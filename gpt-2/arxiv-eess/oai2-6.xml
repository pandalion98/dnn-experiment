<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2020-03-01T06:59:59Z</responseDate>
<request verb="ListRecords" resumptionToken="4250076|5001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1902.10828</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.10828</id><created>2019-02-27</created><authors><author><keyname>Nandwana</keyname><forenames>Mahesh Kumar</forenames></author><author><keyname>van Hout</keyname><forenames>Julien</forenames></author><author><keyname>McLaren</keyname><forenames>Mitchell</forenames></author><author><keyname>Richey</keyname><forenames>Colleen</forenames></author><author><keyname>Lawson</keyname><forenames>Aaron</forenames></author><author><keyname>Barrios</keyname><forenames>Maria Alejandra</forenames></author></authors><title>The VOiCES from a Distance Challenge 2019 Evaluation Plan</title><categories>eess.AS cs.SD</categories><comments>Special Session for Interspeech 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The &quot;VOiCES from a Distance Challenge 2019&quot; is designed to foster research in
the area of speaker recognition and automatic speech recognition (ASR) with the
special focus on single channel distant/far-field audio, under noisy
conditions. The main objectives of this challenge are to: (i) benchmark
state-of-the-art technology in the area of speaker recognition and automatic
speech recognition (ASR), (ii) support the development of new ideas and
technologies in speaker recognition and ASR, (iii) support new research groups
entering the field of distant/far-field speech processing, and (iv) provide a
new, publicly available dataset to the community that exhibits realistic
distance characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.10896</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.10896</id><created>2019-02-28</created><authors><author><keyname>Gayan</keyname><forenames>Samiru</forenames></author><author><keyname>Senanayake</keyname><forenames>Rajitha</forenames></author><author><keyname>Inaltekin</keyname><forenames>Hazer</forenames></author><author><keyname>Evans</keyname><forenames>Jamie</forenames></author></authors><title>Low-Resolution Quantization in Phase Modulated Systems: Optimum
  Detectors and Error Rate Analysis</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a wireless communication system with low-resolution
quantizers, in which transmitted signals are corrupted by fading and additive
noise. For such wireless systems, a universal lower bound on the average symbol
error probability (SEP), correct for all M-ary modulation schemes, is obtained
when the number of quantization bits is not enough to resolve M signal points.
In the special case of M-ary phase shift keying (M-PSK), the optimum maximum
likelihood detector for equi-probable signal points is derived. Utilizing the
structure of the derived optimum receiver, a general average SEP expression for
M-PSK modulation with n-bit quantization is obtained when the wireless channel
is subject to fading with a circularly-symmetric distribution. Adopting this
result for Nakagami-m fading channels, easy-to-evaluate expressions for the
average SEP for M-PSK modulation are further derived. It is shown that a
transceiver architecture with n-bit quantization is asymptotically optimum in
terms of communication reliability if n is greater than or equal to log_2(M
+1). That is, the decay exponent for the average SEP is the same and equal to m
with infinite-bit and n-bit quantizers for n is greater than or equal to
log_2(M+1). On the other hand, it is only equal to half and 0 for n = log_2(M)
and n &lt; log_2(M), respectively. An extensive simulation study is performed to
illustrate the derived results and energy efficiency gains obtained by means of
low-resolution quantizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.10982</identifier>
 <datestamp>2020-02-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.10982</id><created>2019-02-28</created><updated>2020-02-13</updated><authors><author><keyname>Rousse</keyname><forenames>Paul</forenames><affiliation>LAAS-MAC</affiliation></author><author><keyname>Garoche</keyname><forenames>Pierre-Lo&#xef;c</forenames><affiliation>LAAS-MAC</affiliation></author><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS-MAC</affiliation></author></authors><title>Parabolic Set Simulation for Reachability Analysis of Linear Time
  Invariant Systems with Integral Quadratic Constraint</title><categories>math.OC cs.SY eess.SY</categories><proxy>ccsd</proxy><report-no>Rapport LAAS n{\textdegree} 19066</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work extends reachability analyses based on ellipsoidal techniques to
Linear Time Invariant (LTI) systems subject to an integral quadratic constraint
(IQC) between the past state and disturbance signals , interpreted as an
input-output energetic constraint. To compute the reachable set, the LTI system
is augmented with a state corresponding to the amount of energy still available
before the constraint is violated. For a given parabolic set of initial states,
the reachable set of the augmented system is overapproximated with a
time-varying parabolic set. Parameters of this paraboloid are expressed as the
solution of an Initial Value Problem (IVP) and the overapproximation
relationship with the reachable set is proved. This paraboloid is actually
supported by the reachable set on so-called touching trajectories. Finally , we
describe a method to generate all the supporting paraboloids and prove that
their intersection is an exact characterization of the reachable set. This work
provides new practical means to compute overapproximation of reachable sets for
a wide variety of systems such as delayed systems, rate limiters or
energy-bounded linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11023</identifier>
 <datestamp>2019-06-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11023</id><created>2019-02-28</created><updated>2019-06-18</updated><authors><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Yuan</keyname><forenames>Yue</forenames></author><author><keyname>Zhen</keyname><forenames>Li</forenames></author></authors><title>Multi-User Hybrid Precoding for Dynamic Subarrays in MmWave Massive MIMO
  Systems</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Dynamic subarray achieves a compromise between sum rate and hardware
complexity for millimeter wave (mmWave) massive multiple-input multiple-output
(MIMO) systems in which antenna elements are dynamically partitioned to radio
frequency (RF) chain according to the channel state information.} However,
multi-user hybrid precoding for the dynamic subarray is intractable to solve as
the antenna partitioning would result in the user unfairness and multi-user
interference (MUI). In this paper, a novel multi-user hybrid precoding
framework is proposed for the dynamic subarray architecture. Different from the
existing schemes, the base station (BS) firstly selects the multi-user set
based on the analog effective channel. And then the antenna partitioning
algorithm allocates each antenna element to RF chain according to the maximal
increment of the signal to the interference noise ratio (SINR). Finally, the
hybrid precoding is optimized for the dynamic subarray architecture. By
calculating SINRs on the analog effective channels of the selected users, the
antenna partitioning can greatly reduce computation complexity and the size of
the search space. Moreover, it also guarantees the user fairness since each
antenna element is allocated to acquire the maximal SINR increment of all
selected users. \textcolor{blue}{Extensive simulation results demonstrate that
both the energy efficiency and sum rate of the proposed solution obviously
outperforms that of the fixed subarrays, and obtains higher energy efficiency
with slight loss of sum rate compared with the fully-connected architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11043</identifier>
 <datestamp>2019-06-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11043</id><created>2019-02-28</created><updated>2019-06-12</updated><authors><author><keyname>Nie</keyname><forenames>Yuanbo</forenames></author><author><keyname>Kerrigan</keyname><forenames>Eric C.</forenames></author></authors><title>External Constraint Handling for Solving Optimal Control Problems with
  Simultaneous Approaches and Interior Point Methods</title><categories>eess.SY cs.SY math.OC</categories><comments>6 pages, 2 figures</comments><doi>10.1109/LCSYS.2019.2921700</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inactive constraints do not contribute to the solution of an optimal control
problem, but increase the problem size and burden the numerical computations.
We present a novel strategy for handling inactive constraints efficiently by
systematically removing the inactive and redundant constraints. The method is
designed to be used together with simultaneous approaches under a mesh
refinement framework, with mild assumptions that the original problem has
feasible solutions, and the initial solve of the problem is successful. The
method is tailored for interior point-based solvers, which are known to be very
sensitive to the choice of initial points in terms of feasibility. In the
example problem shown, the proposed scheme achieves more than a 40% reduction
in computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11061</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11061</id><created>2019-02-28</created><updated>2019-07-15</updated><authors><author><keyname>Or&#x161;uli&#x107;</keyname><forenames>Juraj</forenames></author><author><keyname>Mikli&#x107;</keyname><forenames>Damjan</forenames></author><author><keyname>Kova&#x10d;i&#x107;</keyname><forenames>Zdenko</forenames></author></authors><title>Efficient Dense Frontier Detection for 2D Graph SLAM Based on Occupancy
  Grid Submaps</title><categories>cs.RO eess.IV</categories><journal-ref>J. Orsulic, D. Miklic and Z. Kovacic, &quot;Efficient Dense Frontier
  Detection for 2D Graph SLAM Based on Occupancy Grid Submaps,&quot; in IEEE
  Robotics and Automation Letters, 2019</journal-ref><doi>10.1109/LRA.2019.2928203</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In autonomous robot exploration, the frontier is the border in the world map
between the explored space and unexplored space. The frontier plays an
important role when deciding where in the environment the robots should go
explore next. We examine a modular control system pipeline for autonomous
exploration where a 2D graph SLAM algorithm based on occupancy grid submaps
performs map building and localization. We provide an overview of the state of
the art in frontier detection and the relevant SLAM concepts and propose a
specialized frontier detection method which is efficiently constrained to
active submaps, yet robust to SLAM loop closures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11085</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11085</id><created>2019-02-28</created><authors><author><keyname>Sidorenkoab</keyname><forenames>Juri</forenames></author><author><keyname>Schatza</keyname><forenames>Volker</forenames></author><author><keyname>Scherer-Negenborna</keyname><forenames>Norbert</forenames></author><author><keyname>Arensa</keyname><forenames>Michael</forenames></author><author><keyname>Hugentobler</keyname><forenames>Urs</forenames></author></authors><title>Decawave UWB clock drift correction and powerself-calibration</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The position accuracy based on Decawave Ultra-Wideband (UWB) is
affectedmainly by three factors: hardware delays, clock drift, and signal
power. Thisarticle discusses the last two factors. The general approach to
clock drift correc-tion uses the phase-locked loop (PLL) integrator, which we
show is subject tosignal power variations, and therefore, is less suitable for
clock drift correction.The general approach to the estimation of signal power
correction curves re-quires additional measurement equipment. This article
presents a new methodfor obtaining the curve without additional hardware and
clock drift correctionwithout the PLL integrator. Both correction methods were
fused together toimprove two-way ranging (TWR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11090</identifier>
 <datestamp>2019-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11090</id><created>2019-02-28</created><updated>2019-10-07</updated><authors><author><keyname>Faisal</keyname><forenames>Alice</forenames></author><author><keyname>Sarieddeen</keyname><forenames>Hadi</forenames></author><author><keyname>Dahrouj</keyname><forenames>Hayssam</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Ultra-Massive MIMO Systems at Terahertz Bands: Prospects and Challenges</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Terahertz (THz) band communications are currently being celebrated as a key
technology that could fulfill the increasing demands for wireless data traffic
and higher speed wireless communications. Many challenges, however, have yet to
be addressed for this technology to be realized, such as high propagation
losses and power limitations, which result in short communication distances.
Ultra-massive multiple input multiple output (UM-MIMO) antenna systems have
emerged as practical means for combatting the distance problem at the THz
range; thereby increasing system capacity. Towards that direction,
graphene-based nano-antennas of small footprints have been recently proposed,
as they can be individually tuned and collectively controlled in an UM-MIMO
array of sub-arrays architecture. In this paper, we present a holistic overview
of THz UM-MIMO systems by assessing recent advancements in transceiver design
and channel modeling. We discuss the major challenges and shortcomings of such
designs from a signal processing perspective, by deriving the relation between
system performance, communication range, and array dimensions. We further
highlight several research advances that could enhance resource allocation at
the THz band, including waveform designs, multi-carrier antenna configurations,
and spatial modulations. Based on this discussion, we highlight prospect use
cases that can bring THz UM-MIMO into reality, in the context of sensing, data
centers, and future mid-range wireless communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11130</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11130</id><created>2019-02-27</created><authors><author><keyname>Polyzos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Dermatas</keyname><forenames>Evangelos</forenames></author></authors><title>Real-Time detection, classification and DOA estimation of Unmanned
  Aerial Vehicle</title><categories>eess.AS cs.SD</categories><comments>ACOUSTICS 2018, Oral Presentation</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The present work deals with a new passive system for real-time detection,
classification and direction of arrival estimator of Unmanned Aerial Vehicles
(UAVs). The proposed system composed of a very low cost hardware components,
comprises two different arrays of three or six-microphones, non-linear
amplification and filtering of the analog acoustic signal, avoiding also the
saturation effect in case where the UAV is located nearby to the microphones.
Advance array processing methods are used to detect and locate the wide-band
sources in the near and far-field including array calibration and energy based
beamforming techniques. Moreover, oversampling techniques are adopted to
increase the acquired signals accuracy and to also decrease the quantization
noise. The classifier is based on the nearest neighbor rule of a normalized
Power Spectral Density, the acoustic signature of the UAV spectrum in short
periods of time. The low-cost, low-power and high efficiency embedded processor
STM32F405RG is used for system implementation. Preliminary experimental results
have shown the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11173</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11173</id><created>2019-02-28</created><updated>2019-03-08</updated><authors><author><keyname>Ghulyani</keyname><forenames>Manu</forenames></author><author><keyname>Arigovindan</keyname><forenames>Muthuvel</forenames></author></authors><title>Fast roughness minimizing image restoration under mixed Poisson-Gaussian
  noise</title><categories>eess.IV</categories><comments>19 pages, 6 figures, 4 tables Submitted to IEEE-TIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image acquisition in many biomedical imaging modalities is corrupted by
Poisson noise followed by additive Gaussian noise. While total variation and
related regularization methods for solving biomedical inverse problems are
known to yield high quality reconstructions, such methods mostly use
log-likelihood of either Gaussian or Poisson noise models, and rarely use mixed
Poisson-Gaussian (PG) noise model. There is a recent work which deals with
exact PG likelihood and totalariation regularization. This method is developed
using the log-likelihood of PG model along with total variation regularization
adapts the primal-dual splitting algorithm, whose step size is restricted to be
bounded by the inverse of the Lipschitz constant of PG log-likelihood. This
leads to limitations in the convergence speed. On the other hand, ADMM methods
do not have such step size restrictions; however, ADDM has never been applied
for this problem, for the possible reason that PG log-likelihood is quite
complex. In this paper, we develop an ADMM based optimization for total
variation minimizing image restoration under PG log-likelihood. We achieve this
by first developing a novel iterative method for computing the proximal
solution of PG log-likelihood, deriving the termination conditions for this
iterative method, and then integrating into a provable convergent ADMM scheme.
The effectiveness of the proposed methods is demonstrated using restoration
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11187</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11187</id><created>2019-01-15</created><authors><author><keyname>Draganic</keyname><forenames>Andjela</forenames></author></authors><title>Analysis of non-stationary multicomponent signals with a focus on the
  Compressive Sensing approach</title><categories>eess.SP cs.MM</categories><comments>PhD Thesis, in Bosnian</comments><journal-ref>University of Montenegro, Faculty of Electrical Engineering, 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The characterization of multicomponent signals with a particular emphasis on
musical and communication signals is one of the problems studied in the
dissertation. In order to provide an efficient analysis of the multicomponent
signals, the possibility to separate signal components is observed. The
procedure for decomposition and classification of the signal components whose
energy and physical characteristics differ in the time-frequency domain is
proposed in this work. A special focus in the dissertation is on the
application of the compressive sensing approach in multicomponent signals. The
compressive sensing method becomes popular in the field of signal processing
until recently, and its application in various fields can increase the
acquisition and transmission speed, reduce the complexity of devices, and
reduce energy consumption. The procedure that applies the compressive sensing
in the classification of the wireless communication signals is proposed. The
algorithms for reconstruction of the compressive sensed signals are intensively
developing, and therefore special emphasis in the dissertation is devoted to
the hardware implementation of one of the algorithms for sparse signal
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11210</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11210</id><created>2019-02-19</created><authors><author><keyname>Kapse</keyname><forenames>Ritesh</forenames></author><author><keyname>Adarsh</keyname><forenames>S.</forenames></author></authors><title>Implementing an Autonomous Emergency Braking with Simulink using two
  Radar Sensors</title><categories>eess.SP</categories><comments>10 pages, 10 figures, 2 equations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we have implemented the autonomous emergency braking using two
radar sensors with different angle of coverage. The synthetic radar data is
generated by radar detection generator block available in AEBTestBench
simulation module. AEBTestBench is autonomous emergency simulation module
available in Matlab 2018b version under ADAS toolbox. From different EURO NCAP
standard scenarios available, we have covered 5 scenarios with their results
analysis showing where forward collision warning is displayed and what is the
AEB status. The observation of simulation results are carried out for 10
seconds. Data fusion for two radar sensors is carried by Kalman filter
algorithm present inside the simulation module.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11243</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11243</id><created>2019-02-20</created><authors><author><keyname>Dambal</keyname><forenames>Vageesh Anand</forenames></author><author><keyname>Mohadikar</keyname><forenames>Sameer</forenames></author><author><keyname>Kumbhar</keyname><forenames>Abhaykumar</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author></authors><title>Improving LoRa Signal Coverage in Urban and Sub-Urban Environments with
  UAVs</title><categories>eess.SP</categories><comments>2019 International Workshop on Antenna Technology (iWAT) (iWAT2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LoRa technology enables long-range communication with low-power consumption
for the Internet-of-Things (IoT) devices in the urban and suburban environment.
However, due to terrestrial structures in urban and suburban environments, the
link distance of LoRa transmissions can be reduced. In this paper, we report
signal strength measurements for the in-building and inter-building LoRa links
and provide insights on factors that affect signal quality such as the
spreading factor and antenna orientation. Subsequently, we also provide
measurement results in urban and suburban environments when the LoRa
transmitter is deployed at different heights using an unmanned aerial vehicle
(UAV). Our findings show that the UAV deployment height is critical for
improving coverage in the suburban environment and antenna orientation affects
the communication range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11245</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11245</id><created>2019-02-28</created><authors><author><keyname>Lakomkin</keyname><forenames>Egor</forenames></author><author><keyname>Zamani</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Weber</keyname><forenames>Cornelius</forenames></author><author><keyname>Magg</keyname><forenames>Sven</forenames></author><author><keyname>Wermter</keyname><forenames>Stefan</forenames></author></authors><title>Incorporating End-to-End Speech Recognition Models for Sentiment
  Analysis</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Accepted at the 2019 International Conference on Robotics and
  Automation (ICRA) will be held on May 20-24, 2019 in Montreal, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work on emotion recognition demonstrated a synergistic effect of
combining several modalities such as auditory, visual, and transcribed text to
estimate the affective state of a speaker. Among these, the linguistic modality
is crucial for the evaluation of an expressed emotion. However, manually
transcribed spoken text cannot be given as input to a system practically. We
argue that using ground-truth transcriptions during training and evaluation
phases leads to a significant discrepancy in performance compared to real-world
conditions, as the spoken text has to be recognized on the fly and can contain
speech recognition mistakes. In this paper, we propose a method of integrating
an automatic speech recognition (ASR) output with a character-level recurrent
neural network for sentiment recognition. In addition, we conduct several
experiments investigating sentiment recognition for human-robot interaction in
a noise-realistic scenario which is challenging for the ASR systems. We
quantify the improvement compared to using only the acoustic modality in
sentiment recognition. We demonstrate the effectiveness of this approach on the
Multimodal Corpus of Sentiment Intensity (MOSI) by achieving 73,6% accuracy in
a binary sentiment classification task, exceeding previously reported results
that use only acoustic input. In addition, we set a new state-of-the-art
performance on the MOSI dataset (80.4% accuracy, 2% absolute improvement).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11275</identifier>
 <datestamp>2019-09-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11275</id><created>2019-02-28</created><updated>2019-09-06</updated><authors><author><keyname>Interdonato</keyname><forenames>Giovanni</forenames></author><author><keyname>Frenger</keyname><forenames>P&#xe5;l</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Scalability Aspects of Cell-Free Massive MIMO</title><categories>cs.IT eess.SP math.IT</categories><comments>Paper published in ICC 2019 - 2019 IEEE International Conference on
  Communications (ICC). {\copyright} 2019 IEEE. Personal use of this material
  is permitted. Permission from IEEE must be obtained for all other uses</comments><doi>10.1109/ICC.2019.8761828</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ubiquitous cell-free massive MIMO (multiple-input multiple-output) combines
massive MIMO technology and user-centric transmission in a distributed
architecture. All the access points (APs) in the network cooperate to jointly
and coherently serve a smaller number of users in the same time-frequency
resource. However, this coordination needs significant amounts of control
signalling which introduces additional overhead, while data co-processing
increases the back/front-haul requirements. Hence, the notion that the &quot;whole
world&quot; could constitute one network, and that all APs would act as a single
base station, is not scalable. In this study, we address some system
scalability aspects of cell-free massive MIMO that have been neglected in
literature until now. In particular, we propose and evaluate a solution related
to data processing, network topology and power control. Results indicate that
our proposed framework achieves full scalability at the cost of a modest
performance loss compared to the canonical form of cell-free massive MIMO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1902.11280</identifier>
 <datestamp>2019-03-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1902.11280</id><created>2019-02-28</created><authors><author><keyname>Abdelnour</keyname><forenames>Jerome</forenames></author><author><keyname>Salvi</keyname><forenames>Giampiero</forenames></author><author><keyname>Rouat</keyname><forenames>Jean</forenames></author></authors><title>From Visual to Acoustic Question Answering</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the new task of Acoustic Question Answering (AQA) to promote
research in acoustic reasoning. The AQA task consists of analyzing an acoustic
scene composed by a combination of elementary sounds and answering questions
that relate the position and properties of these sounds. The kind of relational
questions asked, require that the models perform non-trivial reasoning in order
to answer correctly. Although similar problems have been extensively studied in
the domain of visual reasoning, we are not aware of any previous studies
addressing the problem in the acoustic domain. We propose a method for
generating the acoustic scenes from elementary sounds and a number of relevant
questions for each scene using templates. We also present preliminary results
obtained with two models (FiLM and MAC) that have been shown to work for visual
reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00095</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00095</id><created>2019-02-28</created><updated>2019-05-23</updated><authors><author><keyname>Barbieri</keyname><forenames>Sebastiano</forenames></author><author><keyname>Gurney-Champion</keyname><forenames>Oliver J.</forenames></author><author><keyname>Klaassen</keyname><forenames>Remy</forenames></author><author><keyname>Thoeny</keyname><forenames>Harriet C.</forenames></author></authors><title>Deep Learning How to Fit an Intravoxel Incoherent Motion Model to
  Diffusion-Weighted MRI</title><categories>q-bio.QM cs.LG eess.IV</categories><journal-ref>Magnetic Resonance in Medicine 2019</journal-ref><doi>10.1002/mrm.27910</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: This prospective clinical study assesses the feasibility of training
a deep neural network (DNN) for intravoxel incoherent motion (IVIM) model
fitting to diffusion-weighted magnetic resonance imaging (DW-MRI) data and
evaluates its performance. Methods: In May 2011, ten male volunteers (age
range: 29 to 53 years, mean: 37 years) underwent DW-MRI of the upper abdomen on
1.5T and 3.0T magnetic resonance scanners. Regions of interest in the left and
right liver lobe, pancreas, spleen, renal cortex, and renal medulla were
delineated independently by two readers. DNNs were trained for IVIM model
fitting using these data; results were compared to least-squares and Bayesian
approaches to IVIM fitting. Intraclass Correlation Coefficients (ICC) were used
to assess consistency of measurements between readers. Intersubject variability
was evaluated using Coefficients of Variation (CV). The fitting error was
calculated based on simulated data and the average fitting time of each method
was recorded. Results: DNNs were trained successfully for IVIM parameter
estimation. This approach was associated with high consistency between the two
readers (ICCs between 50 and 97%), low intersubject variability of estimated
parameter values (CVs between 9.2 and 28.4), and the lowest error when compared
with least-squares and Bayesian approaches. Fitting by DNNs was several orders
of magnitude quicker than the other methods but the networks may need to be
re-trained for different acquisition protocols or imaged anatomical regions.
Conclusion: DNNs are recommended for accurate and robust IVIM model fitting to
DW-MRI data. Suitable software is available at (1).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00101</identifier>
 <datestamp>2019-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00101</id><created>2019-02-28</created><authors><author><keyname>Luong</keyname><forenames>David</forenames></author><author><keyname>Chang</keyname><forenames>C. W. Sandbo</forenames></author><author><keyname>Vadiraj</keyname><forenames>A. M.</forenames></author><author><keyname>Damini</keyname><forenames>Anthony</forenames></author><author><keyname>Wilson</keyname><forenames>C. M.</forenames></author><author><keyname>Balaji</keyname><forenames>Bhashyam</forenames></author></authors><title>Receiver Operating Characteristics for a Prototype Quantum Two-Mode
  Squeezing Radar</title><categories>quant-ph eess.SP</categories><comments>17 pages, 17 figures; submitted to IEEE Transactions on Aerospace and
  Electronic Systems</comments><doi>10.1109/TAES.2019.2951213</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have built and evaluated a prototype quantum radar, which we call a
quantum two-mode squeezing radar (QTMS radar), in the laboratory. It operates
solely at microwave frequencies; there is no downconversion from optical
frequencies. Because the signal generation process relies on quantum mechanical
principles, the system is considered to contain a quantum-enhanced radar
transmitter. This transmitter generates a pair of entangled microwave signals
and transmits one of them through free space, where the signal is measured
using a simple and rudimentary receiver.
  At the heart of the transmitter is a device called a Josephson parametric
amplifier (JPA), which generates a pair of entangled signals called two-mode
squeezed vacuum (TMSV) at 6.1445 GHz and 7.5376 GHz. These are then sent
through a chain of amplifiers. The 7.5376 GHz beam passes through 0.5 m of free
space; the 6.1445 GHz signal is measured directly after amplification. The two
measurement results are correlated in order to distinguish signal from noise.
  We compare our QTMS radar to a classical radar setup using conventional
components, which we call a two-mode noise radar (TMN radar), and find that
there is a significant gain when both systems broadcast signals at -82 dBm.
This is shown via a comparison of receiver operator characteristic (ROC)
curves. In particular, we find that the quantum radar requires 8 times fewer
integrated samples compared to its classical counterpart to achieve the same
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00107</identifier>
 <datestamp>2019-03-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00107</id><created>2019-02-28</created><authors><author><keyname>Zhang</keyname><forenames>Shuang</forenames></author><author><keyname>Zhen</keyname><forenames>Ada</forenames></author><author><keyname>Stevenson</keyname><forenames>Robert L.</forenames></author></authors><title>GAN Based Image Deblurring Using Dark Channel Prior</title><categories>cs.CV eess.IV</categories><comments>5 pages, 3 figures. Conference: Electronic Imaging</comments><doi>10.2352/ISSN.2470-1173.2019.13.COIMG-136</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A conditional general adversarial network (GAN) is proposed for image
deblurring problem. It is tailored for image deblurring instead of just
applying GAN on the deblurring problem. Motivated by that, dark channel prior
is carefully picked to be incorporated into the loss function for network
training. To make it more compatible with neuron networks, its original
indifferentiable form is discarded and L2 norm is adopted instead. On both
synthetic datasets and noisy natural images, the proposed network shows
improved deblurring performance and robustness to image noise qualitatively and
quantitatively. Additionally, compared to the existing end-to-end deblurring
networks, our network structure is light-weight, which ensures less training
and testing time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00165</identifier>
 <datestamp>2019-03-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00165</id><created>2019-03-01</created><authors><author><keyname>Xu</keyname><forenames>Di</forenames></author><author><keyname>Che</keyname><forenames>Xiaojing</forenames></author><author><keyname>Wu</keyname><forenames>Changhao</forenames></author><author><keyname>Zhang</keyname><forenames>Shunqing</forenames></author><author><keyname>Xu</keyname><forenames>Shugong</forenames></author><author><keyname>Cao</keyname><forenames>Shan</forenames></author></authors><title>Energy-Efficient Subchannel and Power Allocation for HetNets Based on
  Convolutional Neural Network</title><categories>eess.SP</categories><comments>5 pages, 7 figures, VTC2019-Spring</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous network (HetNet) has been proposed as a promising solution for
handling the wireless traffic explosion in future fifth-generation (5G) system.
In this paper, a joint subchannel and power allocation problem is formulated
for HetNets to maximize the energy efficiency (EE). By decomposing the original
problem into a classification subproblem and a regression subproblem, a
convolutional neural network (CNN) based approach is developed to obtain the
decisions on subchannel and power allocation with a much lower complexity than
conventional iterative methods. Numerical results further demonstrate that the
proposed CNN can achieve similar performance as the Exhaustive method, while
needs only 6.76% of its CPU runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00216</identifier>
 <datestamp>2019-03-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00216</id><created>2019-03-01</created><authors><author><keyname>Lakomkin</keyname><forenames>Egor</forenames></author><author><keyname>Magg</keyname><forenames>Sven</forenames></author><author><keyname>Weber</keyname><forenames>Cornelius</forenames></author><author><keyname>Wermter</keyname><forenames>Stefan</forenames></author></authors><title>KT-Speech-Crawler: Automatic Dataset Construction for Speech Recognition
  from YouTube Videos</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Accepted at the Conference on Empirical Methods in Natural Language
  Processing 2018, Brussels, Belgium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe KT-Speech-Crawler: an approach for automatic
dataset construction for speech recognition by crawling YouTube videos. We
outline several filtering and post-processing steps, which extract samples that
can be used for training end-to-end neural speech recognition systems. In our
experiments, we demonstrate that a single-core version of the crawler can
obtain around 150 hours of transcribed speech within a day, containing an
estimated 3.5% word error rate in the transcriptions. Automatically collected
samples contain reading and spontaneous speech recorded in various conditions
including background noise and music, distant microphone recordings, and a
variety of accents and reverberation. When training a deep neural network on
speech recognition, we observed around 40\% word error rate reduction on the
Wall Street Journal dataset by integrating 200 hours of the collected samples
into the training set. The demo (http://emnlp-demo.lakomkin.me/) and the
crawler code (https://github.com/EgorLakomkin/KTSpeechCrawler) are publicly
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00290</identifier>
 <datestamp>2019-06-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00290</id><created>2019-03-01</created><updated>2019-06-10</updated><authors><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Gerencser</keyname><forenames>Balazs</forenames></author><author><keyname>Fidan</keyname><forenames>Baris</forenames></author></authors><title>Trajectory convergence from coordinate-wise decrease of quadratic energy
  functions, and applications to platoons</title><categories>math.DS cs.MA cs.SY eess.SY</categories><comments>6 pages, 2 figures, double columns</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider trajectories where the sign of the derivative of each entry is
opposite to that of the corresponding entry in the gradient of an energy
function. We show that this condition guarantees convergence when the energy
function is quadratic and positive definite and partly extend that result to
some classes of positive semi-definite quadratic functions including those
defined using a graph Laplacian. We show how this condition allows establishing
the convergence of a platoon application in which it naturally appears, due to
deadzones in the control laws designed to avoid instabilities caused by
inconsistent measurements of the same distance by different agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00332</identifier>
 <datestamp>2019-08-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00332</id><created>2019-02-26</created><authors><author><keyname>Sarkar</keyname><forenames>Debdeep</forenames></author><author><keyname>Mikki</keyname><forenames>Said</forenames></author><author><keyname>Antar</keyname><forenames>Yahia</forenames></author></authors><title>An Efficient Analytical Evaluation of the Electromagnetic
  Cross-Correlation Green's Function in MIMO Systems</title><categories>physics.class-ph eess.SP physics.app-ph</categories><doi>10.1109/TAP.2019.2927662</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we completely eliminate all numerical integrations needed to
compute the far-field envelope cross-correlation (ECC) in
multiple-input-multiple-output (MIMO) systems by deriving accurate and
efficient analytical expressions for the frequency-domain cross-correlation
Green's functions (CGF), the most fundamental electromagnetic kernel needed for
understanding and estimating spatial correlation metrics in multiple-antenna
configurations. The analytical CGF is derived for the most general
three-dimensional case, which can be used for fast CGF-based correlation matrix
calculations in MIMO systems valid for arbitrary locations and relative
polarizations of the constituent elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00403</identifier>
 <datestamp>2019-03-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00403</id><created>2019-03-01</created><authors><author><keyname>Fortunati</keyname><forenames>Stefano</forenames></author><author><keyname>Gini</keyname><forenames>Fulvio</forenames></author><author><keyname>Greco</keyname><forenames>Maria S.</forenames></author></authors><title>Semiparametric Stochastic CRB for DOA Estimation in Elliptical Data
  Model</title><categories>eess.SP</categories><comments>Submitted to 27th European Signal Processing Conference (EUSIPCO
  2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at presenting a numerical investigation of the statistical
efficiency of the MUSIC (with different covariance matrix estimates) and the
IAA-APES Direction of Arrivals (DOAs) estimation algorithms under a general
Complex Elliptically Symmetric (CES) distributed measurement model.
Specifically, the density generator of the CES-distributed data snapshots is
considered as an additional, infinite-dimensional, nuisance parameter. To
assess the efficiency in the considered semiparametric setting, the
Semiparametric Stochastic Cram\'er-Rao Bound (SSCRB) is adopted as lower bound
for the Mean Square Error (MSE) of the DOA estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00435</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00435</id><created>2019-03-01</created><authors><author><keyname>Kanatsoulis</keyname><forenames>Charilaos I.</forenames></author><author><keyname>Fu</keyname><forenames>Xiao</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Nicholas D.</forenames></author><author><keyname>Ak&#xe7;akaya</keyname><forenames>Mehmet</forenames></author></authors><title>Tensor Completion from Regular Sub-Nyquist Samples</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2952044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal sampling and reconstruction is a fundamental engineering task at the
heart of signal processing. The celebrated Shannon-Nyquist theorem guarantees
perfect signal reconstruction from uniform samples, obtained at a rate twice
the maximum frequency present in the signal. Unfortunately a large number of
signals of interest are far from being band-limited. This motivated research on
reconstruction from sub-Nyquist samples, which mainly hinges on the use of
random / incoherent sampling procedures. However, uniform or regular sampling
is more appealing in practice and from the system design point of view, as it
is far simpler to implement, and often necessary due to system constraints. In
this work, we study regular sampling and reconstruction of three- or
higher-dimensional signals (tensors). We show that reconstructing a tensor
signal from regular samples is feasible. Under the proposed framework, the
sample complexity is determined by the tensor rank---rather than the signal
bandwidth. This result offers new perspectives for designing practical regular
sampling patterns and systems for signals that are naturally tensors, e.g.,
images and video. For a concrete application, we show that functional magnetic
resonance imaging (fMRI) acceleration is a tensor sampling problem, and design
practical sampling schemes and an algorithmic framework to handle it. Numerical
results show that our tensor sampling strategy accelerates the fMRI sampling
process significantly without sacrificing reconstruction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00473</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00473</id><created>2019-03-01</created><authors><author><keyname>Lin</keyname><forenames>Liqun</forenames></author><author><keyname>Yu</keyname><forenames>Shiqi</forenames></author><author><keyname>Zhao</keyname><forenames>Tiesong</forenames></author><author><keyname>Member</keyname></author><author><keyname>IEEE</keyname></author><author><keyname>Wang</keyname><forenames>Zhou</forenames></author><author><keyname>Fellow</keyname></author><author><keyname>IEEE</keyname></author></authors><title>PEA265: Perceptual Assessment of Video Compression Artifacts</title><categories>cs.CV eess.IV</categories><comments>10 pages,15 figures,4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most widely used video encoders share a common hybrid coding framework
that includes block-based motion estimation/compensation and block-based
transform coding. Despite their high coding efficiency, the encoded videos
often exhibit visually annoying artifacts, denoted as Perceivable Encoding
Artifacts (PEAs), which significantly degrade the visual Qualityof- Experience
(QoE) of end users. To monitor and improve visual QoE, it is crucial to develop
subjective and objective measures that can identify and quantify various types
of PEAs. In this work, we make the first attempt to build a large-scale
subjectlabelled database composed of H.265/HEVC compressed videos containing
various PEAs. The database, namely the PEA265 database, includes 4 types of
spatial PEAs (i.e. blurring, blocking, ringing and color bleeding) and 2 types
of temporal PEAs (i.e. flickering and floating). Each containing at least
60,000 image or video patches with positive and negative labels. To objectively
identify these PEAs, we train Convolutional Neural Networks (CNNs) using the
PEA265 database. It appears that state-of-theart ResNeXt is capable of
identifying each type of PEAs with high accuracy. Furthermore, we define PEA
pattern and PEA intensity measures to quantify PEA levels of compressed video
sequence. We believe that the PEA265 database and our findings will benefit the
future development of video quality assessment methods and perceptually
motivated video encoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00495</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00495</id><created>2019-03-01</created><authors><author><keyname>Alam</keyname><forenames>Md Sahabul</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author><author><keyname>Kaddoum</keyname><forenames>Georges</forenames></author></authors><title>Performance Analysis of DF Cooperative Relaying over Bursty Impulsive
  Noise Channel</title><categories>eess.SP</categories><comments>11 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we consider the performance analysis of a decode-and-forward
(DF) cooperative relaying (CR) scheme over channels impaired by bursty
impulsive noise. Although, Middleton class-A model and Bernoulli-Gaussian model
give good results to generate a sample distribution of impulsive noise, they
fail in replicating the bursty behavior of impulsive noise, as encountered for
instance within power substations. To deal with that, we adopt a two-state
Markov-Gaussian process for the noise distribution. For this channel, we
evaluate the bit error rate (BER) performance of direct transmission (DT) and a
DF relaying scheme using M-ary phase shift keying (M-PSK) modulation in the
presence of Rayleigh fading with a maximum a posteriori (MAP) receiver. From
the obtained results, it is seen that the DF CR scheme in bursty impulsive
noise channel still achieves the space diversity and performs significantly
better than DT under the same power consumption. Moreover, the proposed MAP
receiver attains the lower bound derived for DF CR scheme, and leads to large
performance gains compared to the conventional receiving criteria which were
optimized for additive white Gaussian noise (AWGN) channel and memoryless
impulsive noise channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00626</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00626</id><created>2019-03-02</created><updated>2019-04-27</updated><authors><author><keyname>Ouyang</keyname><forenames>Chongjun</forenames></author><author><keyname>Ou</keyname><forenames>Zeliang</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Yang</keyname><forenames>Hongwen</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>Secrecy Performance of Antenna-Selection-Aided MIMOME Channels with
  BPSK/QPSK Modulations</title><categories>eess.SP</categories><comments>6 pages</comments><msc-class>26A03</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the secrecy performance of multiple-input multiple-output
(MIMO) wiretap channels, also termed as multiple-input multiple-output
multiple-eavesdropper (MIMOME) channels, under transmit antenna selection (TAS)
and BPSK/QPSK modulations. In the main channel between the transmitter and the
legitimate receiver, a single transmit antenna is selected to maximizes the
instantaneous Signal to Noise Ratio (SNR) at the receiver. At the receiver and
the eavesdropper, selection combination (SC) is utilized. By assuming Rayleigh
flat fading, we first derive the closed-form approximated expression for the
ergodic secrecy rate when the channel state information of the eavesdropper
(CSIE) is available at the transmitter. Next, analytical formulas for the
approximated and asymptotic secrecy outage probability (SOP) are also developed
when CSIE is unavailable. Besides theoretical derivations, simulation results
are provided to demonstrate the approximation precision of the derived results.
Furthermore, the asymptotic results reveal that the secrecy diversity order
degrades into 0 due to the finitealphabet inputs, which is totally different
from that driven by the Gaussian inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00650</identifier>
 <datestamp>2019-07-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00650</id><created>2019-03-02</created><updated>2019-07-21</updated><authors><author><keyname>Liang</keyname><forenames>Hongzhuo</forenames></author><author><keyname>Li</keyname><forenames>Shuang</forenames></author><author><keyname>Ma</keyname><forenames>Xiaojian</forenames></author><author><keyname>Hendrich</keyname><forenames>Norman</forenames></author><author><keyname>Gerkmann</keyname><forenames>Timo</forenames></author><author><keyname>Sun</keyname><forenames>Fuchun</forenames></author><author><keyname>Zhang</keyname><forenames>Jianwei</forenames></author></authors><title>Making Sense of Audio Vibration for Liquid Height Estimation in Robotic
  Pouring</title><categories>cs.RO cs.SD eess.AS</categories><comments>Accepted to IROS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on the challenging perception problem in robotic
pouring. Most of the existing approaches either leverage visual or haptic
information. However, these techniques may suffer from poor generalization
performances on opaque containers or concerning measuring precision. To tackle
these drawbacks, we propose to make use of audio vibration sensing and design a
deep neural network PouringNet to predict the liquid height from the audio
fragment during the robotic pouring task. PouringNet is trained on our
collected real-world pouring dataset with multimodal sensing data, which
contains more than 3000 recordings of audio, force feedback, video and
trajectory data of the human hand that performs the pouring task. Each record
represents a complete pouring procedure. We conduct several evaluations on
PouringNet with our dataset and robotic hardware. The results demonstrate that
our PouringNet generalizes well across different liquid containers, positions
of the audio receiver, initial liquid heights and types of liquid, and
facilitates a more robust and accurate audio-based perception for robotic
pouring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00691</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00691</id><created>2019-03-02</created><updated>2019-04-30</updated><authors><author><keyname>Kopanoglu</keyname><forenames>Emre</forenames><affiliation>Cardiff University Brain Research Imaging Centre</affiliation><affiliation>ASELSAN Research Center</affiliation></author><author><keyname>G&#xfc;ng&#xf6;r</keyname><forenames>Alper</forenames><affiliation>ASELSAN Research Center</affiliation><affiliation>Department of Electrical and Electronics Engineering, Bilkent University</affiliation></author><author><keyname>Kilic</keyname><forenames>Toygan</forenames><affiliation>Department of Electrical and Electronics Engineering, Bilkent University</affiliation><affiliation>National Magnetic Resonance Research Center</affiliation><affiliation>Neuroscience Program, Sabuncu Brain Research Center, Bilkent University</affiliation></author><author><keyname>Saritas</keyname><forenames>Emine Ulku</forenames><affiliation>Department of Electrical and Electronics Engineering, Bilkent University</affiliation><affiliation>National Magnetic Resonance Research Center</affiliation><affiliation>Neuroscience Program, Sabuncu Brain Research Center, Bilkent University</affiliation></author><author><keyname>Oguz</keyname><forenames>Kader K.</forenames><affiliation>National Magnetic Resonance Research Center</affiliation><affiliation>Department of Radiology, Hacettepe University</affiliation></author><author><keyname>&#xc7;ukur</keyname><forenames>Tolga</forenames><affiliation>Department of Electrical and Electronics Engineering, Bilkent University</affiliation><affiliation>National Magnetic Resonance Research Center</affiliation><affiliation>Neuroscience Program, Sabuncu Brain Research Center, Bilkent University</affiliation></author><author><keyname>G&#xfc;ven</keyname><forenames>H. Emre</forenames><affiliation>ASELSAN Research Center</affiliation></author></authors><title>Simultaneous use of Individual and Joint Regularization Terms in
  Compressive Sensing: Joint Reconstruction of Multi-Channel Multi-Contrast MRI
  Acquisitions</title><categories>physics.med-ph eess.IV</categories><comments>13 pages, 13 figures. Submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: A time-efficient strategy to acquire high-quality multi-contrast
images is to reconstruct undersampled data with joint regularization terms that
leverage common information across contrasts. However, these terms can cause
leakage of uncommon features among contrasts, compromising diagnostic utility.
The goal of this study is to develop a compressive sensing method for
multi-channel multi-contrast magnetic resonance imaging (MRI) that optimally
utilizes shared information while preventing feature leakage.
  Theory: Joint regularization terms group sparsity and colour total variation
are used to exploit common features across images while individual sparsity and
total variation are also used to prevent leakage of distinct features across
contrasts. The multi-channel multi-contrast reconstruction problem is solved
via a fast algorithm based on Alternating Direction Method of Multipliers.
  Methods: The proposed method is compared against using only individual and
only joint regularization terms in reconstruction. Comparisons were performed
on single-channel simulated and multi-channel in-vivo datasets in terms of
reconstruction quality and neuroradiologist reader scores.
  Results: The proposed method demonstrates rapid convergence and improved
image quality for both simulated and in-vivo datasets. Furthermore, while
reconstructions that solely use joint regularization terms are prone to
leakage-of-features, the proposed method reliably avoids leakage via
simultaneous use of joint and individual terms.
  Conclusion: The proposed compressive sensing method performs fast
reconstruction of multi-channel multi-contrast MRI data with improved image
quality. It offers reliability against feature leakage in joint
reconstructions, thereby holding great promise for clinical use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00741</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00741</id><created>2019-03-02</created><updated>2019-03-05</updated><authors><author><keyname>Deledalle</keyname><forenames>Charles-Alban</forenames></author><author><keyname>Papadakis</keyname><forenames>Nicolas</forenames></author><author><keyname>Salmon</keyname><forenames>Joseph</forenames></author><author><keyname>Vaiter</keyname><forenames>Samuel</forenames></author></authors><title>Refitting solutions promoted by $\ell_{12}$ sparse analysis
  regularization with block penalties</title><categories>math.OC eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In inverse problems, the use of an $\ell_{12}$ analysis regularizer induces a
bias in the estimated solution. We propose a general refitting framework for
removing this artifact while keeping information of interest contained in the
biased solution. This is done through the use of refitting block penalties that
only act on the co-support of the estimation. Based on an analysis of related
works in the literature, we propose a new penalty that is well suited for
refitting purposes. We also present an efficient algorithmic method to obtain
the refitted solution along with the original (biased) solution for any convex
refitting block penalty. Experiments illustrate the good behavior of the
proposed block penalty for refitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00765</identifier>
 <datestamp>2019-12-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00765</id><created>2019-03-02</created><updated>2019-12-10</updated><authors><author><keyname>Kong</keyname><forenames>Qiuqiang</forenames></author><author><keyname>Yu</keyname><forenames>Changsong</forenames></author><author><keyname>Iqbal</keyname><forenames>Turab</forenames></author><author><keyname>Xu</keyname><forenames>Yong</forenames></author><author><keyname>Wang</keyname><forenames>Wenwu</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Weakly Labelled AudioSet Tagging with Attention Neural Networks</title><categories>cs.SD eess.AS</categories><comments>13 pages</comments><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  vol. 27, no. 11, pp. 1791-1802, Nov. 2019</journal-ref><doi>10.1109/TASLP.2019.2930913</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio tagging is the task of predicting the presence or absence of sound
classes within an audio clip. Previous work in audio tagging focused on
relatively small datasets limited to recognising a small number of sound
classes. We investigate audio tagging on AudioSet, which is a dataset
consisting of over 2 million audio clips and 527 classes. AudioSet is weakly
labelled, in that only the presence or absence of sound classes is known for
each clip, while the onset and offset times are unknown. To address the
weakly-labelled audio tagging problem, we propose attention neural networks as
a way to attend the most salient parts of an audio clip. We bridge the
connection between attention neural networks and multiple instance learning
(MIL) methods, and propose decision-level and feature-level attention neural
networks for audio tagging. We investigate attention neural networks modeled by
different functions, depths and widths. Experiments on AudioSet show that the
feature-level attention neural network achieves a state-of-the-art mean average
precision (mAP) of 0.369, outperforming the best multiple instance learning
(MIL) method of 0.317 and Google's deep neural network baseline of 0.314. In
addition, we discover that the audio tagging performance on AudioSet embedding
features has a weak correlation with the number of training samples and the
quality of labels of each sound class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00767</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00767</id><created>2019-03-02</created><authors><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author></authors><title>Large Scale 2D Spectral Compressed Sensing in Continuous Domain</title><categories>eess.SP</categories><comments>5 pages, 2 figures, 2017 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</comments><journal-ref>ICASSP, IEEE International Conference on Acoustics, Speech and
  Signal Processing - Proceedings, IEEE, 2017, p. 5905-5909, Article number
  7953289</journal-ref><doi>10.1109/ICASSP.2017.7953289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of spectral compressed sensing in continuous domain,
which aims to recover a 2-dimensional spectrally sparse signal from partially
observed time samples. The signal is assumed to be a superposition of s complex
sinusoids. We propose a semidefinite program for the 2D signal recovery
problem. Our model is able to handle large scale 2D signals of size 500*500,
whereas traditional approaches only handle signals of size around 20*20.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00888</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00888</id><created>2019-03-03</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Xu</keyname><forenames>Kele</forenames></author><author><keyname>Feng</keyname><forenames>Dawei</forenames></author><author><keyname>Mi</keyname><forenames>Haibo</forenames></author><author><keyname>Wang</keyname><forenames>Huaimin</forenames></author><author><keyname>Zhu</keyname><forenames>Jian</forenames></author></authors><title>Denoising convolutional autoencoder based B-mode ultrasound tongue image
  feature extraction</title><categories>eess.IV</categories><comments>Accepted by ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  B-mode ultrasound tongue imaging is widely used in the speech production
field. However, efficient interpretation is in a great need for the tongue
image sequences. Inspired by the recent success of unsupervised deep learning
approach, we explore unsupervised convolutional network architecture for the
feature extraction in the ultrasound tongue image, which can be helpful for the
clinical linguist and phonetics. By quantitative comparison between different
unsupervised feature extraction approaches, the denoising convolutional
autoencoder (DCAE)-based method outperforms the other feature extraction
methods on the reconstruction task and the 2010 silent speech interface
challenge. A Word Error Rate of 6.17% is obtained with DCAE, compared to the
state-of-the-art value of 6.45% using Discrete cosine transform as the feature
extractor. Our codes are available at
https://github.com/DeePBluE666/Source-code1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00901</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00901</id><created>2019-03-03</created><authors><author><keyname>Sidorenko</keyname><forenames>Juri</forenames></author><author><keyname>Schatz</keyname><forenames>Volker</forenames></author><author><keyname>Scherer-Negenborn</keyname><forenames>Norbert</forenames></author><author><keyname>Arens</keyname><forenames>Michael</forenames></author><author><keyname>Hugentobler</keyname><forenames>Urs</forenames></author></authors><title>Fusion of time of arrival and time difference of arrival for
  ultra-wideband indoor localization</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a new approach for the wireless clock synchronization
of Decawave ultra-wideband transceivers based on the time difference of
arrival. The presented techniques combine the time-of-arrival and
time-difference-of-arrival measurements without losing the advantages of each
approach. The precision and accuracy of the distances measured by the Decawave
devices depends on three effects: signal power, clock drift, and uncertainty in
the hardware delay. This article shows how all three effects may be compensated
with both measurement techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00935</identifier>
 <datestamp>2019-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00935</id><created>2019-03-03</created><updated>2019-11-18</updated><authors><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Sun</keyname><forenames>Kai</forenames></author></authors><title>Solving Power System Differential Algebraic Equations Using Differential
  Transformation</title><categories>eess.SY cs.SY</categories><comments>Published by IEEE Transactions on Power Systems in 2019</comments><doi>10.1109/TPWRS.2019.2945512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel non-iterative method to solve power system
differential algebraic equations (DAEs) using the differential transformation,
a mathematical tool that can obtain power series coefficients by transformation
rules instead of calculating high order derivatives and has proved to be
effective in solving state variables of nonlinear differential equations in our
previous study. This paper further solves non-state variables, e.g. current
injections and bus voltages, directly with a realistic DAE model of power
grids. These non-state variables, nonlinearly coupled in network equations, are
conventionally solved by numerical methods with time-consuming iterations, but
their differential transformations are proved to satisfy formally linear
equations in this paper. Thus, a non-iterative algorithm is designed to
analytically solve all variables of a power system DAE model with ZIP loads.
From test results on a Polish 2383-bus system, the proposed method demonstrates
fast and reliable time performance compared to traditional numerical methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.00965</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.00965</id><created>2019-03-03</created><authors><author><keyname>Zou</keyname><forenames>Qing</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Sampling of surfaces and functions in high dimensional spaces</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a sampling theoretic framework for the recovery of smooth
surfaces and functions living on smooth surfaces from few samples. The proposed
approach can be thought of as a nonlinear generalization of union of subspace
models widely used in signal processing. This scheme relies on an exponential
lifting of the original data points to feature space, where the features live
on union of subspaces. The low-rank property of the features are used to
recover the surfaces as well as to determine the number of measurements needed
to recover the surface. The low-rank property of the features also provides an
efficient approach which resembles a neural network for the local
representation of multidimensional functions on the surface; the significantly
reduced number of parameters make the computational structure attractive for
learning inference from limited labeled training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01161</identifier>
 <datestamp>2019-07-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01161</id><created>2019-03-04</created><authors><author><keyname>Bous</keyname><forenames>Frederik</forenames></author><author><keyname>Roebel</keyname><forenames>Axel</forenames></author></authors><title>Analysing Deep Learning-Spectral Envelope Prediction Methods for Singing
  Synthesis</title><categories>eess.AS cs.SD</categories><journal-ref>Published in Proceedings of the 27th European Signal Processing
  Conference (EUSIPCO), 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct an investigation on various hyper-parameters regarding neural
networks used to generate spectral envelopes for singing synthesis. Two
perceptive tests, where the first compares two models directly and the other
ranks models with a mean opinion score, are performed. With these tests we show
that when learning to predict spectral envelopes, 2d-convolutions are superior
over previously proposed 1d-convolutions and that predicting multiple frames in
an iterated fashion during training is superior over injecting noise to the
input data. An experimental investigation whether learning to predict a
probability distribution vs.\ single samples was performed but turned out to be
inconclusive. A network architecture is proposed that incorporates the
improvements which we found to be useful and we show in our experiments that
this network produces better results than other stat-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01277</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01277</id><created>2019-02-28</created><authors><author><keyname>Kinoshita</keyname><forenames>Yuma</forenames></author><author><keyname>Kiya</keyname><forenames>Hitoshi</forenames></author></authors><title>Deep Inverse Tone Mapping Using LDR Based Learning for Estimating HDR
  Images with Absolute Luminance</title><categories>eess.IV cs.MM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1901.05686</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel inverse tone mapping method using a convolutional
neural network (CNN) with LDR based learning is proposed. In conventional
inverse tone mapping with CNNs, generated HDR images cannot have absolute
luminance, although relative luminance can. Moreover, loss functions suitable
for learning HDR images are problematic, so it is difficult to train CNNs by
directly using HDR images. In contrast, the proposed method enables us not only
to estimate absolute luminance, but also to train a CNN by using LDR images.
The CNN used in the proposed method learns a transformation from various input
LDR images to LDR images mapped by Reinhard's global operator. Experimental
results show that HDR images generated by the proposed method have
higher-quality than HDR ones generated by conventional inverse tone mapping
methods,in terms of HDR-VDP-2.2 and PU encoding + MS-SSIM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01281</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01281</id><created>2019-03-01</created><authors><author><keyname>Sun</keyname><forenames>Chunlong</forenames></author><author><keyname>Jiang</keyname><forenames>Yu</forenames></author><author><keyname>Liu</keyname><forenames>Jijun</forenames></author><author><keyname>Machida</keyname><forenames>Manabu</forenames></author><author><keyname>Nakamura</keyname><forenames>Gen</forenames></author><author><keyname>Nishimura</keyname><forenames>Goro</forenames></author></authors><title>Fast and robust reconstruction algorithm for fluorescence diffuse
  optical tomography assuming a cuboid target</title><categories>eess.IV physics.optics</categories><doi>10.1364/JOSAA.37.000231</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fast and robust algorithm for fluorescence diffuse optical tomography is
proposed. We identify the location of a fluorescence target by assuming a
cuboid. The proposed numerical method is verified by a numerical experiment and
an in vivo experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01288</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01288</id><created>2019-03-01</created><authors><author><keyname>Choudhury</keyname><forenames>Debaditya</forenames></author><author><keyname>McNicholl</keyname><forenames>Duncan K.</forenames></author><author><keyname>RepettI</keyname><forenames>Audrey</forenames></author><author><keyname>Gris-S&#xe1;nchez</keyname><forenames>Itandehui</forenames></author><author><keyname>Birks</keyname><forenames>Tim A.</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author><author><keyname>Thomson</keyname><forenames>Robert R.</forenames></author></authors><title>Compressive optical imaging with a photonic lantern</title><categories>physics.optics eess.IV</categories><comments>24 pages, 5 figs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The thin and flexible nature of optical fibres often makes them the ideal
technology to view biological processes in-vivo, but current microendoscopic
approaches are limited in spatial resolution. Here, we demonstrate a new route
to high resolution microendoscopy using a multicore fibre (MCF) with an
adiabatic multimode-to-singlemode photonic lantern transition formed at the
distal end by tapering. We show that distinct multimode patterns of light can
be projected from the output of the lantern by individually exciting the
single-mode MCF cores, and that these patterns are highly stable to fibre
movement. This capability is then exploited to demonstrate a form of
single-pixel imaging, where a single pixel detector is used to detect the
fraction of light transmitted through the object for each multimode pattern. A
custom compressive imaging algorithm we call SARA-COIL is used to reconstruct
the object using only the pre-measured multimode patterns themselves and the
detector signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01290</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01290</id><created>2019-03-04</created><authors><author><keyname>Drugman</keyname><forenames>Thomas</forenames></author><author><keyname>Huybrechts</keyname><forenames>Goeric</forenames></author><author><keyname>Klimkov</keyname><forenames>Viacheslav</forenames></author><author><keyname>Moinet</keyname><forenames>Alexis</forenames></author></authors><title>Traditional Machine Learning for Pitch Detection</title><categories>cs.SD cs.CL eess.AS</categories><journal-ref>IEEE Signal Processing Letters, Vol. 25, Issue 11, pp. 1745-1749,
  2018</journal-ref><doi>10.1109/LSP.2018.2874155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pitch detection is a fundamental problem in speech processing as F0 is used
in a large number of applications. Recent articles have proposed deep learning
for robust pitch tracking. In this paper, we consider voicing detection as a
classification problem and F0 contour estimation as a regression problem. For
both tasks, acoustic features from multiple domains and traditional machine
learning methods are used. The discrimination power of existing and proposed
features is assessed through mutual information. Multiple supervised and
unsupervised approaches are compared. A significant relative reduction of
voicing errors over the best baseline is obtained: 20% with the best clustering
method (K-means) and 45% with a Multi-Layer Perceptron. For F0 contour
estimation, the benefits of regression techniques are limited though. We
investigate whether those objective gains translate in a parametric synthesis
task. Clear perceptual preferences are observed for the proposed approach over
two widely-used baselines (RAPT and DIO).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01298</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01298</id><created>2019-03-04</created><authors><author><keyname>Isufi</keyname><forenames>Elvin</forenames></author><author><keyname>Gama</keyname><forenames>Fernando</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Generalizing Graph Convolutional Neural Networks with Edge-Variant
  Recursions on Graphs</title><categories>cs.LG eess.SP stat.ML</categories><comments>submitted to EUSIPCO 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews graph convolutional neural networks (GCNNs) through the
lens of edge-variant graph filters. The edge-variant graph filter is a finite
order, linear, and local recursion that allows each node, in each iteration, to
weigh differently the information of its neighbors. By exploiting this
recursion, we formulate a general framework for GCNNs which considers
state-of-the-art solutions as particular cases. This framework results useful
to i) understand the tradeoff between local detail and the number of parameters
of each solution and ii) provide guidelines for developing a myriad of novel
approaches that can be implemented locally in the vertex domain. One of such
approaches is presented here showing superior performance w.r.t. current
alternatives in graph signal classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01320</identifier>
 <datestamp>2019-06-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01320</id><created>2019-03-04</created><updated>2019-06-11</updated><authors><author><keyname>Bergerhoff</keyname><forenames>Leif</forenames></author><author><keyname>Weickert</keyname><forenames>Joachim</forenames></author><author><keyname>Dar</keyname><forenames>Yehuda</forenames></author></authors><title>Algorithms for Piecewise Constant Signal Approximations</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding optimal piecewise constant approximations
of one-dimensional signals. These approximations should consist of a specified
number of segments (samples) and minimise the mean squared error to the
original signal. We formalise this goal as a discrete nonconvex optimisation
problem, for which we study two algorithms. First we reformulate a recent
adaptive sampling method by Dar and Bruckstein in a compact and transparent
way. This allows us to analyse its limitations when it comes to violations of
its three key assumptions: signal smoothness, local linearity, and error
balancing. As a remedy, we propose a direct optimisation approach which does
not rely on any of these assumptions and employs a particle swarm optimisation
algorithm. Our experiments show that for nonsmooth signals or low sample
numbers, the direct optimisation approach offers substantial qualitative
advantages over the Dar--Bruckstein method. As a more general contribution, we
disprove the optimality of the principle of error balancing for optimising data
in the l^2 norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01322</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01322</id><created>2019-03-04</created><authors><author><keyname>Pi&#xf1;ol</keyname><forenames>David Castro</forenames></author><author><keyname>Reyes</keyname><forenames>Enrique Juan Mara&#xf1;&#xf3;n</forenames></author></authors><title>Automatic Handgun Detection in X-ray Images using Bag of Words Model
  with Selective Search</title><categories>eess.IV cs.CV</categories><comments>in Spanish</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Baggage inspection systems using X-ray screening are crucial for security.
Only 90% of threat objects are recognized from the X-ray system based in human
inspection. Manual detection requires high concentration due to the images
complexity and the challenges objects points of view. An algorithm based on Bag
of Visual Word (BoVW) with Selective Search is proposed in this paper for
handguns detection in single energy X-ray images from the public GDXray
database. This approach is an adaptation of BoVW for X-ray baggage images
context. In order to evaluate the proposed method the algorithm effectiveness
recognition was tested on all bounding boxes returned by selective search
algorithm in 200 images. The most relevant result is the precision and true
positive rate (PPV = 80%, TPR= 92%). This approach achieves good performance
for handgun recognition. In addition, it is the first time the Selective Search
localization algorithm was tested in baggage X-ray images and showed
possibilities with Bag of Visual Words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01369</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01369</id><created>2019-03-04</created><authors><author><keyname>Erez</keyname><forenames>Uri</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>Eliminating Interference in LOS Massive Multi-User MIMO with a Few
  Transceivers</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless cellular communication networks are bandwidth and interference
limited. An important means to overcome these resource limitations is the use
of multiple antennas. Base stations equipped with a very large (massive) number
of antennas have been the focus of recent research. A bottleneck in such
systems is the limited number of transmit/receive chains. In this work, a
line-of-sight (LOS) channel model is considered. It is shown that for a given
number of interferers, it suffices that the number of transmit/receive chains
exceeds the number of desired users by one, assuming a sufficiently large
antenna array. From a theoretical point of view, this is the first result
proving the near-optimal performance of antenna selection, even when the total
number of signals (desired and interfering) is larger than the number of
receive chains. Specifically, a single additional chain suffices to reduce the
interference to any desired level. We prove that using the proposed selection,
a simple linear receiver/transmitter for the uplink/downlink provides
near-optimal rates. In particular, in the downlink direction, there is no need
for complicated dirty paper coding; each user can use an optimal code for a
single user interference-free channel. In the uplink direction, there is almost
no gain in implementing joint decoding. The proposed approach is also a
significant improvement both from system and computational perspectives.
Simulation results demonstrating the performance of the proposed method are
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01380</identifier>
 <datestamp>2019-03-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01380</id><created>2019-03-04</created><authors><author><keyname>Dedhia</keyname><forenames>Bhishma</forenames></author><author><keyname>Chiang</keyname><forenames>Jui-Chiu</forenames></author><author><keyname>Char</keyname><forenames>Yi-Fan</forenames></author></authors><title>Saliency Prediction for Omnidirectional Images Considering Optimization
  on Sphere Domain</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are several formats to describe the omnidirectional images. Among them,
equirectangular projection (ERP), represented as 2D image, is the most widely
used format. There exist many outstanding methods capable of well predicting
the saliency maps for the conventional 2D images. But these works cannot be
directly extended to predict the saliency map of the ERP image, since the
content on ERP is not for direct display. Instead, the viewport image on demand
is generated after converting the ERP image to the sphere domain, followed by
rectilinear projection. In this paper, we propose a model to predict the
saliency maps of the ERP images using existing saliency predictors for the 2D
image. Some pre-processing and post-processing are used to manage the problem
mentioned above. In particular, a smoothing based optimization is realized on
the sphere domain. A public dataset of omnidirectional images is used to
perform all the experiments and competitive results are achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01415</identifier>
 <datestamp>2019-06-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01415</id><created>2019-03-04</created><authors><author><keyname>Cohen-Hadria</keyname><forenames>Alice</forenames></author><author><keyname>Roebel</keyname><forenames>Axel</forenames></author><author><keyname>Peeters</keyname><forenames>Geoffroy</forenames></author></authors><title>Improving singing voice separation using Deep U-Net and Wave-U-Net with
  data augmentation</title><categories>cs.SD eess.AS</categories><journal-ref>Published in Proceedings of the 27th European Signal Processing
  Conference (EUSIPCO), 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art singing voice separation is based on deep learning making
use of CNN structures with skip connections (like U-net model, Wave-U-Net
model, or MSDENSELSTM). A key to the success of these models is the
availability of a large amount of training data. In the following study, we are
interested in singing voice separation for mono signals and will investigate
into comparing the U-Net and the Wave-U-Net that are structurally similar, but
work on different input representations. First, we report a few results on
variations of the U-Net model. Second, we will discuss the potential of state
of the art speech and music transformation algorithms for augmentation of
existing data sets and demonstrate that the effect of these augmentations
depends on the signal representations used by the model. The results
demonstrate a considerable improvement due to the augmentation for both models.
But pitch transposition is the most effective augmentation strategy for the
U-Net model, while transposition, time stretching, and formant shifting have a
much more balanced effect on the Wave-U-Net model. Finally, we compare the two
models on the same dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01416</identifier>
 <datestamp>2019-06-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01416</id><created>2019-03-04</created><authors><author><keyname>Jacques</keyname><forenames>Celine</forenames></author><author><keyname>Roebel</keyname><forenames>Axel</forenames></author></authors><title>Data Augmentation for Drum Transcription with Convolutional Neural
  Networks</title><categories>cs.SD eess.AS</categories><journal-ref>Published in Proceedings of the 27th European Signal Processing
  Conference (EUSIPCO), 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recurrent issue in deep learning is the scarcity of data, in particular
precisely annotated data. Few publicly available databases are correctly
annotated and generating correct labels is very time consuming. The present
article investigates into data augmentation strategies for Neural Networks
training, particularly for tasks related to drum transcription. These tasks
need very precise annotations. This article investigates state-of-the-art sound
transformation algorithms for remixing noise and sinusoidal parts, remixing
attacks, transposing with and without time compensation and compares them to
basic regularization methods such as using dropout and additive Gaussian noise.
And it shows how a drum transcription algorithm based on CNN benefits from the
proposed data augmentation strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01428</identifier>
 <datestamp>2019-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01428</id><created>2019-03-04</created><updated>2019-03-21</updated><authors><author><keyname>Hosseinalipour</keyname><forenames>Seyyedali</forenames></author><author><keyname>Rahmati</keyname><forenames>Ali</forenames></author><author><keyname>Dai</keyname><forenames>Huaiyu</forenames></author></authors><title>Interference Avoidance Position Planning in UAV-assisted Wireless
  Communication</title><categories>eess.SP cs.NI</categories><comments>6 pages, 8 figures</comments><journal-ref>IEEE International Conference on Communications (ICC), 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider unmanned aerial vehicle (UAV)-assisted wireless communication
employing UAVs as relay nodes to increase the throughput between a pair of
transmitter and receiver. We focus on developing effective methods to position
the UAV(s) in the sky in the presence of a major source of interference, the
existence of which makes the problem non-trivial. First, we consider utilizing
a single UAV, for which we develop a theoretical framework to determine its
optimal position aiming to maximize the SIR of the system. To this end, we
investigate the problem for three practical scenarios, in which the position of
the UAV is: (i) vertically fixed, horizontally adjustable; (ii) horizontally
fixed, vertically adjustable; (iii) both horizontally and vertically
adjustable. Afterward, we consider employing multiple UAVs, for which we
propose a cost-effective method that simultaneously minimizes the number of
required UAVs and determines their optimal positions so as to guarantee a
certain SIR of the system. We further develop a distributed placement
algorithm, which can increase the SIR of the system given an arbitrary number
of UAVs. Numerical simulations are provided to evaluate the performance of our
proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01443</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01443</id><created>2019-03-03</created><authors><author><keyname>Chowdhury</keyname><forenames>Md Moin Uddin</forenames></author><author><keyname>Maeng</keyname><forenames>Sung Joon</forenames></author><author><keyname>Bulut</keyname><forenames>Eyuphan</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author></authors><title>Effects of 3D Antenna Radiation and Two-Hop Relaying on Optimal UAV
  Trajectory in Cellular Networks</title><categories>eess.SP</categories><comments>Accepted at IEEE Aerospace Conference 2019, Big sky, Montana. arXiv
  admin note: text overlap with arXiv:1902.04762</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, considering an interference limited inband downlink cellular
network, we study the effects of scheduling criteria, mobility constraints,
path loss models, backhaul constraints, and 3D antenna radiation pattern on
trajectory optimization problem of an unmanned aerial vehicle (UAV). In
particular, we consider a UAV that is tasked to travel between two locations
within a given amount of time (e.g., for delivery or surveillance purposes),
and we consider that such a UAV can be used to improve cellular connectivity of
mobile users by serving as a relay for the terrestrial network. As the
optimization problem is hard to solve numerically, we explore the dynamic
programming (DP) technique for finding the optimum UAV trajectory. We utilize
capacity and coverage performance of the terrestrial network while studying all
the effects of different techniques and phenomenon. Extensive simulations show
that the maximum sum-rate trajectory provides the best per user capacity
whereas, the optimal proportional fair (PF) rate trajectory provides higher
coverage probability than the other two. Since, the generated trajectories are
infeasible for the UAV to follow exactly as it can not take sharp turns due to
kinematic constraints, we generate smooth trajectory using Bezier curves. Our
results show that the cellular capacity using the Bezier curves is close to the
capacity observed when using the optimal trajectories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01450</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01450</id><created>2019-03-03</created><authors><author><keyname>Yao</keyname><forenames>Yu</forenames></author><author><keyname>Atkins</keyname><forenames>Ella M.</forenames></author></authors><title>The Smart Black Box: A Value-Driven High-Bandwidth Automotive Event Data
  Recorder</title><categories>eess.SP</categories><comments>Submitted to IEEE Transactions on Intelligent Transportation Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous vehicles require reliable and resilient sensor suites and ongoing
validation through fleet-wide data collection. This paper proposes a Smart
Black Box (SBB) to augment traditional low-bandwidth data logging with
value-driven high-bandwidth data capture. The SBB caches short-term histories
of data as buffers through a deterministic Mealy machine based on data value
and similarity. Compression quality for each frame is determined by optimizing
the trade-off between value and storage cost. With finite storage, prioritized
data recording discards low-value buffers to make room for new data. This paper
formulates SBB compression decision making as a constrained multi-objective
optimization problem with novel value metrics and filtering. The SBB has been
evaluated on a traffic simulator which generates trajectories containing events
of interest (EOIs) and corresponding first-person view videos. SBB compression
efficiency is assessed by comparing storage requirements with different
compression quality levels and event capture ratios. Performance is evaluated
by comparing results with a traditional first-in-first-out (FIFO) recording
scheme. Deep learning performance on images recorded at different compression
levels is evaluated to illustrate the reproducibility of SBB recorded data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01530</identifier>
 <datestamp>2020-02-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01530</id><created>2019-03-04</created><updated>2019-06-22</updated><authors><author><keyname>de Blois</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Hedhli</keyname><forenames>Ihsen</forenames></author><author><keyname>Gagn&#xe9;</keyname><forenames>Christian</forenames></author></authors><title>Learning of Image Dehazing Models for Segmentation Tasks</title><categories>cs.CV eess.IV</categories><comments>Accepted in EUSIPCO 2019</comments><doi>10.23919/EUSIPCO.2019.8903046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To evaluate their performance, existing dehazing approaches generally rely on
distance measures between the generated image and its corresponding ground
truth. Despite its ability to produce visually good images, using pixel-based
or even perceptual metrics do not guarantee, in general, that the produced
image is fit for being used as input for low-level computer vision tasks such
as segmentation. To overcome this weakness, we are proposing a novel end-to-end
approach for image dehazing, fit for being used as input to an image
segmentation procedure, while maintaining the visual quality of the generated
images. Inspired by the success of Generative Adversarial Networks (GAN), we
propose to optimize the generator by introducing a discriminator network and a
loss function that evaluates segmentation quality of dehazed images. In
addition, we make use of a supplementary loss function that verifies that the
visual and the perceptual quality of the generated image are preserved in hazy
conditions. Results obtained using the proposed technique are appealing, with a
favorable comparison to state-of-the-art approaches when considering the
performance of segmentation algorithms on the hazy images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01544</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01544</id><created>2019-02-26</created><authors><author><keyname>Da Ros</keyname><forenames>F.</forenames></author><author><keyname>Civelli</keyname><forenames>S.</forenames></author><author><keyname>Gaiarin</keyname><forenames>S.</forenames></author><author><keyname>da Silva</keyname><forenames>E. P.</forenames></author><author><keyname>De Renzis</keyname><forenames>N.</forenames></author><author><keyname>Secondini</keyname><forenames>M.</forenames></author><author><keyname>Zibar</keyname><forenames>D.</forenames></author></authors><title>Dual-polarization NFDM transmission with continuous and discrete
  spectral modulation</title><categories>eess.SP physics.optics</categories><journal-ref>Journal of Lightwave Technology, pre-print 2019</journal-ref><doi>10.1109/JLT.2019.2904102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear distortion experienced by signals during their propagation through
optical fibers strongly limits the throughput of optical communication systems.
Recently, a strong research focus has been dedicated to nonlinearity mitigation
and compensation techniques. At the same time, a more disruptive approach, the
nonlinear Fourier transform (NFT), aims at designing signaling schemes more
suited to the nonlinear fiber channel. In a short period, impressive results
have been reported by modulating either the continuous spectrum or the discrete
spectrum. Additionally, very recent works further introduced the opportunity to
modulate both spectra for single polarization transmission. Here, we extend the
joint modulation scheme to dual-polarization transmission by introducing the
framework to construct a dual-polarization optical signal with the desired
continuous and discrete spectra. After a brief analysis of the numerical
algorithms used to implement the proposed scheme, the first experimental
demonstration of dual-polarization joint nonlinear frequency division
multiplexing (NFDM) modulation is reported for up to 3200 km of low-loss
transmission fiber. The proposed dual-polarization joint modulation schemes
enables to exploit all the degrees of freedom for modulation (both
polarizations and both spectra) provided by a single-mode fiber (SMF).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01549</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01549</id><created>2019-02-27</created><updated>2019-03-06</updated><authors><author><keyname>Amari</keyname><forenames>Abdelkerim</forenames></author><author><keyname>Lin</keyname><forenames>Xiang</forenames></author><author><keyname>Dobre</keyname><forenames>Octavia A.</forenames></author><author><keyname>Venkatesan</keyname><forenames>Ramachandran</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author></authors><title>A Machine Learning-Based Detection Technique for Optical Fiber
  Nonlinearity Mitigation</title><categories>eess.SP cs.LG</categories><comments>Accepted for publication in IEEE Photonics Technology Letters</comments><doi>10.1109/LPT.2019.2902973</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the performance of a machine learning classification
technique, called the Parzen window, to mitigate the fiber nonlinearity in the
context of dispersion managed and dispersion unmanaged systems. The technique
is applied for detection at the receiver side, and deals with the non-Gaussian
nonlinear effects by designing improved decision boundaries. We also propose a
two-stage mitigation technique using digital back propagation and Parzen window
for dispersion unmanaged systems. In this case, digital back propagation
compensates for the deterministic nonlinearity and the Parzen window deals with
the stochastic nonlinear signal-noise interactions, which are not taken into
account by digital back propagation. A performance improvement up to 0:4 dB in
terms of Q factor is observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01551</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01551</id><created>2019-02-27</created><authors><author><keyname>Gao</keyname><forenames>Dawei</forenames></author><author><keyname>Guo</keyname><forenames>Qinghua</forenames></author></authors><title>Extreme Learning Machine-Based Receiver for MIMO LED Communications</title><categories>eess.SP cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns receiver design for light-emitting diode (LED) multiple
input multiple output (MIMO) communications where the LED nonlinearity can
severely degrade the performance of communications. In this paper, we propose
an extreme learning machine (ELM) based receiver to jointly handle the LED
nonlinearity and cross-LED interference, and a circulant input weight matrix is
employed, which significantly reduces the complexity of the receiver with the
fast Fourier transform (FFT). It is demonstrated that the proposed receiver can
efficiently handle the LED nonlinearity and cross-LED interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01552</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01552</id><created>2019-03-01</created><authors><author><keyname>Zabihi</keyname><forenames>Morteza</forenames></author><author><keyname>Rad</keyname><forenames>Ali Bahrami</forenames></author><author><keyname>Kiranyaz</keyname><forenames>Serkan</forenames></author><author><keyname>S&#xe4;rkk&#xe4;</keyname><forenames>Simo</forenames></author><author><keyname>Gabbouj</keyname><forenames>Moncef</forenames></author></authors><title>1D Convolutional Neural Network Models for Sleep Arousal Detection</title><categories>eess.SP cs.LG stat.ML</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sleep arousals transition the depth of sleep to a more superficial stage. The
occurrence of such events is often considered as a protective mechanism to
alert the body of harmful stimuli. Thus, accurate sleep arousal detection can
lead to an enhanced understanding of the underlying causes and influencing the
assessment of sleep quality. Previous studies and guidelines have suggested
that sleep arousals are linked mainly to abrupt frequency shifts in EEG
signals, but the proposed rules are shown to be insufficient for a
comprehensive characterization of arousals. This study investigates the
application of five recent convolutional neural networks (CNNs) for sleep
arousal detection and performs comparative evaluations to determine the best
model for this task. The investigated state-of-the-art CNN models have
originally been designed for image or speech processing. A detailed set of
evaluations is performed on the benchmark dataset provided by
PhysioNet/Computing in Cardiology Challenge 2018, and the results show that the
best 1D CNN model has achieved an average of 0.31 and 0.84 for the area under
the precision-recall and area under the ROC curves, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01556</identifier>
 <datestamp>2019-05-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01556</id><created>2019-02-12</created><updated>2019-05-22</updated><authors><author><keyname>M&#xfc;ller</keyname><forenames>Johannes</forenames></author><author><keyname>Gabb</keyname><forenames>Michael</forenames></author><author><keyname>Buchholz</keyname><forenames>Michael</forenames></author></authors><title>A Subjective-Logic-based Reliability Estimation Mechanism for
  Cooperative Information with Application to IV's Safety</title><categories>eess.SP cs.MA cs.SY</categories><comments>7 pages, accepted at 30th IEEE Intelligent Vehicles Symposium (IV
  2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Use of cooperative information, distributed by road-side units, offers large
potential for intelligent vehicles (IVs). As vehicle automation progresses and
cooperative perception is used to fill the blind spots of onboard sensors, the
question of reliability of the data becomes increasingly important in safety
considerations (SOTIF, Safety of the Intended Functionality).
  This paper addresses the problem to estimate the reliability of cooperative
information for in-vehicle use. We propose a novel method to infer the
reliability of received data based on the theory of Subjective Logic (SL).
Using SL, we fuse multiple information sources, which individually only provide
mild cues of the reliability, into a holistic estimate, which is statistically
sound through an end-to-end modeling within the theory of SL.
  Using the proposed scheme for probabilistic SL-based fusion, IVs are able to
separate faulty from correct data samples with a large margin of safety. Real
world experiments show the applicability and effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01563</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01563</id><created>2019-03-01</created><authors><author><keyname>Flowers</keyname><forenames>Bryse</forenames></author><author><keyname>Buehrer</keyname><forenames>R. Michael</forenames></author><author><keyname>Headley</keyname><forenames>William C.</forenames></author></authors><title>Evaluating Adversarial Evasion Attacks in the Context of Wireless
  Communications</title><categories>eess.SP cs.LG stat.ML</categories><comments>13 pages, 14 figures, Submitted to IEEE Transactions on Information
  Forensics &amp; Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in radio frequency machine learning (RFML) have
demonstrated the use of raw in-phase and quadrature (IQ) samples for multiple
spectrum sensing tasks. Yet, deep learning techniques have been shown, in other
applications, to be vulnerable to adversarial machine learning (ML) techniques,
which seek to craft small perturbations that are added to the input to cause a
misclassification. The current work differentiates the threats that adversarial
ML poses to RFML systems based on where the attack is executed from: direct
access to classifier input, synchronously transmitted over the air (OTA), or
asynchronously transmitted from a separate device. Additionally, the current
work develops a methodology for evaluating adversarial success in the context
of wireless communications, where the primary metric of interest is bit error
rate and not human perception, as is the case in image recognition. The
methodology is demonstrated using the well known Fast Gradient Sign Method to
evaluate the vulnerabilities of raw IQ based Automatic Modulation
Classification and concludes RFML is vulnerable to adversarial examples, even
in OTA attacks. However, RFML domain specific receiver effects, which would be
encountered in an OTA attack, can present significant impairments to
adversarial evasion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01564</identifier>
 <datestamp>2019-05-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01564</id><created>2019-02-27</created><updated>2019-05-16</updated><authors><author><keyname>Yin</keyname><forenames>Li</forenames></author><author><keyname>Zhou</keyname><forenames>Y. M.</forenames></author></authors><title>Life detection strategy based on infrared vision and ultra-wideband
  radar data fusion</title><categories>eess.SP</categories><comments>6 pages, 7 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The life detection method based on a single type of information source cannot
meet the requirement of post-earthquake rescue due to its limitations in
different scenes and bad robustness in life detection. This paper proposes a
method based on deep neural network for multi-sensor decision-level fusion
which concludes Convolutional Neural Network and Long Short Term Memory neural
network (CNN+LSTM). Firstly, we calculate the value of the life detection
probability of each sensor with various methods in the same scene
simultaneously, which will be gathered to make samples for inputs of the deep
neural network. Then we use Convolutional Neural Network (CNN) to extract the
distribution characteristics of the spatial domain from inputs which is the
two-channel combination of the probability values and the smoothing probability
values of each life detection sensor respectively. Furthermore, the sequence
time relationship of the outputs from the last layers will be analyzed with
Long Short Term Memory (LSTM) layers, then we concatenate the results from
three branches of LSTM layers. Finally, two sets of LSTM neural networks that
is different from the previous layers are used to integrate the three branches
of the features, and the results of the two classifications are output using
the fully connected network with Binary Cross Entropy (BEC) loss function.
Therefore, the classification results of the life detection can be concluded
accurately with the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01576</identifier>
 <datestamp>2019-03-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01576</id><created>2019-03-04</created><updated>2019-03-05</updated><authors><author><keyname>Mahjoub</keyname><forenames>Hossein Nourkhiz</forenames></author><author><keyname>Toghi</keyname><forenames>Behrad</forenames></author><author><keyname>Gani</keyname><forenames>S M Osman</forenames></author><author><keyname>Fallah</keyname><forenames>Yaser P.</forenames></author></authors><title>V2X System Architecture Utilizing Hybrid Gaussian Process-based Model
  Structures</title><categories>eess.SP cs.LG</categories><comments>Accepted for Oral Presentation at the 13th IEEE Systems Conference
  (SysCon 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable communication is of utmost importance for reliable dissemination of
time-sensitive information in cooperative vehicular ad-hoc networks (VANETs),
which is, in turn, an essential prerequisite for the proper operation of the
critical cooperative safety applications. The model-based communication (MBC)
is a recently-explored scalability solution proposed in the literature, which
has shown a promising potential to reduce the channel congestion to a great
extent. In this work, based on the MBC notion, a technology-agnostic hybrid
model selection policy for Vehicle-to-Everything (V2X) communication is
proposed which benefits from the characteristics of the non-parametric Bayesian
inference techniques, specifically Gaussian Processes. The results show the
effectiveness of the proposed communication architecture on both reducing the
required message exchange rate and increasing the remote agent tracking
precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01625</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01625</id><created>2019-03-04</created><authors><author><keyname>Yang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Study of Sparsity-Aware Reduced-Dimension Beam-Doppler Space-Time
  Adaptive Processing</title><categories>eess.SP cs.IT math.IT</categories><comments>6 figures, 7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing reduced-dimension beam-Doppler space-time adaptive processing
(RD-BD-STAP) algorithms are confined to the beam-Doppler cells used for
adaptation, which often leads to some performance degradation. In this work, a
novel sparsity-aware RD-BD-STAP algorithm, denoted Sparse Constraint on
Beam-Doppler Selection Reduced-Dimension Space-Time Adaptive Processing
(SCBDS-RD-STAP), is proposed can adaptively selects the best beam-Doppler cells
for adaptation. The proposed SCBDS-RD-STAP approach formulates the filter
design as a sparse representation problem and enforcing most of the elements in
the weight vector to be zero (or sufficiently small in amplitude). Simulation
results illustrate that the proposed SCBDS-RD-STAP algorithm outperforms the
traditional RD-BD-STAP approaches with fixed beam-Doppler localized processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01688</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01688</id><created>2019-03-05</created><authors><author><keyname>Favaretto</keyname><forenames>Rodolfo Migon</forenames></author><author><keyname>Dihl</keyname><forenames>Leandro</forenames></author><author><keyname>Musse</keyname><forenames>Soraia Raupp</forenames></author><author><keyname>Vilanova</keyname><forenames>Felipe</forenames></author><author><keyname>Costa</keyname><forenames>Angelo Brandelli</forenames></author></authors><title>Using Big Five Personality Model to Detect Cultural Aspects in Crowds</title><categories>cs.CV eess.IV</categories><doi>10.1109/SIBGRAPI.2017.36</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The use of information technology in the study of human behavior is a subject
of great scientific interest. Cultural and personality aspects are factors that
influence how people interact with one another in a crowd. This paper presents
a methodology to detect cultural characteristics of crowds in video sequences.
Based on filmed sequences, pedestrians are detected, tracked and characterized.
Such information is then used to find out cultural differences in those videos,
based on the Big-five personality model. Regarding cultural differences of each
country, results indicate that this model generates coherent information when
compared to data provided in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01731</identifier>
 <datestamp>2019-03-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01731</id><created>2019-03-05</created><updated>2019-03-14</updated><authors><author><keyname>Wang</keyname><forenames>Haifeng</forenames></author><author><keyname>Qiu</keyname><forenames>Zhilang</forenames></author><author><keyname>Su</keyname><forenames>Shi</forenames></author><author><keyname>Ying</keyname><forenames>Leslie</forenames></author><author><keyname>Liang</keyname><forenames>Dong</forenames></author></authors><title>Fast Calculation Method of Average g-Factor for Wave-CAIPI Imaging</title><categories>physics.med-ph eess.IV</categories><comments>4 pages, 2 figures, 1 table, Accepted at 2019 IEEE 16th International
  Symposium on Biomedical Imaging (ISBI 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wave-CAIPI MR imaging is a 3D imaging technique which can uniformize the
g-factor maps and significantly reduce g-factor penalty at high acceleration
factors. But it is time-consuming to calculate the average g-factor penalty for
optimizing the parameters of Wave-CAIPI. In this paper, we propose a novel fast
calculation method to calculate the average g-factor in Wave-CAIPI imaging.
Wherein, the g-factor value in the arbitrary (e.g. the central) position is
separately calculated and then approximated to the average g-factor using
Taylor linear approximation. The verification experiments have demonstrated
that the average g-factors of Wave-CAIPI imaging which are calculated by the
proposed method is consistent with the previous time-consuming theoretical
calculation method and the conventional pseudo multiple replica method.
Comparison experiments show that the proposed method is averagely about 1000
times faster than the previous theoretical calculation method and about 1700
times faster than the conventional pseudo multiple replica method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01811</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01811</id><created>2019-03-05</created><authors><author><keyname>Ahmad</keyname><forenames>Afzal</forenames></author><author><keyname>Pasha</keyname><forenames>Muhammad Adeel</forenames></author></authors><title>Towards Design Space Exploration and Optimization of Fast Algorithms for
  Convolutional Neural Networks (CNNs) on FPGAs</title><categories>eess.SP cs.CV eess.IV</categories><comments>Preprint: Accepted in 22nd IEEE Design, Automation &amp; Test in Europe
  Conference and Exhibition (DATE'19), Florence, Italy, March 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) have gained widespread popularity in the
field of computer vision and image processing. Due to huge computational
requirements of CNNs, dedicated hardware-based implementations are being
explored to improve their performance. Hardware platforms such as Field
Programmable Gate Arrays (FPGAs) are widely being used to design parallel
architectures for this purpose. In this paper, we analyze Winograd minimal
filtering or fast convolution algorithms to reduce the arithmetic complexity of
convolutional layers of CNNs. We explore a complex design space to find the
sets of parameters that result in improved throughput and power-efficiency. We
also design a pipelined and parallel Winograd convolution engine that improves
the throughput and power-efficiency while reducing the computational complexity
of the overall system. Our proposed designs show up to 4.75$\times$ and
1.44$\times$ improvements in throughput and power-efficiency, respectively, in
comparison to the state-of-the-art design while using approximately
2.67$\times$ more multipliers. Furthermore, we obtain savings of up to 53.6\%
in logic resources compared with the state-of-the-art implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01826</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01826</id><created>2019-03-05</created><authors><author><keyname>Jelic</keyname><forenames>Drazen</forenames></author><author><keyname>Scekic</keyname><forenames>Ana</forenames></author><author><keyname>Hot</keyname><forenames>Melvudin</forenames></author><author><keyname>Sevaljevic</keyname><forenames>Nemanja</forenames></author></authors><title>Comparison of different algorithms for under-sampled image
  reconstruction</title><categories>eess.SP</categories><comments>Student paper submitted to The 8th Mediterranean Conference on
  Embedded Computing - MECO'2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Compressive Sensing (CS) as a novel acquisition approach that finds its
usage in image processing. The hypothesis like this one assures signal recovery
with high quality from decreased number of samples compared with the number
required by the Nyquist - Shannon sampling theorem. It includes a gathering of
strategies for representing a signal that are based on the predetermined number
of estimations and after that signal reconstruction. The CS has been broadly
utilized and applied in numerous applications including computed tomography,
WiFi communication, image processing and camera design. Complex mathematics is
developed in order to ensure signal reconstruction from relatively small
information. Two commonly used groups of the algorithms are convex optimization
and greedy approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01856</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01856</id><created>2019-03-05</created><authors><author><keyname>Liu</keyname><forenames>Xiaolan</forenames></author><author><keyname>Qin</keyname><forenames>Zhijin</forenames></author><author><keyname>Gao</keyname><forenames>Yue</forenames></author></authors><title>Resource Allocation for Edge Computing in IoT Networks via Reinforcement
  Learning</title><categories>eess.SP</categories><comments>6 pages, 5 figures, ICC'19 accepted paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider resource allocation for edge computing in internet
of things (IoT) networks. Specifically, each end device is considered as an
agent, which makes its decisions on whether offloading the computation tasks to
the edge devices or not. To minimise the long-term weighted sum cost which
includes the power consumption and the task execution latency, we consider the
channel conditions between the end devices and the gateway, the computation
task queue as well as the remaining computation resource of the end devices as
the network states. The problem of making a series of decisions at the end
devices is modelled as a Markov decision process and solved by the
reinforcement learning approach. Therefore, we propose a near optimal task
offloading algorithm based on Q-learning. Simulations validate the feasibility
of our proposed algorithm, which achieves a better trade-off between the power
consumption and the task execution latency compared to these of edge computing
and local computing modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01878</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01878</id><created>2019-02-25</created><authors><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Gong</keyname><forenames>Feng-Kui</forenames></author><author><keyname>Song</keyname><forenames>Pei-Yang</forenames></author><author><keyname>Zhai</keyname><forenames>Sheng-Hua</forenames></author></authors><title>Symbol-Based Multi-Layer Iterative Successive Interference Cancellation
  for Faster-Than-Nyquist Signaling</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A symbol-based multi-layer iterative successive interference cancellation
(MLISIC) algorithm is proposed to eliminate the inter-symbol interference (ISI)
for faster-than-Nyquist (FTN) signaling. The computational complexity of the
proposed MLISIC algorithm is much lower than most existing block-based
estimation algorithms. By comparison with existing symbol-based algorithms, the
proposed MLISIC algorithm has higher estimation accuracy. Furthermore, by means
of utilizing more accurate symbols and elaborate length of successive
interference cancellation, an improved MLISIC (IMLISIC) algorithm is presented
to further promote the estimation accuracy. Simulation results show in mild ISI
cases, the proposed MLISIC and IMLISIC algorithms can approximate the
theoretical performance even using 256-amplitude phase shift keying (APSK)
which is adopted in digital video broadcasting-satellite-second generation
extension (DVB-S2X), i.e., bit error rate (BER) performance degradation is
about 0.03 dB when the signal-to-noise ratio is high. In other words, using the
proposed MLISIC and IMLISIC algorithms in mild ISI cases, the spectrum
efficiency can be improved with negligible BER performance degradation. In
moderate ISI cases, BER performance degradation of our proposed IMLISIC
algorithm is no more than 0.13 dB when adopting 64/128/256-APSK, which is still
satisfying. Besides, compared with existing symbol-based algorithms, the
greater BER performance improvement can be achieved under severer ISI
circumstances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01907</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01907</id><created>2019-01-23</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Gu</keyname><forenames>Xueping</forenames></author></authors><title>Feature Selection for Transient Stability Assessment Based on Improved
  Maximal Relevance and Minimal Redundancy Criterion</title><categories>eess.SP cs.SY</categories><journal-ref>Proceedings of the CSEE 33 (2013) 179-186</journal-ref><doi>10.13334/j.0258-8013.pcsee.2013.34.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new feature selection method based on an improved maximal relevance and
minimal redundancy (mRMR) criterion was proposed for power system transient
stability assessment. First, the standard mRMR was improved by introducing a
weight coefficient in the evaluation criteria to refine the measurement of the
features correlation and redundancy. Then, the possible real-time information
provided by phasor measurement unit (PMU) considered, a group of system-level
classification features were extracted from the power system operation
parameters to build the original feature set, and the improved mRMR was
employed to evaluate the classification capability of the original features for
feature selection. A group of nested candidate feature subsets were obtained by
using the incremental search technique, and each candidate feature subset was
tested by a support vector machine classifier to find the optimal feature
subset with the highest classification accuracy. The effectiveness of the
proposed method was validated by the simulation results on the New England
39-bus system and IEEE 50-generator test system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01974</identifier>
 <datestamp>2019-03-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01974</id><created>2019-03-05</created><authors><author><keyname>Ozfatura</keyname><forenames>Emre</forenames></author><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Gradient Coding with Clustering and Multi-message Communication</title><categories>cs.IT cs.DC cs.LG eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gradient descent (GD) methods are commonly employed in machine learning
problems to optimize the parameters of the model in an iterative fashion. For
problems with massive datasets, computations are distributed to many parallel
computing servers (i.e., workers) to speed up GD iterations. While distributed
computing can increase the computation speed significantly, the per-iteration
completion time is limited by the slowest straggling workers. Coded distributed
computing can mitigate straggling workers by introducing redundant
computations; however, existing coded computing schemes are mainly designed
against persistent stragglers, and partial computations at straggling workers
are discarded, leading to wasted computational capacity. In this paper, we
propose a novel gradient coding (GC) scheme which allows multiple coded
computations to be conveyed from each worker to the master per iteration. We
numerically show that the proposed GC with multi-message communication (MMC)
together with clustering provides significant improvements in the average
completion time (of each iteration), with minimal or no increase in the
communication load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.01976</identifier>
 <datestamp>2019-06-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.01976</id><created>2019-03-05</created><updated>2019-06-20</updated><authors><author><keyname>Yela</keyname><forenames>Delia Fano</forenames></author><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Sandler</keyname><forenames>Mark</forenames></author></authors><title>Spectral Visibility Graphs: Application to Similarity of Harmonic
  Signals</title><categories>cs.SD eess.AS</categories><comments>European Signal Processing Conference (EUSIPCO)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph theory is emerging as a new source of tools for time series analysis.
One promising method is to transform a signal into its visibility graph, a
representation which captures many interesting aspects of the signal. Here we
introduce the visibility graph for audio spectra and propose a novel
representation for audio analysis: the spectral visibility graph degree. Such
representation inherently captures the harmonic content of the signal whilst
being resilient to broadband noise. We present experiments demonstrating its
utility to measure robust similarity between harmonic signals in real and
synthesised audio data. The source code is available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02026</identifier>
 <datestamp>2020-01-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02026</id><created>2019-03-05</created><updated>2020-01-21</updated><authors><author><keyname>Haskins</keyname><forenames>Grant</forenames></author><author><keyname>Kruger</keyname><forenames>Uwe</forenames></author><author><keyname>Yan</keyname><forenames>Pingkun</forenames></author></authors><title>Deep Learning in Medical Image Registration: A Survey</title><categories>q-bio.QM cs.CV eess.IV</categories><comments>Accepted for publication by Machine Vision and Applications on
  January 8, 2020</comments><doi>10.1007/s00138-020-01060-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The establishment of image correspondence through robust image registration
is critical to many clinical tasks such as image fusion, organ atlas creation,
and tumor growth monitoring, and is a very challenging problem. Since the
beginning of the recent deep learning renaissance, the medical imaging research
community has developed deep learning based approaches and achieved the
state-of-the-art in many applications, including image registration. The rapid
adoption of deep learning for image registration applications over the past few
years necessitates a comprehensive summary and outlook, which is the main scope
of this survey. This requires placing a focus on the different research areas
as well as highlighting challenges that practitioners face. This survey,
therefore, outlines the evolution of deep learning based medical image
registration in the context of both research challenges and relevant
innovations in the past few years. Further, this survey highlights future
research directions to show how this field may be possibly moved forward to the
next level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02094</identifier>
 <datestamp>2019-12-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02094</id><created>2019-03-05</created><authors><author><keyname>Corey</keyname><forenames>Ryan M.</forenames></author><author><keyname>Tsuda</keyname><forenames>Naoki</forenames></author><author><keyname>Singer</keyname><forenames>Andrew C.</forenames></author></authors><title>Acoustic Impulse Responses for Wearable Audio Devices</title><categories>eess.AS cs.SD</categories><comments>To appear at ICASSP 2019</comments><journal-ref>2019 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)</journal-ref><doi>10.1109/ICASSP.2019.8682733</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an open-access dataset of over 8000 acoustic impulse from 160
microphones spread across the body and affixed to wearable accessories. The
data can be used to evaluate audio capture and array processing systems using
wearable devices such as hearing aids, headphones, eyeglasses, jewelry, and
clothing. We analyze the acoustic transfer functions of different parts of the
body, measure the effects of clothing worn over microphones, compare
measurements from a live human subject to those from a mannequin, and simulate
the noise-reduction performance of several beamformers. The results suggest
that arrays of microphones spread across the body are more effective than those
confined to a single device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02097</identifier>
 <datestamp>2019-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02097</id><created>2019-03-05</created><authors><author><keyname>Ryu</keyname><forenames>Donghun</forenames></author><author><keyname>Jo</keyname><forenames>Youngju</forenames></author><author><keyname>Yoo</keyname><forenames>Jihyeong</forenames></author><author><keyname>Chang</keyname><forenames>Taean</forenames></author><author><keyname>Ahn</keyname><forenames>Daewoong</forenames></author><author><keyname>Kim</keyname><forenames>Young Seo</forenames></author><author><keyname>Kim</keyname><forenames>Geon</forenames></author><author><keyname>Min</keyname><forenames>Hyun-seok</forenames></author><author><keyname>Park</keyname><forenames>Yongkeun</forenames></author></authors><title>Deep learning-enabled image quality control in tomographic
  reconstruction: Robust optical diffraction tomography</title><categories>eess.IV physics.optics</categories><journal-ref>Scientific Reports volume 9, Article number: 15239 (2019)</journal-ref><doi>10.1038/s41598-019-51363-x</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In tomographic reconstruction, the image quality of the reconstructed images
can be significantly degraded by defects in the measured two-dimensional (2D)
raw image data. Despite the importance of screening defective 2D images for
robust tomographic reconstruction, manual inspection and rule-based automation
suffer from low-throughput and insufficient accuracy, respectively. Here, we
present deep learning-enabled quality control for holographic data to produce
robust and high-throughput optical diffraction tomography (ODT). The key idea
is to distill the knowledge of an expert into a deep convolutional neural
network. We built an extensive database of optical field images with
clean/noisy annotations, and then trained a binary classification network based
upon the data. The trained network outperformed visual inspection by non-expert
users and a widely used rule-based algorithm, with &gt; 90% test accuracy.
Subsequently, we confirmed that the superior screening performance
significantly improved the tomogram quality. To further confirm the trained
model's performance and generalizability, we evaluated it on unseen biological
cell data obtained with a setup that was not used to generate the training
dataset. Lastly, we interpreted the trained model using various visualization
techniques that provided the saliency map underlying each model inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02108</identifier>
 <datestamp>2019-06-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02108</id><created>2019-03-05</created><authors><author><keyname>Mousavi</keyname><forenames>Sajad</forenames></author><author><keyname>Afghah</keyname><forenames>Fatemeh</forenames></author><author><keyname>Acharya</keyname><forenames>U. Rajendra</forenames></author></authors><title>SleepEEGNet: Automated Sleep Stage Scoring with Sequence to Sequence
  Deep Learning Approach</title><categories>eess.SP cs.LG q-bio.QM</categories><doi>10.1371/journal.pone.0216456</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electroencephalogram (EEG) is a common base signal used to monitor brain
activity and diagnose sleep disorders. Manual sleep stage scoring is a
time-consuming task for sleep experts and is limited by inter-rater
reliability. In this paper, we propose an automatic sleep stage annotation
method called SleepEEGNet using a single-channel EEG signal. The SleepEEGNet is
composed of deep convolutional neural networks (CNNs) to extract time-invariant
features, frequency information, and a sequence to sequence model to capture
the complex and long short-term context dependencies between sleep epochs and
scores. In addition, to reduce the effect of the class imbalance problem
presented in the available sleep datasets, we applied novel loss functions to
have an equal misclassified error for each sleep stage while training the
network. We evaluated the proposed method on different single-EEG channels
(i.e., Fpz-Cz and Pz-Oz EEG channels) from the Physionet Sleep-EDF datasets
published in 2013 and 2018. The evaluation results demonstrate that the
proposed method achieved the best annotation performance compared to current
literature, with an overall accuracy of 84.26%, a macro F1-score of 79.66% and
Cohen's Kappa coefficient = 0.79. Our developed model is ready to test with
more sleep EEG signals and aid the sleep specialists to arrive at an accurate
diagnosis. The source code is available at
https://github.com/SajadMo/SleepEEGNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02122</identifier>
 <datestamp>2019-12-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02122</id><created>2019-03-05</created><authors><author><keyname>Lyu</keyname><forenames>Yecheng</forenames></author><author><keyname>Bai</keyname><forenames>Lin</forenames></author><author><keyname>Elhousni</keyname><forenames>Mahdi</forenames></author><author><keyname>Huang</keyname><forenames>Xinming</forenames></author></authors><title>An Interactive LiDAR to Camera Calibration</title><categories>eess.IV</categories><comments>Submitted to IV 2019</comments><doi>10.1109/HPEC.2019.8916441</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in the automated driving system (ADS) and advanced driver
assistant system (ADAS) has shown that the combined use of 3D light detection
and ranging (LiDAR) and the camera is essential for an intelligent vehicle to
perceive and understand its surroundings. LiDAR-camera fusion requires precise
intrinsic and extrinsic calibrations between the sensors. However, due to the
limitation of the calibration equipment and susceptibility to noise, algorithms
in existing methods tend to fail in finding LiDAR-camera correspondences in
long-range. In this paper, we introduced an interactive LiDAR to camera
calibration toolbox to estimate the intrinsic and extrinsic transforms. This
toolbox automatically detects the corner of a planer board from a sequence of
LiDAR frames and provides a convenient user interface for annotating the
corresponding pixels on camera frames. Since the toolbox only detects the top
corner of the board, there is no need to prepare a precise polygon planar board
or a checkerboard with different reflectivity areas as in the existing methods.
Furthermore, the toolbox uses genetic algorithms to estimate the transforms and
supports multiple camera models such as the pinhole camera model and the
fisheye camera model. Experiments using Velodyne VLP-16 LiDAR and Point Grey
Chameleon 3 camera show robust results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02184</identifier>
 <datestamp>2019-03-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02184</id><created>2019-03-06</created><authors><author><keyname>Park</keyname><forenames>Sangwoo</forenames></author><author><keyname>Jang</keyname><forenames>Hyeryung</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames></author></authors><title>Learning How to Demodulate from Few Pilots via Meta-Learning</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an Internet-of-Things (IoT) scenario in which devices transmit
sporadically using short packets with few pilot symbols. Each device transmits
over a fading channel and is characterized by an amplifier with a unique
non-linear transfer function. The number of pilots is generally insufficient to
obtain an accurate estimate of the end-to-end channel, which includes the
effects of fading and of the amplifier's distortion. This paper proposes to
tackle this problem using meta-learning. Accordingly, pilots from previous IoT
transmissions are used as meta-training in order to learn a demodulator that is
able to quickly adapt to new end-to-end channel conditions from few pilots.
Numerical results validate the advantages of the approach as compared to
training schemes that either do not leverage prior transmissions or apply a
standard learning algorithm on previously received data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02189</identifier>
 <datestamp>2019-03-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02189</id><created>2019-03-06</created><authors><author><keyname>Chowdhury</keyname><forenames>Dhiman</forenames></author><author><keyname>Miah</keyname><forenames>Mohammad Sharif</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Feroz</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Mostafijur</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Marzan</forenames></author><author><keyname>Sheikh</keyname><forenames>Md. Nazim Uddin</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Mehedi</forenames></author><author><keyname>Sarker</keyname><forenames>Uzzal</forenames></author><author><keyname>Hasan</keyname><forenames>Abu Shahir Md. Khalid</forenames></author></authors><title>Grid-Connected Emergency Back-Up Power Supply</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper documents a design and modelling of a grid-connected emergency
back-up power supply for medium power applications. There are a rectifier-link
boost derived battery charging circuit and a 4-switch push-pull power inverter
circuit which are controlled by pulse width modulation (PWM) signals. This
paper presents a state averaging model and Laplace domain transfer function of
the charging circuit and a switching converter model of the power inverter
circuit. A changeover relay based transfer switch controls the power flow
towards the utility loads. During off-grid situations, loads are fed power by
the proposed inverter circuit and during on-grid situations, battery is charged
by an ac-link rectifier-fed boost converter. There is a relay switching circuit
to control the charging phenomenon of the battery. The proposed design has been
simulated in PLECS and the simulation results corroborate the reliability of
the presented framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02191</identifier>
 <datestamp>2020-01-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02191</id><created>2019-03-06</created><updated>2020-01-30</updated><authors><author><keyname>Dutreix</keyname><forenames>Maxence</forenames></author><author><keyname>Coogan</keyname><forenames>Samuel</forenames></author></authors><title>Specification-Guided Verification and Abstraction Refinement of Mixed
  Monotone Stochastic Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of verifying discrete-time stochastic
systems against omega-regular specifications using finite-state abstractions.
Omega-regular properties allow specifying complex behavior and encompass, for
example, linear temporal logic. We focus on a class of systems with mixed
monotone dynamics. This class has recently been show to be amenable to
efficient reachable set computation and models a wide-range of physically
relevant systems. In general, finite-state abstractions of continuous state
stochastic systems give rise to augmented Markov Chains wherein the
probabilities of transition between states are restricted to an interval. We
present a procedure to compute a finite-state Interval-valued Markov Chain
abstraction of discrete-time, mixed monotone stochastic systems subject to
affine disturbances given a rectangular partition of the state-space. Then, we
suggest an algorithm for performing verification against omega-regular
properties in IMCs. Specifically, we aim to compute bounds on the probability
of satisfying the specification of interest from any initial state in the IMC.
This is achieved by solving a reachability problem on sets of so-called winning
and losing components in the Cartesian product between the IMC and a Rabin
automaton representing the specification. Next, the verification of IMCs may
yield a set of states whose acceptance status is undecided with respect to the
specification, requiring a refinement of the abstraction. We describe a
specification-guided approach that compares the best-case and worst-case
behaviors of accepting paths in the IMC and targets the appropriate states
accordingly. Finally, we show a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02225</identifier>
 <datestamp>2019-03-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02225</id><created>2019-03-06</created><authors><author><keyname>Ngxande</keyname><forenames>Mkhuseli</forenames></author><author><keyname>Tapamo</keyname><forenames>Jules-Raymond</forenames></author><author><keyname>Burke</keyname><forenames>Michael</forenames></author></authors><title>DepthwiseGANs: Fast Training Generative Adversarial Networks for
  Realistic Image Synthesis</title><categories>cs.CV eess.IV</categories><comments>6 pages, 8 figures, To appear in the Proceedings of Southern African
  Universities Power EngineeringConference/Robotics and Mechatronics/Pattern
  Recognition Association of South Africa(SAUPEC/RobMech/PRASA), January 20-30
  2019, Bloemfotein, South Africa</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent work has shown significant progress in the direction of synthetic data
generation using Generative Adversarial Networks (GANs). GANs have been applied
in many fields of computer vision including text-to-image conversion, domain
transfer, super-resolution, and image-to-video applications. In computer
vision, traditional GANs are based on deep convolutional neural networks.
However, deep convolutional neural networks can require extensive computational
resources because they are based on multiple operations performed by
convolutional layers, which can consist of millions of trainable parameters.
Training a GAN model can be difficult and it takes a significant amount of time
to reach an equilibrium point. In this paper, we investigate the use of
depthwise separable convolutions to reduce training time while maintaining data
generation performance. Our results show that a DepthwiseGAN architecture can
generate realistic images in shorter training periods when compared to a
StarGan architecture, but that model capacity still plays a significant role in
generative modelling. In addition, we show that depthwise separable
convolutions perform best when only applied to the generator. For quality
evaluation of generated images, we use the Fr\'echet Inception Distance (FID),
which compares the similarity between the generated image distribution and that
of the training dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02268</identifier>
 <datestamp>2019-03-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02268</id><created>2019-03-06</created><authors><author><keyname>Jiang</keyname><forenames>Yihan</forenames></author><author><keyname>Kim</keyname><forenames>Hyeji</forenames></author><author><keyname>Asnani</keyname><forenames>Himanshu</forenames></author><author><keyname>Kannan</keyname><forenames>Sreeram</forenames></author></authors><title>MIND: Model Independent Neural Decoder</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard decoding approaches rely on model-based channel estimation methods
to compensate for varying channel effects, which degrade in performance
whenever there is a model mismatch. Recently proposed Deep learning based
neural decoders address this problem by leveraging a model-free approach via
gradient-based training. However, they require large amounts of data to retrain
to achieve the desired adaptivity, which becomes intractable in practical
systems.
  In this paper, we propose a new decoder: Model Independent Neural Decoder
(MIND), which builds on the top of neural decoders and equips them with a fast
adaptation capability to varying channels. This feature is achieved via the
methodology of Model-Agnostic Meta-Learning (MAML). Here the decoder: (a)
learns a &quot;good&quot; parameter initialization in the meta-training stage where the
model is exposed to a set of archetypal channels and (b) updates the parameter
with respect to the observed channel in the meta-testing phase using minimal
adaptation data and pilot bits. Building on top of existing state-of-the-art
neural Convolutional and Turbo decoders, MIND outperforms the static benchmarks
by a large margin and shows minimal performance gap when compared to the neural
(Convolutional or Turbo) decoders designed for that particular channel. In
addition, MIND also shows strong learning capability for channels not exposed
during the meta training phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02294</identifier>
 <datestamp>2019-03-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02294</id><created>2019-03-06</created><authors><author><keyname>Paris</keyname><forenames>Antoine</forenames></author><author><keyname>Mirghasemi</keyname><forenames>Hamed</forenames></author><author><keyname>Stupia</keyname><forenames>Ivan</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Energy-Efficient Edge-Facilitated Wireless Collaborative Computing using
  Map-Reduce</title><categories>eess.SP cs.DC cs.IT math.IT</categories><comments>5 pages, 5 figures, submitted to SPAWC19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a heterogeneous set of wireless devices sharing a common access
point collaborates to perform a set of tasks. Using the Map-Reduce distributed
computing framework, the tasks are optimally distributed amongst the nodes with
the objective of minimizing the total energy consumption of the nodes while
satisfying a latency constraint. The derived optimal collaborative-computing
scheme takes into account both the computing capabilities of the nodes and the
strength of their communication links. Numerical simulations illustrate the
benefits of the proposed optimal collaborative-computing scheme over a blind
collaborative-computing scheme and the non-collaborative scenario, both in term
of energy savings and achievable latency. The proposed optimal scheme also
exhibits the interesting feature of allowing to trade energy for latency, and
vice versa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02295</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02295</id><created>2019-03-06</created><updated>2019-04-24</updated><authors><author><keyname>Jiang</keyname><forenames>Yihan</forenames></author><author><keyname>Kim</keyname><forenames>Hyeji</forenames></author><author><keyname>Asnani</keyname><forenames>Himanshu</forenames></author><author><keyname>Kannan</keyname><forenames>Sreeram</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>DeepTurbo: Deep Turbo Decoder</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Present-day communication systems routinely use codes that approach the
channel capacity when coupled with a computationally efficient decoder.
However, the decoder is typically designed for the Gaussian noise channel and
is known to be sub-optimal for non-Gaussian noise distribution. Deep learning
methods offer a new approach for designing decoders that can be trained and
tailored for arbitrary channel statistics. We focus on Turbo codes and propose
DeepTurbo, a novel deep learning based architecture for Turbo decoding.
  The standard Turbo decoder (Turbo) iteratively applies the
Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm with an interleaver in the middle. A
neural architecture for Turbo decoding termed (NeuralBCJR), was proposed
recently. There, the key idea is to create a module that imitates the BCJR
algorithm using supervised learning, and to use the interleaver architecture
along with this module, which is then fine-tuned using end-to-end training.
However, knowledge of the BCJR algorithm is required to design such an
architecture, which also constrains the resulting learned decoder. Here we
remedy this requirement and propose a fully end-to-end trained neural decoder -
Deep Turbo Decoder (DeepTurbo). With novel learnable decoder structure and
training methodology, DeepTurbo reveals superior performance under both AWGN
and non-AWGN settings as compared to the other two decoders - Turbo and
NeuralBCJR. Furthermore, among all the three, DeepTurbo exhibits the lowest
error floor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02424</identifier>
 <datestamp>2019-03-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02424</id><created>2019-03-06</created><authors><author><keyname>Chen</keyname><forenames>Haoshuo</forenames></author><author><keyname>Fontaine</keyname><forenames>Nicolas K.</forenames></author><author><keyname>Gen&#xe9;</keyname><forenames>Joan M.</forenames></author><author><keyname>Ryf</keyname><forenames>Roland</forenames></author><author><keyname>Neilson</keyname><forenames>David T.</forenames></author><author><keyname>Raybon</keyname><forenames>Gregory</forenames></author></authors><title>Full-Field, Carrier-Less, Polarization-Diversity, Direct Detection
  Receiver based on Phase Retrieval</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We realize dual-polarization full-field recovery using intensity only
measurements and phase retrieval techniques based on dispersive elements.
30-Gbaud QPSK waveforms are transmitted over 520-km standard single-mode fiber
and equalized from the receiver outputs using 2X2 MIMO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02506</identifier>
 <datestamp>2019-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02506</id><created>2019-03-06</created><updated>2019-09-29</updated><authors><author><keyname>Semrau</keyname><forenames>Daniel</forenames></author><author><keyname>Sillekens</keyname><forenames>Eric</forenames></author><author><keyname>Killey</keyname><forenames>Robert I.</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author></authors><title>A Modulation Format Correction Formula for the Gaussian Noise Model in
  the Presence of Inter-Channel Stimulated Raman Scattering</title><categories>eess.SP</categories><comments>Version 3: Typos have been corrected in Eq. (4), (15) and (16)</comments><doi>10.1109/JLT.2019.2929461</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A closed-form formula is derived, which corrects for the modulation format
dependence of the Gaussian Noise (GN) model in the presence of inter-channel
stimulated Raman scattering (ISRS). The analytical result enables a rapid
estimate of the nonlinear interference (NLI) for arbitrary modulation formats
and avoids the need for complex integral evaluations and split-step
simulations. It is shown that the modulation format dependent NLI can be
approximated by two contributions, one originating from a single span and one
asymptotic contribution for a large number of spans. The asymptotic
contribution is solved in closed-form for an arbitrary link function, making
the result applicable for generic fiber systems using lumped, distributed or
hybrid amplification schemes. The methodology is applied to the ISRS GN model
and a modulation format correction formula in closed-form is derived which
accounts for an arbitrary number of spans, inter-channel stimulated Raman
scattering, arbitrary launch power distributions and wavelength dependent
dispersion and attenuation. The proposed formula is validated by numerical
simulations over the entire C+L band for multiple fiber types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02657</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02657</id><created>2019-03-06</created><updated>2019-03-08</updated><authors><author><keyname>Ju</keyname><forenames>Shihao</forenames></author><author><keyname>Shah</keyname><forenames>Syed Hashim Ali</forenames></author><author><keyname>Javed</keyname><forenames>Muhammad Affan</forenames></author><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Palteru</keyname><forenames>Girish</forenames></author><author><keyname>Robin</keyname><forenames>Jyotish</forenames></author><author><keyname>Xing</keyname><forenames>Yunchou</forenames></author><author><keyname>Kanhere</keyname><forenames>Ojas</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>Scattering Mechanisms and Modeling for Terahertz Wireless Communications</title><categories>eess.SP cs.IT math.IT physics.optics</categories><comments>7 pages, 7 figures, ICC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an analysis of radio wave scattering for frequencies
ranging from the microwave to the Terahertz band (e.g., 1 GHz - 1 THz), by
studying the scattering power reradiated from various types of materials with
different surface roughnesses. First, fundamentals of scattering and reflection
are developed and explained for use in wireless mobile radio, and the effect of
scattering on the reflection coefficient for rough surfaces is investigated.
Received power is derived using two popular scattering models - the directive
scattering (DS) model and the radar cross section (RCS) model through
simulations over a wide range of frequencies, materials, and orientations for
the two models, and measurements confirm the accuracy of the DS model at 140
GHz. This paper shows that scattering can become a prominent propagation
mechanism as frequencies extend to millimeter-wave (mmWave) and beyond, but at
other times can be treated like simple reflection. Knowledge of scattering
effects is critical for appropriate and realistic channel models, which further
support the development of massive multiple input-multiple output (MIMO)
techniques, localization, ray tracing tool design, and imaging for future 5G
and 6G wireless systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02658</identifier>
 <datestamp>2019-03-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02658</id><created>2019-03-06</created><authors><author><keyname>Zhang</keyname><forenames>Zhaorong</forenames></author><author><keyname>Fu</keyname><forenames>Minyue</forenames></author></authors><title>On Convergence Rate of the Gaussian Belief Propagation Algorithm for
  Markov Networks</title><categories>stat.ML eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Belief Propagation (BP) algorithm is one of the most important
distributed algorithms in signal processing and statistical learning involving
Markov networks. It is well known that the algorithm correctly computes
marginal density functions from a high dimensional joint density function over
a Markov network in a finite number of iterations when the underlying Gaussian
graph is acyclic. It is also known more recently that the algorithm produces
correct marginal means asymptotically for cyclic Gaussian graphs under the
condition of walk summability. This paper extends this convergence result
further by showing that the convergence is exponential under the walk
summability condition, and provides a simple bound for the convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02664</identifier>
 <datestamp>2019-03-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02664</id><created>2019-03-06</created><authors><author><keyname>Xu</keyname><forenames>Pengfei</forenames></author><author><keyname>Jia</keyname><forenames>Yinjie</forenames></author><author><keyname>Wang</keyname><forenames>Zhijian</forenames></author></authors><title>Blind Source Separation of Optical Wireless Communications in Noisy
  Environments</title><categories>eess.SP</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind source separation is a research hotspot in the field of signal
processing because it aims to separate unknown source signals from observed
mixtures through an unknown transmission channel. A low computational
complexity instantaneous linear mixture signals blind separation algorithm was
introduced and improved. There is only one variable parameter named the length
of moving average in the algorithm, which has a significant impact on the
separation effect. This paper gives some suggestions on the reasonable value
through experiments. The algorithm is extended to the separation of visible
light communication signals in different noise environments, and has achieved
certain results, which further expands the applicability of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02794</identifier>
 <datestamp>2019-03-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02794</id><created>2019-03-07</created><authors><author><keyname>Lee</keyname><forenames>Donmoon</forenames></author><author><keyname>Lee</keyname><forenames>Jaejun</forenames></author><author><keyname>Park</keyname><forenames>Jeongsoo</forenames></author><author><keyname>Lee</keyname><forenames>Kyogu</forenames></author></authors><title>Enhancing Music Features by Knowledge Transfer from User-item Log Data</title><categories>cs.SD eess.AS</categories><comments>5 pages, 4 figures, and 1 table. Accepted paper at the International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel method that exploits music listening log
data for general-purpose music feature extraction. Despite the wealth of
information available in the log data of user-item interactions, it has been
mostly used for collaborative filtering to find similar items or users and was
not fully investigated for content-based music applications. We resolve this
problem by extending intra-domain knowledge distillation to cross-domain: i.e.,
by transferring knowledge obtained from the user-item domain to the music
content domain. The proposed system first trains the model that estimates log
information from the audio contents; then it uses the model to improve other
task-specific models. The experiments on various music classification and
regression tasks show that the proposed method successfully improves the
performances of the task-specific models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02844</identifier>
 <datestamp>2019-03-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02844</id><created>2019-03-07</created><authors><author><keyname>Drugman</keyname><forenames>Thomas</forenames></author><author><keyname>Stylianou</keyname><forenames>Yannis</forenames></author><author><keyname>Kida</keyname><forenames>Yusuke</forenames></author><author><keyname>Akamine</keyname><forenames>Masami</forenames></author></authors><title>Voice Activity Detection: Merging Source and Filter-based Information</title><categories>cs.SD eess.AS</categories><journal-ref>IEEE Signal Processing Letters, Volume 23, Issue 2, pp. 252-256,
  2015</journal-ref><doi>10.1109/LSP.2015.2495219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice Activity Detection (VAD) refers to the problem of distinguishing speech
segments from background noise. Numerous approaches have been proposed for this
purpose. Some are based on features derived from the power spectral density,
others exploit the periodicity of the signal. The goal of this paper is to
investigate the joint use of source and filter-based features. Interestingly, a
mutual information-based assessment shows superior discrimination power for the
source-related features, especially the proposed ones. The features are further
the input of an artificial neural network-based classifier trained on a
multi-condition database. Two strategies are proposed to merge source and
filter information: feature and decision fusion. Our experiments indicate an
absolute reduction of 3% of the equal error rate when using decision fusion.
The final proposed system is compared to four state-of-the-art methods on 150
minutes of data recorded in real environments. Thanks to the robustness of its
source-related features, its multi-condition training and its efficient
information fusion, the proposed system yields over the best state-of-the-art
VAD a substantial increase of accuracy across all conditions (24% absolute on
average).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.02975</identifier>
 <datestamp>2019-03-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.02975</id><created>2019-03-07</created><authors><author><keyname>Haigh</keyname><forenames>Paul Anthony</forenames></author><author><keyname>Darwazeh</keyname><forenames>Izzat</forenames></author></authors><title>Real-Time Experimental Demonstration of Multi-band CAP Modulation in a
  VLC System with Off-the-Shelf LEDs</title><categories>eess.SP cs.NI</categories><comments>2 pages, 4 figures, IEEE INFOCOM Demonstrations</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We demonstrate, for the first time, m-CAP modulation using off-the-shelf LEDs
in a VLC in real time experimental setup using field programmable gate arrays
based in universal software radio peripherals (USRPs). We demonstrate
transmission speeds up to ~30 Mb/s can be achieved, which supports high
definition television streaming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03032</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03032</id><created>2019-03-07</created><updated>2019-07-13</updated><authors><author><keyname>Ouyang</keyname><forenames>Chongjun</forenames></author><author><keyname>Ou</keyname><forenames>Zeliang</forenames></author><author><keyname>Yang</keyname><forenames>Pei</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Yang</keyname><forenames>Hongwen</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>Performance of Multiple Antennas Selection in SIMO Systems with
  BPSK/QPSK Modulations</title><categories>eess.SP</categories><comments>This article has been rejected by an IEEE conference. And I hope to
  polish this paper. Because of this, I want to withdrawl it</comments><msc-class>00-01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performance of single-input multiple-output (SIMO)
systems under receive antenna selection (RAS) and BPSK/QPSK modulations. At the
receiver, a subset of branches are selected and combined using maximal-ratio
combining (MRC) to maximize the instantaneous Signal to Noise Ratio (SNR). By
assuming independent and identical distributed (i.i.d.) Rayleigh flat fading, a
closed-form expression, with considerably high precision, is developed to
approximate the average input-output mutual information, also termed as
symmetric capacity, of the whole system. Later, this approximated expression is
further utilized to investigate the efficient capacity and energy efficiency of
the SIMO system under BPSK/QPSK modulations and RAS. Besides analytical
derivations, simulations are provided to demonstrate the approximation
precision, feasibility and validity of the derived results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03059</identifier>
 <datestamp>2019-03-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03059</id><created>2019-01-30</created><authors><author><keyname>Beberoglu</keyname><forenames>Ersin</forenames></author><author><keyname>Tokmakci</keyname><forenames>M.</forenames></author><author><keyname>Ozdemir</keyname><forenames>A T.</forenames></author></authors><title>Is kazalarini onleyebilmek icin bakim/onarim personelinin
  kullanabilecegi bir giyilebilir emniyet sisteminin tasarlanmasi</title><categories>eess.SP</categories><comments>6th International GAP Engineering Conference</comments><report-no>6th International GAP Engineering Conference, pp 73-78,
  Sanliurfa/TURKEY</report-no><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study aims to develop a wearable device that collect health data from
maintenance personnel and environmental conditions data in order to ensure the
safety of the staff in industrial work areas where have different levels of
risk categories. This device can stop machine immediately according to the
health parameters that collected from the maintenance personnel to prevent work
accidents and also with measuring stress level of this personnel, system make
prediction of suitability of personnel for maintenance operation according to
degree of risk level of machine. System consists from four parts. Wearable
unit, smartphone, server computer and machine mounted unit. The wearable device
can measure personnel; heart rate, oxygen saturation, body temperature, skin
resistance; and also collect data from environment; heat, light, humidity, CO2.
The results which obtained by decision support system algorithm can stop
machine immediately or predict the suitability of corresponding operator for
this maintenance operation. This application is an important personal safety
system and can prevent work accidents that may occur during maintenance or
repair operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03060</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03060</id><created>2019-02-27</created><updated>2019-04-25</updated><authors><author><keyname>Tang</keyname><forenames>Wankai</forenames></author><author><keyname>Dai</keyname><forenames>Jun Yan</forenames></author><author><keyname>Chen</keyname><forenames>Mingzheng</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Cheng</keyname><forenames>Qiang</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Cui</keyname><forenames>Tie Jun</forenames></author></authors><title>Programmable Metasurface-based RF Chain-free 8PSK Wireless Transmitter</title><categories>eess.SP</categories><comments>3 pages, 9 figures, 1 table, published in Electronics Letters</comments><journal-ref>IET Electronics Letters, vol. 55, no. 7, pp. 417-420, Apr. 2019</journal-ref><doi>10.1049/el.2019.0400</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, a wireless transmitter using the new architecture of
programmable metasurface is presented. The proposed transmitter does not
require any filter, nor wideband mixer or wideband power amplifier, thereby
making it a promising hardware architecture for cost-effective wireless
communications systems in the future. Using experimental results, we
demonstrate that a programmable metasurface-based 8-phase shift-keying (8PSK)
transmitter with 8*32 phase adjustable unit cells can achieve 6.144 Mbps data
rate over the air at 4.25 GHz with a comparable bit error rate (BER)
performance as the conventional approach without channel coding, but with less
hardware complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03063</identifier>
 <datestamp>2019-03-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03063</id><created>2019-03-01</created><authors><author><keyname>Clazzer</keyname><forenames>Federico</forenames></author><author><keyname>Munari</keyname><forenames>Andrea</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author><author><keyname>Lazaro</keyname><forenames>Francisco</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>From 5G to 6G: Has the Time for Modern Random Access Come?</title><categories>eess.SP</categories><comments>2 pages, 1 figure, presented at 6G Summit, Levi, Finland, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper proposes the use of modern random access for IoT
applications in 6G. A short overview of recent advances in uncoordinated medium
access is provided, highlighting the gains that can be achieved by leveraging
smart protocol design intertwined with advanced signal processing techniques at
the receiver. The authors' vision on the benefits such schemes can yield for
beyond-5G systems is presented, with the aim to trigger further discussion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03065</identifier>
 <datestamp>2019-03-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03065</id><created>2019-03-07</created><authors><author><keyname>Mehrizi</keyname><forenames>Sajad</forenames></author><author><keyname>Tsakmalis</keyname><forenames>Anestis</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>A Bayesian Poisson-Gaussian Process Model for Popularity Learning in
  Edge-Caching Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge-caching is recognized as an efficient technique for future cellular
networks to improve network capacity and user-perceived quality of experience.
To enhance the performance of caching systems, designing an accurate content
request prediction algorithm plays an important role. In this paper, we develop
a flexible model, a Poisson regressor based on a Gaussian process, for the
content request distribution.
  The first important advantage of the proposed model is that it encourages the
already existing or seen contents with similar features to be correlated in the
feature space and therefore it acts as a regularizer for the estimation.
Second, it allows to predict the popularities of newly-added or unseen contents
whose statistical data is not available in advance. In order to learn the model
parameters, which yield the Poisson arrival rates or alternatively the content
\textit{popularities}, we invoke the Bayesian approach which is robust against
over-fitting.
  However, the resulting posterior distribution is analytically intractable to
compute. To tackle this, we apply a Markov Chain Monte Carlo (MCMC) method to
approximate this distribution which is also asymptotically exact. Nevertheless,
the MCMC is computationally demanding especially when the number of contents is
large. Thus, we employ the Variational Bayes (VB) method as an alternative low
complexity solution. More specifically, the VB method addresses the
approximation of the posterior distribution through an optimization problem.
Subsequently, we present a fast block-coordinate descent algorithm to solve
this optimization problem. Finally, extensive simulation results both on
synthetic and real-world datasets are provided to show the accuracy of our
prediction algorithm and the cache hit ratio (CHR) gain compared to existing
methods from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03085</identifier>
 <datestamp>2019-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03085</id><created>2019-03-07</created><updated>2019-10-07</updated><authors><author><keyname>De Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Ali</keyname><forenames>Anum</forenames></author><author><keyname>Amiri</keyname><forenames>Abolfazl</forenames></author><author><keyname>Angjelichinoski</keyname><forenames>Marko</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Non-Stationarities in Extra-Large Scale Massive MIMO</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO, a key technology for increasing area spectral efficiency in
cellular systems, was developed assuming moderately sized apertures. In this
paper, we argue that massive MIMO systems behave differently in large-scale
regimes due to spatial non-stationarity. In the large-scale regime, with arrays
of around fifty wavelengths, the terminals see the whole array but
non-stationarities occur because different regions of the array see different
propagation paths. At even larger dimensions, which we call the extra-large
scale regime, terminals see a portion of the array and inside the first type of
non-stationarities might occur. We show that the non-stationarity properties of
the massive MIMO channel changes several important MIMO design aspects. In
simulations, we demonstrate how non-stationarity is a curse when neglected but
a blessing when embraced in terms of computational load and multi-user
transceiver design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03105</identifier>
 <datestamp>2019-05-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03105</id><created>2019-03-06</created><authors><author><keyname>Shen</keyname><forenames>Hongyu</forenames></author><author><keyname>George</keyname><forenames>Daniel</forenames></author><author><keyname>Huerta</keyname><forenames>E. A.</forenames></author><author><keyname>Zhao</keyname><forenames>Zhizhen</forenames></author></authors><title>Denoising Gravitational Waves with Enhanced Deep Recurrent Denoising
  Auto-Encoders</title><categories>astro-ph.CO astro-ph.IM cs.LG eess.SP gr-qc</categories><comments>5 pages, 11 figures and 3 tables, accepted to ICASSP 2019</comments><msc-class>97R40</msc-class><acm-class>I.2</acm-class><journal-ref>ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</journal-ref><doi>10.1109/ICASSP.2019.8683061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denoising of time domain data is a crucial task for many applications such as
communication, translation, virtual assistants etc. For this task, a
combination of a recurrent neural net (RNNs) with a Denoising Auto-Encoder
(DAEs) has shown promising results. However, this combined model is challenged
when operating with low signal-to-noise ratio (SNR) data embedded in
non-Gaussian and non-stationary noise. To address this issue, we design a novel
model, referred to as 'Enhanced Deep Recurrent Denoising Auto-Encoder'
(EDRDAE), that incorporates a signal amplifier layer, and applies curriculum
learning by first denoising high SNR signals, before gradually decreasing the
SNR until the signals become noise dominated. We showcase the performance of
EDRDAE using time-series data that describes gravitational waves embedded in
very noisy backgrounds. In addition, we show that EDRDAE can accurately denoise
signals whose topology is significantly more complex than those used for
training, demonstrating that our model generalizes to new classes of
gravitational waves that are beyond the scope of established denoising
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03107</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03107</id><created>2019-03-07</created><updated>2019-04-02</updated><authors><author><keyname>Choi</keyname><forenames>Hyeong-Seok</forenames></author><author><keyname>Kim</keyname><forenames>Jang-Hyun</forenames></author><author><keyname>Huh</keyname><forenames>Jaesung</forenames></author><author><keyname>Kim</keyname><forenames>Adrian</forenames></author><author><keyname>Ha</keyname><forenames>Jung-Woo</forenames></author><author><keyname>Lee</keyname><forenames>Kyogu</forenames></author></authors><title>Phase-aware Speech Enhancement with Deep Complex U-Net</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Significant error was found in data processing step, therefore will
  be retracted from International Conference on Learning Representations (ICLR)
  2019. It is not recommended to read current version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most deep learning-based models for speech enhancement have mainly focused on
estimating the magnitude of spectrogram while reusing the phase from noisy
speech for reconstruction. This is due to the difficulty of estimating the
phase of clean speech. To improve speech enhancement performance, we tackle the
phase estimation problem in three ways. First, we propose Deep Complex U-Net,
an advanced U-Net structured model incorporating well-defined complex-valued
building blocks to deal with complex-valued spectrograms. Second, we propose a
polar coordinate-wise complex-valued masking method to reflect the distribution
of complex ideal ratio masks. Third, we define a novel loss function, weighted
source-to-distortion ratio (wSDR) loss, which is designed to directly correlate
with a quantitative evaluation measure. Our model was evaluated on a mixture of
the Voice Bank corpus and DEMAND database, which has been widely used by many
deep learning models for speech enhancement. Ablation experiments were
conducted on the mixed dataset showing that all three proposed approaches are
empirically valid. Experimental results show that the proposed method achieves
state-of-the-art performance in all metrics, outperforming previous approaches
by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03133</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03133</id><created>2019-03-07</created><updated>2019-04-08</updated><authors><author><keyname>Viswanath</keyname><forenames>Sanjay</forenames></author><author><keyname>de Beco</keyname><forenames>Simon</forenames></author><author><keyname>Dahan</keyname><forenames>Maxime</forenames></author><author><keyname>Arigovindan</keyname><forenames>Muthuvel</forenames></author></authors><title>Image Restoration by Combined Order Regularization with Optimal Spatial
  Adaptation</title><categories>eess.IV</categories><comments>17 pages, Journal Draft 19 pages, 9 figures, 2 tables, Updated
  journal draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Total Variation (TV) and related extensions have been popular in image
restoration due to their robust performance and wide applicability. While the
original formulation is still relevant after two decades of extensive research,
its extensions that combine derivatives of first- and second-order are now
being explored for better performance, with examples being Combined Order TV
(COTV) and Total Generalized Variation (TGV). As an improvement over such
multi-order convex formulations, we propose a novel non-convex regularization
functional which adaptively combines Hessian-Schatten (HS) norm and first order
TV (TV1) functionals with spatially varying weight. This adaptive weight itself
is controlled by another regularization term; the total cost becomes the sum of
this adaptively weighted HS-TV1 term, the regularization term for the adaptive
weight, and the data-fitting term. The reconstruction is obtained by jointly
minimizing w.r.t. the required image and the adaptive weight. We construct a
block coordinate descent method for this minimization with proof of
convergence, which alternates between minimization w.r.t. the required image
and the adaptive weights. We derive exact computational formula for
minimization w.r.t. the adaptive weight, and construct an ADMM algorithm for
minimization w.r.t. to the required image. We compare the proposed method using
image recovery examples including MRI reconstruction and microscopy
deconvolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03141</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03141</id><created>2019-03-07</created><updated>2019-06-18</updated><authors><author><keyname>Haldar</keyname><forenames>Justin P.</forenames></author><author><keyname>Setsompop</keyname><forenames>Kawin</forenames></author></authors><title>Linear Predictability in MRI Reconstruction: Leveraging Shift-Invariant
  Fourier Structure for Faster and Better Imaging</title><categories>eess.SP</categories><comments>Submitted to IEEE Signal Processing Magazine, Special Issue on
  Computational MRI: Compressed Sensing and Beyond</comments><doi>10.1109/MSP.2019.2949570</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past several decades, many different types of computational imaging
approaches have been proposed for improving MRI. In this paper, we provide an
overview of methods that assume that MRI Fourier data is linearly predictable.
Linear prediction is well known in signal processing and may be most
recognizable for its usefulness in speech processing and spectrum estimation
applications. In MRI, linear predictability implies that data can be sampled
below the conventional Nyquist rate, since unmeasured data may be imputed as a
shift-invariant linear combination of measured samples. Linear predictive
methods include some of the earliest methods in the computational MRI
reconstruction field, some of the most widely utilized computational MRI
methods in modern clinical practice, and some of the most flexible and
versatile modern computational imaging approaches that are enabling
unprecedented new styles of data acquisition. In addition, the concept of
linear predictability can be used to unify a number of more classical MRI
reconstruction constraints, but without needing to make the strong assumptions
of classical constrained reconstruction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03175</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03175</id><created>2019-03-07</created><authors><author><keyname>Bensenouci</keyname><forenames>A.</forenames></author><author><keyname>Ghany</keyname><forenames>A. M. Abdel</forenames></author></authors><title>Optimal discrete-time output feedback control for multi-area load
  frequency control using evolutionary programming</title><categories>cs.SY eess.SP</categories><comments>IEEE Conference paper, scopus</comments><journal-ref>The IEEE International Symposium on Industrial Electronics
  (ISIE2005) Dubrovnik, Croatia, June 20-23, 2005</journal-ref><doi>10.1109/ISIE.2005.1528894</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interconnected power system presents a great challenge to both system
analyzers and control designers. The load-frequency control (LFC) problem has
gained much importance because of the complexity and size of modern
interconnected power systems. In this work, the original (full) system is
decomposed into subsystems using the overlapping decentralization technique. A
discrete-time output feedback control is then designed using Evolutionary
Programming (EP) technique. EP is selected since it is a good candidate for a
global search for the optimum of a cost function that leads to the optimum
output feedback controller gains in order to achieve the LFC requirements and
improve its performance. The system performance is analyzed through simulating
different disturbances and parameter variations over a wide range. Results from
Dynamic Programming technique are also presented for completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03195</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03195</id><created>2019-03-07</created><updated>2019-03-26</updated><authors><author><keyname>Mydlarz</keyname><forenames>Charlie</forenames></author><author><keyname>Sharma</keyname><forenames>Mohit</forenames></author><author><keyname>Lockerman</keyname><forenames>Yitzchak</forenames></author><author><keyname>Steers</keyname><forenames>Ben</forenames></author><author><keyname>Silva</keyname><forenames>Claudio</forenames></author><author><keyname>Bello</keyname><forenames>Juan Pablo</forenames></author></authors><title>The life of a New York City noise sensor network</title><categories>cs.SD eess.AS</categories><comments>This article belongs to the Section Intelligent Sensors, 24 pages, 15
  figures, 3 tables, 45 references</comments><journal-ref>Mydlarz, C.; Sharma, M.; Lockerman, Y.; Steers, B.; Silva, C.;
  Bello, J.P. The Life of a New York City Noise Sensor Network. Sensors 2019,
  19, 1415</journal-ref><doi>10.3390/s19061415</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Noise pollution is one of the topmost quality of life issues for urban
residents in the United States. Continued exposure to high levels of noise has
proven effects on health, including acute effects such as sleep disruption, and
long-term effects such as hypertension, heart disease, and hearing loss. To
investigate and ultimately aid in the mitigation of urban noise, a network of
55 sensor nodes has been deployed across New York City for over two years,
collecting sound pressure level (SPL) and audio data. This network has
cumulatively amassed over 75 years of calibrated, high-resolution SPL
measurements and 35 years of audio data. In addition, high frequency telemetry
data has been collected that provides an indication of a sensors' health. This
telemetry data was analyzed over an 18 month period across 31 of the sensors.
It has been used to develop a prototype model for pre-failure detection which
has the ability to identify sensors in a prefail state 69.1% of the time. The
entire network infrastructure is outlined, including the operation of the
sensors, followed by an analysis of its data yield and the development of the
fault detection approach and the future system integration plans for this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03237</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03237</id><created>2019-03-07</created><authors><author><keyname>Sekiguchi</keyname><forenames>Kouhei</forenames></author><author><keyname>Nugraha</keyname><forenames>Aditya Arie</forenames></author><author><keyname>Bando</keyname><forenames>Yoshiaki</forenames></author><author><keyname>Yoshii</keyname><forenames>Kazuyoshi</forenames></author></authors><title>Fast Multichannel Source Separation Based on Jointly Diagonalizable
  Spatial Covariance Matrices</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a versatile method that accelerates multichannel source
separation methods based on full-rank spatial modeling. A popular approach to
multichannel source separation is to integrate a spatial model with a source
model for estimating the spatial covariance matrices (SCMs) and power spectral
densities (PSDs) of each sound source in the time-frequency domain. One of the
most successful examples of this approach is multichannel nonnegative matrix
factorization (MNMF) based on a full-rank spatial model and a low-rank source
model. MNMF, however, is computationally expensive and often works poorly due
to the difficulty of estimating the unconstrained full-rank SCMs. Instead of
restricting the SCMs to rank-1 matrices with the severe loss of the spatial
modeling ability as in independent low-rank matrix analysis (ILRMA), we
restrict the SCMs of each frequency bin to jointly-diagonalizable but still
full-rank matrices. For such a fast version of MNMF, we propose a
computationally-efficient and convergence-guaranteed algorithm that is similar
in form to that of ILRMA. Similarly, we propose a fast version of a
state-of-the-art speech enhancement method based on a deep speech model and a
low-rank noise model. Experimental results showed that the fast versions of
MNMF and the deep speech enhancement method were several times faster and
performed even better than the original versions of those methods,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03239</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03239</id><created>2019-03-07</created><authors><author><keyname>Chen</keyname><forenames>Yuquan</forenames></author><author><keyname>Wei</keyname><forenames>Yiheng</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>A novel perspective to gradient method: the fractional order approach</title><categories>math.OC eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give some new thoughts about the classical gradient method
(GM) and recall the proposed fractional order gradient method (FOGM). It is
proven that the proposed FOGM holds a super convergence capacity and a faster
convergence rate around the extreme point than the conventional GM. The
property of asymptotic convergence of conventional GM and FOGM is also
discussed. To achieve both a super convergence capability and an even faster
convergence rate, a novel switching FOGM is proposed. Moreover, we extend the
obtained conclusion to a more general case by introducing the concept of
p-order Lipschitz continuous gradient and p-order strong convex. Numerous
simulation examples are provided to validate the effectiveness of proposed
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03247</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03247</id><created>2019-03-07</created><authors><author><keyname>Fujihashi</keyname><forenames>Takuya</forenames></author><author><keyname>Koike-Akino</keyname><forenames>Toshiaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Takashi</forenames></author><author><keyname>Orlik</keyname><forenames>Philip V.</forenames></author></authors><title>HoloCast: Graph Signal Processing for Graceful Point Cloud Delivery</title><categories>cs.MM eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In conventional point cloud delivery, a sender uses octree-based digital
video compression to stream three-dimensional (3D) points and the corresponding
color attributes over band-limited links, e.g., wireless channels, for 3D scene
reconstructions. However, the digital-based delivery schemes have an issue
called cliff effect, where the 3D reconstruction quality is a step function in
terms of wireless channel quality. We propose a novel scheme of point cloud
delivery, called HoloCast, to gracefully improve the reconstruction quality
with the improvement of wireless channel quality. HoloCast regards the 3D
points and color components as graph signals and directly transmits
linear-transformed signals based on graph Fourier transform (GFT), without
digital quantization and entropy coding operations. One of main contributions
in HoloCast is that the use of GFT can deal with non-ordered and non-uniformly
distributed multi-dimensional signals such as holographic data unlike
conventional delivery schemes. Performance results with point cloud data show
that HoloCast yields better 3D reconstruction quality compared to digital-based
methods in noisy wireless environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03269</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03269</id><created>2019-03-07</created><authors><author><keyname>Nugraha</keyname><forenames>Aditya Arie</forenames></author><author><keyname>Sekiguchi</keyname><forenames>Kouhei</forenames></author><author><keyname>Yoshii</keyname><forenames>Kazuyoshi</forenames></author></authors><title>A Deep Generative Model of Speech Complex Spectrograms</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an approach to the joint modeling of the short-time
Fourier transform magnitude and phase spectrograms with a deep generative
model. We assume that the magnitude follows a Gaussian distribution and the
phase follows a von Mises distribution. To improve the consistency of the phase
values in the time-frequency domain, we also apply the von Mises distribution
to the phase derivatives, i.e., the group delay and the instantaneous
frequency. Based on these assumptions, we explore and compare several
combinations of loss functions for training our models. Built upon the
variational autoencoder framework, our model consists of three convolutional
neural networks acting as an encoder, a magnitude decoder, and a phase decoder.
In addition to the latent variables, we propose to also condition the phase
estimation on the estimated magnitude. Evaluated for a time-domain speech
reconstruction task, our models could generate speech with a high perceptual
quality and a high intelligibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03319</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03319</id><created>2019-03-08</created><updated>2019-08-16</updated><authors><author><keyname>Shao</keyname><forenames>Mingjie</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Swindlehurst</keyname><forenames>Lee</forenames></author></authors><title>One-Bit Sigma-Delta MIMO Precoding</title><categories>cs.IT eess.SP math.IT</categories><doi>10.1109/JSTSP.2019.2938687</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coarsely quantized MIMO signalling methods have gained popularity in the
recent developments of massive MIMO as they open up opportunities for massive
MIMO implementation using cheap and power-efficient radio-frequency front-ends.
This paper presents a new one-bit MIMO precoding approach using spatial
Sigma-Delta ($\Sigma\Delta$) modulation. In previous one-bit MIMO precoding
research, one mainly focuses on using optimization to tackle the difficult
binary signal optimization problem that arises from the precoding design. Our
approach attempts a different route. Assuming angular MIMO channels, we apply
$\Sigma\Delta$ modulation---a classical concept in analog-to-digital conversion
of temporal signals---in space. The resulting $\Sigma\Delta$ precoding approach
has two main advantages: First, we no longer need to deal with binary
optimization in $\Sigma\Delta$ precoding design. Particularly, the binary
signal restriction is replaced by peak signal amplitude constraints. Second,
the impact of the quantization error can be well controlled via modulator
design and under appropriate operating conditions. Through symbol error
probability analysis, we reveal that the very large number of antennas in
massive MIMO provides favorable operating conditions for $\Sigma\Delta$
precoding. In addition, we develop a new $\Sigma\Delta$ modulation architecture
that is capable of adapting the channel to achieve nearly zero quantization
error for a targeted user. Furthermore, we consider multi-user $\Sigma\Delta$
precoding using the zero-forcing and symbol-level precoding schemes. These two
$\Sigma\Delta$ precoding schemes perform considerably better than their direct
one-bit quantized counterparts, as simulation results show.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03349</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03349</id><created>2019-03-08</created><authors><author><keyname>Murphy</keyname><forenames>Keelin</forenames></author><author><keyname>Habib</keyname><forenames>Shifa Salman</forenames></author><author><keyname>Zaidi</keyname><forenames>Syed Mohammad Asad</forenames></author><author><keyname>Khowaja</keyname><forenames>Saira</forenames></author><author><keyname>Khan</keyname><forenames>Aamir</forenames></author><author><keyname>Melendez</keyname><forenames>Jaime</forenames></author><author><keyname>Scholten</keyname><forenames>Ernst T.</forenames></author><author><keyname>Amad</keyname><forenames>Farhan</forenames></author><author><keyname>Schalekamp</keyname><forenames>Steven</forenames></author><author><keyname>Verhagen</keyname><forenames>Maurits</forenames></author><author><keyname>Philipsen</keyname><forenames>Rick H. H. M.</forenames></author><author><keyname>Meijers</keyname><forenames>Annet</forenames></author><author><keyname>van Ginneken</keyname><forenames>Bram</forenames></author></authors><title>Computer aided detection of tuberculosis on chest radiographs: An
  evaluation of the CAD4TB v6 system</title><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  There is a growing interest in the automated analysis of chest X-Ray (CXR) as
a sensitive and inexpensive means of screening susceptible populations for
pulmonary tuberculosis. In this work we evaluate the latest version of CAD4TB,
a software platform designed for this purpose. Version 6 of CAD4TB was released
in 2018 and is here tested on an independent dataset of 5565 CXR images with
GeneXpert (Xpert) sputum test results available (854 Xpert positive subjects).
A subset of 500 subjects (50% Xpert positive) was reviewed and annotated by 5
expert observers independently to obtain a radiological reference standard. The
latest version of CAD4TB is found to outperform all previous versions in terms
of area under receiver operating curve (ROC) with respect to both Xpert and
radiological reference standards. Improvements with respect to Xpert are most
apparent at high sensitivity levels with a specificity of 76% obtained at 90%
sensitivity. When compared with the radiological reference standard, CAD4TB v6
also outperformed previous versions by a considerable margin and achieved 98%
specificity at 90% sensitivity. No substantial difference was found between the
performance of CAD4TB v6 and any of the various expert observers against the
Xpert reference standard. A cost and efficiency analysis on this dataset
demonstrates that in a standard clinical situation, operating at 90%
sensitivity, users of CAD4TB v6 can process 132 subjects per day at an average
cost per screen of \$5.95 per subject, while users of version 3 process only 85
subjects per day at a cost of \$8.41 per subject. At all tested operating
points version 6 is shown to be more efficient and cost effective than any
other version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03371</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03371</id><created>2019-03-08</created><authors><author><keyname>Haqiqatnejad</keyname><forenames>Alireza</forenames></author><author><keyname>Kayhan</keyname><forenames>Farbod</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Robust SINR-Constrained Symbol-Level Multiuser Precoding with Imperfect
  Channel Knowledge</title><categories>eess.SP</categories><comments>19 pages, 9 figures, Submitted to IEEE Transactions in Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address robust design of symbol-level precoding for the
downlink of multiuser multiple-input multiple-output wireless channels, in the
presence of imperfect channel state information (CSI) at the transmitter. In
particular, we consider two common uncertainty models for the CSI imperfection,
namely, spherical (bounded) and stochastic (Gaussian). Our design objective is
to minimize the total (per-symbol) transmission power subject to constructive
interference (CI) constraints as well as users' quality-of-service requirements
in terms of signal-to-interference-plus-noise ratio. Assuming bounded channel
uncertainties, we obtain a convex CI constraint based on the worst-case robust
analysis, whereas in the case of Gaussian uncertainties, we define
probabilistic CI constraints in order to achieve robustness to
statistically-known CSI errors. Since the probabilistic constraints of actual
interest are difficult to handle, we resort to their convex approximations,
yielding tractable (deterministic) robust constraints. Three convex
approximations are developed based on different robust conservatism approaches,
among which one is introduced as a benchmark for comparison. We show that each
of our proposed approximations is tighter than the other under specific
robustness conditions, while both always outperform the benchmark. Using the
developed CI constraints, we formulate the robust precoding optimization as a
convex conic quadratic program. Extensive simulation results are provided to
validate our analytic discussions and to make comparisons with existing robust
precoding schemes. We also show that the robust design increases the
computational complexity by an order of the number of users in the large system
limit, compared to its non-robust counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03377</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03377</id><created>2019-03-08</created><authors><author><keyname>Haqiqatnejad</keyname><forenames>Alireza</forenames></author><author><keyname>Kayhan</keyname><forenames>Farbod</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>An Approximate Solution for Symbol-Level Multiuser Precoding Using
  Support Recovery</title><categories>eess.SP</categories><comments>7 pages, 2 figures, Submitted to IEEE SPAWC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a low-complexity method to approximately solve the
SINR-constrained optimization problem of symbol-level precoding (SLP). First,
assuming a generic modulation scheme, the precoding optimization problem is
recast as a standard non-negative least squares (NNLS). Then, we improve an
existing closed-form SLP (CF-SLP) scheme using the conditions for nearly
perfect recovery of the optimal solution support, followed by solving a reduced
system of linear equations. We show through simulation results that in
comparison with the CF-SLP method, the improved approximate solution of this
paper, referred to as ICF-SLP, significantly enhances the performance with a
negligible increase in complexity. We also provide comparisons with a
fast-converging iterative NNLS algorithm, where it is shown that the ICF-SLP
method is comparable in performance to the iterative algorithm with a limited
maximum number of iterations. Analytic discussions on the complexities of
different methods are provided, verifying the computational efficiency of the
proposed method. Our results further indicate that the ICF-SLP scheme performs
quite close to the optimal SLP, particularly in the large system regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03381</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03381</id><created>2019-03-08</created><authors><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Chen</keyname><forenames>Lirui</forenames></author><author><keyname>Wang</keyname><forenames>Qinglin</forenames></author><author><keyname>Zhang</keyname><forenames>Yang</forenames></author><author><keyname>Xing</keyname><forenames>Zuocheng</forenames></author></authors><title>Algorithm and Architecture for Path Metric Aided Bit-Flipping Decoding
  of Polar Codes</title><categories>eess.SP</categories><comments>6 pages, 6 figures, IEEE Wireless Communications and Networking
  Conference (2019 WCNC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes attract more and more attention of researchers in recent years,
since its capacity achieving property. However, their error-correction
performance under successive cancellation (SC) decoding is inferior to other
modern channel codes at short or moderate blocklengths. SC-Flip (SCF) decoding
algorithm shows higher performance than SC decoding by identifying possibly
erroneous decisions made in initial SC decoding and flipping them in the
sequential decoding attempts. However, it performs not well when there are more
than one erroneous decisions in a codeword. In this paper, we propose a path
metric aided bit-flipping decoding algorithm to identify and correct more
errors efficiently. In this algorithm, the bit-flipping list is generated based
on both log likelihood ratio (LLR) based path metric and bit-flipping metric.
The path metric is used to verify the effectiveness of bit-flipping. In order
to reduce the decoding latency and computational complexity, its corresponding
pipeline architecture is designed. By applying these decoding algorithms and
pipeline architecture, an improvement on error-correction performance can be
got up to 0.25dB compared with SCF decoding at the frame error rate of
$10^{-4}$, with low average decoding latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03474</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03474</id><created>2019-02-10</created><authors><author><keyname>Tait</keyname><forenames>Alexander N.</forenames></author><author><keyname>Ma</keyname><forenames>Philip Y.</forenames></author><author><keyname>de Lima</keyname><forenames>Thomas Ferreira</forenames></author><author><keyname>Blow</keyname><forenames>Eric C.</forenames></author><author><keyname>Chang</keyname><forenames>Matthew P.</forenames></author><author><keyname>Nahmias</keyname><forenames>Mitchell A.</forenames></author><author><keyname>Shastri</keyname><forenames>Bhavin J.</forenames></author><author><keyname>Prucnal</keyname><forenames>Paul R.</forenames></author></authors><title>Demonstration of multivariate photonics: blind dimensionality reduction
  with analog integrated photonics</title><categories>physics.app-ph eess.SP physics.optics</categories><comments>24 pages, 7 figures</comments><doi>10.1109/JLT.2019.2945017</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Multi-antenna radio front-ends generate a multi-dimensional flood of
information, most of which is partially redundant. Redundancy is eliminated by
dimensionality reduction, but contemporary digital processing techniques face
harsh fundamental tradeoffs when implementing this class of functions. These
tradeoffs can be broken in the analog domain, in which the performance of
optical technologies greatly exceeds that of electronic counterparts. Here, we
present concepts, methods, and a first demonstration of multivariate photonics:
a combination of integrated photonic hardware, analog dimensionality reduction,
and blind algorithmic techniques. We experimentally demonstrate 2-channel, 1.0
GHz principal component analysis in a photonic weight bank using recently
proposed algorithms for synthesizing the multivariate properties of signals to
which the receiver is blind. Novel methods are introduced for controlling
blindness conditions in a laboratory context. This work provides a foundation
for further research in multivariate photonic information processing, which is
poised to play a role in future generations of wireless technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03491</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03491</id><created>2019-03-08</created><authors><author><keyname>Bergerhoff</keyname><forenames>Leif</forenames></author><author><keyname>C&#xe1;rdenas</keyname><forenames>Marcelo</forenames></author><author><keyname>Weickert</keyname><forenames>Joachim</forenames></author><author><keyname>Welk</keyname><forenames>Martin</forenames></author></authors><title>Stable Backward Diffusion Models that Minimise Convex Energies</title><categories>math.NA cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backward diffusion processes appear naturally in image enhancement and
deblurring applications. However, the inverse problem of backward diffusion is
known to be ill-posed and straightforward numerical algorithms are unstable. So
far, existing stabilisation strategies in the literature require sophisticated
numerics to solve the underlying initial value problem. Therefore, it is
desirable to establish a backward diffusion model which implements a smart
stabilisation approach that can be used in combination with a simple numerical
scheme. We derive a class of space-discrete one-dimensional backward diffusion
as gradient descent of energies where we gain stability by imposing range
constraints. Interestingly, these energies are even convex. Furthermore, we
establish a comprehensive theory for the time-continuous evolution and we show
that stability carries over to a simple explicit time discretisation of our
model. Finally, we confirm the stability and usefulness of our technique in
experiments in which we enhance the contrast of digital greyscale and colour
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03510</identifier>
 <datestamp>2019-06-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03510</id><created>2019-03-08</created><updated>2019-06-13</updated><authors><author><keyname>Fessler</keyname><forenames>Jeffrey A</forenames></author></authors><title>Optimization methods for MR image reconstruction (long version)</title><categories>eess.IV math.OC</categories><comments>Extended (and revised) version of invited paper submitted to IEEE
  SPMag special issue on &quot;Computational MRI: Compressed Sensing and Beyond.&quot;</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The development of compressed sensing methods for magnetic resonance (MR)
image reconstruction led to an explosion of research on models and optimization
algorithms for MR imaging (MRI). Roughly 10 years after such methods first
appeared in the MRI literature, the U.S. Food and Drug Administration (FDA)
approved certain compressed sensing methods for commercial use, making
compressed sensing a clinical success story for MRI. This review paper
summarizes several key models and optimization algorithms for MR image
reconstruction, including both the type of methods that have FDA approval for
clinical use, as well as more recent methods being considered in the research
community that use data-adaptive regularizers. Many algorithms have been
devised that exploit the structure of the system model and regularizers used in
MRI; this paper strives to collect such algorithms in a single survey. Many of
the ideas used in optimization methods for MRI are also useful for solving
other inverse problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03547</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03547</id><created>2019-03-08</created><authors><author><keyname>Addabbo</keyname><forenames>Pia</forenames></author><author><keyname>Besson</keyname><forenames>Olivier</forenames></author><author><keyname>Orlando</keyname><forenames>Danilo</forenames></author><author><keyname>Ricci</keyname><forenames>Giuseppe</forenames></author></authors><title>Adaptive Detection of Coherent Radar Targets in the Presence of Noise
  Jamming</title><categories>eess.SP</categories><comments>11 pages, 11 figures</comments><doi>10.1109/TSP.2019.2954499</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we devise adaptive decision schemes to detect targets
competing against clutter and smart noise-like jammers (NLJ) which illuminate
the radar system from the sidelobes. Specifically, the considered class of NLJs
generates a pulse of noise (noise cover pulse) that is triggered by and
concurrent with the received uncompressed pulse in order to mask the skin echo
and, hence, to hide the true target range. The detection problem is formulated
as a binary hypothesis test and two different models for the NLJ are
considered. Then, ad hoc modifications of the generalized likelihood ratio test
are exploited where the unknown parameters are estimated by means of cyclic
optimization procedures. The performance analysis is carried out using
simulated data and proves the effectiveness of the proposed approach for both
situations where the NLJ is either active or switched off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03600</identifier>
 <datestamp>2019-03-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03600</id><created>2019-03-08</created><authors><author><keyname>Rahmati</keyname><forenames>Ali</forenames></author><author><keyname>Dai</keyname><forenames>Huaiyu</forenames></author></authors><title>Reinforcement Learning for Interference Avoidance Game in RF-Powered
  Backscatter Communications</title><categories>eess.SP</categories><comments>6 pages, 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RF-powered backscatter communication is a promising new technology that can
be deployed for battery-free applications such as internet of things (IoT) and
wireless sensor networks (WSN). However, since this kind of communication is
based on the ambient RF signals and battery-free devices, they are vulnerable
to interference and jamming. In this paper, we model the interaction between
the user and a smart interferer in an ambient backscatter communication network
as a game. We design the utility functions of both the user and interferer in
which the backscattering time is taken into the account. The convexity of both
sub-game optimization problems is proved and the closed-form expression for the
equilibrium of the Stackelberg game is obtained. Due to lack of information
about the system SNR and transmission strategy of the interferer, the optimal
strategy is obtained using the Q-learning algorithm in a dynamic iterative
manner. We further introduce hotbooting Q-learning as an effective approach to
expedite the convergence of the traditional Q-learning. Simulation results show
that our approach can obtain considerable performance improvement in comparison
to random and fixed backscattering time transmission strategies and improves
the convergence speed of Q-Learning by about 31%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03652</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03652</id><created>2019-03-08</created><authors><author><keyname>Sharma</keyname><forenames>Mohit K</forenames></author><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Assaad</keyname><forenames>Mohamad</forenames></author></authors><title>Deep Learning Based Online Power Control for Large Energy Harvesting
  Networks</title><categories>eess.SP cs.LG math.OC</categories><comments>5 pages, to appear at ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a deep learning based approach to design online
power control policies for large EH networks, which are often intractable
stochastic control problems. In the proposed approach, for a given EH network,
the optimal online power control rule is learned by training a deep neural
network (DNN), using the solution of offline policy design problem. Under the
proposed scheme, in a given time slot, the transmit power is obtained by
feeding the current system state to the trained DNN. Our results illustrate
that the DNN based online power control scheme outperforms a Markov decision
process based policy. In general, the proposed deep learning based approach can
be used to find solutions to large intractable stochastic control problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03682</identifier>
 <datestamp>2020-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03682</id><created>2019-03-08</created><updated>2020-02-18</updated><authors><author><keyname>Dimas</keyname><forenames>Anastasios</forenames></author><author><keyname>Kalogerias</keyname><forenames>Dionysios S.</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author></authors><title>Joint Beamforming and Dynamic Relay Positioning for mmWave Urban
  Communications</title><categories>eess.SP</categories><comments>This paper has never been published nor is it submitted anywhere. I
  am requesting a withdrawal due to the presence of some technical errors that
  don't represent the content</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of millimeter wave (mmWave) spectrum for commercial wireless
communications is expected to offer data rates in the order of
Gigabits-per-second, thus able to support future applications such as
Vehicle-to-Vehicle or Vehicle-to-Infrastructure communication. However,
especially in urban settings, mmWave signal propagation is sensitive to
blockage by surrounding objects, resulting in significant signal attenuation.
One approach to mitigate the effect of attenuation is through multi-hop
communication with the help of relays. Leveraging the unique characteristics of
the mmWave medium, we consider a single-source/destination $2$-hop system,
where a cluster of spatially distributed and reconfigurable relays is used to
cooperatively amplify-and-forward the source signal to the destination. Our
system evolves in time slots, during which not only are optimal beamforming
weights centrally determined, but also future relay positions for the
subsequent time slot are optimally selected, jointly maximizing the expected
signal-to-interference+noise ratio at the destination. Optimal predictive relay
positioning is achieved by formulating a 2-stage stochastic programming
problem, which is efficiently approximated via a conditional
sample-average-approximation surrogate, and solved in a purely distributed
fashion across relays. The efficacy of the proposed near-optimal positioning
policy is corroborated by comparison against a randomized relay positioning
policy, clearly confirming its superiority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03709</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03709</id><created>2019-03-08</created><authors><author><keyname>Matsumine</keyname><forenames>Toshiki</forenames></author><author><keyname>Koike-Akino</keyname><forenames>Toshiaki</forenames></author><author><keyname>Wang</keyname><forenames>Ye</forenames></author></authors><title>Polar Coding with Chemical Reaction Networks</title><categories>cs.IT cs.ET cs.LO eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new polar coding scheme with molecular
programming, which is capable of highly parallel implementation at a nano-scale
without a need of electrical power sources. We designed chemical reaction
networks (CRN) to employ either successive cancellation (SC) or
maximum-likelihood (ML) decoding schemes for short polar codes. From ordinary
differential equation (ODE) analysis of the proposed CRNs, we demonstrate that
SC and ML decoding achieve accurate computations across fully-parallel chemical
reactions. We also make a comparison in terms of the number of required
chemical reactions and species, where the superiority of ML decoder over SC
decoder is observed for very short block lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03711</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03711</id><created>2019-03-08</created><authors><author><keyname>Wang</keyname><forenames>Ye</forenames></author><author><keyname>Koike-Akino</keyname><forenames>Toshiaki</forenames></author></authors><title>Learning to Modulate for Non-coherent MIMO</title><categories>eess.SP cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deep learning trend has recently impacted a variety of fields, including
communication systems, where various approaches have explored the application
of neural networks in place of traditional designs. Neural networks flexibly
allow for data/simulation-driven optimization, but are often employed as black
boxes detached from direct application of domain knowledge. Our work considers
learning-based approaches addressing modulation and signal detection design for
the non-coherent MIMO channel. We demonstrate that simulation-driven
optimization can be performed while entirely avoiding neural networks, yet
still perform comparably. Additionally, we show the feasibility of MIMO
communications over extremely short coherence windows (i.e., channel
coefficient stability period), with as few as two time slots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03713</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03713</id><created>2019-03-08</created><authors><author><keyname>Matsumine</keyname><forenames>Toshiki</forenames></author><author><keyname>Koike-Akino</keyname><forenames>Toshiaki</forenames></author><author><keyname>Wang</keyname><forenames>Ye</forenames></author></authors><title>Deep Learning-Based Constellation Optimization for Physical Network
  Coding in Two-Way Relay Networks</title><categories>cs.IT cs.LG eess.SP math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a new application of deep learning (DL) for optimizing
constellations in two-way relaying with physical-layer network coding (PNC),
where deep neural network (DNN)-based modulation and demodulation are employed
at each terminal and relay node. We train DNNs such that the cross entropy loss
is directly minimized, and thus it maximizes the likelihood, rather than
considering the Euclidean distance of the constellations. The proposed scheme
can be extended to higher level constellations with slight modification of the
DNN structure. Simulation results demonstrate a significant performance gain in
terms of the achievable sum rate over conventional relaying schemes.
Furthermore, since our DNN demodulator directly outputs bit-wise probabilities,
it is straightforward to concatenate with soft-decision channel decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03718</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03718</id><created>2019-03-08</created><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Casta&#xf1;eda</keyname><forenames>Oscar</forenames></author><author><keyname>Jacobsson</keyname><forenames>Sven</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Neural-Network Optimized 1-bit Precoding for Massive MU-MIMO</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Base station (BS) architectures for massive multi-user (MU) multiple-input
multiple-output (MIMO) wireless systems are equipped with hundreds of antennas
to serve tens of users on the same time-frequency channel. The immense number
of BS antennas incurs high system costs, power, and interconnect bandwidth. To
circumvent these obstacles, sophisticated MU precoding algorithms that enable
the use of 1-bit DACs have been proposed. Many of these precoders feature
parameters that are, traditionally, tuned manually to optimize their
performance. We propose to use deep-learning tools to automatically tune such
1-bit precoders. Specifically, we optimize the biConvex 1-bit PrecOding (C2PO)
algorithm using neural networks. Compared to the original C2PO algorithm, our
neural-network optimized (NNO-)C2PO achieves the same error-rate performance at
$\bf 2\boldsymbol\times$ lower complexity. Moreover, by training NNO-C2PO for
different channel models, we show that 1-bit precoding can be made robust to
vastly changing propagation conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03725</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03725</id><created>2019-03-08</created><authors><author><keyname>Sharma</keyname><forenames>Vishal</forenames></author><author><keyname>Sharma</keyname><forenames>Navuday</forenames></author><author><keyname>Rehmani</keyname><forenames>Mubashir Husain</forenames></author></authors><title>Control over Skies: Survivability, Coverage and Mobility Laws for
  Hierarchical Aerial Base Stations</title><categories>eess.SP cs.NI</categories><comments>7 pages, 6 figures, Submitted to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aerial Base Stations (ABSs) have gained significant importance in the next
generation of wireless networks for accommodating mobile ground users and flash
crowds with high convenience and quality. However, to achieve an efficient ABS
network, many factors pertaining to ABS flight, governing laws and information
transmissions have to be studied. In this article, multi-drone communications
are studied in three major aspects, survivability, coverage and mobility laws,
which optimize the multi-tier ABS network to avoid issues related to inter-cell
interference, deficient energy, frequent handovers, and lifetime. Moreover,
this article discusses several optimization constraints along with the proposed
solution for management of the hierarchical ABS network. In addition, the
article includes simulation results of hierarchical ABS allocations for
handling a set of users over a defined geographical area. Further, several open
issues and challenges are presented to provide deep insights into the ABS
network management and its utility framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03741</identifier>
 <datestamp>2020-01-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03741</id><created>2019-03-09</created><updated>2020-01-19</updated><authors><author><keyname>Ji</keyname><forenames>Feng</forenames></author><author><keyname>Pratibha</keyname></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author></authors><title>Folded Graph Signals: Sensing with Unlimited Dynamic Range</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-reset analog-to-digital converters (ADCs) are used to sample high
dynamic range signals resulting in modulo-operation based folded signal
samples. We consider the case where each vertex of a graph (e.g., sensors in a
network) is equipped with a self-reset ADC and senses a time series. Graph
sampling allows the graph time series to be represented by the signals at a
subset of sampled vertices and time instances. We investigate the problem of
recovering bandlimited continuous-time graph signals from folded signal
samples. We derive sufficient conditions to achieve successful recovery of the
graph signal from the folded signal samples, which can be achieved via integer
programming. To resolve the scalability issue of integer programming, we
propose a sparse optimization recovery method for graph signals satisfying
certain technical conditions. Such an approach requires a novel graph sampling
scheme that selects vertices with small signal variation. The proposed
algorithm exploits the inherent relationship among the graph vertices in both
the vertex and time domains to recover the graph signal. Simulations and
experiments on images validate the feasibility of our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03895</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03895</id><created>2019-03-09</created><authors><author><keyname>Zhong</keyname><forenames>Wei</forenames></author><author><keyname>He</keyname><forenames>Kai</forenames></author><author><keyname>Li</keyname><forenames>Lianlin</forenames></author></authors><title>Through-the-Wall Imaging Exploiting 2.4GHz Commodity Wi-Fi</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we experimentally investigate a low-cost through-the-wall
imaging exploiting Wi-Fi signals in an indoor environment from the perspective
of holographic imaging. In our experiments, a pair of antennas in a synthetic
aperture mode is used to acquire signals produced by commodity Wi-Fi devices
and reflected from the scene in a synthetic aperture mode. The classical
filtered back propagation (FBP) algorithm is then employed to form the image
based on these signals. We use an IEEE 802.11n wireless router working at
2.4GHz with bandwidth of 20MHz. Selected experimental results are provided to
demonstrate the performance of the proposed Wi-Fi based imaging scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03913</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03913</id><created>2019-03-09</created><authors><author><keyname>Feng</keyname><forenames>Daquan</forenames></author><author><keyname>She</keyname><forenames>Changyang</forenames></author><author><keyname>Ying</keyname><forenames>Kai</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author><author><keyname>Hou</keyname><forenames>Zhanwei</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Towards Ultra-Reliable Low-Latency Communications: Typical Scenarios,
  Possible Solutions, and Open Issues</title><categories>eess.SP cs.IT math.IT</categories><comments>8 pages, 7 figures. Accepted by IEEE Vehicular Technology Magazine</comments><journal-ref>IEEE Vehicular Technology Magazine, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultra-reliable low-latency communications (URLLC) has been considered as one
of the three new application scenarios in the \emph{5th Generation} (5G) \emph
{New Radio} (NR), where the physical layer design aspects have been specified.
With the 5G NR, we can guarantee the reliability and latency in radio access
networks. However, for communication scenarios where the transmission involves
both radio access and wide area core networks, the delay in radio access
networks only contributes to part of the \emph{end-to-end} (E2E) delay. In this
paper, we outline the delay components and packet loss probabilities in typical
communication scenarios of URLLC, and formulate the constraints on E2E delay
and overall packet loss probability. Then, we summarize possible solutions in
the physical layer, the link layer, the network layer, and the cross-layer
design, respectively. Finally, we discuss the open issues in prediction and
communication co-design for URLLC in wide area large scale networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03942</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03942</id><created>2019-03-10</created><authors><author><keyname>Peters</keyname><forenames>Bas</forenames></author><author><keyname>Herrmann</keyname><forenames>Felix J.</forenames></author></authors><title>Generalized Minkowski sets for the regularization of inverse problems</title><categories>eess.IV cs.NA math.OC</categories><comments>18 pages, 3 figures</comments><msc-class>68U10, 86A22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many works on inverse problems in the imaging sciences consider
regularization via one or more penalty functions or constraint sets. When the
models/images are not easily described using one or a few penalty
functions/constraints, additive model descriptions for regularization lead to
better imaging results. These include cartoon-texture decomposition,
morphological component analysis, and robust principal component analysis;
methods that typically rely on penalty functions. We propose a regularization
framework, based on the Minkowski set, that merges the strengths of additive
models and constrained formulations. We generalize the Minkowski set, such that
the model parameters are the sum of two components, each of which is
constrained to an intersection of sets. Furthermore, the sum of the components
is also an element of another intersection of sets. These generalizations allow
us to include multiple pieces of prior knowledge on each of the components, as
well as on the sum of components, which is necessary to ensure physical
feasibility of partial-differential-equation based parameters estimation
problems. We derive the projection operation onto the generalized Minkowski
sets and construct an algorithm based on the alternating direction method of
multipliers. We illustrate how we benefit from using more prior knowledge in
the form of the generalized Minkowski set using seismic waveform inversion and
video background-anomaly separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03949</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03949</id><created>2019-03-10</created><authors><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Zadik</keyname><forenames>Ilias</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>A simple bound on the BER of the MAP decoder for massive MIMO systems</title><categories>cs.IT eess.SP math.IT</categories><comments>To appear in ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of massive MIMO systems has revived much of the interest in
the study of the large-system performance of multiuser detection systems. In
this paper, we prove a non-trivial upper bound on the bit-error rate (BER) of
the MAP detector for BPSK signal transmission and equal-power condition. In
particular, our bound is approximately tight at high-SNR. The proof is simple
and relies on Gordon's comparison inequality. Interestingly, we show that under
the assumption that Gordon's inequality is tight, the resulting BER prediction
matches that of the replica method under the replica symmetry (RS) ansatz.
Also, we prove that, when the ratio of receive to transmit antennas exceeds
$0.9251$, the replica prediction matches the matched filter lower bound (MFB)
at high-SNR. We corroborate our results by numerical evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03962</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03962</id><created>2019-03-10</created><authors><author><keyname>Azhiri</keyname><forenames>Fatemeh Asghari</forenames></author><author><keyname>Abdolee</keyname><forenames>Reza</forenames></author><author><keyname>Tazehkand</keyname><forenames>Behzad Mozaffari</forenames></author></authors><title>Effect of Mutual Coupling on the Performance of STCM-MIMO Systems</title><categories>eess.SP</categories><comments>The paper is accepted in 2019 IEEE Wireless Network Communication
  Conference. The original paper is 5 pages and it has 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space-time coded massive (STCM) multiple-input multiple-output (MIMO) system
provides superior bit error rate (BER) performance compared with the
conventional space-time coding and massive MIMO techniques. The transmitter of
the STCM-MIMO system consists of a large antenna array. In a practical system,
the self-interference created by the signals transmitted by the elements of
this antenna array, known as mutual coupling (MC), degrades the performance of
the system. The MC effect is pronounced in communication systems with a large
antenna array. On the other hand, increasing the number of transmitting
antennas results in improved BER performance. Hence, there is a trade off in
selecting the optimum number of transmitting antennas in an STCM-MIMO system.
In order to take the impact of MC into account, we have derived an analytical
expression for the received signal to accurately model the STCM-MIMO system
under the existence of the MC effect. We present an algorithm to select the
optimal number of antennas to minimize mutual coupling and the system bit error
rate (BER). Through computer simulations, we investigate the BER performance of
the STCM-MIMO system for different numbers of array elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.03971</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.03971</id><created>2019-03-10</created><authors><author><keyname>Masuyama</keyname><forenames>Yoshiki</forenames></author><author><keyname>Yatabe</keyname><forenames>Kohei</forenames></author><author><keyname>Koizumi</keyname><forenames>Yuma</forenames></author><author><keyname>Oikawa</keyname><forenames>Yasuhiro</forenames></author><author><keyname>Harada</keyname><forenames>Noboru</forenames></author></authors><title>Deep Griffin-Lim Iteration</title><categories>cs.SD cs.LG eess.AS</categories><comments>5 pages, to appear in IEEE ICASSP 2019 (Paper Code: AASP-L3.1,
  Session: Source Separation and Speech Enhancement I)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel phase reconstruction method (only from a given
amplitude spectrogram) by combining a signal-processing-based approach and a
deep neural network (DNN). To retrieve a time-domain signal from its amplitude
spectrogram, the corresponding phase is required. One of the popular phase
reconstruction methods is the Griffin-Lim algorithm (GLA), which is based on
the redundancy of the short-time Fourier transform. However, GLA often involves
many iterations and produces low-quality signals owing to the lack of prior
knowledge of the target signal. In order to address these issues, in this
study, we propose an architecture which stacks a sub-block including two
GLA-inspired fixed layers and a DNN. The number of stacked sub-blocks is
adjustable, and we can trade the performance and computational load based on
requirements of applications. The effectiveness of the proposed method is
investigated by reconstructing phases from amplitude spectrograms of speeches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04085</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04085</id><created>2019-03-10</created><authors><author><keyname>Tabaghi</keyname><forenames>Puoya</forenames></author><author><keyname>Dokmani&#x107;</keyname><forenames>Ivan</forenames></author></authors><title>Real Polynomial Gram Matrices Without Real Spectral Factors</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that a non-negative definite polynomial matrix (a polynomial
Gramian) $G(t)$ can be written as a product of its polynomial spectral factors,
$G(t) = X(t)^H X(t)$. In this paper, we give a new algebraic characterization
of spectral factors when $G(t)$ is real-valued. The key idea is to construct a
representation set that is in bijection with the set of real polynomial
Gramians. We use the derived characterization to identify the set of all
complex polynomial matrices that generate real-valued Gramians, and we
formulate a conjecture that typical rank-deficient real polynomial Gramians
have real spectral factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04124</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04124</id><created>2019-03-11</created><authors><author><keyname>Chen</keyname><forenames>Xin</forenames></author><author><keyname>Chu</keyname><forenames>Wei</forenames></author><author><keyname>Guo</keyname><forenames>Jinxi</forenames></author><author><keyname>Xu</keyname><forenames>Ning</forenames></author></authors><title>Singing voice conversion with non-parallel data</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>Accepted to MIPR 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Singing voice conversion is a task to convert a song sang by a source singer
to the voice of a target singer. In this paper, we propose using a parallel
data free, many-to-one voice conversion technique on singing voices. A phonetic
posterior feature is first generated by decoding singing voices through a
robust Automatic Speech Recognition Engine (ASR). Then, a trained Recurrent
Neural Network (RNN) with a Deep Bidirectional Long Short Term Memory (DBLSTM)
structure is used to model the mapping from person-independent content to the
acoustic features of the target person. F0 and aperiodic are obtained through
the original singing voice, and used with acoustic features to reconstruct the
target singing voice through a vocoder. In the obtained singing voice, the
targeted and sourced singers sound similar. To our knowledge, this is the first
study that uses non parallel data to train a singing voice conversion system.
Subjective evaluations demonstrate that the proposed method effectively
converts singing voices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04131</identifier>
 <datestamp>2019-12-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04131</id><created>2019-03-11</created><updated>2019-05-22</updated><authors><author><keyname>Kumari</keyname><forenames>Vineeta</forenames></author><author><keyname>Sheoran</keyname><forenames>Gyanendra</forenames></author><author><keyname>Kanumuri</keyname><forenames>Tirupathiraju</forenames></author></authors><title>SAR Analysis of Directive Antenna on Anatomically Real Breast Phantoms
  for Microwave Holography</title><categories>eess.IV</categories><comments>11 pages, 10 figures</comments><journal-ref>Microwave and Optics Technology Letters, 2019</journal-ref><doi>10.1002/mop.32037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microwave imaging is emerging as a promising substitution diagnostic tool,
for breast cancer detection. It has ignited the interest in the interaction of
the microwaves with biological tissues and their respective effects on the
human body which requires the investigation of safety issues on human health
under the exposure of antenna's microwave radiation. In this work a
specifically designed directive and ultra-wideband antenna is developed for
microwave holographic system. A detailed simulation analysis of Specific
Absorption Rate (SAR) and temperature variation due to the radiation exposure
of the designed antenna on self-designed anatomically real breast phantoms and
Computer Simulation Technology (CST) voxel breast models are done in CST
environment. Thereafter, the antenna and anatomical phantom are fabricated for
the experimental verification of the simulation analysis of SAR. It is found
that the SAR due to the radiation of the designed antenna lies in the
permissible limit and the variation in temperature is only 0.096%. Hence, the
antenna can be used in microwave holography for breast cancer detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04141</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04141</id><created>2019-03-11</created><updated>2019-03-12</updated><authors><author><keyname>Roy</keyname><forenames>Sandip</forenames></author><author><keyname>Xue</keyname><forenames>Mengran</forenames></author></authors><title>Sign Patterns of Inverse Doubly-Nonnegative Matrices</title><categories>cs.SY eess.SP math.NA math.OC</categories><comments>technical note, submitted to Linear Algebra and its applications.
  After the original submission, we became aware of a work of M. Fiedler that
  is related to our work; the draft has been updated to acknowledge the work of
  Fiedler and position our contribution relative to it</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sign patterns of inverse doubly-nonnegative matrices are examined. A
necessary and sufficient condition is developed for a sign matrix to correspond
to an inverse doubly-nonnegative matrix. In addition, for a doubly-nonnegative
matrix whose graph is a tree, the inverse is shown to have a unique sign
pattern, which can be expressed in terms of a two-coloring of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04249</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04249</id><created>2019-03-11</created><authors><author><keyname>Kruber</keyname><forenames>Friedrich</forenames></author><author><keyname>Wurst</keyname><forenames>Jonas</forenames></author><author><keyname>Chakraborty</keyname><forenames>Samarjit</forenames></author><author><keyname>Botsch</keyname><forenames>Michael</forenames></author></authors><title>Highway traffic data: macroscopic, microscopic and criticality analysis
  for capturing relevant traffic scenarios and traffic modeling based on the
  highD data set</title><categories>eess.SP physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work provides a comprehensive analysis on naturalistic driving behavior
for highways based on the highD data set. Two thematic fields are considered.
First, some macroscopic and microscopic traffic statistics are provided. These
include the traffic flow rate and the traffic density, as well as velocity,
acceleration and distance distributions. Additionally, the dependencies to each
other are examined and compared to related work. The second part investigates
the distributions of criticality measures. The Time-To-Collision, Time-Headway
and a third measure, which couples both, are analyzed. These measures are also
combined with other indicators. Scenarios, in which these measures reach a
critical level, are separately discussed. The results are compared to related
work as well. The two main contributions of this work can be stated as follows.
First, the analysis on the criticality measures can be used to find suitable
thresholds for rare traffic scenarios. Second, the statistics provided in this
work can also be utilized for traffic modeling, for example in simulation
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04267</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04267</id><created>2019-03-07</created><authors><author><keyname>Wu</keyname><forenames>Yawen</forenames></author><author><keyname>Jia</keyname><forenames>Zhenge</forenames></author><author><keyname>Hu</keyname><forenames>Jingtong</forenames></author></authors><title>Energy Harvesting Powered Embedded Systems</title><categories>eess.SP</categories><comments>2 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Historically, battery is the power source for mobile, embedded and remote
system applications. However, the development of battery techniques does not
follow the Moore's Law. The large physical size, limited electric quantity and
high-cost replacement process always restrict the performance of the
application such as embedded systems, wireless sensors networks and lower-power
electronics. Energy harvesting, a technique which enables the applications to
scavenge energy from RF signal from TV towers, solar energy, piezoelectric
driven by motion of people and thermal energy from the temperature difference,
which could dramatically extend the operating lifetime of applications. Thus,
energy harvesting is important for the sustainable operations of an
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04297</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04297</id><created>2019-03-08</created><authors><author><keyname>Wang</keyname><forenames>Hongmei</forenames></author><author><keyname>Wu</keyname><forenames>Zhenzhen</forenames></author><author><keyname>Ma</keyname><forenames>Shuai</forenames></author><author><keyname>Lu</keyname><forenames>Songtao</forenames></author><author><keyname>Zhang</keyname><forenames>Han</forenames></author><author><keyname>Ding</keyname><forenames>Guoru</forenames></author><author><keyname>Li</keyname><forenames>Shiyin</forenames></author></authors><title>Deep Learning for Signal Demodulation in Physical Layer Wireless
  Communications: Prototype Platform, Open Dataset, and Analytics</title><categories>eess.SP cs.LG stat.ML</categories><doi>10.1109/ACCESS.2019.2903130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate deep learning (DL)-enabled signal demodulation
methods and establish the first open dataset of real modulated signals for
wireless communication systems. Specifically, we propose a flexible
communication prototype platform for measuring real modulation dataset. Then,
based on the measured dataset, two DL-based demodulators, called deep belief
network (DBN)-support vector machine (SVM) demodulator and adaptive boosting
(AdaBoost) based demodulator, are proposed. The proposed DBN-SVM based
demodulator exploits the advantages of both DBN and SVM, i.e., the advantage of
DBN as a feature extractor and SVM as a feature classifier. In DBN-SVM based
demodulator, the received signals are normalized before being fed to the DBN
network. Furthermore, an AdaBoost based demodulator is developed, which employs
the $k$-Nearest Neighbor (KNN) as a weak classifier to form a strong combined
classifier. Finally, experimental results indicate that the proposed DBN-SVM
based demodulator and AdaBoost based demodulator are superior to the single
classification method using DBN, SVM, and maximum likelihood (MLD) based
demodulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04460</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04460</id><created>2019-03-11</created><authors><author><keyname>Gecgel</keyname><forenames>Selen</forenames></author><author><keyname>Goztepe</keyname><forenames>Caner</forenames></author><author><keyname>Kurt</keyname><forenames>Gunes Karabulut</forenames></author></authors><title>Transmit Antenna Selection for Massive MIMO-GSM with Machine Learning</title><categories>eess.SP cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dynamic and flexible generalized spatial modulation (GSM) framework is
proposed for massive MIMO systems. Our framework is leveraged on the
utilization of machine learning methods for GSM in order to improve the error
performance in presence of correlated channels and channel estimation errors.
Both decision tree and multi-layer perceptrons approaches are adopted for the
GSM transmitter. Simulation results indicate that in presence of real-life
impairments machine learning based approaches provide a superior performance
when compared to the classical Euclidean distance based approach. The
observations are validated through measurement results over the designed
$16\times 4$ MIMO test-bed using software defined radio nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04486</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04486</id><created>2019-03-09</created><authors><author><keyname>Niazazari</keyname><forenames>Iman</forenames></author><author><keyname>Hamidi</keyname><forenames>Reza Jalilzadeh</forenames></author><author><keyname>Livani</keyname><forenames>Hanif</forenames></author><author><keyname>Arghandeh</keyname><forenames>Reza</forenames></author></authors><title>Cause Identification of Electromagnetic Transient Events using
  Spatiotemporal Feature Learning</title><categories>eess.SP cs.LG stat.ML</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a spatiotemporal unsupervised feature learning method for
cause identification of electromagnetic transient events (EMTE) in power grids.
The proposed method is formulated based on the availability of
time-synchronized high-frequency measurement, and using the convolutional
neural network (CNN) as the spatiotemporal feature representation along with
softmax function. Despite the existing threshold-based, or energy-based events
analysis methods, such as support vector machine (SVM), autoencoder, and
tapered multi-layer perception (t-MLP) neural network, the proposed feature
learning is carried out with respect to both time and space. The effectiveness
of the proposed feature learning and the subsequent cause identification is
validated through the EMTP simulation of different events such as line
energization, capacitor bank energization, lightning, fault, and high-impedance
fault in the IEEE 30-bus, and the real-time digital simulation (RTDS) of the
WSCC 9-bus system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04547</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04547</id><created>2019-03-07</created><authors><author><keyname>Li</keyname><forenames>Shaoyan</forenames></author><author><keyname>Gu</keyname><forenames>Xueping</forenames></author><author><keyname>Zhou</keyname><forenames>Guangqi</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author></authors><title>Optimisation and Comprehensive Evaluation of Alternative Energising
  Paths for Power System Restoration</title><categories>eess.SP cs.CE math.OC</categories><comments>Accepted by IET Generation Transmission &amp; Distribution</comments><doi>10.1049/iet-gtd.2018.6277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power system restoration after a major blackout is a complex process, in
which selection of energising paths is a key issue to realize unit and load
restoration safely and efficiently. In general, the energising path scheme made
beforehand may not be executed successfully due to the possible faults on the
related lines under the extreme system condition, so it is necessary to provide
alternative path schemes for system restoration. In view of this, the
energising path optimisation based on the minimum cost flow model is
investigated, then an iterative searching method for alternative path schemes
based on mixed integer linear programming is proposed. The iterative method for
alternative path schemes could determine more than one scheme with minimal
charging reactive power efficiently. In order to make a comprehensive
evaluation of the alternative schemes, an evaluation index set is established,
and the method based on similarity to ideal grey relational projection is
introduced to achieve the final evaluation. The New England 10-unit 39-bus
system and the southern Hebei power system of China are employed to demonstrate
the effectiveness of the proposed method. The proposed method can provide more
efficient and comprehensive decision support for the dispatchers to select
reasonable energising paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04567</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04567</id><created>2019-03-11</created><updated>2019-03-12</updated><authors><author><keyname>Wang</keyname><forenames>Peidong</forenames></author><author><keyname>Tan</keyname><forenames>Ke</forenames></author><author><keyname>Wang</keyname><forenames>DeLiang</forenames></author></authors><title>Bridging the Gap Between Monaural Speech Enhancement and Recognition
  with Distortion-Independent Acoustic Modeling</title><categories>eess.AS cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monaural speech enhancement has made dramatic advances since the introduction
of deep learning a few years ago. Although enhanced speech has been
demonstrated to have better intelligibility and quality for human listeners,
feeding it directly to automatic speech recognition (ASR) systems trained with
noisy speech has not produced expected improvements in ASR performance. The
lack of an enhancement benefit on recognition, or the gap between monaural
speech enhancement and recognition, is often attributed to speech distortions
introduced in the enhancement process. In this study, we analyze the distortion
problem, compare different acoustic models, and investigate a
distortion-independent training scheme for monaural speech recognition.
Experimental results suggest that distortion-independent acoustic modeling is
able to overcome the distortion problem. Such an acoustic model can also work
with speech enhancement models different from the one used during training.
Moreover, the models investigated in this paper outperform the previous best
system on the CHiME-2 corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04579</identifier>
 <datestamp>2019-08-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04579</id><created>2019-03-12</created><updated>2019-07-22</updated><authors><author><keyname>Williamson</keyname><forenames>Ian A. D.</forenames></author><author><keyname>Hughes</keyname><forenames>Tyler W.</forenames></author><author><keyname>Minkov</keyname><forenames>Momchil</forenames></author><author><keyname>Bartlett</keyname><forenames>Ben</forenames></author><author><keyname>Pai</keyname><forenames>Sunil</forenames></author><author><keyname>Fan</keyname><forenames>Shanhui</forenames></author></authors><title>Reprogrammable Electro-Optic Nonlinear Activation Functions for Optical
  Neural Networks</title><categories>eess.SP cs.NE physics.optics</categories><comments>12 pages, 6 figures</comments><journal-ref>IEEE Journal of Selected Topics in Quantum Electronics, vol. 26,
  no. 1, pp. 1-12, Jan. 2020</journal-ref><doi>10.1109/JSTQE.2019.2930455</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an electro-optic hardware platform for nonlinear activation
functions in optical neural networks. The optical-to-optical nonlinearity
operates by converting a small portion of the input optical signal into an
analog electric signal, which is used to intensity-modulate the original
optical signal with no reduction in processing speed. Our scheme allows for
complete nonlinear on-off contrast in transmission at relatively low optical
power thresholds and eliminates the requirement of having additional optical
sources between each layer of the network. Moreover, the activation function is
reconfigurable via electrical bias, allowing it to be programmed or trained to
synthesize a variety of nonlinear responses. Using numerical simulations, we
demonstrate that this activation function significantly improves the
expressiveness of optical neural networks, allowing them to perform well on two
benchmark machine learning tasks: learning a multi-input exclusive-OR (XOR)
logic function and classification of images of handwritten numbers from the
MNIST dataset. The addition of the nonlinear activation function improves test
accuracy on the MNIST task from 85% to 94%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04595</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04595</id><created>2019-03-11</created><authors><author><keyname>Flores</keyname><forenames>V&#xed;ctor H.</forenames></author><author><keyname>Rivera</keyname><forenames>Mariano</forenames></author></authors><title>Computation of the phase step between two-step fringe patterns based on
  Gram--Schmidt algorithm</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the evaluation of a closed form formula for the calculation of the
original step between two randomly shifted fringe patterns. Our proposal
extends the Gram--Schmidt orthonormalization algorithm for fringe pattern.
Experimentally, the phase shift is introduced by a electro--mechanical devices
(such as piezoelectric or moving mounts).The estimation of the actual phase
step allows us to improve the phase shifting device calibration. The evaluation
consists of three cases that represent different pre-normalization processes:
First, we evaluate the accuracy of the method in the orthonormalization process
by estimating the test step using synthetic normalized fringe patterns with no
background, constant amplitude and different noise levels. Second, we evaluate
the formula with a variable amplitude function on the fringe patterns but with
no background. Third, we evaluate non-normalized noisy fringe patterns
including the comparison of pre-filtering processes such as the Gabor filter
banks and the isotropic normalization process, in order to emphasize how they
affect in the calculation of the phase step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04601</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04601</id><created>2019-02-11</created><authors><author><keyname>Perez</keyname><forenames>Daniel</forenames></author><author><keyname>Gasulla</keyname><forenames>Ivana</forenames></author><author><keyname>Capmany</keyname><forenames>Jose</forenames></author></authors><title>Reconfigurable integrated waveguide meshes for photonic signal
  processing and emerging applications</title><categories>physics.app-ph eess.SP physics.optics</categories><comments>Author copy</comments><journal-ref>Published in Photonics West 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review the recent advances reported in the field of integrated photonic
waveguide meshes, both from the theoretical as well as from the experimental
point of view. We show how these devices can be programmed to implement both
traditional signal processing structures, such as finite and infinite impulse
response filters, delay lines, beamforming networks as well as more advanced
linear matrix optics functionalities. Experimental results reported both in
Silicon and Silicon Nitride material platforms will be presented. We will also
discuss the main programming algorithms to implement these structures and
discuss their applications either as standalone systems or as part of more
elaborated subsystems in microwave photonics, quantum information and machine
learning
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04602</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04602</id><created>2019-02-11</created><authors><author><keyname>Perez</keyname><forenames>Daniel</forenames></author><author><keyname>Gasulla</keyname><forenames>Ivana</forenames></author><author><keyname>Capmany</keyname><forenames>Jose</forenames></author></authors><title>Programmable Multifuctional Photonics ICs</title><categories>physics.app-ph eess.SP</categories><comments>Author copy</comments><journal-ref>Paper presented at 2018 IEEE Photonics in switching conference,
  Cyprus</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we review present and future work in the next photonic IC generation
aiming at integration of multi-functional software-defined systems for signal
processing operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04651</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04651</id><created>2019-03-11</created><authors><author><keyname>Pillay</keyname><forenames>Ruven</forenames></author><author><keyname>Hardeberg</keyname><forenames>Jon Y</forenames></author><author><keyname>George</keyname><forenames>Sony</forenames></author></authors><title>Hyperspectral Calibration of Art: Acquisition and Calibration Workflows</title><categories>eess.IV cs.OH</categories><doi>10.1080/01971360.2018.1549919</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral imaging has become an increasingly used tool in the analysis of
works of art. However, the quality of the acquired data and the processing of
that data to produce accurate and reproducible spectral image cubes can be a
challenge to many cultural heritage users. The calibration of data that is both
spectrally and spatially accurate is an essential step in order to obtain
useful and relevant results from hyperspectral imaging. Data that is too noisy
or inaccurate will produce sub-optimal results when used for pigment mapping,
the detection of hidden features, change detection or for quantitative spectral
documentation. To help address this, therefore, we will examine the specific
acquisition and calibration workflows necessary for works of art. These
workflows includes the key parameters that must be addressed during acquisition
and the essential steps and issues at each of the stages required during
post-processing in order to fully calibrate hyperspectral data. In addition we
will look in detail at the key issues that affect data quality and propose
practical solutions that can make significant differences to overall
hyperspectral image quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04656</identifier>
 <datestamp>2019-06-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04656</id><created>2019-03-11</created><updated>2019-06-20</updated><authors><author><keyname>Arvinte</keyname><forenames>Marius</forenames></author><author><keyname>Tewfik</keyname><forenames>Ahmed H.</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Deep Log-Likelihood Ratio Quantization</title><categories>cs.LG eess.SP stat.ML</categories><comments>Accepted for publication at EUSIPCO 2019. Camera-ready version</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, a deep learning-based method for log-likelihood ratio (LLR)
lossy compression and quantization is proposed, with emphasis on a single-input
single-output uncorrelated fading communication setting. A deep autoencoder
network is trained to compress, quantize and reconstruct the bit log-likelihood
ratios corresponding to a single transmitted symbol. Specifically, the encoder
maps to a latent space with dimension equal to the number of sufficient
statistics required to recover the inputs - equal to three in this case - while
the decoder aims to reconstruct a noisy version of the latent representation
with the purpose of modeling quantization effects in a differentiable way.
Simulation results show that, when applied to a standard rate-1/2 low-density
parity-check (LDPC) code, a finite precision compression factor of nearly three
times is achieved when storing an entire codeword, with an incurred loss of
performance lower than 0.1 dB compared to straightforward scalar quantization
of the log-likelihood ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04659</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04659</id><created>2019-03-11</created><updated>2019-03-26</updated><authors><author><keyname>Colbert</keyname><forenames>Ian</forenames></author><author><keyname>Kreutz-Delgado</keyname><forenames>Ken</forenames></author><author><keyname>Das</keyname><forenames>Srinjoy</forenames></author></authors><title>AX-DBN: An Approximate Computing Framework for the Design of Low-Power
  Discriminative Deep Belief Networks</title><categories>eess.IV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power budget for embedded hardware implementations of Deep Learning
algorithms can be extremely tight. To address implementation challenges in such
domains, new design paradigms, like Approximate Computing, have drawn
significant attention. Approximate Computing exploits the innate
error-resilience of Deep Learning algorithms, a property that makes them
amenable for deployment on low-power computing platforms. This paper describes
an Approximate Computing design methodology, AX-DBN, for an architecture
belonging to the class of stochastic Deep Learning algorithms known as Deep
Belief Networks (DBNs). Specifically, we consider procedures for efficiently
implementing the Discriminative Deep Belief Network (DDBN), a stochastic neural
network which is used for classification tasks, extending Approximation
Computing from the analysis of deterministic to stochastic neural networks. For
the purpose of optimizing the DDBN for hardware implementations, we explore the
use of: (a)Limited precision of neurons and functional approximations of
activation functions; (b) Criticality analysis to identify nodes in the network
which can operate at reduced precision while allowing the network to maintain
target accuracy levels; and (c) A greedy search methodology with incremental
retraining to determine the optimal reduction in precision for all neurons to
maximize power savings. Using the AX-DBN methodology proposed in this paper, we
present experimental results across several network architectures that show
significant power savings under a user-specified accuracy loss constraint with
respect to ideal full precision implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04722</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04722</id><created>2019-03-12</created><authors><author><keyname>Oza</keyname><forenames>Manan</forenames></author><author><keyname>Vaghela</keyname><forenames>Himanshu</forenames></author><author><keyname>Srivastava</keyname><forenames>Kriti</forenames></author></authors><title>Progressive Generative Adversarial Binary Networks for Music Generation</title><categories>cs.SD cs.CV cs.LG eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent improvements in generative adversarial network (GAN) training
techniques prove that progressively training a GAN drastically stabilizes the
training and improves the quality of outputs produced. Adding layers after the
previous ones have converged has proven to help in better overall convergence
and stability of the model as well as reducing the training time by a
sufficient amount. Thus we use this training technique to train the model
progressively in the time and pitch domain i.e. starting from a very small time
value and pitch range we gradually expand the matrix sizes until the end result
is a completely trained model giving outputs having tensor sizes [4 (bar) x 96
(time steps) x 84 (pitch values) x 8 (tracks)]. As proven in previously
proposed models deterministic binary neurons also help in improving the
results. Thus we make use of a layer of deterministic binary neurons at the end
of the generator to get binary valued outputs instead of fractional values
existing between 0 and 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04740</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04740</id><created>2019-03-12</created><authors><author><keyname>You</keyname><forenames>Yuning</forenames></author><author><keyname>Lv</keyname><forenames>Gangming</forenames></author></authors><title>Sphere Bounding Scheme for Probabilistic Robust Constructive
  Interference Precoding in MISO Downlink Transmission</title><categories>eess.SP</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we propose a sphere bounding scheme for probabilistic robust
constructive interference (CI) power minimizing precoding, to address the
imperfect channel state information (CSI) caused by the channel error (CE),
which satisfies the known distribution in single-cell multiuser multipleinput
single-output (MISO) downlink transmission. In the proposed scheme, we
transform the probabilistic quality of service (QoS) constraints into tractable
sphere bounding second-order cone (SOC) constraints through taking two-step
tightening, and then we model tightened CI max-min signal-to-noise ratio (SNR)
precoding, proving that its lower bound can be solved through tightened CI
power minimizing precoding. Besides, in tightened CI power minimizing
precoding, we propose the relaxation iteration to relax the connect probability
requirement. Finally, we analyze the complexity of our proposed scheme.
Numerical results show that our proposed schemes perform well in the
satisfaction of the connect probability requirement, resulting in lower symbol
error rate (SER) and higher transmit power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04749</identifier>
 <datestamp>2019-05-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04749</id><created>2019-03-12</created><updated>2019-05-15</updated><authors><author><keyname>Rudsari</keyname><forenames>Hamid Khoshfekr</forenames></author><author><keyname>Orooji</keyname><forenames>Mahdi</forenames></author><author><keyname>Javan</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Mokari</keyname><forenames>Nader</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Non-Uniform BCSK Modulation in Nutrient-Limited Relay-Assisted Molecular
  Communication System: Optimization and Performance Evaluation</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel non-uniform Binary Concentration Shift Keying (BCSK)
modulation in the course of molecular communication is introduced. We consider
the nutrient limiting as the main reason for avoiding the nanotransmitters to
release huge number of molecules at once. The solution of this problem is in
the utilization of the BCSK modulation. In this scheme, nanotransmitter
releases the information molecules non-uniformly during the time slot. The
3-dimensional diffusion channel with 3-dimensional drift is considered in this
paper. To boost the bit error rate (BER) performance, we consider a
relay-assisted molecular communication via diffusion. Our computations
demonstrate how the pulse shape of BCSK modulation affects the BER, and we also
derive the energy consumption of non-uniform BCSK in the closed-form
expression. We study the parameters that can affect the BER performance, in
particular the distance between the nanotransmitter and the nanoreceiver, the
drift velocity of the medium, and the symbol duration. Furthermore, we propose
an optimization problem that is designed to find the optimal symbol duration
value that maximizes the number of successful received bits. The proposed
algorithm to solve the optimization problem is based on the bisection method.
The analytical results show that non-uniform BCSK modulation outperforms
uniform BCSK modulation in BER performance, when the aggregate energy is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04788</identifier>
 <datestamp>2020-01-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04788</id><created>2019-03-12</created><updated>2020-01-14</updated><authors><author><keyname>Gustafson</keyname><forenames>Carl</forenames></author><author><keyname>Mahler</keyname><forenames>Kim</forenames></author><author><keyname>Bolin</keyname><forenames>David</forenames></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames></author></authors><title>The COST IRACON Geometry-based Stochastic Channel Model for
  Vehicle-to-Vehicle Communication in Intersections</title><categories>eess.SP</categories><comments>Submitted to IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle-to-vehicle (V2V) wireless communications can improve traffic safety
at road intersections and enable congestion avoidance. However, detailed
knowledge about the wireless propagation channel is needed for the development
and realistic assessment of V2V communication systems. We present a novel
geometry-based stochastic MIMO channel model with support for frequencies in
the band of 5.2-6.2 GHz. The model is based on extensive high-resolution
measurements at different road intersections in the city of Berlin, Germany. We
extend existing models, by including the effects of various obstructions,
higher order interactions, and by introducing an angular gain function for the
scatterers. Scatterer locations have been identified and mapped to measured
multi-path trajectories using a measurement-based ray tracing method and a
subsequent RANSAC algorithm. The developed model is parameterized, and using
the measured propagation paths that have been mapped to scatterer locations,
model parameters are estimated. The time variant power fading of individual
multi-path components is found to be best modeled by a Gamma process with an
exponential autocorrelation. The path coherence distance is estimated to be in
the range of 0-2 m. The model is also validated against measurement data,
showing that the developed model accurately captures the behavior of the
measured channel gain, Doppler spread, and delay spread. This is also the case
for intersections that have not been used when estimating model parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04811</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04811</id><created>2019-03-12</created><authors><author><keyname>Routray</keyname><forenames>Sudhir K.</forenames></author><author><keyname>Mishra</keyname><forenames>Pallavi</forenames></author><author><keyname>Sarkar</keyname><forenames>Sutapa</forenames></author><author><keyname>Javali</keyname><forenames>Abhishek</forenames></author><author><keyname>Ramnath</keyname><forenames>Swathi</forenames></author></authors><title>Communication Bandwidth for Emerging Networks: Trends and Prospects</title><categories>eess.SP cs.NI</categories><comments>4 Pages, 2 figures, Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bandwidth is one of the essential resources for communication. Due to the
emergence of large number of new services in different types of communications
and their value added entities, the demand for bandwidth has gone up more than
ever before. The Internet and its allied services are one of the main users of
the global bandwidth. With this increasing demand, effective usage of the
available bandwidth and the discovery of new bands become very important. In
this article, we show current bandwidth usages and their utilities in different
application domains. We show the present trends of bandwidth used for global
communication by taking the international bandwidth of the core networks in to
account. We analyzed the bandwidth trends in optical and wireless communication
domains. Emerging services such as Internet of Things and their bandwidth
provisioning too have been discussed in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04812</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04812</id><created>2019-03-12</created><authors><author><keyname>Musarra</keyname><forenames>G.</forenames></author><author><keyname>Lyons</keyname><forenames>A.</forenames></author><author><keyname>Conca</keyname><forenames>E.</forenames></author><author><keyname>Altmann</keyname><forenames>Y.</forenames></author><author><keyname>Villa</keyname><forenames>F.</forenames></author><author><keyname>Zappa</keyname><forenames>F.</forenames></author><author><keyname>Padgett</keyname><forenames>M. J.</forenames></author><author><keyname>Faccio</keyname><forenames>D.</forenames></author></authors><title>Non-line-of-sight 3D imaging with a single-pixel camera</title><categories>physics.optics eess.IV</categories><comments>6 pages, 4 figures</comments><journal-ref>Phys. Rev. Applied 12, 011002 (2019)</journal-ref><doi>10.1103/PhysRevApplied.12.011002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real time, high resolution 3D reconstruction of scenes hidden from the direct
field of view is a challenging field of research with applications in real-life
situations related e.g. to surveillance, self-driving cars and rescue missions.
Most current techniques recover the 3D structure of a non-lineof-sight (NLOS)
static scene by detecting the return signal from the hidden object on a
scattering observation area. Here, we demonstrate the full colour retrieval of
the 3D shape of a hidden scene by coupling back-projection imaging algorithms
with the high-resolution time-of-flight information provided by a single-pixel
camera. By using a high effciency Single-Photon Avalanche Diode (SPAD)
detector, this technique provides the advantage of imaging with no mechanical
scanning parts, with acquisition times down to sub-seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04837</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04837</id><created>2019-03-12</created><authors><author><keyname>Routray</keyname><forenames>Sudhir K.</forenames></author><author><keyname>Mohanty</keyname><forenames>Sasmita</forenames></author></authors><title>Why 6G?</title><categories>eess.SP</categories><comments>5 Pages, 1 Figure, Research Article, Provides 6G basics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the 1980s, the world has witnessed new mobile generations every decade.
Each new generation is better than the previous in some ways. The recently
emerging generation, 5G has several advanced features. However, it is doubted
that there will be several short comings of this generation when compared with
the other contemporary ICT alternatives. These short comings are going to be
the main motivation for the next new mobile generation. According to the
existing trends, this new version will be known as the Sixth Generation of
Mobile Communication (6G). In this article, we show the main driving forces
behind 6G, its expected features and key technologies are also been discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04844</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04844</id><created>2019-03-12</created><authors><author><keyname>Routray</keyname><forenames>Sudhir</forenames></author><author><keyname>Javali</keyname><forenames>Abhishek</forenames></author><author><keyname>Sharma</keyname><forenames>Laxmi</forenames></author><author><keyname>Tengshe</keyname><forenames>Richa</forenames></author><author><keyname>Sarkar</keyname><forenames>Sutapa</forenames></author><author><keyname>Ghosh</keyname><forenames>Aritri</forenames></author></authors><title>Satellite Based IoT for MC Applications</title><categories>eess.SP cs.CY cs.NI</categories><comments>6 Pages, 1 Figure, Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, world has witnessed the ubiquitous applications of
Internet of things (IoT) for many different scenarios. There are several
critical applications where the results are essential and the mission has to be
successful at any cost. Such applications are well known as mission critical
applications. These applications are really critical and deal with very serious
situations such as disaster management, rescue operations and military
applications. IoT can provide both accuracy and sustainability in these
applications. IoT in fact, is suitable for several critical applications
because it can be deployed at locations where human presence is not possible
due to the dangers to human life. In such cases, collection of information can
be done through IoT sensors and it can be sent directly to the processing hubs.
These days we find several mission critical applications where both increased
reliability and coverage have very high priorities. Hybridization of IoT and
satellite networks can be a game changer in these applications. In this
article, we present the general features of mission critical IoT and the
motivation for connecting it with the satellite networks. Then we present the
main deployment related issues of these hybrid networks. We focused on the
hybridization aspects of narrowband IoT (NBIoT) with the satellite networks.
Because NBIoT has the energy efficiency which can make the satellite based IoT
networks sustainable in the long term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04846</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04846</id><created>2019-03-12</created><authors><author><keyname>P</keyname><forenames>Aswathylakshmi</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author></authors><title>QR Approximation for Massive MIMO Fronthaul Compression</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO's immense potential to serve large number of users at fast data
rates also comes with the caveat of requiring tremendous processing power. This
favours a centralized radio access network (C-RAN) architecture that
concentrates the processing power at a common baseband unit (BBU) connected to
multiple remote radio heads (RRH) via fronthaul links. The high bandwidths of
5G make the fronthaul data rate a major bottleneck. Since the number of active
users in a massive MIMO system is much smaller than the number of antennas, we
propose a dimension reduction scheme based on low rank approximation for
fronthaul data compression. Link level simulations show that the proposed
method achieves more than 17x compression while also improving the error
performance of the system through denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04850</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04850</id><created>2019-03-12</created><authors><author><keyname>Routray</keyname><forenames>Sudhir K.</forenames></author><author><keyname>Hussein</keyname><forenames>Habib Mohammed</forenames></author></authors><title>Narrowband IoT: An Appropriate Solution for Developing Countries</title><categories>eess.SP cs.NI</categories><comments>5 Pages, 3 Figures, Conference paper, IoT for developing countries</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of things (IoT) is very much attractive for several sensor based
applications. It provides large coverage of the services with small amount of
resources. Its applications span from the ordinary scenarios such as sensing in
the common digital ecosystem to the far more complicated processes of modern
manufacturing, agriculture, security provisioning, location tracking and health
care. Several types of IoTs have been proposed for the recent applications.
Narrowband IoT (NBIoT) is one of the economical versions of the IoTs. It is a
low power wide area network technology and thus suitable for resource limited
scenarios. In the developing countries, the resources are scarce and economical
solutions are always preferable. Therefore, NBIoT is an attractive solution for
the developing countries. In this article, we present its features and
functions which make it suitable for developing countries. We also provide
several sector based analysis which are suitable for the NBIoT deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04918</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04918</id><created>2019-03-10</created><authors><author><keyname>Chen</keyname><forenames>Mimi</forenames></author><author><keyname>Chen</keyname><forenames>Jiajun</forenames></author><author><keyname>Chen</keyname><forenames>Xiaojing</forenames></author><author><keyname>Zhang</keyname><forenames>Shunqing</forenames></author><author><keyname>Xu</keyname><forenames>Shugong</forenames></author></authors><title>A Deep Learning Based Resource Allocation Scheme in Vehicular
  Communication Systems</title><categories>eess.SP cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1903.00165</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In vehicular communications, intracell interference and the stringent latency
requirement are challenging issues. In this paper, a joint spectrum reuse and
power allocation problem is formulated for hybrid vehicle-to-vehicle (V2V) and
vehicle-to-infrastructure (V2I) communications. Recognizing the high capacity
and low-latency requirements for V2I and V2V links, respectively, we aim to
maximize the weighted sum of the capacities and latency requirement. By
decomposing the original problem into a classification subproblem and a
regression sub-problem, a convolutional neural network (CNN) based approach is
developed to obtain real-time decisions on spectrum reuse and power allocation.
Numerical results further demonstrate that the proposed CNN can achieve similar
performance as the Exhaustive method, while needs only 3.62% of its CPU
runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.04979</identifier>
 <datestamp>2019-03-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.04979</id><created>2019-03-12</created><authors><author><keyname>Liu</keyname><forenames>Qingchen</forenames></author><author><keyname>Drake</keyname><forenames>Samuel P.</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author></authors><title>Mapping Target Location from Doppler Data</title><categories>eess.SP</categories><comments>14 pages, 15 figures, journal. arXiv admin note: text overlap with
  arXiv:1504.04349 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an algorithm for determining a curve on the earth's
terrain on which a stationary emitter must lie according to a single Doppler
shift measured on an unmanned aerial vehicle (UAV) or a low earth orbit
satellite (LEOS). The mobile vehicle measures the Doppler shift and uses it to
build equations for a particular right circular cone according to the Doppler
shift and the vehicle's velocity, then determines a curve consisting of points
which represents the intersections of the cone with an ellipsoid that
approximately describes the earth's surface. The intersection points of the
cone with the ellipsoid are mapped into a digital terrain data set, namely
Digital Terrain Elevation Data (DTED), to generate the intersection points on
the earth's terrain. The work includes consideration of the possibility that
the rotation of the earth could affect the Doppler shift, and of the errors
resulting from the non-constant refractive index of the atmosphere and from
lack of precise knowledge of the transmitter frequency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05248</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05248</id><created>2019-03-12</created><authors><author><keyname>Noe</keyname><forenames>Reinhold</forenames></author><author><keyname>Koch</keyname><forenames>Benjamin</forenames></author></authors><title>Structure and Needed Properties of Reasonable Polarization Mode
  Dispersion Emulators for Coherent Optical Fiber Transmission</title><categories>eess.SP physics.optics</categories><comments>5 pages, 4 figures</comments><report-no>PMDE2019_Paderborn_Univ_Novoptel_n12.docx</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a scientifically reasonable polarization mode dispersion
(PMD) emulator (PMDE) for coherent optical fiber transmission. Guidelines are
physically correct modeling of the polarization-dispersive fiber, the
time-variable polarization transformations occurring in there, including
emulation of polarization events caused by lightning strikes, the adoption of
acceptable compromise to keep implementation cost low enough and competitive
industrial basis for production of such PMDE. We propse a PMDE consisting of N
differential group delay (DGD) sections placed between N+1 time-variable
general retarders or polarization scramblers. These should be general
elliptical retarders, capable of changing polarization with rates up to 20
Mrad/s on the Poincar\'e sphere. That should include bursts of polarization
rotations forth and back at up to 20 Mrad/s. The DGD sections can be fixed or
variable and should be able to constitute a total PMD of alternatively, say, 20
ps, 50 ps, 100 ps, 200 ps, or another set of various discrete values. We
propose even N and equal individual DGDs, which allows the total PMDE to assume
a neutral state without any PMD of whatever order. Number N may be chosen
relatively small; N = 2 seems acceptable. A variety of component and subsystem
suppliers is available, and the proposed PMDE is available on the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05289</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05289</id><created>2019-03-12</created><updated>2019-03-14</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Wu</keyname><forenames>Qingqing</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Accessing From The Sky: A Tutorial on UAV Communications for 5G and
  Beyond</title><categories>eess.SP cs.IT math.IT</categories><comments>Submitted to Proceedings of the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unmanned aerial vehicles (UAVs) have found numerous applications and are
expected to bring fertile business opportunities in the next decade. Among
various enabling technologies for UAVs, wireless communication is essential and
has drawn significantly growing attention in recent years. Compared to the
conventional terrestrial communications, UAVs' communications face new
challenges due to their high altitude above the ground and great flexibility of
movement in the three-dimensional (3D) space. Several critical issues arise,
including the line-of-sight (LoS) dominant UAV-ground channels and resultant
strong aerial-terrestrial network interference, the distinct communication
quality of service (QoS) requirements for UAV control messages versus payload
data, the stringent constraints imposed by the size, weight and power (SWAP)
limitations of UAVs, as well as the exploitation of the new design degree of
freedom (DoF) brought by the highly controllable 3D UAV mobility. In this
paper, we give a tutorial overview of the recent advances in UAV communications
to address the above issues, with an emphasis on how to integrate UAVs into the
forthcoming fifth-generation (5G) and future cellular networks. In particular,
we partition our discussions into two promising research and application
frameworks of UAV communications, namely UAV-assisted wireless communications
and cellular-connected UAVs,where UAVs serve as aerial communication platforms
and users, respectively. Furthermore, we point out promising directions for
future research and investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05291</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05291</id><created>2019-03-12</created><authors><author><keyname>Yazdani</keyname><forenames>Hassan</forenames></author><author><keyname>Vosoughi</keyname><forenames>Azadeh</forenames></author></authors><title>On the Spectrum Sensing, Beam Selection and Power Allocation in
  Cognitive Radio Networks Using Reconfigurable Antennas</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a cognitive radio (CR) system consisting of a
primary user (PU) and a pair of secondary user transmitter (SUtx) and secondary
user receiver (SUrx). The SUtx is equipped with a reconfigurable antenna (RA)
which divides the angular space into M sectors. The RA chooses one sector among
M sectors for its data transmission to SUrx. The SUtx first senses the channel
and monitors the activity of PU for a duration of T_{sen} seconds. We refer to
this period as channel sensing phase. Depending on the outcome of this phase,
SUtx stays in this phase or enters the next phase, which we refer to as
transmission phase. The transmission phase itself consists of two phases:
channel training phase followed by data transmission phase. During the former
phase, SUtx sends pilot symbols to enable channel training and estimation at
SUrx. The SUrx selects the best beam (sector) for data transmission and feeds
back the index of the selected beam as well as its corresponding channel gain.
We also derive the probability of determining the true beam and take into
account this probability in our system design. During the latter phase, SUtx
sends data symbols to SUrx over the selected beam with constant power {\Phi} if
the gain corresponding to the selected beam is bigger than the threshold
{\zeta}. We find the optimal channel sensing duration T_{sen}, the optimal
power level {\Phi} and a optimal threshold {\zeta}, such that the ergodic
capacity of CR system is maximized, subject to average interference and power
constraints. In addition, we derive closed form expressions for outage and
symbol error probabilities of our CR system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05299</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05299</id><created>2019-03-12</created><updated>2019-04-28</updated><authors><author><keyname>Wu</keyname><forenames>Minhua</forenames></author><author><keyname>Kumatani</keyname><forenames>Kenichi</forenames></author><author><keyname>Sundaram</keyname><forenames>Shiva</forenames></author><author><keyname>Strom</keyname><forenames>Nikko</forenames></author><author><keyname>Hoffmeister</keyname><forenames>Bjorn</forenames></author></authors><title>Frequency Domain Multi-channel Acoustic Modeling for Distant Speech
  Recognition</title><categories>eess.AS cs.SD</categories><comments>ICASSP 2019, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional far-field automatic speech recognition (ASR) systems typically
employ microphone array techniques for speech enhancement in order to improve
robustness against noise or reverberation. However, such speech enhancement
techniques do not always yield ASR accuracy improvement because the
optimization criterion for speech enhancement is not directly relevant to the
ASR objective. In this work, we develop new acoustic modeling techniques that
optimize spatial filtering and long short-term memory (LSTM) layers from
multi-channel (MC) input based on an ASR criterion directly. In contrast to
conventional methods, we incorporate array processing knowledge into the
acoustic model. Moreover, we initialize the network with beamformers'
coefficients. We investigate effects of such MC neural networks through ASR
experiments on the real-world far-field data where users are interacting with
an ASR system in uncontrolled acoustic environments. We show that our MC
acoustic model can reduce a word error rate (WER) by~16.5\% compared to a
single channel ASR system with the traditional log-mel filter bank energy
(LFBE) feature on average. Our result also shows that our network with the
spatial filtering layer on two-channel input achieves a relative WER reduction
of~9.5\% compared to conventional beamforming with seven microphones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05316</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05316</id><created>2019-03-13</created><authors><author><keyname>Liu</keyname><forenames>Shangqing</forenames></author><author><keyname>Zhao</keyname><forenames>Yanchao</forenames></author><author><keyname>Xue</keyname><forenames>Fanggang</forenames></author><author><keyname>Chen</keyname><forenames>Bing</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author></authors><title>DeepCount: Crowd Counting with WiFi via Deep Learning</title><categories>cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the research of wireless sensing has achieved more intelligent
results, and the intelligent sensing of human location and activity can be
realized by means of WiFi devices. However, most of the current human
environment perception work is limited to a single person's environment,
because the environment in which multiple people exist is more complicated than
the environment in which a single person exists. In order to solve the problem
of human behavior perception in a multi-human environment, we first proposed a
solution to achieve crowd counting (inferred population) using deep learning in
a closed environment with WIFI signals - DeepCout, which is the first in a
multi-human environment. step. Since the use of WiFi to directly count the
crowd is too complicated, we use deep learning to solve this problem, use
Convolutional Neural Network(CNN) to automatically extract the relationship
between the number of people and the channel, and use Long Short Term
Memory(LSTM) to resolve the dependencies of number of people and Channel State
Information(CSI) . To overcome the massive labelled data required by deep
learning method, we add an online learning mechanism to determine whether or
not someone is entering/leaving the room by activity recognition model, so as
to correct the deep learning model in the fine-tune stage, which, in turn,
reduces the required training data and make our method evolving over time. The
system of DeepCount is performed and evaluated on the commercial WiFi devices.
By massive training samples, our end-to-end learning approach can achieve an
average of 86.4% prediction accuracy in an environment of up to 5 people.
Meanwhile, by the amendment mechanism of the activity recognition model to
judge door switch to get the variance of crowd to amend deep learning predicted
results, the accuracy is up to 90%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05379</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05379</id><created>2019-03-13</created><authors><author><keyname>Ancora</keyname><forenames>Daniele</forenames></author><author><keyname>Leuzzi</keyname><forenames>Luca</forenames></author></authors><title>Transmission Matrix Inference via Pseudolikelihood Decimation</title><categories>stat.ML cond-mat.dis-nn cs.LG eess.IV physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest challenges in the field of biomedical imaging is the
comprehension and the exploitation of the photon scattering through disordered
media. Many studies have pursued the solution to this puzzle, achieving
light-focusing control or reconstructing images in complex media. In the
present work, we investigate how statistical inference helps the calculation of
the transmission matrix in a complex scrambling environment, enabling its usage
like a normal optical element. We convert a linear input-output transmission
problem into a statistical formulation based on pseudolikelihood maximization,
learning the coupling matrix via random sampling of intensity realizations. Our
aim is to uncover insights from the scattering problem, encouraging the
development of novel imaging techniques for better medical investigations,
borrowing a number of statistical tools from spin-glass theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05408</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05408</id><created>2019-03-13</created><authors><author><keyname>Rajendran</keyname><forenames>Sreeraj</forenames></author><author><keyname>Lenders</keyname><forenames>Vincent</forenames></author><author><keyname>Meert</keyname><forenames>Wannes</forenames></author><author><keyname>Pollin</keyname><forenames>Sofie</forenames></author></authors><title>Crowdsourced wireless spectrum anomaly detection</title><categories>eess.SP cs.NI</categories><comments>IEEE: under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated wireless spectrum monitoring across frequency, time and space will
be essential for many future applications. Manual and fine-grained spectrum
analysis is becoming impossible because of the large number of measurement
locations and complexity of the spectrum use landscape. Detecting unexpected
behaviors in the wireless spectrum from the collected data is a crucial part of
this automated monitoring, and the control of detected anomalies is a key
functionality to enable interaction between the automated system and the end
user. In this paper we look into the wireless spectrum anomaly detection
problem for crowdsourced sensors. We first analyze in detail the nature of
these anomalies and design effective algorithms to bring the higher dimensional
input data to a common feature space across sensors. Anomalies can then be
detected as outliers in this feature space. In addition, we investigate the
importance of user feedback in the anomaly detection process to improve the
performance of unsupervised anomaly detection. Furthermore, schemes for
generalizing user feedback across sensors are also developed to close the
anomaly detection loop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05460</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05460</id><created>2019-03-12</created><authors><author><keyname>Restuccia</keyname><forenames>Francesco</forenames></author><author><keyname>Melodia</keyname><forenames>Tommaso</forenames></author></authors><title>Big Data Goes Small: Real-Time Spectrum-Driven Embedded Wireless
  Networking Through Deep Learning in the RF Loop</title><categories>cs.NI cs.LG eess.SP</categories><comments>Accepted to IEEE INFOCOM 2019. arXiv admin note: text overlap with
  arXiv:1901.07947</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosion of 5G networks and the Internet of Things will result in an
exceptionally crowded RF environment, where techniques such as spectrum sharing
and dynamic spectrum access will become essential components of the wireless
communication process. In this vision, wireless devices must be able to (i)
learn to autonomously extract knowledge from the spectrum on-the-fly; and (ii)
react in real time to the inferred spectrum knowledge by appropriately changing
communication parameters, including frequency band, symbol modulation, coding
rate, among others. Traditional CPU-based machine learning suffers from high
latency, and requires application-specific and computationally-intensive
feature extraction/selection algorithms. In this paper, we present RFLearn, the
first system enabling spectrum knowledge extraction from unprocessed I/Q
samples by deep learning directly in the RF loop. RFLearn provides (i) a
complete hardware/software architecture where the CPU, radio transceiver and
learning/actuation circuits are tightly connected for maximum performance; and
(ii) a learning circuit design framework where the latency vs. hardware
resource consumption trade-off can explored. We implement and evaluate the
performance of RFLearn on custom software-defined radio built on a
system-on-chip (SoC) ZYNQ-7000 device mounting AD9361 radio transceivers and
VERT2450 antennas. We showcase the capabilities of RFLearn by applying it to
solving the fundamental problems of modulation and OFDM parameter recognition.
Experimental results reveal that RFLearn decreases latency and power by about
17x and 15x with respect to a software-based solution, with a comparatively low
hardware resource consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05464</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05464</id><created>2019-03-12</created><authors><author><keyname>Ahmed</keyname><forenames>Ali</forenames></author><author><keyname>Hameed</keyname><forenames>Humera</forenames></author></authors><title>Channel Protection using Random Modulation</title><categories>eess.SP</categories><comments>5 pages, 2 figures, to be published in ICASSP 2019. arXiv admin note:
  substantial text overlap with arXiv:1811.08453</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that modulation protects a bandlimited signal against
convolutive interference. A signal $s(t)$, bandlimited to $B$Hz, is modulated
(pointwise multiplied) with a known random sign sequence $r(t)$, alternating at
a rate $Q$, and the resultant \textit{spread spectrum} signal $s(t) \odot r(t)$
is convolved against an $M$-tap channel impulse response $h(t)$ to yield the
observed signal $y(t)= (s(t)\odot r(t))\circledast h(t),$ where $\odot$ and
$\circledast$ denote pointwise multiplication, and circular convolution,
respectively.
  We show that both $s(t)$, and $h(t)$ can be provably recovered using a simple
gradient descent scheme by alternating the binary waveform $r(t)$ at a rate $Q
\gtrsim B + M$(to within log factors and a signal coherences) and sampling
$y(t)$ at a rate $Q$. We also present a comprehensive set of phase transitions
to depict the trade-off between $Q$, $M$, and $B$ for successful recovery.
Moreover, we show stable recovery results under noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05481</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05481</id><created>2019-03-13</created><authors><author><keyname>Rached</keyname><forenames>Nadhir Ben</forenames></author><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Tempone</keyname><forenames>Raul</forenames></author></authors><title>An Accurate Sample Rejection Estimator for the Estimation of Outage
  Probability of EGC Receivers</title><categories>eess.SP stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we evaluate the outage probability (OP) for L-branch equal gain
combining (EGC) diversity receivers operating over fading channels, i.e.
equivalently the cumulative distribution function (CDF) of the sum of the L
channel envelopes. In general, closed form expressions of OP values are
unobtainable. The use of Monte Carlo (MC) simulations is not considered a good
alternative as it requires a large number of samples for small values of OP,
making MC simulations very expensive. In this paper, we use the concept of
importance sampling (IS), being known to yield accurate estimates using fewer
simulation runs. Our proposed IS scheme is essentially based on sample
rejection where the IS probability density function (PDF) is the truncation of
the underlying PDF over the L dimensional sphere. It assumes the knowledge of
the CDF of the sum of the L channel gains in a closed-form expression. Such an
assumption is not restrictive since it holds for various challenging fading
models. We apply our approach to the case of independent Rayleigh, correlated
Rayleigh, and independent and identically distributed Rice fading models. Next,
we extend our approach to the interesting scenario of generalised selection
combining receivers combined with EGC under the independent Rayleigh fading
environment. For each case, we prove the desired bounded relative error
property. Finally, we validate these theoretical results through some selected
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05525</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05525</id><created>2019-03-13</created><authors><author><keyname>Kigka</keyname><forenames>Vassiliki I.</forenames></author><author><keyname>Rigas</keyname><forenames>George</forenames></author><author><keyname>Sakellarios</keyname><forenames>Antonis</forenames></author><author><keyname>Panagiotis</keyname><forenames>Siogkas</forenames></author><author><keyname>Andrikos</keyname><forenames>Ioannis O.</forenames></author><author><keyname>Exarchos</keyname><forenames>Themis P.</forenames></author><author><keyname>Loggitsi</keyname><forenames>Dimitra</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Constantinos D.</forenames></author><author><keyname>Michalis</keyname><forenames>Lampros K.</forenames></author><author><keyname>Neglia</keyname><forenames>Danilo</forenames></author><author><keyname>Pelosi</keyname><forenames>Gualtriero</forenames></author><author><keyname>Parodi</keyname><forenames>Oberdan</forenames></author><author><keyname>Fotiadis</keyname><forenames>Dimitrios I.</forenames></author></authors><title>3D Reconstruction of Coronary Arteries and Atherosclerotic Plaques based
  on Computed Tomography Angiography Images</title><categories>eess.IV physics.med-ph</categories><comments>This is the pre-final version. The final version must be cited using
  the DOI: 10.1016/j.bspc.2017.09.009. \c{opyright} &lt;2018&gt;. This manuscript
  version is made available under the CC-BY-NC-ND 4.0 license:
  http://creativecommons.org/licenses/by-nc-nd/4.0/</comments><journal-ref>Biomedical Signal Processing and Control, Volume 40, February
  2018, Pages 286-294</journal-ref><doi>10.1016/j.bspc.2017.09.009</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The purpose of this study is to present a new semi-automated methodology for
three-dimensional (3D) reconstruction of coronary arteries and their plaque
morphology using Computed Tomography Angiography (CTA) images. The methodology
is summarized in seven stages: pre-processing of the acquired CTA images,
extraction of the vessel tree centerline, estimation of a weight function for
lumen, outer wall and calcified plaque, lumen segmentation, outer wall
segmentation, plaque detection, and finally 3D surfaces construction. The
methodology was evaluated using both expert manual annotations and estimations
of a recently presented Intravascular Ultrasound (IVUS) reconstruction method.
As far as the manual annotation validation process is concerned, the mean value
of the comparison metrics for the 3D segmentation were 0.749 and 1.746 for the
Dice coefficient and Hausdorff distance, respectively. On the other hand, the
correlation coefficients for the degree of stenosis 1, the degree of stenosis
2, the plaque burden, the minimal lumen area and the minimal lumen diameter,
when comparing the derived from the proposed methodology 3D models with the
IVUS reconstructed models, were 0.79, 0.77, 0.75, 0.85, 0.81, respectively. The
proposed methodology is an innovative approach for reconstruction of coronary
arteries, since it provides 3D models of the lumen, the outer wall and the CP
plaques, using the minimal user interaction. Its first implementation
demonstrated that it provides an accurate reconstruction of coronary arteries
and thus, it may have a wide clinical applicability
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05600</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05600</id><created>2019-03-13</created><authors><author><keyname>Masuyama</keyname><forenames>Yoshiki</forenames></author><author><keyname>Yatabe</keyname><forenames>Kohei</forenames></author><author><keyname>Oikawa</keyname><forenames>Yasuhiro</forenames></author></authors><title>Phase-aware Harmonic/Percussive Source Separation via Convex
  Optimization</title><categories>eess.AS cs.SD eess.SP</categories><comments>5 pages, to appear in IEEE ICASSP 2019 (Paper Code: AASP-P16.5,
  Session: Music Signal Analysis, Feedback and Echo Cancellation and
  Equalization)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decomposition of an audio mixture into harmonic and percussive components,
namely harmonic/percussive source separation (HPSS), is a useful pre-processing
tool for many audio applications. Popular approaches to HPSS exploit the
distinctive source-specific structures of power spectrograms. However, such
approaches consider only power spectrograms, and the phase remains intact for
resynthesizing the separated signals. In this paper, we propose a phase-aware
HPSS method based on the structure of the phase of harmonic components. It is
formulated as a convex optimization problem in the time domain, which enables
the simultaneous treatment of both amplitude and phase. The numerical
experiment validates the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05603</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05603</id><created>2019-03-13</created><authors><author><keyname>Masuyama</keyname><forenames>Yoshiki</forenames></author><author><keyname>Yatabe</keyname><forenames>Kohei</forenames></author><author><keyname>Oikawa</keyname><forenames>Yasuhiro</forenames></author></authors><title>Low-rankness of Complex-valued Spectrogram and Its Application to
  Phase-aware Audio Processing</title><categories>eess.AS cs.SD eess.SP</categories><comments>5 pages, to appear in IEEE ICASSP 2019 (Paper Code: AASP-P13.9,
  Session: Acoustic Scene Classification and Music Signal Analysis)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rankness of amplitude spectrograms has been effectively utilized in audio
signal processing methods including non-negative matrix factorization. However,
such methods have a fundamental limitation owing to their amplitude-only
treatment where the phase of the observed signal is utilized for resynthesizing
the estimated signal. In order to address this limitation, we directly treat a
complex-valued spectrogram and show a complex-valued spectrogram of a sum of
sinusoids can be approximately low-rank by modifying its phase. For evaluating
the applicability of the proposed low-rank representation, we further propose a
convex prior emphasizing harmonic signals, and it is applied to audio
denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05636</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05636</id><created>2019-03-13</created><authors><author><keyname>Manshouri</keyname><forenames>Negin</forenames></author><author><keyname>Kayikcioglu</keyname><forenames>Temel</forenames></author></authors><title>A Comprehensive Analysis of 2D&amp;3D Video Watching of EEG Signals by
  Increasing PLSR and SVM Classification Results</title><categories>cs.HC cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the development of two and three dimensional (2D&amp;3D) technology, it
has attracted the attention of researchers in recent years. This research is
done to reveal the detailed effects of 2D in comparison with 3D technology on
the human brain waves. The impact of 2D&amp;3D video watching using
electroencephalography (EEG) brain signals is studied. A group of eight healthy
volunteers with the average age of 31+-3.06 years old participated in this
three-stage test. EEG signal recording consisted of three stages: After a bit
of relaxation (a), a 2D video was displayed (b), the recording of the signal
continued for a short period of time as rest (c), and finally the trial ended.
Exactly the same steps were repeated for the 3D video. Power spectrum density
(PSD) based on short time Fourier transform (STFT) was used to analyze the
brain signals of 2D&amp;3D video viewers. After testing all the EEG frequency
bands, delta and theta were extracted as the features. Partial least squares
regression (PLSR) and Support vector machine (SVM) classification algorithms
were considered in order to classify EEG signals obtained as the result of
2D&amp;3D video watching. Successful classification results were obtained by
selecting the correct combinations of effective channels representing the brain
regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05794</identifier>
 <datestamp>2020-01-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05794</id><created>2019-03-13</created><updated>2019-03-19</updated><authors><author><keyname>Liu</keyname><forenames>Zhenwei</forenames></author><author><keyname>Saberi</keyname><forenames>Ali</forenames></author><author><keyname>Stoorvogel</keyname><forenames>Anton A.</forenames></author><author><keyname>Li</keyname><forenames>Rong</forenames></author></authors><title>Delayed state synchronization of continuous-time multi-agent systems in
  the presence of unknown communication delays (including complete proofs)</title><categories>cs.SY eess.SP eess.SY</categories><comments>Accepted by the 31th CCDC held by Nanchang, China, June 3rd-5th, 2019</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper studies delayed synchronization of continuous-time multi-agent
systems (MAS) in the presence of unknown nonuniform communication delays. A
delay-free transformation is developed based on a communication network which
is a directed spanning tree, which can transform the original MAS to a new one
without delays. By using this transformation, we design a static protocol for
full-state coupling and a dynamic protocol for delayed state synchronization
for homogeneous MAS via full- and partial-state coupling. Meanwhile, the
delayed output synchronization is also studied for heterogeneous MAS, which is
achieved by using a low-gain and output regulation based dynamic protocol
design via the delay-free transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05808</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05808</id><created>2019-03-14</created><authors><author><keyname>Liu</keyname><forenames>Binghong</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author></authors><title>Joint Resource Block-Power Allocation for NOMA-Enabled Fog Radio Access
  Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to achieve efficient communication in the fifth generation (5G)
networks, non-orthogonal multiple access (NOMA) technique has been utilized in
fog radio access networks (F-RANs). In this paper, we investigate the resource
allocation problem in a NOMA-enabled downlink F-RAN. To maximize the weighted
sum rate of NOMA users served by fog-computing-based access points (F-APs), the
resource block (RB) allocation and power allocation are optimized.
Specifically, we decouple the problem into RB allocation and power allocation
problems. The former is modeled as a many-to-one matching game and we propose a
modified swap-enabled matching algorithm to solve it, which takes interference
threshold into consideration. The later is a non-convex problem, we transform
it into a tractable one via some approximations and get the closed-form
expressions of power allocation coefficients. Finally, we combine the both to
propose a joint resource allocation algorithm, which is preformed iteratively
to obtain the optimal result. Simulation results are provided to show the
performance of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05820</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05820</id><created>2019-03-14</created><authors><author><keyname>Zhao</keyname><forenames>Tongtong</forenames></author><author><keyname>Yan</keyname><forenames>Yuxiao</forenames></author><author><keyname>Shehu</keyname><forenames>Ibrahim Shehi</forenames></author><author><keyname>Fu</keyname><forenames>Xianping</forenames></author><author><keyname>Wang</keyname><forenames>Huibing</forenames></author></authors><title>Purifying Naturalistic Images through a Real-time Style Transfer
  Semantics Network</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the progress of learning-by-synthesis has proposed a training model
for synthetic images, which can effectively reduce the cost of human and
material resources. However, due to the different distribution of synthetic
images compared to real images, the desired performance cannot still be
achieved. Real images consist of multiple forms of light orientation, while
synthetic images consist of a uniform light orientation. These features are
considered to be characteristic of outdoor and indoor scenes, respectively. To
solve this problem, the previous method learned a model to improve the realism
of the synthetic image. Different from the previous methods, this paper takes
the first step to purify real images. Through the style transfer task, the
distribution of outdoor real images is converted into indoor synthetic images,
thereby reducing the influence of light. Therefore, this paper proposes a
real-time style transfer network that preserves image content information (eg,
gaze direction, pupil center position) of an input image (real image) while
inferring style information (eg, image color structure, semantic features) of
style image (synthetic image). In addition, the network accelerates the
convergence speed of the model and adapts to multi-scale images. Experiments
were performed using mixed studies (qualitative and quantitative) methods to
demonstrate the possibility of purifying real images in complex directions.
Qualitatively, it compares the proposed method with the available methods in a
series of indoor and outdoor scenarios of the LPW dataset. In quantitative
terms, it evaluates the purified image by training a gaze estimation model on
the cross data set. The results show a significant improvement over the
baseline method compared to the raw real image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05904</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05904</id><created>2019-03-14</created><authors><author><keyname>Choi</keyname><forenames>Hayoung</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author></authors><title>Large-Scale Beamforming for Massive MIMO via Randomized Sketching</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO system yields significant improvements in spectral and energy
efficiency for future wireless communication systems. The regularized
zero-forcing (RZF) beamforming is able to provide good performance with the
capability of achieving numerical stability and robustness to the channel
uncertainty. However, in massive MIMO systems, the matrix inversion operation
in RZF beamforming becomes computationally expensive. To address this
computational issue, we shall propose a novel randomized sketching based RZF
beamforming approach with low computational latency. This is achieved by
solving a linear system via randomized sketching based on the preconditioned
Richard iteration, which guarantees high quality approximations to the optimal
solution. We theoretically prove that the sequence of approximations obtained
iteratively converges to the exact RZF beamforming matrix linearly fast as the
number of iterations increases. Also, it turns out that the system sum-rate for
such sequence of approximations converges to the exact one at a linear
convergence rate. Our simulation results verify our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05955</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05955</id><created>2019-03-14</created><authors><author><keyname>Bollepalli</keyname><forenames>Bajibabu</forenames></author><author><keyname>Juvela</keyname><forenames>Lauri</forenames></author><author><keyname>Alku</keyname><forenames>Paavo</forenames></author></authors><title>Generative adversarial network-based glottal waveform model for
  statistical parametric speech synthesis</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>Accepted in Interspeech</comments><journal-ref>Interspeech-2017</journal-ref><doi>10.21437/Interspeech.2017-1288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have shown that text-to-speech synthesis quality can be
improved by using glottal vocoding. This refers to vocoders that parameterize
speech into two parts, the glottal excitation and vocal tract, that occur in
the human speech production apparatus. Current glottal vocoders generate the
glottal excitation waveform by using deep neural networks (DNNs). However, the
squared error-based training of the present glottal excitation models is
limited to generating conditional average waveforms, which fails to capture the
stochastic variation of the waveforms. As a result, shaped noise is added as
post-processing. In this study, we propose a new method for predicting glottal
waveforms by generative adversarial networks (GANs). GANs are generative models
that aim to embed the data distribution in a latent space, enabling generation
of new instances very similar to the original by randomly sampling the latent
distribution. The glottal pulses generated by GANs show a stochastic component
similar to natural glottal pulses. In our experiments, we compare synthetic
speech generated using glottal waveforms produced by both DNNs and GANs. The
results show that the newly proposed GANs achieve synthesis quality comparable
to that of widely-used DNNs, without using an additive noise component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.05969</identifier>
 <datestamp>2019-12-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.05969</id><created>2019-03-14</created><updated>2019-12-16</updated><authors><author><keyname>Rajanna</keyname><forenames>Amogh</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author></authors><title>Rate Statistics in Cellular Downlink: A Per-User Analysis of Rateless
  Coded Transmission</title><categories>cs.IT eess.SP math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we focus on rateless coded adaptive transmission in a
cellular downlink. Based on a stochastic geometry model for the locations of
BSs, we evaluate the meta-distribution of rate, i.e., the distribution of rate
conditioned on the point process. An accurate approximation to the distribution
of the per-user rate is proposed and clearly shown to provide a good match to
the simulation results. We illustrate the gain in the per-user rate due to
physical layer rateless codes relative to the fixed-rate adaptive modulation
and coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06007</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06007</id><created>2019-03-11</created><authors><author><keyname>Bautista</keyname><forenames>Esteban</forenames></author><author><keyname>Abry</keyname><forenames>Patrice</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Paulo</forenames></author></authors><title>$L^\gamma$-PageRank for Semi-Supervised Learning</title><categories>cs.SI cs.LG eess.SP stat.ML</categories><comments>Submitted to Applied Network Science (special issue on machine
  learning with graphs)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PageRank for Semi-Supervised Learning has shown to leverage data structures
and limited tagged examples to yield meaningful classification. Despite
successes, classification performance can still be improved, particularly in
cases of fuzzy graphs or unbalanced labeled data. To address such limitations,
a novel approach based on powers of the Laplacian matrix $L^\gamma$ ($\gamma &gt;
0$), referred to as $L^\gamma$-PageRank, is proposed. Its theoretical study
shows that it operates on signed graphs, where nodes belonging to one same
class are more likely to share positive edges while nodes from different
classes are more likely to be connected with negative edges. It is shown that
by selecting an optimal $\gamma$, classification performance can be
significantly enhanced. A procedure for the automated estimation of the optimal
$\gamma$, from a unique observation of data, is devised and assessed.
Experiments on several datasets demonstrate the effectiveness of both
$L^\gamma$-PageRank classification and the optimal $\gamma$ estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06008</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06008</id><created>2019-03-12</created><authors><author><keyname>Montecchio</keyname><forenames>Nicola</forenames></author><author><keyname>Roy</keyname><forenames>Pierre</forenames></author><author><keyname>Pachet</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>The Skipping Behavior of Users of Music Streaming Services and its
  Relation to Musical Structure</title><categories>cs.IR cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The behavior of users of music streaming services is investigated from the
point of view of the temporal dimension of individual songs; specifically, the
main object of the analysis is the point in time within a song at which users
stop listening and start streaming another song (&quot;skip&quot;). The main contribution
of this study is the ascertainment of a correlation between the distribution in
time of skipping events and the musical structure of songs. It is also shown
that such distribution is not only specific to the individual songs, but also
independent of the cohort of users and, under stationary conditions, date of
observation. Finally, user behavioral data is used to train a predictor of the
musical structure of a song solely from its acoustic content; it is shown that
the use of such data, available in large quantities to music streaming
services, yields significant improvements in accuracy over the customary
fashion of training this class of algorithms, in which only smaller amounts of
hand-labeled data are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06009</identifier>
 <datestamp>2019-05-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06009</id><created>2019-03-14</created><updated>2019-05-29</updated><authors><author><keyname>Sato</keyname><forenames>Issei</forenames></author></authors><title>On Learning from Ghost Imaging without Imaging</title><categories>eess.IV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational ghost imaging is an imaging technique in which an object is
imaged from light collected using a single-pixel detector with no spatial
resolution. Recently, ghost cytometry has been proposed for a high-speed
cell-classification method that involves ghost imaging and machine learning in
flow cytometry. Ghost cytometry skips the reconstruction of cell images from
signals and directly used signals for cell-classification because this
reconstruction is what creates the bottleneck in the high-speed analysis. In
this paper, we provide theoretical analysis for learning from ghost imaging
without imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06031</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06031</id><created>2019-03-14</created><authors><author><keyname>Schymura</keyname><forenames>Christopher</forenames></author><author><keyname>Kolossa</keyname><forenames>Dorothea</forenames></author></authors><title>Audiovisual Speaker Tracking using Nonlinear Dynamical Systems with
  Dynamic Stream Weights</title><categories>cs.CV cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data fusion plays an important role in many technical applications that
require efficient processing of multimodal sensory observations. A prominent
example is audiovisual signal processing, which has gained increasing attention
in automatic speech recognition, speaker localization and related tasks. If
appropriately combined with acoustic information, additional visual cues can
help to improve the performance in these applications, especially under adverse
acoustic conditions. A dynamic weighting of acoustic and visual streams based
on instantaneous sensor reliability measures is an efficient approach to data
fusion in this context. This paper presents a framework that extends the
well-established theory of nonlinear dynamical systems with the notion of
dynamic stream weights for an arbitrary number of sensory observations. It
comprises a recursive state estimator based on the Gaussian filtering paradigm,
which incorporates dynamic stream weights into a framework closely related to
the extended Kalman filter. Additionally, a convex optimization approach to
estimate oracle dynamic stream weights in fully observed dynamical systems
utilizing a Dirichlet prior is presented. This serves as a basis for a generic
parameter learning framework of dynamic stream weight estimators. The proposed
system is application-independent and can be easily adapted to specific tasks
and requirements. A study using audiovisual speaker tracking tasks is
considered as an exemplary application in this work. An improved tracking
performance of the dynamic stream weight-based estimation framework over
state-of-the-art methods is demonstrated in the experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06046</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06046</id><created>2019-03-13</created><authors><author><keyname>Carbonell-Leal</keyname><forenames>Miguel</forenames></author><author><keyname>Mendoza-Yero</keyname><forenames>Omel</forenames></author></authors><title>Encoding complex fields by using a phase-only optical element:
  mitigation of pixel crosstalk effects</title><categories>eess.IV physics.optics</categories><comments>5 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this letter we report on the effects of pixel crosstalk on the
experimental realization of a reported encoding method (Opt. Lett. 39, 1740
(2014)) with PA-LCoS SLMs. We found that, under Nyquist limit condition, about
70% of a single pixel cell can generate unexpected phase modulation. In order
to approach uniform phase modulation, and consequently improve the quality of
measured amplitude and phase images, a generalized sampling scheme is proposed.
To corroborate our proposal, proper experiments were carried out. On this
point, a particular implementation of the well-established phase shifting
technique allows us to measure the retrieved complex field by using just a
single camera.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06120</identifier>
 <datestamp>2019-12-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06120</id><created>2019-03-11</created><updated>2019-12-23</updated><authors><author><keyname>Bharati</keyname><forenames>Alok Kumar</forenames></author><author><keyname>Ajjarapu</keyname><forenames>Venkataramana</forenames></author><author><keyname>Wang</keyname><forenames>Zhaoyu</forenames></author></authors><title>Counterintuitive VSM Behavior under CVR Incorporating Distribution
  System</title><categories>eess.SP cs.SY</categories><comments>3 pages, 5 figures, 1 tables, Submitted to IEEE Power Engineering
  Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyses the impact of conservation by voltage reduction (CVR) on
voltage stability margin (VSM) considering transmission and distribution (T&amp;D)
systems. VSM is determined by P-V curve analysis using PSSE and GridLAB-D
solvers to co-simulate the T&amp;D systems under CVR and No CVR conditions. ZIP
loads with profile [ZIP] = [0.4 0.3 0.3] are used to model the load. The paper
discusses the counterintuitive result: under CVR, the VSM is reduced.
Theoretical justification for the reduced VSM under CVR is the increase in the
effective impedance between generation and load and this is proved using an
extended 2-bus system. The paper shares T&amp;D co-simulation results with IEEE
9-bus transmission system and a larger 123-bus distribution system and with
distributed generation (DG) in unity power factor (UPF) and volt-VAR control
(VVC) mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06121</identifier>
 <datestamp>2019-03-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06121</id><created>2019-03-13</created><authors><author><keyname>Manshouri</keyname><forenames>Negin</forenames></author><author><keyname>Maleki</keyname><forenames>Masoud</forenames></author><author><keyname>Kayikcioglu</keyname><forenames>Temel</forenames></author></authors><title>An EEG-based Stereoscopic Research to Reveal the Brain's Response to
  What Happens Before and After Watching 2D and 3D Movies</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite knowing the reality of three-dimensional (3D) technology in the form
of eye fatigue, this technology continues to be retained by people (especially
the young community). To check what happens before and after watching a 2D and
3D movie and how this condition influences the human brain's power spectrum
density (PSD), a five-member test group was arranged. In this study,
electroencephalogram (EEG) was used as a neuroimaging method. EEG recordings of
five individuals were taken both before and after watching 2D and 3D movies.
After 2D/3D EEG recording, this record was divided into three stages for
analysis. These stages consisted of Relax, Watching, and Rest. This
benchmarking analysis included I) before and after watching the 2D movie (R2b
and R2a), II) before and after watching the 3D movie (R3b and R3a), and III)
after watching the 2D/3D movie (R2a and R3a). In the Relax and Rest stages, the
2D/3D EEG power differences in all channels of brain regions for the five EEG
bands, including delta ({\delta}), theta ({\theta}), alpha ({\alpha}), beta
(\b{eta}), and gamma ({\gamma}), were analyzed and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06133</identifier>
 <datestamp>2020-01-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06133</id><created>2019-03-14</created><updated>2020-01-06</updated><authors><author><keyname>Calvi</keyname><forenames>Giuseppe G.</forenames></author><author><keyname>Moniri</keyname><forenames>Ahmad</forenames></author><author><keyname>Mahfouz</keyname><forenames>Mahmoud</forenames></author><author><keyname>Zhao</keyname><forenames>Qibin</forenames></author><author><keyname>Mandic</keyname><forenames>Danilo P.</forenames></author></authors><title>Compression and Interpretability of Deep Neural Networks via Tucker
  Tensor Layer: From First Principles to Tensor Valued Back-Propagation</title><categories>cs.LG cs.CV eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims to help resolve the two main stumbling blocks in the
application of Deep Neural Networks (DNNs), that is, the exceedingly large
number of trainable parameters and their physical interpretability. This is
achieved through a tensor valued approach, based on the proposed Tucker Tensor
Layer (TTL), as an alternative to the dense weight-matrices of DNNs. This
allows us to treat the weight-matrices of general DNNs as a matrix unfolding of
a higher order weight-tensor. By virtue of the compression properties of tensor
decompositions, this enables us to introduce a novel and efficient framework
for exploiting the multi-way nature of the weight-tensor in order to
dramatically reduce the number of DNN parameters. We also derive the tensor
valued back-propagation algorithm within the TTL framework, by extending the
notion of matrix derivatives to tensors. In this way, the physical
interpretability of the Tucker decomposition is exploited to gain physical
insights into the NN training, through the process of computing gradients with
respect to each factor matrix. The proposed framework is validated on both
synthetic data, and the benchmark datasets MNIST, Fashion-MNIST, and CIFAR-10.
Overall, through the ability to provide the relative importance of each data
feature in training, the TTL back-propagation is shown to help mitigate the
&quot;black-box&quot; nature inherent to NNs. Experiments also illustrate that the TTL
achieves a 66.63-fold compression on MNIST and Fashion-MNIST, while, by
simplifying the VGG-16 network, it achieves a 10\% speed up in training time,
at a comparable performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06228</identifier>
 <datestamp>2019-03-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06228</id><created>2019-03-14</created><authors><author><keyname>Nguyen</keyname><forenames>Duc-Phuc</forenames></author><author><keyname>Le</keyname><forenames>Dinh-Dung</forenames></author></authors><title>An FPGA-based centralized visible light beacon network</title><categories>eess.SP</categories><comments>Submitted to IEICE Electronics Express (ELEX)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indoor localization systems based on Visible Light Communication (VLC) have
shown promising advantages compared with systems based on other wireless
technologies. In these systems, many VLC light-emitting diode (LED) anchors are
employed in an indoor space in which location identification messages are sent
to user devices in small packets. In normal beacon network models,
micro-controller (MCU) or low-end system-on-chip (SoC) are often the
coordinators which configure messages for one or many VLC-LED bulbs. In this
paper, we discuss about processing overload and implementation cost of the two
typical models of VLC beacon network in scenarios of a hundred of VLC-LED
anchors are taken into account. Finally, an FPGA-based centralized VLC
transmitter and its aided Nios II-based system has been introduced to enhance
the performance of the VLC beacon network. Besides, due to the centralized
processing, our system model is considered to be more cost-efficient than the
dedicated-processor-based models
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06235</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06235</id><created>2019-03-14</created><updated>2019-03-30</updated><authors><author><keyname>Yang</keyname><forenames>Zhong</forenames></author><author><keyname>Liu</keyname><forenames>Yuanwei</forenames></author><author><keyname>Chen</keyname><forenames>Yue</forenames></author><author><keyname>Jiao</keyname><forenames>Lei</forenames></author></authors><title>Learning Automata Based Q-learning for Content Placement in Cooperative
  Caching</title><categories>eess.SP</categories><comments>30 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An optimization problem of content placement in cooperative caching is
formulated, with the aim of maximizing sum mean opinion score (MOS) of mobile
users. Firstly, a supervised feed-forward back-propagation connectionist model
based neural network (SFBC-NN) is invoked for user mobility and content
popularity prediction. More particularly, practical data collected from
GPS-tracker app on smartphones is tackled to test the accuracy of mobility
prediction. Then, a learning automata-based Q-learning (LAQL) algorithm for
cooperative caching is proposed, in which learning automata (LA) is invoked for
Q-learning to obtain an optimal action selection in a random and stationary
environment. It is proven that the LA-based action selection scheme is capable
of enabling every state to select the optimal action with arbitrarily high
probability if Q-learning is able to converge to the optimal Q value
eventually. To characterize the performance of the proposed algorithms, the sum
MOS of users is applied to define the reward function. Extensive simulations
reveal that: 1) The prediction error of SFBC-NN lessen with the increase of
iterations and nodes; 2) the proposed LAQL achieves significant performance
improvement against traditional Q-learning; 3) the cooperative caching scheme
is capable of outperforming non-cooperative caching and random caching of 3%
and 4%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06254</identifier>
 <datestamp>2019-03-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06254</id><created>2019-03-11</created><authors><author><keyname>Robins</keyname><forenames>Thomas</forenames></author><author><keyname>Stanziola</keyname><forenames>Antonio</forenames></author><author><keyname>Reimer</keyname><forenames>Kai</forenames></author><author><keyname>Weinberg</keyname><forenames>Peter</forenames></author><author><keyname>Tang</keyname><forenames>Meng-Xing</forenames></author></authors><title>Demonstration of Vector Flow Imaging using Convolutional Neural Networks</title><categories>cs.CV eess.IV eess.SP</categories><comments>2018 IEEE International Ultrasonics Symposium, Convolutional Neural
  Network, DeepLearning, Echo-PIV, UltrasoundToolbox, FieldII</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthetic Aperture Vector Flow Imaging (SA-VFI) can visualize complex cardiac
and vascular blood flow patterns at high temporal resolution with a large field
of view. Convolutional neural networks (CNNs) are commonly used in image and
video recognition and classification. However, most recently presented CNNs
also allow for making per-pixel predictions as needed in optical flow
velocimetry. To our knowledge we demonstrate here for the first time a CNN
architecture to produce 2D full flow field predictions from high frame rate SA
ultrasound images using supervised learning. The CNN was initially trained
using CFD-generated and augmented noiseless SA ultrasound data of a realistic
vessel geometry. Subsequently, a mix of noisy simulated and real \textit{in
vivo} acquisitions were added to increase the network's robustness. The
resulting flow field of the CNN resembled the ground truth accurately with an
endpoint-error percentage between 6.5\% to 14.5\%. Furthermore, when confronted
with an unknown geometry of an arterial bifurcation, the CNN was able to
predict an accurate flow field indicating its ability for generalization.
Remarkably, the CNN also performed well for rotational flows, which usually
requires advanced, computationally intensive VFI methods. We have demonstrated
that convolutional neural networks can be used to estimate complex
multidirectional flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06266</identifier>
 <datestamp>2019-03-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06266</id><created>2019-03-14</created><authors><author><keyname>Pajovic</keyname><forenames>Milutin</forenames></author><author><keyname>Koike-Akino</keyname><forenames>Toshiaki</forenames></author><author><keyname>Orlik</keyname><forenames>Philip V.</forenames></author></authors><title>Model-Driven Deep Learning Method for Jammer Suppression in Massive
  Connectivity Systems</title><categories>eess.SP</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for separating collided signals from multiple users in
the presence of strong and wideband interference/jamming signal. More
specifically, we consider a massive connectivity setup where few, out of a
large number of users, equipped with spreading codes, synchronously transmit
symbols. The received signal is a noisy mixture of symbols transmitted through
users' flat fading channels, impaired by fast frequency hopping jamming signal
of relatively large power. In the absence of any conventional technique
suitable for the considered setup, we propose a &quot;model-driven&quot; deep learning
method, based on convolution neural network, to suppress jamming signal from
the received signal, and detect active users together with their transmitted
symbols. A numerical study of the proposed method confirms its effectiveness in
scenarios where classical techniques fail. As such, in a two user scenario with
wideband jamming signal of power $20$ dB above the power any active user, the
proposed algorithm achieves error rates $10^{-2}$ for a wide range of AWGN
variances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06338</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06338</id><created>2019-03-14</created><authors><author><keyname>Chaudhari</keyname><forenames>Shailesh</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Power Control and Frequency Band Selection Policies for Underlay MIMO
  Cognitive Radio</title><categories>eess.SP cs.IT math.IT</categories><journal-ref>published in IEEE Transactions on Cognitive Communications and
  Networking, Jan. 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study power control and frequency band selection policies for multi-band
underlay MIMO cognitive radio with the objective of maximizing the rate of a
secondary user (SU) link while limiting the interference leakage towards
primary users (PUs) below a threshold. The goal of the SU in each policy is to
select one frequency band in each time slot and determine the transmit power.
To limit the interference towards PU in time-varying channels, we propose fixed
and dynamic transmit power control schemes which depend on PU traffic and the
temporal correlation of channels between the SU and the PU. We study the
performance of frequency band selection policies that use fixed or dynamic
power control. We show that dynamic frequency band selection policies, e.g.,
policies based on multi-armed bandit framework, wherein SU selects a different
frequency band in each slot, result in higher interference towards PU as
compared to the fixed band policy wherein SU stays on one band. We also provide
an expression for the gap between the rate achieved by SU under a clairvoyant
policy and the fixed band policy. It is observed that this gap reduces with
increased temporal correlation and with increased number of SU antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06339</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06339</id><created>2019-03-14</created><authors><author><keyname>Chaudhari</keyname><forenames>Shailesh</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>QoS Aware Power Allocation and User Selection in Massive MIMO Underlay
  Cognitive Radio Networks</title><categories>eess.SP cs.IT math.IT math.OC</categories><journal-ref>IEEE Transactions on Cognitive Communications and Networking, Jan.
  2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of power allocation and secondary user (SU) selection
in the downlink from a secondary base station (SBS) equipped with a large
number of antennas in an underlay cognitive radio network. A new optimization
framework is proposed in order to select the maximum number of SUs and compute
power allocations in order to satisfy instantaneous rate or QoS requirements of
SUs. The optimization framework also aims to restrict the interference to
primary users (PUs) below a predefined threshold using available imperfect CSI
at the SBS. In order to obtain a feasible solution for power allocation and
user selection, we propose a low-complexity algorithm called
DeleteSU-with-Maximum-Power-allocation (DMP). Theoretical analysis is provided
to compute the interference to PUs and the number of SUs exceeding the required
rate. The analysis and simulations show that the proposed DMP algorithm
outperforms the stateof-the art selection algorithm in terms of serving more
users with minimum rate constraints, and it approaches the optimal solution if
the number of antennas is an order of magnitude greater than the number of
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06380</identifier>
 <datestamp>2019-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06380</id><created>2019-03-15</created><updated>2019-11-11</updated><authors><author><keyname>Mun</keyname><forenames>Jiwoo</forenames></author><author><keyname>Kim</keyname><forenames>Heasung</forenames></author><author><keyname>Lee</keyname><forenames>Jungwoo</forenames></author></authors><title>A Deep Learning Approach for Automotive Radar Interference Mitigation</title><categories>eess.SP</categories><comments>Accepted in 2018 VTC workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In automotive systems, a radar is a key component of autonomous driving.
Using transmit and reflected radar signal by a target, we can capture the
target range and velocity. However, when interference signals exist, noise
floor increases and it severely affects the detectability of target objects.
For these reasons, previous studies have been proposed to cancel interference
or reconstruct original signals. However, the conventional signal processing
methods for canceling the interference or reconstructing the transmit signals
are difficult tasks, and also have many restrictions. In this work, we propose
a novel approach to mitigate interference using deep learning. The proposed
method provides high performance in various interference conditions and has low
processing time. Moreover, we show that our proposed method achieves better
performance compared to existing signal processing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06392</identifier>
 <datestamp>2019-03-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06392</id><created>2019-03-15</created><authors><author><keyname>Yang</keyname><forenames>Jun</forenames></author><author><keyname>Chhetri</keyname><forenames>Amit S.</forenames></author><author><keyname>Murgia</keyname><forenames>Carlo</forenames></author><author><keyname>Hilmes</keyname><forenames>Philip</forenames></author></authors><title>Reconfigurable Multitask Audio Dynamics Processing Scheme</title><categories>eess.SP</categories><comments>5 pages, ICASSP 2019 paper</comments><journal-ref>ICASSP 2019, May 12-17, 2019, Brighton, UK</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic speech recognition (ASR), audio quality, and loudness are key
performance indicators (KPIs) in smart speakers. To improve all these KPIs,
audio dynamics processing is a crucial component in related systems.
Unfortunately, single-band and existing multiband dynamics processing (MBDP)
schemes fail to maximize bass and loudness but even produce unwanted peaks,
distortions, and nonlinear echo so that an optimized ASR performance cannot be
achieved. It has been a goal in both industry and academia to find a better
audio dynamics processing for mitigating these problems. To provide such a
desired solution, this paper proposes a novel reconfigurable multitask MBDP
scheme through a global optimization framework. Through extensive testing, we
show the accuracy and effectiveness of the proposed scheme in terms of bass and
loudness maximization, distortion and nonlinear echo reduction, browning-out
prevention, and significant ASR performance improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06470</identifier>
 <datestamp>2019-03-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06470</id><created>2019-03-15</created><authors><author><keyname>Nguyen</keyname><forenames>Hieu V.</forenames></author><author><keyname>Nguyen</keyname><forenames>Van-Dinh</forenames></author><author><keyname>Dobre</keyname><forenames>Octavia A.</forenames></author><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Shin</keyname><forenames>Oh-Soon</forenames></author></authors><title>Joint Antenna Array Mode Selection and User Assignment for Full-Duplex
  MU-MISO Systems</title><categories>eess.SP</categories><comments>Accepted for publication in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a full-duplex (FD) multiuser multiple-input
single-output system where a base station simultaneously serves both uplink
(UL) and downlink (DL) users on the same time-frequency resource. The crucial
barriers in implementing FD systems reside in the residual self-interference
and co-channel interference. To accelerate the use of FD radio in future
wireless networks, we aim at managing the network interference more effectively
by jointly designing the selection of half-array antenna modes (in the transmit
or receive mode) at the BS with time phases and user assignments. The first
problem of interest is to maximize the overall sum rate subject to
quality-of-service requirements, which is formulated as a highly non-concave
utility function followed by non-convex constraints. To address the design
problem, we propose an iterative low-complexity algorithm by developing new
inner approximations, and its convergence to a stationary point is guaranteed.
To provide more insights into the solution of the proposed design, a general
max-min rate optimization is further considered to maximize the minimum
per-user rate while satisfying a given ratio between UL and DL rates.
Furthermore, a robust algorithm is devised to verify that the proposed scheme
works well under channel uncertainty. Simulation results demonstrate that the
proposed algorithms exhibit fast convergence and substantially outperform
existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06500</identifier>
 <datestamp>2019-06-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06500</id><created>2019-03-15</created><updated>2019-06-12</updated><authors><author><keyname>Xia</keyname><forenames>Rui</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Filstroff</keyname><forenames>Louis</forenames></author><author><keyname>F&#xe9;votte</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>A Ranking Model Motivated by Nonnegative Matrix Factorization with
  Applications to Tennis Tournaments</title><categories>cs.LG eess.SP stat.ML</categories><comments>16 pages, 2 figures, 9 tables. Accepted and to be presented at the
  European Conference on Machine Learning and Principles and Practice of
  Knowledge Discovery in Databases (ECML/PKDD) 2019. Supplementary material,
  code and datasets can be found in this URL
  https://github.com/XiaRui1996/btl-nmf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel ranking model that combines the Bradley-Terry-Luce
probability model with a nonnegative matrix factorization framework to model
and uncover the presence of latent variables that influence the performance of
top tennis players. We derive an efficient, provably convergent, and
numerically stable majorization-minimization-based algorithm to maximize the
likelihood of datasets under the proposed statistical model. The model is
tested on datasets involving the outcomes of matches between 20 top male and
female tennis players over 14 major tournaments for men (including the Grand
Slams and the ATP Masters 1000) and 16 major tournaments for women over the
past 10 years. Our model automatically infers that the surface of the court
(e.g., clay or hard court) is a key determinant of the performances of male
players, but less so for females. Top players on various surfaces over this
longitudinal period are also identified in an objective manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06519</identifier>
 <datestamp>2019-03-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06519</id><created>2019-03-14</created><authors><author><keyname>Platonova</keyname><forenames>Ganna</forenames></author><author><keyname>Stys</keyname><forenames>Dalibor</forenames></author><author><keyname>Soucek</keyname><forenames>Pavel</forenames></author><author><keyname>Machacek</keyname><forenames>Petr</forenames></author><author><keyname>Kotal</keyname><forenames>Vladimir</forenames></author><author><keyname>Rychtarikova</keyname><forenames>Renata</forenames></author></authors><title>Technically correct visualization of biological microscopic experiments</title><categories>eess.IV cs.CV</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most realistic images that reflect native cellular and intracellular
structure and behavior can be obtained only using brightfield microscopy. At
high-intensity pulsing LED illumination, we captured the primary
12-bit-per-channel (bpc) signal from an observed sample using a brightfield
microscope equipped with a high-resolution (4872x3248) camera. In order to
suppress image distortions arising from light passing through the whole
microscope optical path, from camera sensor defects and geometrical
peculiarities of sensor sensitivity, these uncompressed 12-bpc images underwent
a kind of correction after simultaneous calibration of all the parts of the
experimental arrangement. Moreover, the final corrected images (from biological
experiments) show the number of photons reaching each camera pixel and can be
visualized in 8-bpc intensity depth after the Least Information Loss
compression (Stys et al., Lect. Notes Bioinform. 9656, 2016).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06539</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06539</id><created>2019-03-12</created><updated>2019-04-28</updated><authors><author><keyname>Kumatani</keyname><forenames>Kenichi</forenames></author><author><keyname>Wu</keyname><forenames>Minhua</forenames></author><author><keyname>Sundaram</keyname><forenames>Shiva</forenames></author><author><keyname>Strom</keyname><forenames>Nikko</forenames></author><author><keyname>Hoffmeister</keyname><forenames>Bjorn</forenames></author></authors><title>Multi-Geometry Spatial Acoustic Modeling for Distant Speech Recognition</title><categories>eess.AS cs.SD</categories><comments>ICASSP2019, 5 pages. arXiv admin note: substantial text overlap with
  arXiv:1903.05299</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of spatial information with multiple microphones can improve
far-field automatic speech recognition (ASR) accuracy. However, conventional
microphone array techniques degrade speech enhancement performance when there
is an array geometry mismatch between design and test conditions. Moreover,
such speech enhancement techniques do not always yield ASR accuracy improvement
due to the difference between speech enhancement and ASR optimization
objectives. In this work, we propose to unify an acoustic model framework by
optimizing spatial filtering and long short-term memory (LSTM) layers from
multi-channel (MC) input. Our acoustic model subsumes beamformers with multiple
types of array geometry. In contrast to deep clustering methods that treat a
neural network as a black box tool, the network encoding the spatial filters
can process streaming audio data in real time without the accumulation of
target signal statistics. We demonstrate the effectiveness of such MC neural
networks through ASR experiments on the real-world far-field data. We show that
our two-channel acoustic model can on average reduce word error rates (WERs)
by~13.4 and~12.7% compared to a single channel ASR system with the log-mel
filter bank energy (LFBE) feature under the matched and mismatched microphone
placement conditions, respectively. Our result also shows that our two-channel
network achieves a relative WER reduction of over~7.0% compared to conventional
beamforming with seven microphones overall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06658</identifier>
 <datestamp>2019-03-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06658</id><created>2019-01-18</created><authors><author><keyname>Gubran</keyname><forenames>Ayub A.</forenames></author><author><keyname>Huang</keyname><forenames>Felix</forenames></author><author><keyname>Aamodt</keyname><forenames>Tor M.</forenames></author></authors><title>Surface Compression Using Dynamic Color Palettes</title><categories>cs.GR cs.MM eess.IV</categories><comments>13 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Off-chip memory traffic is a major source of power and energy consumption on
mobile platforms. A large amount of this off-chip traffic is used to manipulate
graphics framebuffer surfaces. To cut down the cost of accessing off-chip
memory, framebuffer surfaces are compressed to reduce the bandwidth consumed on
surface manipulation when rendering or displaying.
  In this work, we study the compression properties of framebuffer surfaces and
highlight the fact that surfaces from different applications have different
compression characteristics. We use the results of our analysis to propose a
scheme, Dynamic Color Palettes (DCP), which achieves higher compression rates
with UI and 2D surfaces.
  DCP is a hardware mechanism for exploiting inter-frame coherence in lossless
surface compression; it implements a scheme that dynamically constructs color
palettes, which are then used to efficiently compress framebuffer surfaces. To
evaluate DCP, we created an extensive set of OpenGL workload traces from 124
Android applications. We found that DCP improves compression rates by 91% for
UI and 20% for 2D applications compared to previous proposals. We also evaluate
a hybrid scheme that combines DCP with a generic compression scheme. We found
that compression rates improve over previous proposals by 161%, 124% and 83%
for UI, 2D and 3D applications, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06753</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06753</id><created>2019-03-02</created><authors><author><keyname>Cheng</keyname><forenames>Cheng</forenames></author><author><keyname>Zhou</keyname><forenames>Beitong</forenames></author><author><keyname>Ma</keyname><forenames>Guijun</forenames></author><author><keyname>Wu</keyname><forenames>Dongrui</forenames></author><author><keyname>Yuan</keyname><forenames>Ye</forenames></author></authors><title>Wasserstein Distance based Deep Adversarial Transfer Learning for
  Intelligent Fault Diagnosis</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand of artificial intelligent adoption for condition-based maintenance
strategy is astonishingly increased over the past few years. Intelligent fault
diagnosis is one critical topic of maintenance solution for mechanical systems.
Deep learning models, such as convolutional neural networks (CNNs), have been
successfully applied to fault diagnosis tasks for mechanical systems and
achieved promising results. However, for diverse working conditions in the
industry, deep learning suffers two difficulties: one is that the well-defined
(source domain) and new (target domain) datasets are with different feature
distributions; another one is the fact that insufficient or no labelled data in
target domain significantly reduce the accuracy of fault diagnosis. As a novel
idea, deep transfer learning (DTL) is created to perform learning in the target
domain by leveraging information from the relevant source domain. Inspired by
Wasserstein distance of optimal transport, in this paper, we propose a novel
DTL approach to intelligent fault diagnosis, namely Wasserstein Distance based
Deep Transfer Learning (WD-DTL), to learn domain feature representations
(generated by a CNN based feature extractor) and to minimize the distributions
between the source and target domains through adversarial training. The
effectiveness of the proposed WD-DTL is verified through 3 transfer scenarios
and 16 transfer fault diagnosis experiments of both unsupervised and supervised
(with insufficient labelled data) learning. We also provide a comprehensive
analysis of the network visualization of those transfer tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06764</identifier>
 <datestamp>2019-05-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06764</id><created>2019-03-15</created><authors><author><keyname>Raurale</keyname><forenames>Sumit</forenames></author><author><keyname>McAllister</keyname><forenames>John</forenames></author><author><keyname>del Rincon</keyname><forenames>Jesus Martinez</forenames></author></authors><title>EMG wrist-hand motion recognition system for real-time Embedded platform</title><categories>eess.SP</categories><comments>5 pages, to appear in upcoming IEEE ICASSP 2019 (Paper: 1810,
  Session: DISPS-P2: Algorithm and Architecture Optimization, Topic: Design and
  Implementation of Signal Processing Systems / Low-power signal processing
  techniques and architectures)</comments><doi>10.1109/ICASSP.2019.8683104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electromyography (EMG) signal analysis is a popular method for controlling
prosthetic and gesture control equipment. For portable systems, such as
prosthetic limbs, real-time low-power operation on embedded processors is
critical, but to date, there has been no record of how existing EMG analysis
approaches support such deployments. This paper presents a novel approach to
time-domain classification of multi-channel EMG signals harnessed from
randomly-placed sensors according to the wrist-hand movements which caused
their occurrence. It shows how, by employing a very small set of time-domain
features, Kernel Fisher discriminant feature projection and Radial Bias
Function neural network classifiers, nine wrist-hand movements can be detected
with accuracy exceeding 99% - surpassing the state-of-the-art on record. It
also shows how, when deployed on ARM Cortex-A53, the processing time is not
only sufficient to enable real-time processing but is also a factor 50 shorter
than the leading time-frequency techniques on record.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06828</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06828</id><created>2019-03-15</created><authors><author><keyname>Sharma</keyname><forenames>Pranav</forenames></author><author><keyname>Huang</keyname><forenames>Bowen</forenames></author><author><keyname>Vaidya</keyname><forenames>Umesh</forenames></author><author><keyname>Ajjarapu</keyname><forenames>Venkatramana</forenames></author></authors><title>Data-driven Identification and Prediction of Power System Dynamics Using
  Linear Operators</title><categories>eess.SP cs.SY</categories><comments>Accepted for publication in IEEE Power and Energy System General
  Meeting 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose linear operator theoretic framework involving
Koopman operator for the data-driven identification of power system dynamics.
We explicitly account for noise in the time series measurement data and propose
robust approach for data-driven approximation of Koopman operator for the
identification of nonlinear power system dynamics. The identified model is used
for the prediction of state trajectories in the power system. The application
of the framework is illustrated using an IEEE nine bus test system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06836</identifier>
 <datestamp>2019-10-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06836</id><created>2019-03-15</created><updated>2019-10-02</updated><authors><author><keyname>Nataraj</keyname><forenames>Lakshmanan</forenames></author><author><keyname>Mohammed</keyname><forenames>Tajuddin Manhar</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>Shivkumar</forenames></author><author><keyname>Flenner</keyname><forenames>Arjuna</forenames></author><author><keyname>Bappy</keyname><forenames>Jawadul H.</forenames></author><author><keyname>Roy-Chowdhury</keyname><forenames>Amit K.</forenames></author><author><keyname>Manjunath</keyname><forenames>B. S.</forenames></author></authors><title>Detecting GAN generated Fake Images using Co-occurrence Matrices</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of Generative Adversarial Networks (GANs) has brought about
completely novel ways of transforming and manipulating pixels in digital
images. GAN based techniques such as Image-to-Image translations, DeepFakes,
and other automated methods have become increasingly popular in creating fake
images. In this paper, we propose a novel approach to detect GAN generated fake
images using a combination of co-occurrence matrices and deep learning. We
extract co-occurrence matrices on three color channels in the pixel domain and
train a model using a deep convolutional neural network (CNN) framework.
Experimental results on two diverse and challenging GAN datasets comprising
more than 56,000 images based on unpaired image-to-image translations (cycleGAN
[1]) and facial attributes/expressions (StarGAN [2]) show that our approach is
promising and achieves more than 99% classification accuracy in both datasets.
Further, our approach also generalizes well and achieves good results when
trained on one dataset and tested on the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06886</identifier>
 <datestamp>2019-08-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06886</id><created>2019-03-16</created><updated>2019-08-19</updated><authors><author><keyname>Gu</keyname><forenames>Yifan</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Zhai</keyname><forenames>Chao</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Minimizing Age of Information in Cognitive Radio-based IoT Systems:
  Underlay or Overlay?</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cognitive radio-based Internet-of-Things (CR-IoT) network
consisting of one primary IoT (PIoT) system and one secondary IoT (SIoT)
system. The IoT devices of both the PIoT and the SIoT respectively monitor one
physical process and send randomly generated status updates to their associated
access points (APs). The timeliness of the status updates is important as the
systems are interested in the latest condition (e.g., temperature, speed and
position) of the IoT device. In this context, two natural questions arise: (1)
How to characterize the timeliness of the status updates in CR-IoT systems? (2)
Which scheme, overlay or underlay, is better in terms of the timeliness of the
status updates. To answer these two questions, we adopt a new performance
metric, named the age of information (AoI). We analyze the average peak AoI of
the PIoT and the SIoT for overlay and underlay schemes, respectively. Simple
asymptotic expressions of the average peak AoI are also derived when the PIoT
operates at high signal-to-noise ratio (SNR). Based on the asymptotic
expressions, we characterize a critical generation rate of the PIoT system,
which can determine the superiority of overlay and underlay schemes in terms of
the average peak AoI of the SIoT. Numerical results validate the theoretical
analysis and uncover that the overlay and underlay schemes can outperform each
other in terms of the average peak AoI of the SIoT for different system setups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06908</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06908</id><created>2019-03-16</created><authors><author><keyname>Avila</keyname><forenames>Anderson R.</forenames></author><author><keyname>Gamper</keyname><forenames>Hannes</forenames></author><author><keyname>Reddy</keyname><forenames>Chandan</forenames></author><author><keyname>Cutler</keyname><forenames>Ross</forenames></author><author><keyname>Tashev</keyname><forenames>Ivan</forenames></author><author><keyname>Gehrke</keyname><forenames>Johannes</forenames></author></authors><title>Non-intrusive speech quality assessment using neural networks</title><categories>eess.AS cs.SD</categories><comments>Accepted at ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the perceived quality of an audio signal is critical for many
multimedia and audio processing systems. Providers strive to offer optimal and
reliable services in order to increase the user quality of experience (QoE). In
this work, we present an investigation of the applicability of neural networks
for non-intrusive audio quality assessment. We propose three neural
network-based approaches for mean opinion score (MOS) estimation. We compare
our results to three instrumental measures: the perceptual evaluation of speech
quality (PESQ), the ITU-T Recommendation P.563, and the speech-to-reverberation
energy ratio. Our evaluation uses a speech dataset contaminated with
convolutive and additive noise, labeled using a crowd-based QoE evaluation,
evaluated with Pearson correlation with MOS labels, and mean-squared-error of
the estimated MOS. Our proposed approaches outperform the aforementioned
instrumental measures, with a fully connected deep neural network using
Mel-frequency features providing the best correlation (0.87) and the lowest
mean squared error (0.15)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06909</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06909</id><created>2019-03-16</created><authors><author><keyname>Mousavi</keyname><forenames>Elahe</forenames></author><author><keyname>Kafieh</keyname><forenames>Rahele</forenames></author><author><keyname>Rabbani</keyname><forenames>Hossein</forenames></author></authors><title>Classification of dry age-related macular degeneration and diabetic
  macular edema from optical coherence tomography images using dictionary
  learning</title><categories>eess.IV cs.CV</categories><comments>9 pages, IET journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Age-related Macular Degeneration (AMD) and Diabetic Macular Edema (DME) are
the major causes of vision loss in developed countries. Alteration of retinal
layer structure and appearance of exudate are the most significant signs of
these diseases. With the aim of automatic classification of DME, AMD and normal
subjects from Optical Coherence Tomography (OCT) images, we proposed a
classification algorithm. The two important issues intended in this approach
are, not utilizing retinal layer segmentation which by itself is a challenging
task and attempting to identify diseases in their early stages, where the signs
of diseases appear in a small fraction of B-Scans. We used a histogram of
oriented gradients (HOG) feature descriptor to well characterize the
distribution of local intensity gradients and edge directions. In order to
capture the structure of extracted features, we employed different dictionary
learning-based classifiers. Our dataset consists of 45 subjects: 15 patients
with AMD, 15 patients with DME and 15 normal subjects. The proposed classifier
leads to an accuracy of 95.13%, 100.00%, and 100.00% for DME, AMD, and normal
OCT images, respectively, only by considering the 4% of all B-Scans of a volume
which outperforms the state of the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06914</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06914</id><created>2019-03-16</created><authors><author><keyname>Zhan</keyname><forenames>Junpeng</forenames></author><author><keyname>Ansari</keyname><forenames>Osama Aslam</forenames></author><author><keyname>Liu</keyname><forenames>Weijia</forenames></author><author><keyname>Chung</keyname><forenames>C. Y.</forenames></author></authors><title>An Accurate Bilinear Cavern Model for Compressed Air Energy Storage</title><categories>eess.SP</categories><comments>18 pages, 15 figures, accepted by Applied Energy on March 2019</comments><doi>10.1016/j.apenergy.2019.03.104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed air energy storage is suitable for large-scale electrical energy
storage, which is important for integrating renewable energy sources into
electric power systems. A typical compressed air energy storage plant consists
of compressors, expanders, caverns, and a motor/generator set. Current cavern
models used for compressed air energy storage are either accurate but highly
nonlinear or linear but inaccurate. The application of highly nonlinear cavern
models in power system optimization problems renders them computationally
challenging to solve. In this regard, an accurate bilinear cavern model for
compressed air energy storage is proposed in this paper. The charging and
discharging processes in a cavern are divided into several real/virtual states.
The first law of thermodynamics and ideal gas law are then utilized to derive a
cavern model, i.e., a model for the variation of temperature and pressure in
these processes. Thereafter, the heat transfer between the air in the cavern
and the cavern wall is considered and integrated into the cavern model. By
subsequently eliminating several negligible terms, the cavern model reduces to
a bilinear model. The accuracy of the bilinear cavern model is verified via
comparison with both an accurate nonlinear model and two sets of field-measured
data. The bilinear cavern model can be easily linearized and is then suitable
for integration into optimization problems considering compressed air energy
storage. This is verified via comparatively solving a self-scheduling problem
of compressed air energy storage using different cavern models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06917</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06917</id><created>2019-03-16</created><authors><author><keyname>Zhong</keyname><forenames>Zhiwei</forenames><affiliation>Lab of Efficient Architectures for Digital-communication and Signal-processing</affiliation><affiliation>National Mobile Communications Research Laboratory</affiliation><affiliation>Quantum Information Center, Southeast University, China</affiliation></author><author><keyname>Ge</keyname><forenames>Lulu</forenames><affiliation>Lab of Efficient Architectures for Digital-communication and Signal-processing</affiliation><affiliation>National Mobile Communications Research Laboratory</affiliation><affiliation>Quantum Information Center, Southeast University, China</affiliation></author><author><keyname>Zhang</keyname><forenames>Zaichen</forenames><affiliation>National Mobile Communications Research Laboratory</affiliation><affiliation>Quantum Information Center, Southeast University, China</affiliation></author><author><keyname>You</keyname><forenames>Xiaohu</forenames><affiliation>National Mobile Communications Research Laboratory</affiliation></author><author><keyname>Zhang</keyname><forenames>Chuan</forenames><affiliation>Lab of Efficient Architectures for Digital-communication and Signal-processing</affiliation><affiliation>National Mobile Communications Research Laboratory</affiliation><affiliation>Quantum Information Center, Southeast University, China</affiliation></author></authors><title>Molecular Polar Belief Propagation Decoder and Successive Cancellation
  Decoder</title><categories>eess.SP q-bio.MN</categories><comments>This paper was first submitted to GLOBECOM 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By constructing chemical reaction networks (CRNs), this paper proposes a
method of synthesizing polar decoder using belief propagation (BP) algorithm
and successive cancellation (SC) algorithm, respectively. Theoretical analysis
and simulation results have validated the feasibility of the method. Reactions
in the proposed design could be experimentally implemented with DNA strand
displacement reactions, making the proposed polar decoders promising for wide
application in nanoscale devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.06999</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.06999</id><created>2019-03-16</created><updated>2019-03-21</updated><authors><author><keyname>Zheng</keyname><forenames>Yang</forenames></author><author><keyname>Izzat</keyname><forenames>Izzat H.</forenames></author><author><keyname>Ziaee</keyname><forenames>Shahrzad</forenames></author></authors><title>GFD-SSD: Gated Fusion Double SSD for Multispectral Pedestrian Detection</title><categories>cs.CV cs.LG eess.IV</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pedestrian detection is an essential task in autonomous driving research. In
addition to typical color images, thermal images benefit the detection in dark
environments. Hence, it is worthwhile to explore an integrated approach to take
advantage of both color and thermal images simultaneously. In this paper, we
propose a novel approach to fuse color and thermal sensors using deep neural
networks (DNN). Current state-of-the-art DNN object detectors vary from
two-stage to one-stage mechanisms. Two-stage detectors, like Faster-RCNN,
achieve higher accuracy, while one-stage detectors such as Single Shot Detector
(SSD) demonstrate faster performance. To balance the trade-off, especially in
the consideration of autonomous driving applications, we investigate a fusion
strategy to combine two SSDs on color and thermal inputs. Traditional fusion
methods stack selected features from each channel and adjust their weights. In
this paper, we propose two variations of novel Gated Fusion Units (GFU), that
learn the combination of feature maps generated by the two SSD middle layers.
Leveraging GFUs for the entire feature pyramid structure, we propose several
mixed versions of both stack fusion and gated fusion. Experiments are conducted
on the KAIST multispectral pedestrian detection dataset. Our Gated Fusion
Double SSD (GFD-SSD) outperforms the stacked fusion and achieves the lowest
miss rate in the benchmark, at an inference speed that is two times faster than
Faster-RCNN based fusion networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07011</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07011</id><created>2019-03-16</created><authors><author><keyname>Babaie</keyname><forenames>Morteza</forenames></author><author><keyname>Tizhoosh</keyname><forenames>H. R.</forenames></author></authors><title>Deep Features for Tissue-Fold Detection in Histopathology Images</title><categories>eess.IV cs.CV</categories><comments>Accepted for publication in the 15th European Congress on Digital
  Pathology (ECDP 2019), University of Warwick, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whole slide imaging (WSI) refers to the digitization of a tissue specimen
which enables pathologists to explore high-resolution images on a monitor
rather than through a microscope. The formation of tissue folds occur during
tissue processing. Their presence may not only cause out-of-focus digitization
but can also negatively affect the diagnosis in some cases. In this paper, we
have compared five pre-trained convolutional neural networks (CNNs) of
different depths as feature extractors to characterize tissue folds. We have
also explored common classifiers to discriminate folded tissue against the
normal tissue in hematoxylin and eosin (H\&amp;E) stained biopsy samples. In our
experiments, we manually select the folded area in roughly 2.5mm $\times$ 2.5mm
patches at $20$x magnification level as the training data. The ``DenseNet''
with 201 layers alongside an SVM classifier outperformed all other
configurations. Based on the leave-one-out validation strategy, we achieved
$96.3\%$ accuracy, whereas with augmentation the accuracy increased to
$97.2\%$. We have tested the generalization of our method with five unseen WSIs
from the NIH (National Cancer Institute) dataset. The accuracy for patch-wise
detection was $81\%$. One folded patch within an image suffices to flag the
entire specimen for visual inspection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07013</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07013</id><created>2019-03-16</created><authors><author><keyname>Chenni</keyname><forenames>Wafa</forenames></author><author><keyname>Herbi</keyname><forenames>Habib</forenames></author><author><keyname>Babaie</keyname><forenames>Morteza</forenames></author><author><keyname>Tizhoosh</keyname><forenames>H. R.</forenames></author></authors><title>Patch Clustering for Representation of Histopathology Images</title><categories>eess.IV cs.CV</categories><comments>Accepted for publication in the 15th European Congress on Digital
  Pathology (ECDP 2019), University of Warwick, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whole Slide Imaging (WSI) has become an important topic during the last
decade. Even though significant progress in both medical image processing and
computational resources has been achieved, there are still problems in WSI that
need to be solved. A major challenge is the scan size. The dimensions of
digitized tissue samples may exceed 100,000 by 100,000 pixels causing memory
and efficiency obstacles for real-time processing. The main contribution of
this work is representing a WSI by selecting a small number of patches for
algorithmic processing (e.g., indexing and search). As a result, we reduced the
search time and storage by various factors between ($50\% - 90\%$), while
losing only a few percentages in the patch retrieval accuracy. A
self-organizing map (SOM) has been applied on local binary patterns (LBP) and
deep features of the KimiaPath24 dataset in order to cluster patches that share
the same characteristics. We used a Gaussian mixture model (GMM) to represent
each class with a rather small ($10\%-50\%$) portion of patches. The results
showed that LBP features can outperform deep features. By selecting only $50\%$
of all patches after SOM clustering and GMM patch selection, we received $65\%$
accuracy for retrieval of the best match, while the maximum accuracy (using all
patches) was $69\%$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07066</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07066</id><created>2019-03-17</created><authors><author><keyname>Saeed</keyname><forenames>Nasir</forenames></author><author><keyname>Celik</keyname><forenames>Abdulkadir</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Localization of Energy Harvesting Empowered Underwater Optical Wireless
  Sensor Networks</title><categories>eess.SP</categories><comments>Accepted in IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a received signal strength (RSS) based localization
framework for energy harvesting underwater optical wireless sensor networks
(EH-UOWSNs), where the optical noise sources and channel impairments of
seawater pose significant challenges on range estimation. In UOWSNs energy
limitation is another major problem due to the limited battery power and
difficulty to replace or recharge the battery of an underwater sensor node. In
the proposed framework, sensor nodes with insufficient battery harvest ambient
energy and start communicating once they have sufficient storage of energy.
Network localization is carried out by measuring the RSSs of active nodes,
which are modeled based on the underwater optical communication channel
characteristics. Thereafter, block kernel matrices are computed for RSS-based
range measurements. Unlike the traditional shortest-path approach, the proposed
technique reduces the estimation error of the shortest path for each block
kernel matrix. Once the complete block kernel matrices are available, a closed
form localization technique is developed to find the location of every optical
sensor node in the network. An analytical expression for the Cramer Rao lower
bound (CRLB) is also derived as a benchmark to evaluate the localization
performance of the developed technique. Extensive simulations show that the
proposed framework outperforms the well-known network localization techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07158</identifier>
 <datestamp>2019-06-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07158</id><created>2019-03-17</created><updated>2019-06-03</updated><authors><author><keyname>Hung</keyname><forenames>Cheng-Yu</forenames></author><author><keyname>Kaveh</keyname><forenames>Mostafa</forenames></author></authors><title>Joint Block Low Rank and Sparse Matrix Recovery in Array
  Self-Calibration Off-Grid DoA Estimation</title><categories>eess.SP cs.IR math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter addresses the estimation of directions-of-arrival (DoA) by a
sensor array using a sparse model in the presence of array calibration errors
and off-grid directions. The received signal utilizes previously used models
for unknown errors in calibration and structured linear representation of the
off-grid effect. A convex optimization problem is formulated with an objective
function to promote two-layer joint block-sparsity with its second-order cone
programming (SOCP) representation. The performance of the proposed method is
demonstrated by numerical simulations and compared with the Cramer-Rao Bound
(CRB), and several previously proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07164</identifier>
 <datestamp>2019-06-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07164</id><created>2019-03-17</created><updated>2019-06-03</updated><authors><author><keyname>Hung</keyname><forenames>Cheng-Yu</forenames></author><author><keyname>Kaveh</keyname><forenames>Mostafa</forenames></author></authors><title>Linearly Constrained Smoothing Group Sparsity Solvers in Off-grid Model</title><categories>eess.SP cs.IR math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressed sensing, the sensing matrix is assumed perfectly known.
However, there exists perturbation in the sensing matrix in reality due to
sensor offsets or noise disturbance. Directions-of-arrival (DoA) estimation
with off-grid effect satisfies this situation, and can be formulated into a
(non)convex optimization problem with linear inequalities constraints, which
can be solved by the interior point method (using the CVX tools), but at a
large computational cost. In this work, in order to design efficient
algorithms, we consider various alternative formulations, such as unconstrained
formulation, primal-dual formulation, or conic formulation to develop
group-sparsity promoted solvers. First, the consensus alternating direction
method of multipliers (C-ADMM) is applied. Then, iterative algorithms for the
BPDN formulation is proposed by combining the Nesterov smoothing technique with
accelerated proximal gradient method, and the convergence analysis of the
method is conducted as well.
  We also developed a variant of EGT (Excessive Gap Technique)-based
primal-dual method to systematically reduce the smoothing parameter
sequentially. Finally, we propose algorithms for quadratically constrained
L2-L1 mixed norm minimization problem by using the smoothed dual conic
optimization (SDCO) and continuation technique. The performance of accuracy and
convergence for all the proposed methods are demonstrated in the numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07168</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07168</id><created>2019-03-17</created><authors><author><keyname>Dashmiz</keyname><forenames>Shayan</forenames></author><author><keyname>Mansouri</keyname><forenames>Seyed Mohammad</forenames></author><author><keyname>Najafi</keyname><forenames>Amir</forenames></author><author><keyname>Marvasti</keyname><forenames>Farrokh</forenames></author></authors><title>Sum Capacity Bounds and Design of Suboptimal Codes for Asynchronous CDMA
  Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study two issues in asynchronous communication systems. The
first issue is the derivation of sum capacity bounds for finite dimensional
asynchronous systems. In addition, asymptotic results for the sum capacity
bounds are obtained. The second issue is the design of practical suboptimal
codes for binary chip asynchronous CDMA systems that become optimal for high
Signal-to-Noise (SNR) ratios. The performance of such suboptimal codes is also
compared to Gold and Optical Orthogonal codes. The conclusion is that the
proposed suboptimal codes perform favorably compared to other known codes for
high SNR asynchronous systems and perform more or less the same as the other
codes for the low SNR values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07194</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07194</id><created>2019-03-17</created><authors><author><keyname>Ram&#xed;rez-Chavarr&#xed;a</keyname><forenames>Roberto G.</forenames></author><author><keyname>S&#xe1;nchez-P&#xe9;rez</keyname><forenames>Celia</forenames></author></authors><title>Sensing Micro-colloid Concentration by Spectral Impedance Measurements
  and Relaxation Times Analysis</title><categories>eess.SP physics.data-an physics.ins-det</categories><comments>Conference Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an attractive method towards sensing and quantifying the
concentration of dielectric particles suspended in a highly conductive medium.
Fast electrical impedance spectroscopy (EIS) measurements were performed, based
on an electrochemical setup and a non-parametric unbiased estimator. The
impedance data are then mapped from the frequency domain to a time constant
domain via the distribution of relaxation times (DRT) model. It is shown that
application of DRT can systematically give a distribution function which is
directly related with the concentration of micro-colloid samples. As a result,
the samples were characterized by high sensitivity and resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07227</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07227</id><created>2019-03-17</created><authors><author><keyname>Huang</keyname><forenames>Cheng-Zhi Anna</forenames></author><author><keyname>Cooijmans</keyname><forenames>Tim</forenames></author><author><keyname>Roberts</keyname><forenames>Adam</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Eck</keyname><forenames>Douglas</forenames></author></authors><title>Counterpoint by Convolution</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Proceedings of the 18th International Society for Music Information
  Retrieval Conference, ISMIR 2017</comments><acm-class>H.5.5; I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning models of music typically break up the task of composition
into a chronological process, composing a piece of music in a single pass from
beginning to end. On the contrary, human composers write music in a nonlinear
fashion, scribbling motifs here and there, often revisiting choices previously
made. In order to better approximate this process, we train a convolutional
neural network to complete partial musical scores, and explore the use of
blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the
generative procedure are tied to a particular causal direction of composition.
Our model is an instance of orderless NADE (Uria et al., 2014), which allows
more direct ancestral sampling. However, we find that Gibbs sampling greatly
improves sample quality, which we demonstrate to be due to some conditional
distributions being poorly modeled. Moreover, we show that even the cheap
approximate blocked Gibbs procedure from Yao et al. (2014) yields better
samples than ancestral sampling, based on both log-likelihood and human
evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07243</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07243</id><created>2019-03-17</created><authors><author><keyname>Chen</keyname><forenames>Wenshuai</forenames></author><author><keyname>Gou</keyname><forenames>Shuiping</forenames></author><author><keyname>Wang</keyname><forenames>Xinlin</forenames></author><author><keyname>Jiao</keyname><forenames>Licheng</forenames></author><author><keyname>Jiao</keyname><forenames>Changzhe</forenames></author><author><keyname>Zare</keyname><forenames>Alina</forenames></author></authors><title>Complex Scene Classification of PolSAR Imagery based on a Self-paced
  Learning Approach</title><categories>cs.CV eess.IV</categories><doi>10.1109/JSTARS.2018.2879440</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing polarimetric synthetic aperture radar (PolSAR) image classification
methods cannot achieve satisfactory performance on complex scenes characterized
by several types of land cover with significant levels of noise or similar
scattering properties across land cover types. Hence, we propose a supervised
classification method aimed at constructing a classifier based on self-paced
learning (SPL). SPL has been demonstrated to be effective at dealing with
complex data while providing classifier. In this paper, a novel Support Vector
Machine (SVM) algorithm based on SPL with neighborhood constraints (SVM_SPLNC)
is proposed. The proposed method leverages the easiest samples first to obtain
an initial parameter vector. Then, more complex samples are gradually
incorporated to update the parameter vector iteratively. Moreover, neighborhood
constraints are introduced during the training process to further improve
performance. Experimental results on three real PolSAR images show that the
proposed method performs well on complex scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07300</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07300</id><created>2019-03-18</created><authors><author><keyname>Xu</keyname><forenames>Jun</forenames></author><author><keyname>Zhu</keyname><forenames>Pengcheng</forenames></author><author><keyname>Li</keyname><forenames>Jiamin</forenames></author><author><keyname>You</keyname><forenames>Xiaohu</forenames></author></authors><title>Deep Learning Based Pilot Design for Multi-user Distributed Massive MIMO
  Systems</title><categories>eess.SP</categories><comments>4 Pages, 4 figures, accepted by IEEE Wireless Communications Letters</comments><doi>10.1109/LWC.2019.2904229</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a deep learning based pilot design scheme to minimize
the sum mean square error (MSE) of channel estimation for multi-user
distributed massive multiple-input multiple-output (MIMO) systems. The pilot
signal of each user is expressed as a weighted superposition of orthonormal
pilot sequence basis, where the power assigned to each pilot sequence is the
corresponding weight. A multi-layer fully connected deep neural network (DNN)
is designed to optimize the power allocated to each pilot sequence to minimize
the sum MSE, which takes the channel large-scale fading coefficients as input
and outputs the pilot power allocation vector. The loss function of the DNN is
defined as the sum MSE, and we leverage the unsupervised learning strategy to
train the DNN. Simulation results show that the proposed scheme achieves better
sum MSE performance than other methods with low complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07364</identifier>
 <datestamp>2019-12-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07364</id><created>2019-03-18</created><updated>2019-12-20</updated><authors><author><keyname>Moon</keyname><forenames>Jihwan</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Park</keyname><forenames>Seok-Hwan</forenames></author><author><keyname>Lee</keyname><forenames>Inkyu</forenames></author></authors><title>Online Reinforcement Learning of X-Haul Content Delivery Mode in Fog
  Radio Access Networks</title><categories>eess.SP cs.NI</categories><comments>5 pages, 2 figures</comments><report-no>IEEE Signal Processing Letters ( Volume: 26 , Issue: 10 , Pages:
  1451 - 1455 , Oct. 2019 )</report-no><doi>10.1109/LSP.2019.2932193</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Fog Radio Access Network (F-RAN) with a Base Band Unit (BBU) in
the cloud and multiple cache-enabled enhanced Remote Radio Heads (eRRHs). The
system aims at delivering contents on demand with minimal average latency from
a time-varying library of popular contents. Information about uncached
requested files can be transferred from the cloud to the eRRHs by following
either backhaul or fronthaul modes. The backhaul mode transfers fractions of
the requested files, while the fronthaul mode transmits quantized baseband
samples as in Cloud-RAN (C-RAN). The backhaul mode allows the caches of the
eRRHs to be updated, which may lower future delivery latencies. In contrast,
the fronthaul mode enables cooperative C-RAN transmissions that may reduce the
current delivery latency. Taking into account the trade-off between current and
future delivery performance, this paper proposes an adaptive selection method
between the two delivery modes to minimize the long-term delivery latency.
Assuming an unknown and time-varying popularity model, the method is based on
model-free Reinforcement Learning (RL). Numerical results confirm the
effectiveness of the proposed RL scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07391</identifier>
 <datestamp>2020-02-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07391</id><created>2019-02-24</created><updated>2020-02-16</updated><authors><author><keyname>Wei</keyname><forenames>Yiheng</forenames></author><author><keyname>Chen</keyname><forenames>Yuquan</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author></authors><title>Some fundamental properties on the sampling free nabla Laplace transform</title><categories>math.OC eess.SP</categories><doi>10.1115/DETC2019-97351</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete fractional order systems have attracted more and more attention in
recent years. Nabla Laplace transform is an important tool to deal with the
problem of nabla discrete fractional order systems, but there is still much
room for its development. In this paper, 14 lemmas are listed to conclude the
existing properties and 14 theorems are developed to describe the innovative
features. On one hand, these properties make the N-transform more effective and
efficient. On the other hand, they enrich the discrete fractional order system
theory
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07395</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07395</id><created>2019-03-13</created><authors><author><keyname>Wiest</keyname><forenames>Thomas</forenames></author><author><keyname>Cummins</keyname><forenames>Nicholas</forenames></author><author><keyname>Baird</keyname><forenames>Alice</forenames></author><author><keyname>Hantke</keyname><forenames>Simone</forenames></author><author><keyname>Dineley</keyname><forenames>Judith</forenames></author><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Voice command generation using Progressive Wavegans</title><categories>cs.CL cs.LG cs.SD eess.AS stat.ML</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative Adversarial Networks (GANs) have become exceedingly popular in a
wide range of data-driven research fields, due in part to their success in
image generation. Their ability to generate new samples, often from only a
small amount of input data, makes them an exciting research tool in areas with
limited data resources. One less-explored application of GANs is the synthesis
of speech and audio samples. Herein, we propose a set of extensions to the
WaveGAN paradigm, a recently proposed approach for sound generation using GANs.
The aim of these extensions - preprocessing, Audio-to-Audio generation, skip
connections and progressive structures - is to improve the human likeness of
synthetic speech samples. Scores from listening tests with 30 volunteers
demonstrated a moderate improvement (Cohen's d coefficient of 0.65) in human
likeness using the proposed extensions compared to the original WaveGAN
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07502</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07502</id><created>2019-03-18</created><authors><author><keyname>Pierrou</keyname><forenames>Georgia</forenames></author><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author></authors><title>Investigating the Impacts of Stochastic Load Fluctuation on Dynamic
  Voltage Stability Margin Using Bifurcation Theory</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the impacts of stochastic load fluctuations, namely the
fluctuation intensity and the changing speed of load power, on the size of the
voltage stability margin. To this end, Stochastic Differential-Algebraic
Equations (SDAEs) are used to model the stochastic load variation; bifurcation
analysis is carried out to explain the influence of stochasticity. Numerical
study and Monte Carlo simulations on the IEEE 14-bus system demonstrate that a
larger fluctuation intensity or a slower load power changing speed may lead to
a smaller voltage stability margin. Particularly, this work may represent the
first attempt to reveal the influence of the time evolution property of the
driving parameters on the voltage stability margin in power systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07554</identifier>
 <datestamp>2020-02-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07554</id><created>2019-03-18</created><updated>2019-04-23</updated><authors><author><keyname>Chandna</keyname><forenames>Pritish</forenames></author><author><keyname>Blaauw</keyname><forenames>Merlijn</forenames></author><author><keyname>Bonada</keyname><forenames>Jordi</forenames></author><author><keyname>Gomez</keyname><forenames>Emilia</forenames></author></authors><title>A Vocoder Based Method For Singing Voice Extraction</title><categories>cs.SD eess.AS</categories><journal-ref>ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</journal-ref><doi>10.1109/ICASSP.2019.8683323</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a novel method for extracting the vocal track from a
musical mixture. The musical mixture consists of a singing voice and a backing
track which may comprise of various instruments. We use a convolutional network
with skip and residual connections as well as dilated convolutions to estimate
vocoder parameters, given the spectrogram of an input mixture. The estimated
parameters are then used to synthesize the vocal track, without any
interference from the backing track. We evaluate our system, through objective
metrics pertinent to audio quality and interference from background sources,
and via a comparative subjective evaluation. We use open-source source
separation systems based on Non-negative Matrix Factorization (NMFs) and Deep
Learning methods as benchmarks for our system and discuss future applications
for this particular algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07577</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07577</id><created>2019-03-18</created><authors><author><keyname>&#xd6;zdemir</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author><author><keyname>Anjinappa</keyname><forenames>Chethan Kumar</forenames></author><author><keyname>Hamila</keyname><forenames>Ridha</forenames></author><author><keyname>Al-Dhahir</keyname><forenames>Naofal</forenames></author><author><keyname>G&#xfc;ven&#xe7;</keyname><forenames>&#x130;smail</forenames></author></authors><title>Joint Frame Synchronization and Channel Estimation: Sparse Recovery
  Approach and USRP Implementation</title><categories>eess.SP</categories><comments>Accepted to IEEE Access (Mar. 8, 2019). arXiv admin note: text
  overlap with arXiv:1709.01474</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation-based techniques used for frame synchronization can suffer
significant performance degradation over multi-path frequency-selective
channels. In this paper, we propose a joint frame synchronization and channel
estimation (JFSCE) framework as a remedy to this problem. This framework,
however, increases the size of the resulting combined channel vector which
should capture both the channel impulse response (CIR) vector and the frame
boundary offset and, therefore, its estimation becomes more challenging. On the
other hand, because the combined channel vector is sparse, sparse channel
estimation methods can be applied. We propose several JFSCE methods using
popular sparse signal recovery (SSR) algorithms which exploit the sparsity of
the combined channel vector. Subsequently, the sparse channel vector estimate
is used to design a sparse equalizer. Our simulation results and experimental
measurements using software defined radios (SDRs) show that in some scenarios
our proposed method improves the overall system performance significantly, in
terms of the mean square error (MSE) between the transmitted and the equalized
symbols compared to the conventional method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07654</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07654</id><created>2019-03-18</created><authors><author><keyname>Chaudhari</keyname><forenames>Shailesh</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Cyclic Weighted Centroid Algorithm for Transmitter Localization in the
  Presence of Interference</title><categories>eess.SP</categories><journal-ref>Published in IEEE Transactions on Cognitive Communications and
  Networking, June 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of localizing a non-cooperative transmitter
in the presence of a spectrally overlapped interferer in a cognitive receiver
(CR) network. It has been observed that the performance of non-cooperative
weighted centroid localization (WCL) algorithm degrades in the presence of a
spectrally overlapped interferer. We propose cyclic WCL algorithm that uses
cyclic autocorrelation (CAC) of received signals at CRs in the network to
estimate the location coordinates of the target transmitter. Performance of the
proposed algorithm is further improved by eliminating CRs in the vicinity of
the interferer from the localization process. In order to identify and
eliminate CRs in the vicinity of the interferer, the ratio of the variance and
the mean of the square of absolute value of the CAC, referred to as feature
variation coefficient, is used. Theoretical analysis of the cyclic WCL
algorithm is presented in order to compute the root mean square error in the
location estimates. We further study the impacts of the interferer's power and
location, CR density, and fading environment on the performance of cyclic WCL.
The comparison between cyclic WCL and traditional WCL is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07655</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07655</id><created>2019-03-18</created><authors><author><keyname>Chaudhari</keyname><forenames>Shailesh</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Cyclic weighted centroid localization for spectrally overlapped sources
  in cognitive radio networks</title><categories>eess.SP</categories><journal-ref>Published in IEEE GLOBECOM, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of localizing spectrally overlapped sources in
cognitive radio networks. A new weighted centroid localization algorithm (WCL)
called Cyclic WCL is proposed, which exploits the cyclostationary feature of
the target signal to estimate its location coordinates. In order to analyze the
algorithm in terms of root-mean-square error (RMSE), we model the location
estimates as the ratios of quadratic forms in a Gaussian random vector. With
analysis and simulation, we show the impact of the interferer location and its
modulation scheme on the RMSE. We also study the RMSE performance of the
algorithm for different power levels of the target and the interference.
Further, the comparison between Cyclic WCL and WCL w/o cyclostationarity is
presented. It is observed that the Cyclic WCL provides significant performance
gain over WCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07656</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07656</id><created>2019-03-18</created><authors><author><keyname>Chaudhari</keyname><forenames>Shailesh</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Unsupervised frequency clustering algorithm for null space estimation in
  wideband spectrum sharing networks</title><categories>eess.SP</categories><journal-ref>Published in IEEE GlobalSIP, Nov. 2017</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spectrum sharing networks, a base station (BS) needs to mitigate the
interference to users associated with other coexisting network in the same
band. The BS can achieve this by transmitting its downlink signal in the null
space of channels to such users. However, under a wideband scenario, the BS
needs to estimate null space matrices using the received signal from such
non-cooperative users in each frequency bin where the users are active. To
reduce the computational complexity of this operation, we propose a frequency
clustering algorithm that exploits the channel correlations among adjacent
frequency bins. The proposed algorithm forms clusters of frequency bins with
correlated channel vectors without prior knowledge of the channels and obtains
a single null space matrix for each cluster. We show that the number of
matrices and the number of eigenvalue decompositions required to obtain the
null space significantly reduce using the proposed clustering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07657</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07657</id><created>2019-03-18</created><authors><author><keyname>Chaudhari</keyname><forenames>Shailesh</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Kuiper test based modulation level classification under unknown
  frequency selective channels</title><categories>eess.SP</categories><journal-ref>Published in IEEE GlobalSIP, Nov. 2017</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of Identifying the modulation level of
the received signal under an unknown frequency selective channel. The
modulation level classification is performed using reduced-complexity Kuiper
(rcK) test which utilizes the distribution of signal features such as magnitude
of the received samples or phase difference in consecutive received samples.
However, in frequency selective channels, these features are severely distorted
resulting in a poor classification performance. We propose to use constant
modulus algorithm (CMA) to mitigate the impact of the frequency selective
channel on the signal feature. Simulation and analytical results show that the
proposed CMA-rcK technique outperforms state-of-the-art cumulant-based
technique as well as blind equalizer-based technique that uses Alphabet Matched
Algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07672</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07672</id><created>2019-03-14</created><updated>2019-03-31</updated><authors><author><keyname>Li</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Wang</keyname><forenames>Zhenpo</forenames></author></authors><title>State of health estimation for lithium-ion battery by combining
  incremental capacity analysis with Gaussian process regression</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The state of health for lithium battery is necessary to ensure the
reliability and safety for battery energy storage system. Accurate prediction
battery state of health plays an extremely important role in guaranteeing
safety and minimizing the maintenance costs. However, the complex
physicochemical characteristics of battery degradation cannot be obtained
directly. Here a novel Gaussian process regression model based on partial
incremental capacity curve is proposed. First, an advanced Gaussian filter
method is applied to obtain the smoothing incremental capacity curves. The
health indexes are then extracted from the partial incremental capacity curves
as the input features of the proposed model. Otherwise, the mean and the
covariance function of the proposed method are applied to predict battery state
of health and the model uncertainty, respectively. Four aging datasets from
NASA data repository are employed for demonstrating the predictive capability
and efficacy of the degradation model using the proposed method. Besides,
different initial health conditions of the tested batteries are used to verify
the robustness and reliability of the proposed method. Results show that the
proposed method can provide accurate and robust state of health estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07822</identifier>
 <datestamp>2019-11-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07822</id><created>2019-03-19</created><updated>2019-11-06</updated><authors><author><keyname>Roy</keyname><forenames>Subhrajit</forenames></author><author><keyname>Kate</keyname><forenames>Kiran</forenames></author><author><keyname>Hirzel</keyname><forenames>Martin</forenames></author></authors><title>A semi-supervised deep learning algorithm for abnormal EEG
  identification</title><categories>cs.LG eess.SP stat.ML</categories><comments>Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended
  Abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems that can automatically analyze EEG signals can aid neurologists by
reducing heavy workload and delays. However, such systems need to be first
trained using a labeled dataset. While large corpuses of EEG data exist, a
fraction of them are labeled. Hand-labeling data increases workload for the
very neurologists we try to aid. This paper proposes a semi-supervised learning
workflow that can not only extract meaningful information from large unlabeled
EEG datasets but also make predictions with minimal supervision, using labeled
datasets as small as 5 examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07824</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07824</id><created>2019-03-19</created><authors><author><keyname>Cheng</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Chen</keyname><forenames>Feiyu</forenames></author><author><keyname>Sandino</keyname><forenames>Christopher</forenames></author><author><keyname>Mardani</keyname><forenames>Morteza</forenames></author><author><keyname>Pauly</keyname><forenames>John M.</forenames></author><author><keyname>Vasanawala</keyname><forenames>Shreyas S.</forenames></author></authors><title>Compressed Sensing: From Research to Clinical Practice with Data-Driven
  Learning</title><categories>eess.IV cs.CV cs.LG physics.med-ph stat.ML</categories><comments>Submitted to the Special Issue on Computational MRI: Compressed
  Sensing and Beyond in the IEEE Signal Processing Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing in MRI enables high subsampling factors while maintaining
diagnostic image quality. This technique enables shortened scan durations
and/or improved image resolution. Further, compressed sensing can increase the
diagnostic information and value from each scan performed. Overall, compressed
sensing has significant clinical impact in improving the diagnostic quality and
patient experience for imaging exams. However, a number of challenges exist
when moving compressed sensing from research to the clinic. These challenges
include hand-crafted image priors, sensitive tuning parameters, and long
reconstruction times. Data-driven learning provides a solution to address these
challenges. As a result, compressed sensing can have greater clinical impact.
In this tutorial, we will review the compressed sensing formulation and outline
steps needed to transform this formulation to a deep learning framework.
Supplementary open source code in python will be used to demonstrate this
approach with open databases. Further, we will discuss considerations in
applying data-driven compressed sensing in the clinical setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07825</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07825</id><created>2019-03-19</created><authors><author><keyname>Roy</keyname><forenames>Subhrajit</forenames></author></authors><title>Machine Learning for removing EEG artifacts: Setting the benchmark</title><categories>eess.SP stat.ML</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electroencephalograms (EEG) are often contaminated by artifacts which make
interpreting them more challenging for clinicians. Hence, automated artifact
recognition systems have the potential to aid the clinical workflow. In this
abstract, we share the first results on applying various machine learning
algorithms to the recently released world's largest open-source artifact
recognition dataset. We envision that these results will serve as a benchmark
for researchers who might work with this dataset in future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07831</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07831</id><created>2019-03-19</created><authors><author><keyname>Chen</keyname><forenames>Qian</forenames></author><author><keyname>Zhang</keyname><forenames>Shunqing</forenames></author><author><keyname>Xu</keyname><forenames>Shugong</forenames></author><author><keyname>Cao</keyname><forenames>Shan</forenames></author></authors><title>Efficient MIMO Detection with Imperfect Channel Knowledge - A Deep
  Learning Approach</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input multiple-output (MIMO) system is the key technology for long
term evolution (LTE) and 5G. The information detection problem at the receiver
side is in general difficult due to the imbalance of decoding complexity and
decoding accuracy within conventional methods. Hence, a deep learning based
efficient MIMO detection approach is proposed in this paper. In our work, we
use a neural network to directly get a mapping function of received signals,
channel matrix and transmitted bit streams. Then, we compare the end-to-end
approach using deep learning with the conventional methods in possession of
perfect channel knowledge and imperfect channel knowledge. Simulation results
show that our method presents a better trade-off in the performance for
accuracy versus decoding complexity. At the same time, better robustness can be
achieved in condition of imperfect channel knowledge compared with conventional
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07922</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07922</id><created>2019-03-19</created><authors><author><keyname>Aldababsa</keyname><forenames>Mahmoud</forenames></author><author><keyname>Kucur</keyname><forenames>O&#x11f;uz</forenames></author></authors><title>Performance of Cooperative NOMA with MRT/RAS in Nakagami-m Fading
  Channels with Channel Estimation Errors</title><categories>eess.SP</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performance of a downlink non-orthogonal multiple
access (NOMA) based cooperative network with maximal ratio transmission/receive
antenna selection (MRT/RAS) over Nakagami-m fading channels in the presence of
channel estimation errors (CEEs). In the system, a base station communicates
with multiple mobile users through a half duplex channel state information
based amplify-and-forward relay. All nodes are equipped with multiple antennas
and the hybrid diversity technique MRT/RAS is employed in both hops. The outage
behavior of the system is investigated by driving closed-form expression for
outage probability (OP). In addition, the corresponding lower and upper bounds
of the derived OP are obtained. Moreover, the behavior of the system is studied
in high signal-to-noise ratio region by obtaining an error floor value in the
presence of CEE as well as achieving diversity and array gains in the absence
of CEE. Finally, the analytical results in the presence and absence of the CEEs
are verified by the Monte Carlo simulations. Results show that the MRT/RAS
scheme enhances the OP significantly and is much more robust to the CEEs in
comparison with the single antenna case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07929</identifier>
 <datestamp>2019-05-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07929</id><created>2019-03-19</created><updated>2019-05-13</updated><authors><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Johan</forenames></author><author><keyname>Skog</keyname><forenames>Isaac</forenames></author><author><keyname>Gustafsson</keyname><forenames>Fredrik</forenames></author><author><keyname>Markham</keyname><forenames>Andrew</forenames></author><author><keyname>Trigoni</keyname><forenames>Niki</forenames></author></authors><title>Zero-Velocity Detection - A Bayesian Approach to Adaptive Thresholding</title><categories>eess.SP stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bayesian zero-velocity detector for foot-mounted inertial navigation
systems is presented. The detector extends existing zero-velocity detectors
based on the likelihood-ratio test, and allows, possibly time-dependent, prior
information about the two hypotheses - the sensors being stationary or in
motion - to be incorporated into the test. It is also possible to incorporate
information about the cost of a missed detection or a false alarm.
Specifically, we consider an hypothesis prior based on the velocity estimates
provided by the navigation system and an exponential model for how the cost of
a missed detection increases with the time since the last zero-velocity update.
Thereby, we obtain a detection threshold that adapts to the motion
characteristics of the user. Thus, the proposed detection framework efficiently
solves one of the key challenges in current zero-velocity-aided inertial
navigation systems: the tuning of the zero-velocity detection threshold. A
performance evaluation on data with normal and fast gait demonstrates that the
proposed detection framework outperforms any detector that chooses two separate
fixed thresholds for the two gait speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07978</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07978</id><created>2019-03-15</created><authors><author><keyname>Pinel</keyname><forenames>Dimitri</forenames></author><author><keyname>Korp&#xe5;s</keyname><forenames>Magnus</forenames></author><author><keyname>Lindberg</keyname><forenames>Karen B.</forenames></author></authors><title>Cost Optimal Design of Zero Emission Neighborhoods' (ZENs) Energy
  System: Model Presentation and Case Study on Evenstad</title><categories>physics.soc-ph eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zero Emission Neighborhoods (ZEN) is a concept studied in Norway to reduce
the CO_2 emission of neighborhoods. One question coming along this concept is
how to design the energy system of such neighborhoods to fit the ZEN
definition. From this definition we extract the CO_2 balance, requiring an
annual net zero emission of CO_2 in the lifetime of the neighborhood. This
paper proposes a MILP model for obtaining cost optimal design of ZEN's energy
system and demonstrates it on a case study. Different technologies are included
as investment options and, notably PV as an on-site electricity production
mean. Wind turbines are not included in this study because inappropriate in the
context of most cities. The results for the case study highlight the importance
of PV investment in reaching the ZEN requirements. For example, around 850 kW
of solar is needed for our test cases of 10 000 m^2 of floor area, for an
annual energy demand of around 700 MWh of electricity and 620 MWh of heat. The
investments in other technologies are small in comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07981</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07981</id><created>2019-03-15</created><authors><author><keyname>Ak&#x131;n</keyname><forenames>Cihan</forenames></author><author><keyname>Kacar</keyname><forenames>Umit</forenames></author><author><keyname>Kirci</keyname><forenames>Murvet</forenames></author></authors><title>Twins Recognition with Multi Biometric System: Handcrafted-Deep Learning
  Based Multi Algorithm with Voice-Ear Recognition Based Multi Modal</title><categories>cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of technology, the usage areas and importance of
biometric systems have started to increase. Since the characteristics of each
person are different from each other, a single model biometric system can yield
successful results. However, because the characteristics of twin people are
very close to each other, multiple biometric systems including multiple
characteristics of individuals will be more appropriate and will increase the
recognition rate. In this study, a multiple biometric recognition system
consisting of a combination of multiple algorithms and multiple models was
developed to distinguish people from other people and their twins. Ear and
voice biometric data were used for the multimodal model and 38 pair of twin ear
images and sound recordings were used in the data set. Sound and ear
recognition rates were obtained using classical (hand-crafted) and deep
learning algorithms. The results obtained were combined with the score level
fusion method to achieve a success rate of 94.74% in rank-1 and 100% in rank
-2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07987</identifier>
 <datestamp>2019-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07987</id><created>2019-03-16</created><updated>2019-09-21</updated><authors><author><keyname>Kar</keyname><forenames>O&#x11f;uzhan Fatih</forenames></author><author><keyname>Oktem</keyname><forenames>Figen S.</forenames></author></authors><title>Compressive Spectral Imaging with Diffractive Lenses</title><categories>eess.IV eess.SP physics.optics</categories><comments>4 pages, 4 figures, published in Optics Letters (Vol.44, Issue 18,
  pp. 4582-4585 (2019))</comments><journal-ref>Opt. Lett. 44, 4582-4585 (2019)</journal-ref><doi>10.1364/OL.44.004582</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive spectral imaging enables to reconstruct the entire
three-dimensional (3D) spectral cube from a few multiplexed images. Here, we
develop a novel compressive spectral imaging technique using diffractive
lenses. Our technique uses a coded aperture to spatially modulate the optical
field from the scene and a diffractive lens such as a photon-sieve for
dispersion. The coded field is passed through the diffractive lens and then
measured at a few planes using a monochrome detector. The 3D spectral cube is
then reconstructed from these highly compressed measurements through sparse
recovery. A fast sparse recovery method is developed to solve this large-scale
inverse problem. The imaging performance is illustrated at visible regime for
various scenarios with different compression ratios through numerical
simulations. The results demonstrate that promising reconstruction performance
can be achieved with as little as two measurements. This opens up new
possibilities for high resolution spectral imaging with low-cost and simple
designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.07988</identifier>
 <datestamp>2019-12-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.07988</id><created>2019-03-18</created><authors><author><keyname>Gr&#xf8;vik</keyname><forenames>Endre</forenames></author><author><keyname>Yi</keyname><forenames>Darvin</forenames></author><author><keyname>Iv</keyname><forenames>Michael</forenames></author><author><keyname>Tong</keyname><forenames>Elisabeth</forenames></author><author><keyname>Rubin</keyname><forenames>Daniel L.</forenames></author><author><keyname>Zaharchuk</keyname><forenames>Greg</forenames></author></authors><title>Deep Learning Enables Automatic Detection and Segmentation of Brain
  Metastases on Multi-Sequence MRI</title><categories>eess.IV cs.LG stat.ML</categories><doi>10.1002/jmri.26766</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting and segmenting brain metastases is a tedious and time-consuming
task for many radiologists, particularly with the growing use of multi-sequence
3D imaging. This study demonstrates automated detection and segmentation of
brain metastases on multi-sequence MRI using a deep learning approach based on
a fully convolution neural network (CNN). In this retrospective study, a total
of 156 patients with brain metastases from several primary cancers were
included. Pre-therapy MR images (1.5T and 3T) included pre- and post-gadolinium
T1-weighted 3D fast spin echo, post-gadolinium T1-weighted 3D axial IR-prepped
FSPGR, and 3D fluid attenuated inversion recovery. The ground truth was
established by manual delineation by two experienced neuroradiologists. CNN
training/development was performed using 100 and 5 patients, respectively, with
a 2.5D network based on a GoogLeNet architecture. The results were evaluated in
51 patients, equally separated into those with few (1-3), multiple (4-10), and
many (&gt;10) lesions. Network performance was evaluated using precision, recall,
Dice/F1 score, and ROC-curve statistics. For an optimal probability threshold,
detection and segmentation performance was assessed on a per metastasis basis.
The area under the ROC-curve (AUC), averaged across all patients, was 0.98. The
AUC in the subgroups was 0.99, 0.97, and 0.97 for patients having 1-3, 4-10,
and &gt;10 metastases, respectively. Using an average optimal probability
threshold determined by the development set, precision, recall, and Dice-score
were 0.79, 0.53, and 0.79, respectively. At the same probability threshold, the
network showed an average false positive rate of 8.3/patient (no lesion-size
limit) and 3.4/patient (10 mm3 lesion size limit). In conclusion, a deep
learning approach using multi-sequence MRI can aid in the detection and
segmentation of brain metastases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08068</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08068</id><created>2019-03-19</created><authors><author><keyname>Yang</keyname><forenames>Zhaohui</forenames></author><author><keyname>Chen</keyname><forenames>Mingzhe</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Shikh-Bahaei</keyname><forenames>Mohammad</forenames></author></authors><title>Optimization of Rate Allocation and Power Control for Rate Splitting
  Multiple Access (RSMA)</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, the sum-rate maximization problem is studied for wireless
networks that use downlink rate splitting multiple access (RSMA). In the
considered model, each base station (BS) divides the messages that must be
transmitted to its users into a `private' part and a `common' part. Here, the
common message is a message that all users want to receive and the private
message is a message that is dedicated to only a specific user. The RSMA
mechanism enables a BS to adjust the split of common and private messages so as
to control the interference by decoding and treating interference as noise and,
thus optimizing the data rate of users. To maximize the users' sum-rate, the
network can determine the rate allocation for the common message to meet the
rate demand, and adjust the transmit power for the private message to reduce
the interference. This problem is formulated as an optimization problem whose
goal is to maximize the sum-rate of all users. To solve this nonconvex
maximization problem, the optimal power used for transmitting the private
message to the users is first obtained in closed form for a given rate
allocation and common message power. Based on the optimal private message
transmission power, the optimal rate allocation is then derived under a fixed
common message transmission power. Subsequently, a one-dimensional search
algorithm is proposed to obtain the optimal solution of common message
transmission power. Simulation results show that the RSMA can achieve up to
15.6\% and 21.5\% gains in terms of data rate compared to non-orthogonal
multiple access (NOMA) and orthogonal frequency-division multiple access
(OFDMA), respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08070</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08070</id><created>2019-03-19</created><authors><author><keyname>Bomfin</keyname><forenames>Roberto</forenames></author><author><keyname>Chafii</keyname><forenames>Marwa</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Low-Complexity Iterative Receiver for Orthogonal Chirp Division
  Multiplexing</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a low-complexity iterative receiver for the recently
proposed Orthogonal Chirp Division Multiplexing (OCDM) modulation scheme, where
we consider a system under frequency-selective channels and constrained to
channel state information availability only at the receiver. It has been shown
that under these assumptions, OCDM becomes an optimal waveform in terms of
performance, i.e., frame error rate (FER), when employing a receiver capable of
achieving perfect feedback equalizer (PFE) performance. Thus, this work targets
proposing such a receiver for OCDM with low-complexity. Our approach is based
on the well accepted minimum mean squared error with parallel interference
cancellation (MMSE-PIC), where we derive an approximated equalizer whose
complexity is reduced to two fast Fourier transforms (FFTs) per iteration. The
FER results reveal that i) the proposed low-complexity receiver perform as good
as the original MMSE-PIC, ii) OCDM performs very closely to PFE, and iii) OCDM
has approximately 2.5 dB improvement over OFDM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08100</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08100</id><created>2019-03-19</created><authors><author><keyname>Lu</keyname><forenames>Diyuan</forenames></author><author><keyname>Triesch</keyname><forenames>Jochen</forenames></author></authors><title>Residual Deep Convolutional Neural Network for EEG Signal Classification
  in Epilepsy</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epilepsy is the fourth most common neurological disorder, affecting about 1%
of the population at all ages. As many as 60% of people with epilepsy
experience focal seizures which originate in a certain brain area and are
limited to part of one cerebral hemisphere. In focal epilepsy patients, a
precise surgical removal of the seizure onset zone can lead to effective
seizure control or even a seizure-free outcome. Thus, correct identification of
the seizure onset zone is essential. For clinical evaluation purposes,
electroencephalography (EEG) recordings are commonly used. However, their
interpretation is usually done manually by physicians and is time-consuming and
error-prone. In this work, we propose an automated epileptic signal
classification method based on modern deep learning methods. In contrast to
previous approaches, the network is trained directly on the EEG recordings,
avoiding hand-crafted feature extraction and selection procedures. This
exploits the ability of deep neural networks to detect and extract relevant
features automatically, that may be too complex or subtle to be noticed by
humans. The proposed network structure is based on a convolutional neural
network with residual connections. We demonstrate that our network produces
state-of-the-art performance on two benchmark data sets, a data set from Bonn
University and the Bern-Barcelona data set. We conclude that modern deep
learning approaches can reach state-of-the-art performance on epileptic EEG
classification and automated seizure onset zone identification tasks when
trained on raw EEG data. This suggests that such approaches have potential for
improving clinical practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08111</identifier>
 <datestamp>2020-01-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08111</id><created>2019-03-19</created><updated>2020-01-21</updated><authors><author><keyname>Marie-Caroline</keyname><forenames>Corbineau</forenames></author><author><keyname>Denis</keyname><forenames>Kouam&#xe9;</forenames></author><author><keyname>Emilie</keyname><forenames>Chouzenoux</forenames></author><author><keyname>Jean-Yves</keyname><forenames>Tourneret</forenames></author><author><keyname>Jean-Christophe</keyname><forenames>Pesquet</forenames></author></authors><title>Preconditioned P-ULA for Joint Deconvolution-Segmentation of Ultrasound
  Images -- Extended Version</title><categories>eess.IV cs.CV</categories><journal-ref>In IEEE Signal Processing Letters, vol.26, no.10, pp.1456-1460
  (2019)</journal-ref><doi>10.1109/LSP.2019.2935610</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint deconvolution and segmentation of ultrasound images is a challenging
problem in medical imaging. By adopting a hierarchical Bayesian model, we
propose an accelerated Markov chain Monte Carlo scheme where the tissue
reflectivity function is sampled thanks to a recently introduced proximal
unadjusted Langevin algorithm. This new approach is combined with a
forward-backward step and a preconditioning strategy to accelerate the
convergence, and with a method based on the majorization-minimization principle
to solve the inner nonconvex minimization problems. As demonstrated in
numerical experiments conducted on both simulated and in vivo ultrasound
images, the proposed method provides high-quality restoration and segmentation
results and is up to six times faster than an existing Hamiltonian Monte Carlo
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08154</identifier>
 <datestamp>2019-09-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08154</id><created>2019-03-19</created><updated>2019-09-27</updated><authors><author><keyname>Kim</keyname><forenames>Minsu</forenames></author><author><keyname>Lee</keyname><forenames>Jemin</forenames></author></authors><title>Impact of an Interfering Node on Unmanned Aerial Vehicle Communications</title><categories>eess.SP</categories><comments>12 pages, 10 figures, this paper has been submitted in IEEE
  Transactions on Vehicular Technology. arXiv admin note: substantial text
  overlap with arXiv:1806.09843</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike terrestrial communications, unmanned aerial vehicle (UAV)
communications have some advantages such as the line-of-sight (LoS) environment
and flexible mobility. However, the interference will be still inevitable. In
this paper, we analyze the effect of an interfering node on the UAV
communications by considering the LoS probability and different channel fading
for LoS and non-line-of-sight (NLoS) links, which are affected by the elevation
angle of the communication link. We then derive a closed-form outage
probability in the presence of an interfering node for all the possible
scenarios and environments of main and interference links. After discussing the
impacts of transmitting and interfering node parameters on the outage
probability, we show the existence of the optimal height of the UAV that
minimize the outage probability. We also show the NLoS environment can be
better than the LoS environment if the average received power of the
interference is more dominant than that of the transmitting signal on UAV
communications. Finally, we analyze the outage probability for the case of
multiple interfering nodes using stochastic geometry and the outage probability
of the single interfering node case, and show the effect of the interfering
node density on the optimal height of the UAV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08165</identifier>
 <datestamp>2019-03-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08165</id><created>2019-03-19</created><authors><author><keyname>Cassady</keyname><forenames>Philip</forenames></author></authors><title>Bayesian Analysis of Target Detection with Enhanced Receiver Operating
  Characteristic</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we develop a Bayesian analysis of radar target detection
that uses the parameters of conventional radar analysis to provide a valid
prediction of target presence or absence when received signals cross or fail to
cross chosen threshold values. A Positive Predictive Value parameter is added
to the normal Receiver Operating Characteristic to provide information that
allows the radar operator to make an informed decision in the choice of
threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08207</identifier>
 <datestamp>2019-03-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08207</id><created>2019-03-19</created><authors><author><keyname>Choi</keyname><forenames>Thomas</forenames></author><author><keyname>Rottenberg</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Luo</keyname><forenames>Peng</forenames></author><author><keyname>Zhang</keyname><forenames>Jianzhong</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>How Many Antennas Do We Need for Massive MIMO Channel Sounding? -
  Validating Through Measurement</title><categories>eess.SP</categories><comments>2 pages, 2 figures, IEEE APS/URSI 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the impact of the number of antennas (8 to 64) and
the array configuration on massive MIMO channel parameters estimation for
multiple propagation scenarios at 3.5 GHz. Different measurement environments
are artificially created by placing several reflectors and absorbers in an
anechoic chamber. Ground truth channel parameters, e.g, path angles, are
obtained by geometry and trigonometric rules. Then, these are compared to the
channel parameters extracted by the applying Space-Alternating Generalized
Expectation-Maximization (SAGE) algorithm on the measurements. Overall, the
estimation errors for various array configurations and the multiple
environments are compared. This paper will help to determine the appropriate
configuration of the antenna array and the parameter extraction algorithm for
outdoor massive MIMO channel sounding campaigns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08260</identifier>
 <datestamp>2019-05-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08260</id><created>2019-03-19</created><updated>2019-03-26</updated><authors><author><keyname>Fitzgerald</keyname><forenames>Emma</forenames></author><author><keyname>Pi&#xf3;ro</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames></author></authors><title>Massive MIMO Optimization with Compatible Sets</title><categories>eess.SP cs.NI</categories><journal-ref>IEEE Transactions on Wireless Communications, volume 19, issue 5,
  pages 2794 - 2812, 2019</journal-ref><doi>10.1109/TWC.2019.2908362</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) is expected to be a vital
component in future 5G systems. As such, there is a need for new modeling in
order to investigate the performance of massive MIMO not only at the physical
layer, but also higher up the networking stack. In this paper, we present
general optimization models for massive MIMO, based on mixed-integer
programming and compatible sets, with both maximum ratio combing and zero
forcing precoding schemes. We then apply our models to the case of joint device
scheduling and power control for heterogeneous devices and traffic demands, in
contrast to existing power control schemes that consider only homogeneous users
and saturated scenarios. Our results show substantial benefits in terms of
energy usage can be achieved without sacrificing throughput, and that both
signalling overhead and the complexity of end devices can be reduced by
abrogating the need for uplink power control through efficient scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08345</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08345</id><created>2019-03-20</created><authors><author><keyname>Olbinado</keyname><forenames>Margie P.</forenames></author><author><keyname>Paganin</keyname><forenames>David M.</forenames></author><author><keyname>Cheng</keyname><forenames>Yin</forenames></author><author><keyname>Rack</keyname><forenames>Alexander</forenames></author></authors><title>Phase-sensitive x-ray ghost imaging</title><categories>eess.IV physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imaging with hard x-rays is an invaluable tool in medicine, biology,
materials science, and cultural heritage. Propagation-based x-ray
phase-contrast imaging and tomography have been mostly used to resolve
micrometer-scale structures inside weakly absorbing objects as well as inside
dense specimens. Indirect x-ray detection has been the key technology to
achieve up to sub-micrometer spatial resolutions, albeit inefficiently and
hence at the expense of increased radiation dose to the specimen. A promising
approach to low-dose imaging and high spatial resolution even at high x-ray
energies is ghost imaging, which could use single-pixel, yet efficient direct
x-ray detectors made of high-density materials. However, phase contrast has not
yet been realised with x-ray ghost imaging. We present an approach which
exploits both the advantages of x-ray ghost imaging and the high sensitivity of
phase-contrast imaging. In comparison with existing techniques, our method is
efficient and achieves high-fidelity x-ray ghost images with phase contrast,
accurate density resolution and dramatically higher spatial resolution. The
method is scalable to practical tomography with large fields of view,
micrometer spatial resolution, and with high-energy x-rays above 100 keV. It is
also applicable to other phase-sensitive imaging techniques and with other
probes such as neutrons, alpha rays, and muons, for which high spatial
resolution detectors are limited or even not available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08347</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08347</id><created>2019-03-20</created><updated>2019-04-29</updated><authors><author><keyname>Gupta</keyname><forenames>Anant</forenames></author><author><keyname>Ingle</keyname><forenames>Atul</forenames></author><author><keyname>Velten</keyname><forenames>Andreas</forenames></author><author><keyname>Gupta</keyname><forenames>Mohit</forenames></author></authors><title>Photon-Flooded Single-Photon 3D Cameras</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single photon avalanche diodes (SPADs) are starting to play a pivotal role in
the development of photon-efficient, long-range LiDAR systems. However, due to
non-linearities in their image formation model, a high photon flux (e.g., due
to strong sunlight) leads to distortion of the incident temporal waveform, and
potentially, large depth errors. Operating SPADs in low flux regimes can
mitigate these distortions, but, often requires attenuating the signal and
thus, results in low signal-to-noise ratio. In this paper, we address the
following basic question: what is the optimal photon flux that a SPAD-based
LiDAR should be operated in? We derive a closed form expression for the optimal
flux, which is quasi-depth-invariant, and depends on the ambient light
strength. The optimal flux is lower than what a SPAD typically measures in real
world scenarios, but surprisingly, considerably higher than what is
conventionally suggested for avoiding distortions. We propose a simple,
adaptive approach for achieving the optimal flux by attenuating incident flux
based on an estimate of ambient light strength. Using extensive simulations and
a hardware prototype, we show that the optimal flux criterion holds for several
depth estimators, under a wide range of illumination conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08459</identifier>
 <datestamp>2019-03-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08459</id><created>2019-03-20</created><authors><author><keyname>Roy</keyname><forenames>Pierre</forenames></author><author><keyname>Pachet</keyname><forenames>Francois</forenames></author></authors><title>Smart Edition of MIDI Files</title><categories>cs.SD eess.AS</categories><comments>20 pages, 16 figures, 14 audio (MIID and MP3) examples</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of editing musical performance data, in particular MIDI
files representing human musical performances. Editing such sequences raises
specific issues due to the ambiguous nature of musical objects. The first
source of ambiguity is that musicians naturally produce many deviations from
the metrical frame. These deviations may be intentional or subconscious, but
they play an important role in conveying the groove or feeling of a
performance. Relations between musical elements are also usually implicit,
creating even more ambiguity. A note is in relation with the surrounding notes
in many possible ways: it can be part of a melodic pattern, it can also play a
harmonic role with the simultaneous notes, or be a pedal-tone. All these
aspects play an essential role that should be preserved, as much as possible,
when editing musical sequences.
  In this paper, we contribute specifically to the problem of editing
non-quantized, metrical musical sequences represented as MIDI files. We first
list of number of problems caused by the use of naive edition operations
applied to performance data, using a motivating example. We then introduce a
model, called Dancing MIDI, based on 1) two desirable, well-defined properties
for edit operations and 2) two well-defined operations, Split and Concat, with
an implementation. We show that our model formally satisfies the two
properties, and that it prevents most of the problems that occur with naive
edit operations on our motivating example, as well as on a real-world example
using an automatic harmonizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08468</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08468</id><created>2019-03-20</created><updated>2019-10-01</updated><authors><author><keyname>Coluccia</keyname><forenames>Angelo</forenames></author><author><keyname>Ricci</keyname><forenames>Giuseppe</forenames></author><author><keyname>Besson</keyname><forenames>Olivier</forenames></author></authors><title>Design of robust radar detectors through random perturbation of the
  target signature</title><categories>eess.SP</categories><journal-ref>IEEE Transactions on Signal Processing, Vol. 67, Issue 19, Oct.
  2019</journal-ref><doi>10.1109/TSP.2019.2935915</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses the problem of designing radar detectors more robust than
Kelly's detector to possible mismatches of the assumed target signature, but
with no performance degradation under matched conditions. The idea is to model
the received signal under the signal-plus-noise hypothesis by adding a random
component, parameterized via a design covariance matrix, that makes the
hypothesis more plausible in presence of mismatches. Moreover, an unknown power
of such component, to be estimated from the observables, can lead to no
performance loss. Derivation of the (one-step) GLRT is provided for two choices
of the design matrix, obtaining detectors with different complexity and
behavior. A third parametric detector is also obtained by an ad-hoc
generalization of one of such GLRTs. The analysis shows that the proposed
approach can cover a range of different robustness levels that is not
achievable by state-of-the-art with the same performance of Kelly's detector
under matched conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08514</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08514</id><created>2019-03-20</created><authors><author><keyname>Bello</keyname><forenames>Juan Luis Gonzalez</forenames></author><author><keyname>Kim</keyname><forenames>Munchurl</forenames></author></authors><title>A Novel Monocular Disparity Estimation Network with Domain
  Transformation and Ambiguity Learning</title><categories>eess.IV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNN) have shown state-of-the-art results for
low-level computer vision problems such as stereo and monocular disparity
estimations, but still, have much room to further improve their performance in
terms of accuracy, numbers of parameters, etc. Recent works have uncovered the
advantages of using an unsupervised scheme to train CNN's to estimate monocular
disparity, where only the relatively-easy-to-obtain stereo images are needed
for training. We propose a novel encoder-decoder architecture that outperforms
previous unsupervised monocular depth estimation networks by (i) taking into
account ambiguities, (ii) efficient fusion between encoder and decoder features
with rectangular convolutions and (iii) domain transformations between encoder
and decoder. Our architecture outperforms the Monodepth baseline in all
metrics, even with a considerable reduction of parameters. Furthermore, our
architecture is capable of estimating a full disparity map in a single forward
pass, whereas the baseline needs two passes. We perform extensive experiments
to verify the effectiveness of our method on the KITTI dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08548</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08548</id><created>2019-03-20</created><updated>2019-05-22</updated><authors><author><keyname>Quach</keyname><forenames>Maurice</forenames></author><author><keyname>Valenzise</keyname><forenames>Giuseppe</forenames></author><author><keyname>Dufaux</keyname><forenames>Frederic</forenames></author></authors><title>Learning Convolutional Transforms for Lossy Point Cloud Geometry
  Compression</title><categories>cs.CV cs.LG eess.IV stat.ML</categories><comments>Published in ICIP 2019. The source code can be found at
  https://github.com/mauriceqch/pcc_geo_cnn and the supplementary material can
  be found at https://www.mauricequach.com/pcc_geo_cnn_samples</comments><doi>10.1109/ICIP.2019.8803413</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient point cloud compression is fundamental to enable the deployment of
virtual and mixed reality applications, since the number of points to code can
range in the order of millions. In this paper, we present a novel data-driven
geometry compression method for static point clouds based on learned
convolutional transforms and uniform quantization. We perform joint
optimization of both rate and distortion using a trade-off parameter. In
addition, we cast the decoding process as a binary classification of the point
cloud occupancy map. Our method outperforms the MPEG reference solution in
terms of rate-distortion on the Microsoft Voxelized Upper Bodies dataset with
51.5% BDBR savings on average. Moreover, while octree-based methods face
exponential diminution of the number of points at low bitrates, our method
still produces high resolution outputs even at low bitrates. Code and
supplementary material are available at
https://github.com/mauriceqch/pcc_geo_cnn .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08742</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08742</id><created>2019-03-20</created><authors><author><keyname>Mai</keyname><forenames>Vien V.</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Noisy Accelerated Power Method for Eigenproblems with Applications</title><categories>math.OC cs.LG eess.SP</categories><comments>Accepted for publication in the IEEE Transaction on Signal Processing</comments><doi>10.1109/TSP.2019.2908126</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces an efficient algorithm for finding the dominant
generalized eigenvectors of a pair of symmetric matrices. Combining tools from
approximation theory and convex optimization, we develop a simple scalable
algorithm with strong theoretical performance guarantees. More precisely, the
algorithm retains the simplicity of the well-known power method but enjoys the
asymptotic iteration complexity of the powerful Lanczos method. Unlike these
classic techniques, our algorithm is designed to decompose the overall problem
into a series of subproblems that only need to be solved approximately. The
combination of good initializations, fast iterative solvers, and appropriate
error control in solving the subproblems lead to a linear running time in the
input sizes compared to the superlinear time for the traditional methods. The
improved running time immediately offers acceleration for several applications.
As an example, we demonstrate how the proposed algorithm can be used to
accelerate canonical correlation analysis, which is a fundamental statistical
tool for learning of a low-dimensional representation of high-dimensional
objects. Numerical experiments on real-world data sets confirm that our
approach yields significant improvements over the current state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08756</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08756</id><created>2019-03-20</created><authors><author><keyname>Arronte-Alvarez</keyname><forenames>Aitor</forenames></author><author><keyname>G&#xf3;mez-Martin</keyname><forenames>Francisco</forenames></author></authors><title>Distributed Vector Representations of Folksong Motifs</title><categories>cs.IR cs.CL cs.LG cs.SD eess.AS</categories><comments>MCM 19</comments><acm-class>H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a distributed vector representation model for learning
folksong motifs. A skip-gram version of word2vec with negative sampling is used
to represent high quality embeddings. Motifs from the Essen Folksong collection
are compared based on their cosine similarity. A new evaluation method for
testing the quality of the embeddings based on a melodic similarity task is
presented to show how the vector space can represent complex contextual
features, and how it can be utilized for the study of folksong variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08827</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08827</id><created>2019-03-21</created><authors><author><keyname>Girault</keyname><forenames>Benjamin</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>What's in a frequency: new tools for graph Fourier Transform
  visualization</title><categories>eess.SP</categories><comments>Paper presented at 2019 Information Theory and Applications Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in graph signal processing (GSP) has addressed a number of
problems, including sampling and filtering. Proposed methods have focused on
generic graphs and defined signals with certain characteristics, e.g.,
bandlimited signals, based on t he graph Fourier transform (GFT). However, the
effect of GFT properties (e.g., vertex localization) on the behavior of such
methods is not as well understood. In this paper, we propose novel GFT
visualization tools and provide some examples to illustrate certain GFT
properties and their impact on sampling or wavelet transforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08835</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08835</id><created>2019-03-21</created><authors><author><keyname>Hadizadeh</keyname><forenames>Ehsan</forenames></author><author><keyname>Rabbani</keyname><forenames>Rozhan</forenames></author><author><keyname>Azizi</keyname><forenames>Zohreh</forenames></author><author><keyname>Barekatain</keyname><forenames>Matin</forenames></author><author><keyname>Hakhamaneshi</keyname><forenames>Kourosh</forenames></author><author><keyname>Khoram</keyname><forenames>Erfan</forenames></author><author><keyname>Fotowat-Ahmady</keyname><forenames>Ali</forenames></author></authors><title>Ultra Low-Power System for Remote ECG Monitoring</title><categories>eess.SP</categories><comments>9 pages, 10 figures, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complete system solution extracting signals from the patient chest with
three leads including motion artifact removal in both analog and digital
implementations are described. The resulting ECG signal is transferred via
Bluetooth low energy to a mobile phone. Using deep sleep modes, the overall
power consumption is less than 300 $\mu$A and the device can operate for more
than 20 days using a 150mAh battery. The screening software looks for
suspicious traces such as those with missing pulses, tachycardia, bradycardia,
etc. The mobile phone software also eliminates any remaining motion artifact.
The traces are subsequently processed in detail in a cloud server and to a
physicians dashboard for long-term monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08860</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08860</id><created>2019-03-21</created><authors><author><keyname>Feng</keyname><forenames>Tianxin</forenames></author><author><keyname>Ma</keyname><forenames>Ganggang</forenames></author><author><keyname>Xu</keyname><forenames>Jie</forenames></author></authors><title>Cognitive Wireless Power Transfer in the Presence of Reactive Primary
  Communication User</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a cognitive or secondary multi-antenna wireless power
transfer (WPT) system over a multi-carrier channel, which shares the same
spectrum with a primary wireless information transfer (WIT) system that employs
adaptive water-filling power allocation. By controlling the transmit energy
beamforming over sub-carriers (SCs), the secondary energy transmitter (S-ET)
can directly charge the secondary energy receiver (S-ER), even purposely
interfere with the primary WIT system, such that the primary information
transmitter (P-IT) can reactively adjust its power allocation (based on
water-filling) to facilitate the S-ER's energy harvesting. We investigate how
the secondary WPT system can exploit the primary WIT system's reactive power
allocation, for improving the wireless energy harvesting performance. In
particular, our objective is to maximize the total energy received at the S-ER
from both the S-ET and the P-IT, by optimizing the S-ET's energy beamforming
over SCs, subject to its maximum transmit power constraint, and the maximum
interference power constraint imposed at the primary information receiver
(P-IR) to protect the primary WIT. Although the formulated problem is
non-convex and difficult to be optimally solved in general, we propose an
efficient algorithm to obtain a high-quality solution by employing the Lagrange
dual method together with a one-dimensional search. We also present two
benchmark energy beamforming designs based on the zero-forcing (ZF) and
maximum-ratio-transmission (MRT) principles, respectively, as well as the
conventional design without considering the primary WIT system's reaction.
Numerical results show that our proposed design leads to significantly improved
energy harvesting performance at the S-ER, as compared to these benchmark
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08864</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08864</id><created>2019-03-21</created><authors><author><keyname>Iesmantas</keyname><forenames>Tomas</forenames></author><author><keyname>Alzbutas</keyname><forenames>Robertas</forenames></author></authors><title>Convolutional neural network for detection and classification of
  seizures in clinical data</title><categories>eess.SP cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epileptic seizure detection and classification in clinical
electroencephalogram data still is a challenge, and only low sensitivity with a
high rate of false positives has been achieved with commercially available
seizure detection tools, which usually are patient non-specific. Epilepsy
patients suffer from severe detrimental effects like physical injury or
depression due to unpredictable seizures. However, even in hospitals due to the
high rate of false positives the seizure alert systems are of poor help for
patients as tools of seizure detection are mostly trained on unrealistically
clean data, containing little noise and obtained under controlled laboratory
conditions, where patient groups are homogeneous, e.g. in terms of age or type
of seizures. In this study authors present the approach for detection and
classification of a seizure using clinical data of electroencephalograms and a
convolutional neural network trained on features of brain synchronisation and
power spectrum. Various deep learning methods were applied, and the network was
trained on very heterogeneous clinical electroencephalogram dataset. In total,
eight different types of seizures were considered, and the patients were of
various ages, health conditions and they were observed under clinical
conditions. Despite this, classifier presented in this paper achieved
sensitivity and specificity equal to 0.68 and 0.67, accordingly, which is a
significant improvement as compared to the known results for clinical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08871</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08871</id><created>2019-03-21</created><authors><author><keyname>Tang</keyname><forenames>Xiwei</forenames></author><author><keyname>Bi</keyname><forenames>Xuan</forenames></author><author><keyname>Qu</keyname><forenames>Annie</forenames></author></authors><title>Individualized Multilayer Tensor Learning with An Application in Imaging
  Analysis</title><categories>stat.ML cs.CV cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is motivated by multimodality breast cancer imaging data, which is
quite challenging in that the signals of discrete tumor-associated
microvesicles (TMVs) are randomly distributed with heterogeneous patterns. This
imposes a significant challenge for conventional imaging regression and
dimension reduction models assuming a homogeneous feature structure. We develop
an innovative multilayer tensor learning method to incorporate heterogeneity to
a higher-order tensor decomposition and predict disease status effectively
through utilizing subject-wise imaging features and multimodality information.
Specifically, we construct a multilayer decomposition which leverages an
individualized imaging layer in addition to a modality-specific tensor
structure. One major advantage of our approach is that we are able to
efficiently capture the heterogeneous spatial features of signals that are not
characterized by a population structure as well as integrating multimodality
information simultaneously. To achieve scalable computing, we develop a new
bi-level block improvement algorithm. In theory, we investigate both the
algorithm convergence property, tensor signal recovery error bound and
asymptotic consistency for prediction model estimation. We also apply the
proposed method for simulated and human breast cancer imaging data. Numerical
results demonstrate that the proposed method outperforms other existing
competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08876</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08876</id><created>2019-03-21</created><authors><author><keyname>Takeuchi</keyname><forenames>Daiki</forenames></author><author><keyname>Yatabe</keyname><forenames>Kohei</forenames></author><author><keyname>Koizumi</keyname><forenames>Yuma</forenames></author><author><keyname>Oikawa</keyname><forenames>Yasuhiro</forenames></author><author><keyname>Harada</keyname><forenames>Noboru</forenames></author></authors><title>Data-driven design of perfect reconstruction filterbank for DNN-based
  sound source enhancement</title><categories>eess.AS cs.SD</categories><comments>5 pages, to appear in IEEE ICASSP 2019 (Paper Code: AASP-P8.8,
  Session: Spatial Audio, Audio Enhancement and Bandwidth Extension)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a data-driven design method of perfect-reconstruction filterbank
(PRFB) for sound-source enhancement (SSE) based on deep neural network (DNN).
DNNs have been used to estimate a time-frequency (T-F) mask in the short-time
Fourier transform (STFT) domain. Their training is more stable when a simple
cost function as mean-squared error (MSE) is utilized comparing to some
advanced cost such as objective sound quality assessments. However, such a
simple cost function inherits strong assumptions on the statistics of the
target and/or noise which is often not satisfied, and the mismatch of
assumption results in degraded performance. In this paper, we propose to design
the frequency scale of PRFB from training data so that the assumption on MSE is
satisfied. For designing the frequency scale, the warped filterbank frame
(WFBF) is considered as PRFB. The frequency characteristic of learned WFBF was
in between STFT and the wavelet transform, and its effectiveness was confirmed
by comparison with a standard STFT-based DNN whose input feature is compressed
into the mel scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08895</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08895</id><created>2019-03-21</created><authors><author><keyname>Frigo</keyname><forenames>Guglielmo</forenames></author><author><keyname>Derviskadic</keyname><forenames>Asja</forenames></author><author><keyname>Zuo</keyname><forenames>Yihui</forenames></author><author><keyname>Paolone</keyname><forenames>Mario</forenames></author></authors><title>PMU-Based ROCOF Measurements: Uncertainty Limits and Metrological
  Significance in Power System Applications</title><categories>eess.SP</categories><comments>Manuscript IM-18-20133R. Accepted for publication on IEEE
  Transactions on Instrumentation and Measurement (acceptance date: 9 March
  2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern power systems, the Rate-of-Change-of-Frequency (ROCOF) may be
largely employed in Wide Area Monitoring, Protection and Control (WAMPAC)
applications. However, a standard approach towards ROCOF measurements is still
missing. In this paper, we investigate the feasibility of Phasor Measurement
Units (PMUs) deployment in ROCOF-based applications, with a specific focus on
Under-Frequency Load-Shedding (UFLS). For this analysis, we select three
state-of-the-art window-based synchrophasor estimation algorithms and compare
different signal models, ROCOF estimation techniques and window lengths in
datasets inspired by real-world acquisitions. In this sense, we are able to
carry out a sensitivity analysis of the behavior of a PMU-based UFLS control
scheme. Based on the proposed results, PMUs prove to be accurate ROCOF meters,
as long as the harmonic and inter-harmonic distortion within the measurement
pass-bandwidth is scarce. In the presence of transient events, the
synchrophasor model looses its appropriateness as the signal energy spreads
over the entire spectrum and cannot be approximated as a sequence of
narrow-band components. Finally, we validate the actual feasibility of
PMU-based UFLS in a real-time simulated scenario where we compare two different
ROCOF estimation techniques with a frequency-based control scheme and we show
their impact on the successful grid restoration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08912</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08912</id><created>2019-03-21</created><authors><author><keyname>A</keyname><forenames>Shyam</forenames></author><author><keyname>Ravichandran</keyname><forenames>Vignesh</forenames></author><author><keyname>P</keyname><forenames>Preejith S.</forenames></author><author><keyname>Joseph</keyname><forenames>Jayaraj</forenames></author><author><keyname>Sivaprakasam</keyname><forenames>Mohanasankar</forenames></author></authors><title>PPGnet: Deep Network for Device Independent Heart Rate Estimation from
  Photoplethysmogram</title><categories>cs.LG eess.SP stat.ML</categories><comments>Under review in EMBC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photoplethysmogram (PPG) is increasingly used to provide monitoring of the
cardiovascular system under ambulatory conditions. Wearable devices like
smartwatches use PPG to allow long term unobtrusive monitoring of heart rate in
free living conditions. PPG based heart rate measurement is unfortunately
highly susceptible to motion artifacts, particularly when measured from the
wrist. Traditional machine learning and deep learning approaches rely on
tri-axial accelerometer data along with PPG to perform heart rate estimation.
The conventional learning based approaches have not addressed the need for
device-specific modeling due to differences in hardware design among PPG
devices. In this paper, we propose a novel end to end deep learning model to
perform heart rate estimation using 8 second length input PPG signal. We
evaluate the proposed model on the IEEE SPC 2015 dataset, achieving a mean
absolute error of 3.36+-4.1BPM for HR estimation on 12 subjects without
requiring patient specific training. We also studied the feasibility of
applying transfer learning along with sparse retraining from a comprehensive in
house PPG dataset for heart rate estimation across PPG devices with different
hardware design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08938</identifier>
 <datestamp>2019-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08938</id><created>2019-03-21</created><updated>2019-07-12</updated><authors><author><keyname>Qian</keyname><forenames>Cheng</forenames></author><author><keyname>Fu</keyname><forenames>Xiao</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Nicholas D.</forenames></author></authors><title>Algebraic Channel Estimation Algorithms for FDD Massive MIMO systems</title><categories>eess.SP</categories><doi>10.1109/JSTSP.2019.2930893</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider downlink (DL) channel estimation for frequency division duplex
based massive MIMO systems under the multipath model. Our goal is to provide
fast and accurate channel estimation from a small amount of DL training
overhead. Prior art tackles this problem using compressive sensing or classic
array processing techniques (e.g., ESPRIT and MUSIC). However, these methods
have challenges in some scenarios, e.g., when the number of paths is greater
than the number of receive antennas. Tensor factorization methods can also be
used to handle such challenging cases, but it is hard to solve the associated
optimization problems. In this work, we propose an efficient channel estimation
framework to circumvent such difficulties. Specifically, a structural training
sequence that imposes a tensor structure on the received signal is proposed. We
show that with such a training sequence, the parameters of DL MIMO channels can
be provably identified even when the number of paths largely exceeds the number
of receive antennas---under very small training overhead. Our approach is a
judicious combination of Vandermonde tensor algebra and a carefully designed
conjugate-invariant training sequence. Unlike existing tensor-based channel
estimation methods that involve hard optimization problems, the proposed
approach consists of very lightweight algebraic operations, and thus real-time
implementation is within reach. Simulation results are carried out to showcase
the effectiveness of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.08950</identifier>
 <datestamp>2019-07-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.08950</id><created>2019-03-21</created><updated>2019-07-12</updated><authors><author><keyname>Harar</keyname><forenames>Pavol</forenames></author><author><keyname>Bammer</keyname><forenames>Roswitha</forenames></author><author><keyname>Breger</keyname><forenames>Anna</forenames></author><author><keyname>D&#xf6;rfler</keyname><forenames>Monika</forenames></author><author><keyname>Smekal</keyname><forenames>Zdenek</forenames></author></authors><title>Improving Machine Hearing on Limited Data Sets</title><categories>cs.SD cs.LG eess.AS eess.SP stat.ML</categories><comments>13 pages, 3 figures, 2 tables. Repository for reproducibility:
  https://gitlab.com/hararticles/gs-ms-mt/. Keywords: audio, CNN, limited data,
  Mel scattering, mel-spectrogram, augmented target loss function. Rewritten
  and restructured after peer revision. Recomputed and added new experiments
  and visualizations. Changed the presentation of the results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural network (CNN) architectures have originated and
revolutionized machine learning for images. In order to take advantage of CNNs
in predictive modeling with audio data, standard FFT-based signal processing
methods are often applied to convert the raw audio waveforms into an image-like
representations (e.g. spectrograms). Even though conventional images and
spectrograms differ in their feature properties, this kind of pre-processing
reduces the amount of training data necessary for successful training. In this
contribution we investigate how input and target representations interplay with
the amount of available training data in a music information retrieval setting.
We compare the standard mel-spectrogram inputs with a newly proposed
representation, called Mel scattering. Furthermore, we investigate the impact
of additional target data representations by using an augmented target loss
function which incorporates unused available information. We observe that all
proposed methods outperform the standard mel-transform representation when
using a limited data set and discuss their strengths and limitations. The
source code for reproducibility of our experiments as well as intermediate
results and model checkpoints are available in an online repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09012</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09012</id><created>2019-03-21</created><authors><author><keyname>Sobhani</keyname><forenames>Faranak</forenames></author><author><keyname>Straccia</keyname><forenames>Umberto</forenames></author></authors><title>Towards a Forensic Event Ontology to Assist Video Surveillance-based
  Vandalism Detection</title><categories>cs.AI eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The detection and representation of events is a critical element in automated
surveillance systems. We present here an ontology for representing complex
semantic events to assist video surveillance-based vandalism detection. The
ontology contains the definition of a rich and articulated event vocabulary
that is aimed at aiding forensic analysis to objectively identify and represent
complex events. Our ontology has then been applied in the context of London
Riots, which took place in 2011. We report also on the experiments conducted to
support the classification of complex criminal events from video data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09027</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09027</id><created>2019-03-21</created><authors><author><keyname>Kim</keyname><forenames>Sung</forenames></author><author><keyname>Sathe</keyname><forenames>Visvesh</forenames></author></authors><title>Bandwidth Extension on Raw Audio via Generative Adversarial Networks</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network-based methods have recently demonstrated state-of-the-art
results on image synthesis and super-resolution tasks, in particular by using
variants of generative adversarial networks (GANs) with supervised feature
losses. Nevertheless, previous feature loss formulations rely on the
availability of large auxiliary classifier networks, and labeled datasets that
enable such classifiers to be trained. Furthermore, there has been
comparatively little work to explore the applicability of GAN-based methods to
domains other than images and video. In this work we explore a GAN-based method
for audio processing, and develop a convolutional neural network architecture
to perform audio super-resolution. In addition to several new architectural
building blocks for audio processing, a key component of our approach is the
use of an autoencoder-based loss that enables training in the GAN framework,
with feature losses derived from unlabeled data. We explore the impact of our
architectural choices, and demonstrate significant improvements over previous
works in terms of both objective and perceptual quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09097</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09097</id><created>2019-03-20</created><authors><author><keyname>Folle</keyname><forenames>Lukas</forenames></author><author><keyname>Vesal</keyname><forenames>Sulaiman</forenames></author><author><keyname>Ravikumar</keyname><forenames>Nishant</forenames></author><author><keyname>Maier</keyname><forenames>Andreas</forenames></author></authors><title>Dilated deeply supervised networks for hippocampus segmentation in MRI</title><categories>eess.IV cs.AI cs.LG</categories><comments>BVM 2019 conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tissue loss in the hippocampi has been heavily correlated with the
progression of Alzheimer's Disease (AD). The shape and structure of the
hippocampus are important factors in terms of early AD diagnosis and prognosis
by clinicians. However, manual segmentation of such subcortical structures in
MR studies is a challenging and subjective task. In this paper, we investigate
variants of the well known 3D U-Net, a type of convolution neural network (CNN)
for semantic segmentation tasks. We propose an alternative form of the 3D
U-Net, which uses dilated convolutions and deep supervision to incorporate
multi-scale information into the model. The proposed method is evaluated on the
task of hippocampus head and body segmentation in an MRI dataset, provided as
part of the MICCAI 2018 segmentation decathlon challenge. The experimental
results show that our approach outperforms other conventional methods in terms
of different segmentation accuracy metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09125</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09125</id><created>2019-03-21</created><authors><author><keyname>Roy</keyname><forenames>Sandip</forenames></author><author><keyname>Xue</keyname><forenames>Mengran</forenames></author></authors><title>Controllability-Gramian Submatrices for a Network Consensus Model</title><categories>cs.SY eess.SP math.DS physics.soc-ph</categories><comments>submitted to Systems &amp; Control Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal submatrices of the controllability Gramian and their inverses are
examined, for a network-consensus model with inputs at a subset of network
nodes. Specifically, several properties of the Gramian submatrices and their
inverses -- including dominant eigenvalues and eigenvectors, diagonal entries,
and sign patterns -- are characterized by exploiting the special
doubly-nonnegative structure of the matrices. In addition, majorizations for
these properties are obtained in terms of cutsets in the network's graph, based
on the diffusive form of the model. The asymptotic (long time horizon)
structure of the controllability Gramian is also analyzed. The results on the
Gramian are used to study metrics for target control of the network-consensus
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09136</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09136</id><created>2019-03-21</created><authors><author><keyname>Petersen</keyname><forenames>Eike</forenames></author><author><keyname>Hoffmann</keyname><forenames>Christian</forenames></author><author><keyname>Rostalski</keyname><forenames>Philipp</forenames></author></authors><title>On Approximate Nonlinear Gaussian Message Passing On Factor Graphs</title><categories>stat.ML cs.LG cs.SY eess.SP</categories><journal-ref>2018 IEEE Statistical Signal Processing Workshop (SSP)</journal-ref><doi>10.1109/SSP.2018.8450699</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Factor graphs have recently gained increasing attention as a unified
framework for representing and constructing algorithms for signal processing,
estimation, and control. One capability that does not seem to be well explored
within the factor graph tool kit is the ability to handle deterministic
nonlinear transformations, such as those occurring in nonlinear filtering and
smoothing problems, using tabulated message passing rules. In this
contribution, we provide general forward (filtering) and backward (smoothing)
approximate Gaussian message passing rules for deterministic nonlinear
transformation nodes in arbitrary factor graphs fulfilling a Markov property,
based on numerical quadrature procedures for the forward pass and a
Rauch-Tung-Striebel-type approximation of the backward pass. These message
passing rules can be employed for deriving many algorithms for solving
nonlinear problems using factor graphs, as is illustrated by the proposition of
a nonlinear modified Bryson-Frazier (MBF) smoother based on the presented
message passing rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09141</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09141</id><created>2019-03-21</created><updated>2019-04-01</updated><authors><author><keyname>Kouokam</keyname><forenames>Emmanuel Kiegaing</forenames></author><author><keyname>Dirik</keyname><forenames>Ahmet Emir</forenames></author></authors><title>PRNU-Based Source Device Attribution for YouTube Videos</title><categories>eess.IV eess.SP</categories><comments>Revised (and accepted) version of the original submission. Minor
  changes have been brought to the original manuscript</comments><journal-ref>Digital Investigation, March 2019</journal-ref><doi>10.1016/j.diin.2019.03.005</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Photo Response Non-Uniformity (PRNU) is a camera imaging sensor imperfection
which has earned a great interest for source device attribution of digital
videos. A majority of recent researches about PRNU-based source device
attribution for digital videos do not take into consideration the effects of
video compression on the PRNU noise in video frames, but rather consider video
frames as isolated images of equal importance. As a result, these methods
perform poorly on re-compressed or low bit-rate videos. This paper proposes a
novel method for PRNU fingerprint estimation from video frames taking into
account the effects of video compression on the PRNU noise in these frames.
With this method, we aim to determine whether two videos from unknown sources
originate from the same device or not. Experimental results on a large set of
videos show that the method we propose is more effective than existing
frame-based methods that use either only I frames or all (I-B-P) frames,
especially on YouTube videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09180</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09180</id><created>2019-03-21</created><authors><author><keyname>Lu</keyname><forenames>Mengqi</forenames></author><author><keyname>Li</keyname><forenames>Zuyi</forenames></author></authors><title>A Hybrid Event Detection Approach for Non-Intrusive Load Monitoring</title><categories>eess.SP</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Intrusive Load Monitoring (NILM) is a practical method to provide
appliance-level electricity consumption information. Event detection, as an
important part of event-based NILM methods, has a direct impact on the accuracy
of the ultimate load disaggregation results in the entire NILM framework. This
paper presents a hybrid event detection approach for relatively complex
household load datasets that include appliances with long transients, high
fluctuations, and/or near-simultaneous actions. The proposed approach includes
a base algorithm based on moving average change with time limit, and two
auxiliary algorithms based on derivative analysis and filtering analysis. The
structure, steps, and working principle of this approach are described in
detail. The proposed approach does not require additional information about
household appliances, nor does it require any training sets. Case studies on
different datasets are conducted to evaluate the performance of the proposed
approach in comparison with several existing approaches including log
likelihood ratio detector with maxima (LLD-Max) approach, active window-based
(AWB) approach, and generalized likelihood ratio (GLR) approach. Results show
that the proposed approach works well in detecting events in complex household
load datasets and performs better than the existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09240</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09240</id><created>2019-03-21</created><authors><author><keyname>Banerjee</keyname><forenames>Subhashis</forenames></author><author><keyname>Mitra</keyname><forenames>Sushmita</forenames></author><author><keyname>Masulli</keyname><forenames>Francesco</forenames></author><author><keyname>Rovetta</keyname><forenames>Stefano</forenames></author></authors><title>Deep Radiomics for Brain Tumor Detection and Classification from
  Multi-Sequence MRI</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Glioma constitutes 80% of malignant primary brain tumors and is usually
classified as HGG and LGG. The LGG tumors are less aggressive, with slower
growth rate as compared to HGG, and are responsive to therapy. Tumor biopsy
being challenging for brain tumor patients, noninvasive imaging techniques like
Magnetic Resonance Imaging (MRI) have been extensively employed in diagnosing
brain tumors. Therefore automated systems for the detection and prediction of
the grade of tumors based on MRI data becomes necessary for assisting doctors
in the framework of augmented intelligence. In this paper, we thoroughly
investigate the power of Deep ConvNets for classification of brain tumors using
multi-sequence MR images. We propose novel ConvNet models, which are trained
from scratch, on MRI patches, slices, and multi-planar volumetric slices. The
suitability of transfer learning for the task is next studied by applying two
existing ConvNets models (VGGNet and ResNet) trained on ImageNet dataset,
through fine-tuning of the last few layers. LOPO testing, and testing on the
holdout dataset are used to evaluate the performance of the ConvNets. Results
demonstrate that the proposed ConvNets achieve better accuracy in all cases
where the model is trained on the multi-planar volumetric dataset. Unlike
conventional models, it obtains a testing accuracy of 95% for the low/high
grade glioma classification problem. A score of 97% is generated for
classification of LGG with/without 1p/19q codeletion, without any additional
effort towards extraction and selection of features. We study the properties of
self-learned kernels/ filters in different layers, through visualization of the
intermediate layer outputs. We also compare the results with that of
state-of-the-art methods, demonstrating a maximum improvement of 7% on the
grading performance of ConvNets and 9% on the prediction of 1p/19q codeletion
status.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09284</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09284</id><created>2019-03-21</created><authors><author><keyname>Ghassemi</keyname><forenames>Mohsen</forenames></author><author><keyname>Shakeri</keyname><forenames>Zahra</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author></authors><title>Learning Mixtures of Separable Dictionaries for Tensor Data: Analysis
  and Algorithms</title><categories>cs.LG cs.IT eess.SP math.IT stat.ML</categories><comments>17 pages, 5 figures, 2 tables; in review for journal publication</comments><doi>10.1109/TSP.2019.2952046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses the problem of learning sparse representations of tensor
data using structured dictionary learning. It proposes learning a mixture of
separable dictionaries to better capture the structure of tensor data by
generalizing the separable dictionary learning model. Two different approaches
for learning mixture of separable dictionaries are explored and sufficient
conditions for local identifiability of the underlying dictionary are derived
in each case. Moreover, computational algorithms are developed to solve the
problem of learning mixture of separable dictionaries in both batch and online
settings. Numerical experiments are used to show the usefulness of the proposed
model and the efficacy of the developed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09294</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09294</id><created>2019-03-21</created><authors><author><keyname>Pradhan</keyname><forenames>Chandan</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Zhuo</keyname><forenames>Li</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Hybrid Precoder and Combiner for Imperfect Beam Alignment in mmWave MIMO
  Systems</title><categories>eess.SP cs.NI</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we aim to design a robust hybrid precoder and combiner
against beam misalignment in millimeter-wave (mmWave) communication systems. We
consider the inclusion of the `error statistics' into the precoder and combiner
design, where the array response that incorporates the distribution of the
misalignment error is first derived. An iterative algorithm is then proposed to
design the robust hybrid precoder and combiner to maximize the array gain in
the presence of beam misalignment. To further enhance the spectral efficiency,
a second-stage digital precoder and combiner are included to mitigate the
inter-stream interference. Numerical results show that the proposed robust
hybrid precoder and combiner design can effectively alleviate the performance
degradation incurred by beam misalignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09330</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09330</id><created>2019-03-21</created><authors><author><keyname>Ning</keyname><forenames>Cai</forenames></author><author><keyname>Fei</keyname><forenames>Shi</forenames></author><author><keyname>Dianlin</keyname><forenames>Hu</forenames></author><author><keyname>Yang</keyname><forenames>Chen</forenames></author></authors><title>A resnet-based universal method for speckle reduction in optical
  coherence tomography images</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a ResNet-based universal method for speckle reduction
in optical coherence tomography (OCT) images. The proposed model contains 3
main modules: Convolution-BN-ReLU, Branch and Residual module. Unlike
traditional algorithms, the model can learn from training data instead of
selecting parameters manually such as noise level. Application of this proposed
method to the OCT images shows a more than 22 dB signal-to-noise ratio
improvement in speckle noise reduction with minimal structure blurring. The
proposed method provides strong generalization ability and can process noisy
other types of OCT images without retraining. It outperforms other filtering
methods in suppressing speckle noises and revealing subtle features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09336</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09336</id><created>2019-03-21</created><authors><author><keyname>Wei</keyname><forenames>Xiao</forenames></author><author><keyname>Xiang</keyname><forenames>Lin</forenames></author><author><keyname>Cottatellucci</keyname><forenames>Laura</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Cache-Aided Massive MIMO: Linear Precoding Design and Performance
  Analysis</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel joint caching and massive multiple-input
multiple-output (MIMO) transmission scheme, referred to as cache-aided massive
MIMO, for advanced downlink cellular communications. In addition to reaping the
conventional advantages of caching and massive MIMO, the proposed scheme also
exploits the side information provided by cached files for interference
cancellation at the receivers. This interference cancellation increases the
degrees of freedom available for precoding design. In addition, the power freed
by the cache-enabled offloading can benefit the transmissions to the users
requesting non-cached files. The resulting performance gains are not possible
if caching and massive MIMO are designed separately. We analyze the performance
of cache-aided massive MIMO for cache-dependent maximum-ratio transmission
(MRT), zero-forcing (ZF) precoding, and regularized zero-forcing (RZF)
precoding. Lower bounds on the ergodic achievable rates are derived in closed
form for MRT and ZF precoding. The ergodic achievable rate of RZF precoding is
obtained for the case when the numbers of transmit antennas and users are large
but their ratio is fixed. Compared to conventional massive MIMO, the proposed
cache-aided massive MIMO scheme achieves a significantly higher ergodic rate
especially when the number of users approaches the number of transmit antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09339</identifier>
 <datestamp>2019-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09339</id><created>2019-03-21</created><updated>2019-05-09</updated><authors><author><keyname>Grupp</keyname><forenames>Robert B.</forenames></author><author><keyname>Hegeman</keyname><forenames>Rachel A.</forenames></author><author><keyname>Murphy</keyname><forenames>Ryan J.</forenames></author><author><keyname>Alexander</keyname><forenames>Clayton P.</forenames></author><author><keyname>Otake</keyname><forenames>Yoshito</forenames></author><author><keyname>McArthur</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Armand</keyname><forenames>Mehran</forenames></author><author><keyname>Taylor</keyname><forenames>Russell H.</forenames></author></authors><title>Pose Estimation of Periacetabular Osteotomy Fragments with
  Intraoperative X-Ray Navigation</title><categories>cs.CV eess.IV</categories><comments>Accepted for publication in IEEE Transactions on Biomedical
  Engineering</comments><doi>10.1109/TBME.2019.2915165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: State of the art navigation systems for pelvic osteotomies use
optical systems with external fiducials. We propose the use of X-Ray navigation
for pose estimation of periacetabular fragments without fiducials. Methods: A
2D/3D registration pipeline was developed to recover fragment pose. This
pipeline was tested through an extensive simulation study and 6 cadaveric
surgeries. Using osteotomy boundaries in the fluoroscopic images, the
preoperative plan is refined to more accurately match the intraoperative shape.
Results: In simulation, average fragment pose errors were 1.3{\deg}/1.7 mm when
the planned fragment matched the intraoperative fragment, 2.2{\deg}/2.1 mm when
the plan was not updated to match the true shape, and 1.9{\deg}/2.0 mm when the
fragment shape was intraoperatively estimated. In cadaver experiments, the
average pose errors were 2.2{\deg}/2.2 mm, 3.8{\deg}/2.5 mm, and 3.5{\deg}/2.2
mm when registering with the actual fragment shape, a preoperative plan, and an
intraoperatively refined plan, respectively. Average errors of the lateral
center edge angle were less than 2{\deg} for all fragment shapes in simulation
and cadaver experiments. Conclusion: The proposed pipeline is capable of
accurately reporting femoral head coverage within a range clinically identified
for long-term joint survivability. Significance: Human interpretation of
fragment pose is challenging and usually restricted to rotation about a single
anatomical axis. The proposed pipeline provides an intraoperative estimate of
rigid pose with respect to all anatomical axes, is compatible with minimally
invasive incisions, and has no dependence on external fiducials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09341</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09341</id><created>2019-03-21</created><updated>2019-03-31</updated><authors><author><keyname>Shimada</keyname><forenames>Kazuki</forenames></author><author><keyname>Bando</keyname><forenames>Yoshiaki</forenames></author><author><keyname>Mimura</keyname><forenames>Masato</forenames></author><author><keyname>Itoyama</keyname><forenames>Katsutoshi</forenames></author><author><keyname>Yoshii</keyname><forenames>Kazuyoshi</forenames></author><author><keyname>Kawahara</keyname><forenames>Tatsuya</forenames></author></authors><title>Unsupervised Speech Enhancement Based on Multichannel NMF-Informed
  Beamforming for Noise-Robust Automatic Speech Recognition</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><doi>10.1109/TASLP.2019.2907015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes multichannel speech enhancement for improving automatic
speech recognition (ASR) in noisy environments. Recently, the minimum variance
distortionless response (MVDR) beamforming has widely been used because it
works well if the steering vector of speech and the spatial covariance matrix
(SCM) of noise are given. To estimating such spatial information, conventional
studies take a supervised approach that classifies each time-frequency (TF) bin
into noise or speech by training a deep neural network (DNN). The performance
of ASR, however, is degraded in an unknown noisy environment. To solve this
problem, we take an unsupervised approach that decomposes each TF bin into the
sum of speech and noise by using multichannel nonnegative matrix factorization
(MNMF). This enables us to accurately estimate the SCMs of speech and noise not
from observed noisy mixtures but from separated speech and noise components. In
this paper we propose online MVDR beamforming by effectively initializing and
incrementally updating the parameters of MNMF. Another main contribution is to
comprehensively investigate the performances of ASR obtained by various types
of spatial filters, i.e., time-invariant and variant versions of MVDR
beamformers and those of rank-1 and full-rank multichannel Wiener filters, in
combination with MNMF. The experimental results showed that the proposed method
outperformed the state-of-the-art DNN-based beamforming method in unknown
environments that did not match training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09363</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09363</id><created>2019-03-22</created><authors><author><keyname>Esswie</keyname><forenames>Ali A.</forenames></author><author><keyname>Pedersen</keyname><forenames>Klaus I.</forenames></author><author><keyname>Mogensen</keyname><forenames>Preben E.</forenames></author></authors><title>Quasi-Dynamic Frame Coordination For Ultra- Reliability and Low-Latency
  in 5G TDD Systems</title><categories>eess.SP</categories><journal-ref>IEEE International Conference on Communications 2019 (ICC 2019)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fifth generation (5G) mobile technology features the ultra-reliable and
low-latency communications (URLLC) as a major service class. URLLC applications
demand a tight radio latency with extreme link reliability. In 5G dynamic time
division duplexing (TDD) systems, URLLC requirements become further challenging
to achieve due to the severe and fast-varying cross link interference (CLI) and
the switching time of the radio frame configurations (RFCs). In this work, we
propose a quasi-dynamic inter-cell frame coordination algorithm using hybrid
frame design and a cyclic-offset-based RFC code-book. The proposed solution
adaptively updates the RFCs in time such that both the average CLI and the
user-centric radio latency are minimized. Compared to state-of-the-art dynamic
TDD studies, the proposed scheme shows a significant improvement in the URLLC
outage latency, i.e., 92% reduction gain, while boosting the cell-edge capacity
by 189% and with a greatly reduced coordination overhead space, limited to
B-bit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09394</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09394</id><created>2019-03-22</created><updated>2019-03-28</updated><authors><author><keyname>Brihuega</keyname><forenames>Alberto</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Abdelaziz</keyname><forenames>Mahmoud</forenames></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Digital Predistortion for Multiuser Hybrid MIMO at mmWaves</title><categories>eess.SP</categories><comments>13 pages, updated Journal; submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient mitigation of power amplifier (PA) nonlinear distortion in hybrid
precoding based broadband mmWave systems is an open research problem. In this
article, we first carry out detailed signal and distortion modeling in
broadband multi-user hybrid MIMO systems with a bank of nonlinear PAs in each
subarray. Building on the derived models, we then propose a novel digital
predistortion (DPD) solution that requires only a single DPD unit per transmit
chain or subarray. The proposed DPD system makes use of a closed-loop learning
architecture and combined feedback observation receivers that merge the
individual PA output signals within each subarray for DPD parameter learning
purposes. Such combined feedback signals reflect the true received signals at
the intended users, from the nonlinear distortion point of view. We show that,
under spatially correlated multipath propagation, each DPD unit can provide
linearization towards every intended user, or more generally, towards all
spatial directions where coherent propagation is taking place. In the
directions with less coherent combining, the joint effect of DPD and
beamforming keeps the nonlinear distortion at a sufficiently low level.
Extensive numerical results are provided, demonstrating and verifying the
excellent linearization performance of the proposed DPD system in different
evaluation scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09451</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09451</id><created>2019-03-22</created><authors><author><keyname>Vishwakarma</keyname><forenames>Shelly</forenames></author><author><keyname>Ram</keyname><forenames>Shobha Sundar</forenames></author></authors><title>Mitigation of Through-Wall Distortions of Frontal Radar Images using
  Denoising Autoencoders</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radar images of humans and other concealed objects are considerably distorted
by attenuation, refraction and multipath clutter in indoor through-wall
environments. While several methods have been proposed for removing target
independent static and dynamic clutter, there still remain considerable
challenges in mitigating target dependent clutter especially when the knowledge
of the exact propagation characteristics or analytical framework is
unavailable. In this work we focus on mitigating wall effects using a machine
learning based solution -- denoising autoencoders -- that does not require
prior information of the wall parameters or room geometry. Instead, the method
relies on the availability of a large volume of training radar images gathered
in through-wall conditions and the corresponding clean images captured in
line-of-sight conditions. During the training phase, the autoencoder learns how
to denoise the corrupted through-wall images in order to resemble the free
space images. We have validated the performance of the proposed solution for
both static and dynamic human subjects. The frontal radar images of static
targets are obtained by processing wideband planar array measurement data with
two-dimensional array and range processing. The frontal radar images of dynamic
targets are simulated using narrowband planar array data processed with
two-dimensional array and Doppler processing. In both simulation and
measurement processes, we incorporate considerable diversity in the target and
propagation conditions. Our experimental results, from both simulation and
measurement data, show that the denoised images are considerably more similar
to the free-space images when compared to the original through-wall images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09520</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09520</id><created>2019-03-22</created><authors><author><keyname>Bera</keyname><forenames>Sutanu</forenames></author><author><keyname>Lahiri</keyname><forenames>Avisek</forenames></author><author><keyname>Biswas</keyname><forenames>Prabir Kumar</forenames></author></authors><title>A lightweight convolutional neural network for image denoising with fine
  details preservation capability</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image denoising is a fundamental problem in image processing whose primary
objective is to remove the noise while preserving the original image structure.
In this work, we proposed a new architecture for image denoising. We have used
several dense blocks to design our network. Additionally, we have forwarded
feature extracted in the first layer to the input of every transition layer.
Our experimental result suggests that the use of low-level feature helps in
reconstructing better texture. Furthermore, we had trained our network with a
combination of MSE and a differentiable multi-scale structural similarity
index(MS-SSIM). With proper training, our proposed model with a much lower
parameter can outperform other models which were with trained much higher
parameters. We evaluated our algorithm on two grayscale benchmark dataset BSD68
and SET12. Our model had achieved similar PSNR with the current state of the
art methods and most of the time better SSIM than other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09527</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09527</id><created>2019-03-22</created><authors><author><keyname>Li</keyname><forenames>Hongchang</forenames></author><author><keyname>Fang</keyname><forenames>Jingyang</forenames></author><author><keyname>Tang</keyname><forenames>Yi</forenames></author></authors><title>A Simplified Dynamical Model for Tuned Wireless Power Transfer Systems</title><categories>eess.SP</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamical models of wireless power transfer (WPT) systems are of primary
importance for the dynamical behavior studies and controller design. However,
the existing dynamical models usually suffer from high orders and complicated
forms due to the complex nature of the coupled resonances and switched-mode
power converters in WPT systems. This letter finds that a well-tuned WPT system
can be accurately described by a much simpler dynamical model. Specifically, at
the tuned condition, the existing dynamical model can be decomposed into two
parts. One is controllable and the other one is uncontrollable. The former
should be considered in the modeling while the latter can be ignored because it
always exponentially converges to zero. For illustration, the recently proposed
zero-voltage-switching full-bridge pulse-density modulation WPT system is
modeled as an example since such a system can efficiently operate at the tuned
condition with soft switching and control capabilities. The derived model was
verified in experiments by time-domain and frequency-domain responses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09538</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09538</id><created>2019-03-22</created><authors><author><keyname>Adachi</keyname><forenames>Hiroaki</forenames></author><author><keyname>Kawamura</keyname><forenames>Yoko</forenames></author><author><keyname>Nakagawa</keyname><forenames>Keiji</forenames></author><author><keyname>Horisaki</keyname><forenames>Ryoichi</forenames></author><author><keyname>Sato</keyname><forenames>Issei</forenames></author><author><keyname>Yamaguchi</keyname><forenames>Satoko</forenames></author><author><keyname>Fujiu</keyname><forenames>Katsuhito</forenames></author><author><keyname>Waki</keyname><forenames>Kayo</forenames></author><author><keyname>Noji</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Ota</keyname><forenames>Sadao</forenames></author></authors><title>Use of Ghost Cytometry to Differentiate Cells with Similar Gross
  Morphologic Characteristics</title><categories>q-bio.QM cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imaging flow cytometry shows significant potential for increasing our
understanding of heterogeneous and complex life systems and is useful for
biomedical applications. Ghost cytometry is a recently proposed approach for
directly analyzing compressively measured signals, thereby relieving the
computational bottleneck observed in high-throughput cytometry based on
morphological information. While this image-free approach could distinguish
different cell types using the same fluorescence staining method, further
strict controls are sometimes required to clearly demonstrate that the
classification is based on detailed morphologic analysis. In this study, we
show that ghost cytometry can be used to classify cell populations of the same
type but with different fluorescence distributions in space, supporting the
strength of our image-free approach for morphologic cell analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09548</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09548</id><created>2019-03-22</created><authors><author><keyname>Geier</keyname><forenames>Martin</forenames><affiliation>Technical University of Munich</affiliation></author><author><keyname>Faller</keyname><forenames>Dominik</forenames><affiliation>Technical University of Munich</affiliation></author><author><keyname>Br&#xe4;ndle</keyname><forenames>Marian</forenames><affiliation>Technical University of Munich</affiliation></author><author><keyname>Chakraborty</keyname><forenames>Samarjit</forenames><affiliation>Technical University of Munich</affiliation></author></authors><title>Cost-effective Energy Monitoring of a Zynq-based Real-time System
  including dual Gigabit Ethernet</title><categories>eess.SP cs.DC cs.PF</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ongoing integration of fine-grained power management features already
established in CPU-driven Systems-on-Chip (SoCs) enables both traditional Field
Programmable Gate Arrays (FPGAs) and, more recently, hybrid Programmable SoCs
(pSoCs) to reach more energy-sensitive application domains (such as, e.g.,
automotive and robotics). By combining a fixed-function multi-core SoC with
flexible, configurable FPGA fabric, the latter can be used to realize
heterogeneous Real-time Systems (RTSs) commonly implementing complex
application-specific architectures with high computation and communication
(I/O) densities. Their dynamic changes in workload, currently active power
saving features and thus power consumption require precise voltage and current
sensing on all relevant supply rails to enable dependable evaluation of the
various power management techniques. In this paper, we propose a low-cost
18-channel 16-bit-resolution measurement (sub-)system capable of 200 kSPS
(kilo-samples per second) for instrumentation of current pSoC development
boards. To this end, we join simultaneously sampling analog-to-digital
converters (ADCs) and analog voltage/current sensing circuitry with a Cortex M7
microcontroller using an SD card for storage. In addition, we propose to
include crucial I/O components such as Ethernet PHYs into the power monitoring
to gain a holistic view on the RTS's temporal behavior covering not only
computation on FPGA and CPUs, but also communication in terms of, e.g.,
reception of sensor values and transmission of actuation signals. We present an
FMC-sized implementation of our measurement system combined with two Gigabit
Ethernet PHYs and one HDMI input. Paired with Xilinx' ZC702 development board,
we are able to synchronously acquire power traces of a Zynq pSoC and the two
PHYs precise enough to identify individual Ethernet frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09564</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09564</id><created>2019-03-22</created><authors><author><keyname>Roy</keyname><forenames>Harshit</forenames></author><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author></authors><title>Mixer-First Receiver with wide-RF Range</title><categories>eess.SP</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Passive Mixer first receiver, four mosfet, and four baseband impedance
can synthesize High-Q bandpass filter. This on-chip High-Q bandpass filter can
replace bulky, expensive and off-chip SAW filters. The impedance which is seen
by the antenna is tuned by switch resistance of the Passive Mixer and the input
impedance of the Trans-impedance amplifier (baseband impedance) to match
antenna impedance. The gain of the feedback loop of the TIA controls the input
impedance of the TIA. Further, M baseband impedance can be replaced by M/4
complex impedance. This complex impedance provides a wide RF range by changing
the transconductance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09606</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09606</id><created>2019-03-22</created><authors><author><keyname>Tu</keyname><forenames>Ming</forenames></author><author><keyname>Tang</keyname><forenames>Yun</forenames></author><author><keyname>Huang</keyname><forenames>Jing</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Towards adversarial learning of speaker-invariant representation for
  speech emotion recognition</title><categories>eess.AS cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech emotion recognition (SER) has attracted great attention in recent
years due to the high demand for emotionally intelligent speech interfaces.
Deriving speaker-invariant representations for speech emotion recognition is
crucial. In this paper, we propose to apply adversarial training to SER to
learn speaker-invariant representations. Our model consists of three parts: a
representation learning sub-network with time-delay neural network (TDNN) and
LSTM with statistical pooling, an emotion classification network and a speaker
classification network. Both the emotion and speaker classification network
take the output of the representation learning network as input. Two training
strategies are employed: one based on domain adversarial training (DAT) and the
other one based on cross-gradient training (CGT). Besides the conventional data
set, we also evaluate our proposed models on a much larger publicly available
emotion data set with 250 speakers. Evaluation results show that on IEMOCAP,
DAT and CGT provides 5.6% and 7.4% improvement respectively, over a baseline
system without speaker-invariant representation learning on 5-fold cross
validation. On the larger emotion data set, while CGT fails to yield better
results than baseline, DAT can still provide 9.8% relative improvement on a
standalone test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09631</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09631</id><created>2019-03-19</created><authors><author><keyname>Pandit</keyname><forenames>Parthe</forenames></author><author><keyname>Sahraee-Ardakan</keyname><forenames>Mojtaba</forenames></author><author><keyname>Amini</keyname><forenames>Arash A.</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Fletcher</keyname><forenames>Alyson K.</forenames></author></authors><title>High-Dimensional Bernoulli Autoregressive Process with Long-Range
  Dependence</title><categories>math.ST cs.LG eess.SP stat.ML stat.TH</categories><comments>To appear at AISTATS 2019 titled &quot;Sparse Multivariate Bernoulli
  Processes in High Dimensions&quot;</comments><journal-ref>Proceedings of the 22nd International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2019, Naha, Okinawa, Japan. PMLR:
  Volume 89</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the parameters of a multivariate
Bernoulli process with auto-regressive feedback in the high-dimensional setting
where the number of samples available is much less than the number of
parameters. This problem arises in learning interconnections of networks of
dynamical systems with spiking or binary-valued data. We allow the process to
depend on its past up to a lag $p$, for a general $p \ge 1$, allowing for more
realistic modeling in many applications. We propose and analyze an
$\ell_1$-regularized maximum likelihood estimator (MLE) under the assumption
that the parameter tensor is approximately sparse. Rigorous analysis of such
estimators is made challenging by the dependent and non-Gaussian nature of the
process as well as the presence of the nonlinearities and multi-level feedback.
We derive precise upper bounds on the mean-squared estimation error in terms of
the number of samples, dimensions of the process, the lag $p$ and other key
statistical properties of the model. The ideas presented can be used in the
high-dimensional analysis of regularized $M$-estimators for other sparse
nonlinear and non-Gaussian processes with long-range dependence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09728</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09728</id><created>2019-03-22</created><authors><author><keyname>Akbari</keyname><forenames>Hesam</forenames></author><author><keyname>Esmaili</keyname><forenames>Somayeh Saraf</forenames></author><author><keyname>Zadeh</keyname><forenames>Sima Farzollah</forenames></author></authors><title>Classification of seizure and seizure-free EEG signals based on
  empirical wavelet transform and phase space reconstruction</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epilepsy is a brain disorder due to abnormalactivity of neurons and recording
of seizures is of primary interest in the evaluation of epileptic patients. A
seizureis the phenomenon of rhythmicity discharge from either a local area or
the whole brain and the individual behavior usually lasts from seconds to
minutes.In this work, empirical wavelet transform(EWT) is applied to decompose
signals into Electroencephalography (EEG) rhythms. EEG signals are separated to
delta, theta, alpha, beta and gamma rhythms using EWT.The proposed method has
been evaluated by benchmark dataset which is freely downloadable from Bonn
University website. 95% confident ellipse area is computed from 2D projection
of reconstructed phase space (RPS)of rhythms as features and fed to K-nearest
neighbor classifier for detection of seizure (S) and seizure free (SF) EEG
signals. Our proposed method archived 98% accuracy in classification of S and
SF EEG signals with a tenfold cross-validation strategy that is higher than
previous techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09752</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09752</id><created>2019-03-22</created><authors><author><keyname>Jiao</keyname><forenames>Chunxu</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Feng</keyname><forenames>Zhiyong</forenames></author></authors><title>MmWave Communication With Active Ambient Perception</title><categories>eess.SP</categories><comments>Accepted for publication in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In existing communication systems, the channel state information of each UE
(user equipment) should be repeatedly estimated when it moves to a new position
or another UE takes its place. The underlying ambient information, including
the specific layout of potential reflectors, which provides more detailed
information about all UEs' channel structures, has not been fully explored and
exploited. In this paper, we rethink the mmWave channel estimation problem in a
new and indirect way, i.e., instead of estimating the resultant composite
channel response at each time and for any specific location, we first conduct
the ambient perception exploiting the fascinating radar capability of a mmWave
antenna array and then accomplish the location-based sparse channel
reconstruction. In this way, the sparse channel for a quasi-static UE arriving
at a specific location can be rapidly synthesized based on the perceived
ambient information, thus greatly reducing the signalling overhead and online
computational complexity. Based on the reconstructed mmWave channel,
single-beam mmWave communication is designed and evaluated which shows
excellent performance. Such an approach in fact integrates radar with
communication, which may possibly open a new paradigm for future communication
system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09787</identifier>
 <datestamp>2019-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09787</id><created>2019-03-23</created><updated>2019-11-21</updated><authors><author><keyname>Li</keyname><forenames>Yunyi</forenames></author><author><keyname>Dai</keyname><forenames>Fei</forenames></author><author><keyname>Zhao</keyname><forenames>Yu</forenames></author><author><keyname>Cheng</keyname><forenames>Xiefeng</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author></authors><title>ADMM-IDNN: Iteratively Double-reweighted Nuclear Norm Algorithm for
  Group-prior based Nonconvex Compressed Sensing via ADMM</title><categories>eess.IV</categories><comments>This is a raw manuscript, and all the authors agree to revise and
  improve it in the method, thgeory and experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group-prior based regularization method has led to great successes in various
image processing tasks, which can usually be considered as a low-rank matrix
minimization problem. As a widely used surrogate function of low-rank, the
nuclear norm based convex surrogate usually lead to over-shrinking phenomena,
since the nuclear norm shrinks the rank components (singular value)
simultaneously. In this paper, we propose a novel Group-prior based nonconvex
image compressive sensing (CS) reconstruction framework via a family of
nonconvex nuclear norms functions which contain common concave and
monotonically properties. To solve the resulting nonconvex nuclear norm
minimization (NNM) problem, we develop a Group based iteratively
double-reweighted nuclear norm algorithm (IDNN) via an alternating direction
method of multipliers (ADMM) framework. Our proposed algorithm can convert the
nonconvex nuclear norms optimization problem into a double-reweighted singular
value thresholding (DSVT) problem. Extensive experiments demonstrate our
proposed framework achieved favorable reconstruction performance compared with
current state-of-the-art convex methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09793</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09793</id><created>2019-03-23</created><updated>2019-04-10</updated><authors><author><keyname>Cavalcante</keyname><forenames>Renato Lu&#xed;s Garrido</forenames></author><author><keyname>Liao</keyname><forenames>Qi</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>Slawomir</forenames></author></authors><title>Connections between spectral properties of asymptotic mappings and
  solutions to wireless network problems</title><categories>eess.SP cs.IT math.IT</categories><comments>manuscript accepted for publication (IEEE Transactions on Signal
  Processing)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we establish connections between asymptotic functions and
properties of solutions to important problems in wireless networks. We start by
introducing a class of self-mappings (called asymptotic mappings) constructed
with asymptotic functions, and we show that spectral properties of these
mappings explain the behavior of solutions to some maxmin utility optimization
problems. For example, in a common family of max-min utility power control
problems, we prove that the optimal utility as a function of the power
available to transmitters is approximately linear in the low power regime.
However, as we move away from this regime, there exists a transition point,
easily computed from the spectral radius of an asymptotic mapping, from which
gains in utility become increasingly marginal. From these results we derive
analogous properties of the transmit energy efficiency. In this study we also
generalize and unify existing approaches for feasibility analysis in wireless
networks. Feasibility problems often reduce to determining the existence of the
fixed point of a standard interference mapping, and we show that the spectral
radius of an asymptotic mapping provides a necessary and sufficient condition
for the existence of such a fixed point. We further present a result that
determines whether the fixed point satisfies a constraint given in terms of a
monotone norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09803</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09803</id><created>2019-03-23</created><authors><author><keyname>Shahin</keyname><forenames>Ismail</forenames></author></authors><title>Emotion Recognition based on Third-Order Circular Suprasegmental Hidden
  Markov Model</title><categories>cs.SD cs.HC eess.AS</categories><comments>Accepted at The 2019 IEEE Jordan International Joint Conference on
  Electrical Engineering and Information Technology (JEEIT), Jordan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on recognizing the unknown emotion based on the Third-Order
Circular Suprasegmental Hidden Markov Model (CSPHMM3) as a classifier. Our work
has been tested on Emotional Prosody Speech and Transcripts (EPST) database.
The extracted features of EPST database are Mel-Frequency Cepstral Coefficients
(MFCCs). Our results give average emotion recognition accuracy of 77.8% based
on the CSPHMM3. The results of this work demonstrate that CSPHMM3 is superior
to the Third-Order Hidden Markov Model (HMM3), Gaussian Mixture Model (GMM),
Support Vector Machine (SVM), and Vector Quantization (VQ) by 6.0%, 4.9%, 3.5%,
and 5.4%, respectively, for emotion recognition. The average emotion
recognition accuracy achieved based on the CSPHMM3 is comparable to that found
using subjective assessment by human judges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09836</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09836</id><created>2019-03-23</created><updated>2019-03-28</updated><authors><author><keyname>Yin</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Qian</forenames></author><author><keyname>Feng</keyname><forenames>Shijie</forenames></author><author><keyname>Tao</keyname><forenames>Tianyang</forenames></author><author><keyname>Huang</keyname><forenames>Lei</forenames></author><author><keyname>Trusiak</keyname><forenames>Maciej</forenames></author><author><keyname>Asundi</keyname><forenames>Anand</forenames></author><author><keyname>Zuo</keyname><forenames>Chao</forenames></author></authors><title>Temporal phase unwrapping using deep learning</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-frequency temporal phase unwrapping (MF-TPU) method, as a classical
phase unwrapping algorithm for fringe projection profilometry (FPP), is capable
of eliminating the phase ambiguities even in the presence of surface
discontinuities or spatially isolated objects. For the simplest and most
efficient case, two sets of 3-step phase-shifting fringe patterns are used: the
high-frequency one is for 3D measurement and the unit-frequency one is for
unwrapping the phase obtained from the high-frequency pattern set. The final
measurement precision or sensitivity is determined by the number of fringes
used within the high-frequency pattern, under the precondition that the phase
can be successfully unwrapped without triggering the fringe order error.
Consequently, in order to guarantee a reasonable unwrapping success rate, the
fringe number (or period number) of the high-frequency fringe patterns is
generally restricted to about 16, resulting in limited measurement accuracy. On
the other hand, using additional intermediate sets of fringe patterns can
unwrap the phase with higher frequency, but at the expense of a prolonged
pattern sequence. Inspired by recent successes of deep learning techniques for
computer vision and computational imaging, in this work, we report that the
deep neural networks can learn to perform TPU after appropriate training, as
called deep-learning based temporal phase unwrapping (DL-TPU), which can
substantially improve the unwrapping reliability compared with MF-TPU even in
the presence of different types of error sources, e.g., intensity noise, low
fringe modulation, and projector nonlinearity. We further experimentally
demonstrate for the first time, to our knowledge, that the high-frequency phase
obtained from 64-period 3-step phase-shifting fringe patterns can be directly
and reliably unwrapped from one unit-frequency phase using DL-TPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09893</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09893</id><created>2019-03-23</created><authors><author><keyname>Bai</keyname><forenames>Jingwen</forenames></author><author><keyname>Yeh</keyname><forenames>Shu-ping</forenames></author><author><keyname>Xue</keyname><forenames>Feng</forenames></author><author><keyname>Choi</keyname><forenames>Yang-seok</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Talwar</keyname><forenames>Shilpa</forenames></author></authors><title>Full-duplex in 5G Small Cell Access: SystemDesign and Performance
  Aspects</title><categories>eess.SP cs.NI</categories><comments>Submitted to IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent achievement in self-interference cancellation algorithms enables
potential application of full-duplex (FD) in 5G radio access systems. The
exponential growth of data traffic in 5G can be supported by having more
spectrum and higher spectral efficiency. FD communication promises to double
the spectral efficiency by enabling simultaneous uplink and downlink
transmissions in the same frequency band. Yet for cellular access network with
FD base stations (BS) serving multiple users (UE), additional BS-to-BS and
UE-to-UE interferences due to FD operation could diminish the performance gain
if not tackled properly. In this article, we address the practical system
design aspects to exploit FD gain at network scale. We propose efficient
reference signal design, low-overhead channel state information feedback and
signalling mechanisms to enable FD operation, and develop low-complexity power
control and scheduling algorithms to effectively mitigate new interference
introduced by FD operation. We extensively evaluate FD network-wide performance
in various deployment scenarios and traffic environment with detailed LTE
PHY/MAC modelling. We demonstrate that FD can achieve not only appreciable
throughput gains (1.9x), but also significant transmission latency
reduction~(5-8x) compared with the half-duplex system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09896</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09896</id><created>2019-03-23</created><updated>2019-04-26</updated><authors><author><keyname>Al-Hameed</keyname><forenames>Aubida A.</forenames></author><author><keyname>Younus</keyname><forenames>Safwan Hafeedh</forenames></author><author><keyname>Hussein</keyname><forenames>Ahmed Taha</forenames></author><author><keyname>Alresheedi</keyname><forenames>Mohammed T.</forenames></author><author><keyname>Elmirghani</keyname><forenames>Jaafar M. H.</forenames></author></authors><title>LiDAL: Light Detection and Localization</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the first indoor light-based detection and
localization system that builds on concepts from radio detection and ranging
(radar) making use of the expected growth in the use and adoption of visible
light communication (VLC), which can provide the infrastructure for our LiDAL
system. Our system enables active detection, counting and localization of
people, in addition to being fully compatible with existing VLC systems. In
order to detect human (targets), LiDAL uses the visible light spectrum, it
sends pulses using a VLC transmitter and analyses the reflected signal
collected by a photodetector receiver. Although we examine the use of the
visible spectrum here, LiDAL can be used in the infrared spectrum and other
parts of the light spectrum. We introduce LiDAL with different
transmitter-receiver configurations and optimum detectors considering the
fluctuation of the received reflected signal from the target in the presence of
Gaussian noise. We design an efficient multiple input multiple output (MIMO)
LiDAL system with wide field of view (FOV) single photodetector receiver, and
also design a multiple input single output (MISO) LiDAL system with an imaging
receiver to eliminate the ambiguity in target detection and localization. We
develop models for the human body and its reflections and consider the impact
of the colour and texture of the cloth used as well as the impact of target
mobility. A number of detection and localization methods are developed for our
LiDAL system including cross correaltion, a background subtraction method and a
neural network based method. These methods are considered to distinguish a
mobile target from the ambient reflections due to background obstacles
(furniture) in a realistic indoor environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09952</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09952</id><created>2019-03-24</created><authors><author><keyname>Xu</keyname><forenames>Chenglin</forenames></author><author><keyname>Rao</keyname><forenames>Wei</forenames></author><author><keyname>Chng</keyname><forenames>Eng Siong</forenames></author><author><keyname>Li</keyname><forenames>Haizhou</forenames></author></authors><title>Optimization of Speaker Extraction Neural Network with Magnitude and
  Temporal Spectrum Approximation Loss</title><categories>eess.AS cs.CL cs.SD</categories><comments>Accepted in ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SpeakerBeam-FE (SBF) method is proposed for speaker extraction. It
attempts to overcome the problem of unknown number of speakers in an audio
recording during source separation. The mask approximation loss of SBF is
sub-optimal, which doesn't calculate direct signal reconstruction error and
consider the speech context. To address these problems, this paper proposes a
magnitude and temporal spectrum approximation loss to estimate a phase
sensitive mask for the target speaker with the speaker characteristics.
Moreover, this paper explores a concatenation framework instead of the context
adaptive deep neural network in the SBF method to encode a speaker embedding
into the mask estimation network. Experimental results under open evaluation
condition show that the proposed method achieves 70.4% and 17.7% relative
improvement over the SBF baseline on signal-to-distortion ratio (SDR) and
perceptual evaluation of speech quality (PESQ), respectively. A further
analysis demonstrates 69.1% and 72.3% relative SDR improvements obtained by the
proposed method for different and same gender mixtures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.09966</identifier>
 <datestamp>2019-07-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.09966</id><created>2019-03-24</created><updated>2019-07-01</updated><authors><author><keyname>Ghasemian</keyname><forenames>Amir</forenames></author><author><keyname>Taheri</keyname><forenames>Asghar</forenames></author></authors><title>A Unified Approach to Mitigate Voltage Jump Effects in Near Optimal
  Switching Surface Control of DC-DC Converters</title><categories>cs.SY eess.SP</categories><comments>The article was published without the co-Author's notice, and it is
  withdrawn due to his objection</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Equivalent Series Resistance (ESR) of the output capacitor may cause
output voltage Vo jumps, that are not modeled commonly for second order DC-DC
converters, i.e., converters with two second order switched subsystems. These
jump discontinuities in Vo lead to performance issues in Switching Surface (SS)
controllers. In this paper, these ESR effects are modeled using switched
systems with state jumps, called Jump-Flow Switched (JFS) systems. Furthermore,
it is shown that approximating the capacitor voltage (Vc), with Vo, can cause
undesired limit cycles, oscillations, chattering or instability issues. To
resolve these issues, a non-jumping normal switched system is defined for JFS
systems, that is equivalent to the internal continuous dynamics. Also, the
challenges of designing SS controllers, for this equivalent switched system is
studied, and the Constrained Near Optimal (CNO) SS is designed for the
equivalent switched system of buck, boost, and buck-boost converters. To
eliminate the required estimations, a general class of switching methods are
defined, that also avoids chattering and eliminates the conventional hysteresis
blocks. The proposed controller is implemented using analog op-amp circuits.
Experimental results show fast and robust responses of the controller board
with buck, boost, and buck-boost converters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10027</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10027</id><created>2019-03-24</created><authors><author><keyname>Routray</keyname><forenames>Sudhir K.</forenames></author><author><keyname>Mohammed</keyname><forenames>Habib H.</forenames></author></authors><title>Core Access Hybridization in 5G</title><categories>eess.SP cs.NI</categories><comments>4 pages, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fifth generation (5G) cellular communication system will have several
advanced features, majority of which will come into existence for the first
time. According to the ITU recommendations, 5G will provide minimum 3 GB/s end
to end data rates under the static conditions and in the mobile condition the
minimum data rate is 100 Mb/s. The latency will be brought down to 1 ms and the
device densities will increase by many folds. These features cannot be provided
in the pure wireless domain. Certainly, the support of the optical networks is
essential to provide these advanced features in 5G communication. These
features need special architectures. In this article, we provide advanced
optical wireless hybrid architectures for 5G networks. We explain the
importance of this architecture and also show the nodal configuration of these
hybrid networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10046</identifier>
 <datestamp>2019-09-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10046</id><created>2019-03-24</created><updated>2019-09-06</updated><authors><author><keyname>Interdonato</keyname><forenames>Giovanni</forenames></author><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Frenger</keyname><forenames>P&#xe5;l</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Downlink Training in Cell-Free Massive MIMO: A Blessing in Disguise</title><categories>cs.IT eess.SP math.IT</categories><comments>Published in IEEE Transactions on Wireless Communications on August
  14, 2019. {\copyright} 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses</comments><doi>10.1109/TWC.2019.2933831</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell-free Massive MIMO (multiple-input multiple-output) refers to a
distributed Massive MIMO system where all the access points (APs) cooperate to
coherently serve all the user equipments (UEs), suppress inter-cell
interference and mitigate the multiuser interference. Recent works demonstrated
that, unlike co-located Massive MIMO, the \textit{channel hardening} is, in
general, less pronounced in cell-free Massive MIMO, thus there is much to
benefit from estimating the downlink channel. In this study, we investigate the
gain introduced by the downlink beamforming training, extending the previously
proposed analysis to non-orthogonal uplink and downlink pilots. Assuming
single-antenna APs, conjugate beamforming and independent Rayleigh fading
channel, we derive a closed-form expression for the per-user achievable
downlink rate that addresses channel estimation errors and pilot contamination
both at the AP and UE side. The performance evaluation includes max-min
fairness power control, greedy pilot assignment methods, and a comparison
between achievable rates obtained from different capacity-bounding techniques.
Numerical results show that downlink beamforming training, although increases
pilot overhead and introduces additional pilot contamination, improves
significantly the achievable downlink rate. Even for large number of APs, it is
not fully efficient for the UE relying on the statistical channel state
information for data decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10073</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10073</id><created>2019-03-24</created><authors><author><keyname>Zayyani</keyname><forenames>Hadi</forenames></author><author><keyname>Haddadi</keyname><forenames>Farzan</forenames></author><author><keyname>Korki</keyname><forenames>Mehdi</forenames></author></authors><title>One Bit Spectrum Sensing in Cognitive Radio Sensor Networks</title><categories>eess.SP stat.AP</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a spectrum sensing algorithm from one bit measurements in
a cognitive radio sensor network. A likelihood ratio test (LRT) for the one bit
spectrum sensing problem is derived. Different from the one bit spectrum
sensing research work in the literature, the signal is assumed to be a discrete
random correlated Gaussian process, where the correlation is only available
within immediate successive samples of the received signal. The employed model
facilitates the design of a powerful detection criteria with measurable
analytical performance. One bit spectrum sensing criterion is derived for one
sensor which is then generalized to multiple sensors. Performance of the
detector is analyzed by obtaining closed-form formulas for the probability of
false alarm and the probability of detection. Simulation results corroborate
the theoretical findings and confirm the efficacy of the proposed detector in
the context of highly correlated signals and large number of sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10119</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10119</id><created>2019-03-24</created><authors><author><keyname>Li</keyname><forenames>Shiyong</forenames></author><author><keyname>Amin</keyname><forenames>Moeness</forenames></author><author><keyname>An</keyname><forenames>Qiang</forenames></author><author><keyname>Zhao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Sun</keyname><forenames>Houjun</forenames></author></authors><title>2-D Coherence Factor for Sidelobe and Ghost Suppressions in Radar
  Imaging</title><categories>eess.SP</categories><comments>7 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coherence factor (CF) is defined as the ratio of coherent power to
incoherent power received by the radar aperture. The incoherent power is
computed by the multi-antenna receiver based on only the spatial variable. In
this respect, it is a one-dimensional (1-D) CF, and thereby the image sidelobes
in down-range cannot be effectively suppressed. We propose a two-dimensional
(2-D) CF by supplementing the 1-D CF by an incoherent sum dealing with the
frequency dimension. In essence, we employ both spatial diversity and frequency
diversity which, respectively, enhance imaging quality in cross range and
range. Simulations and experimental results are provided to demonstrate the
performance advantages of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10154</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10154</id><created>2019-03-25</created><authors><author><keyname>Bera</keyname><forenames>Sutanu</forenames></author><author><keyname>Roy</keyname><forenames>Rinku</forenames></author><author><keyname>Sikdar</keyname><forenames>Debdeep</forenames></author><author><keyname>Mahadevappa</keyname><forenames>Manjunatha</forenames></author></authors><title>An Ensemble Learning Based Classification of Individual Finger Movement
  from EEG</title><categories>eess.SP</categories><comments>Brain Computer Interfacing, EEG, Finger movement analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain computer interface based assistive technology are currently promoted
for motor rehabilitation of the neuromuscular ailed individuals. Recent studies
indicate a high potential of utilising electroencephalography (EEG) to extract
motor related intentions. Limbic movement intentions are already exhaustively
studied by the researchers with high accuracy rate. But, capturing movement of
fingers from EEG is still in nascent stage. In this study, we have proposed an
ensemble learning based approach for EEG in distinguishing between movements of
different fingers, namely, thumb, index, and middle. Six healthy subjects
participated in this study. Common spatial patterns (CSP) were extracted as
features to classify with the extra tree or extremely randomized tree binary
classifier. The average classification accuracy of decoding a finger from rest
condition was found to be $74\%$, wheres in discriminating of movement of pair
of fingers average accuracy was $60\%$. Furthermore, error correcting output
coding (ECOC) was added to the binary classifier to use it in multiclass
classification. The proposed algorithm achieved a maximum kappa value of 0.36
among the subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10176</identifier>
 <datestamp>2019-10-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10176</id><created>2019-03-25</created><updated>2019-10-24</updated><authors><author><keyname>Mataev</keyname><forenames>Gary</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author><author><keyname>Milanfar</keyname><forenames>Peyman</forenames></author></authors><title>DeepRED: Deep Image Prior Powered by RED</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverse problems in imaging are extensively studied, with a variety of
strategies, tools, and theory that have been accumulated over the years.
Recently, this field has been immensely influenced by the emergence of
deep-learning techniques. One such contribution, which is the focus of this
paper, is the Deep Image Prior (DIP) work by Ulyanov, Vedaldi, and Lempitsky
(2018). DIP offers a new approach towards the regularization of inverse
problems, obtained by forcing the recovered image to be synthesized from a
given deep architecture. While DIP has been shown to be quite an effective
unsupervised approach, its results still fall short when compared to
state-of-the-art alternatives.
  In this work, we aim to boost DIP by adding an explicit prior, which enriches
the overall regularization effect in order to lead to better-recovered images.
More specifically, we propose to bring-in the concept of Regularization by
Denoising (RED), which leverages existing denoisers for regularizing inverse
problems. Our work shows how the two (DIP and RED) can be merged into a highly
effective unsupervised recovery process while avoiding the need to
differentiate the chosen denoiser, and leading to very effective results,
demonstrated for several tested problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10244</identifier>
 <datestamp>2019-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10244</id><created>2019-03-25</created><updated>2019-11-12</updated><authors><author><keyname>G&#xfc;ltekin</keyname><forenames>Yunus Can</forenames></author><author><keyname>van Houtum</keyname><forenames>Wim J.</forenames></author><author><keyname>Koppelaar</keyname><forenames>Arie</forenames></author><author><keyname>Willems</keyname><forenames>Frans M. J.</forenames></author></authors><title>Enumerative Sphere Shaping for Wireless Communications with Short
  Packets</title><categories>eess.SP</categories><comments>14 pages, 9 figures</comments><doi>10.1109/TWC.2019.2951139</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic amplitude shaping (PAS) combines an outer shaping layer with an
inner, systematic forward error correction (FEC) layer to close the shaping
gap. Proposed for PAS, constant composition distribution matching (CCDM)
produces amplitude sequences with a fixed empirical distribution. We show that
CCDM suffers from high rate losses for small block lengths, and we propose to
use Enumerative Sphere Shaping (ESS) instead. ESS minimizes the rate loss at
any block length. Furthermore, we discuss the computational complexity of ESS
and demonstrate that it is significantly smaller than shell mapping (SM), which
is another method to perform sphere shaping. We then study the choice of design
parameters for PAS. Following Wachsmann et al., we show that for a given
constellation and target rate, there is an optimum balance between the FEC code
rate and the entropy of the Maxwell-Boltzmann distribution that minimizes the
gap-to-capacity. Moreover, we demonstrate how to utilize the non-systematic
convolutional code from IEEE 802.11 in PAS. Simulations over the additive white
Gaussian noise (AWGN) and frequency-selective channels exhibit that ESS is up
to 1.6 and 0.7 dB more energy-efficient than uniform signaling at block lengths
as small as 96 symbols, respectively, with convolutional and low-density
parity-check (LDPC) codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10293</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10293</id><created>2019-03-25</created><authors><author><keyname>Guo</keyname><forenames>Hongzhi</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author></authors><title>Inter-Media Backscatter Communications with Magnetic Induction</title><categories>eess.SP</categories><journal-ref>ICC 2019 - 2019 IEEE International Conference on Communications
  (ICC)</journal-ref><doi>10.1109/ICC.2019.8761753</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensors in extreme environments such as underground, concrete wall,
and the human body, can enable a large number of important applications.
However, deploying wireless sensors in such environments on a large scale is a
great challenge due to the high cost and the large profile of wireless sensors.
Backscatter communications can reduce the cost and size of wireless sensors by
removing most of the typical wireless components. In this paper, we propose to
leverage the RFID sensors for inter-media magnetic induction-based backscatter
communications (MIBC). In this way, the complexity and cost of wireless sensors
can be significantly reduced. The sensors leverage magnetic signals to
backscatter information which demonstrate high penetration efficiency. We
design a system with channel estimation and optimal signal transmission
strategy, and an optimal receiver. The channel between the aboveground reader
and underground sensors are modeled by using a stratified medium model. The
bit-error-rate is evaluated with different configurations. The results suggest
that MIBC can be utilized for most of the inter-media applications with low
power consumption and high penetration efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10299</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10299</id><created>2019-03-25</created><authors><author><keyname>Guo</keyname><forenames>Hongzhi</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author><author><keyname>Wang</keyname><forenames>Pu</forenames></author></authors><title>On Reliability of Underwater Magnetic Induction Communications with
  Tri-Axis Coils</title><categories>eess.SP</categories><journal-ref>ICC 2019 - 2019 IEEE International Conference on Communications
  (ICC)</journal-ref><doi>10.1109/ICC.2019.8762000</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Underwater magnetic induction communications (UWMICs) provide a low-power and
high-throughput solution for autonomous underwater vehicles (AUVs), which are
envisioned to explore and monitor the underwater environment. UWMIC with
tri-axis coils increases the reliability of the wireless channel by exploring
the coil orientation diversity. However, the UWMIC channel is different from
typical fading channels and the mutual inductance information (MII) is not
always available. It is not clear the performance of the tri-axis coil MIMO
without MII. Also, its performances with multiple users have not been
investigated. In this paper, we analyze the reliability and multiplexing gain
of UWMICs with tri-axis coils by using coil selection. We optimally select the
transmit and receive coils to reduce the computation complexity and power
consumption and explore the diversity for multiple users. We find that without
using all the coils and MII, we can still achieve reliability. Also, the
multiplexing gain of UWMIC without MII is 5dB smaller than typical terrestrial
fading channels. The results of this paper provide a more power-efficient way
to use UWMICs with tri-axis coils.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10336</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10336</id><created>2019-03-21</created><updated>2019-04-02</updated><authors><author><keyname>Deng</keyname><forenames>Xianda</forenames></author><author><keyname>Bian</keyname><forenames>Desong</forenames></author><author><keyname>Shi</keyname><forenames>Di</forenames></author><author><keyname>Yao</keyname><forenames>Wenxuan</forenames></author><author><keyname>Jiang</keyname><forenames>Zhihao</forenames></author><author><keyname>Liu</keyname><forenames>Yilu</forenames></author></authors><title>Line Outage Detection and Localization via Synchrophasor Measurement</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since transmission lines are crucial links in the power system, one line
outage event may bring about interruption or even cascading failure of the
power system. If a quick and accurate line outage detection and localization
can be achieved, the system operator can take necessary actions in time to
mitigate the negative impact. Therefore, the objective of this paper is to
study a method for line outage detection and localization via synchrophasor
measurements. The density of deployed phasor measurement units (PMUs) is
increasing recently, which greatly improves the visibility of the power grid.
Taking advantage of the high-resolution synchrophasor data, the proposed method
utilizes frequency measurement for line outage detection and power change for
localization. The procedure of the proposed method is given. Compared with
conventional methods, it does not require the pre-knowledge on the system.
Simulation study validates the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10346</identifier>
 <datestamp>2019-06-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10346</id><created>2019-03-22</created><updated>2019-06-07</updated><authors><author><keyname>Qin</keyname><forenames>Yao</forenames></author><author><keyname>Carlini</keyname><forenames>Nicholas</forenames></author><author><keyname>Goodfellow</keyname><forenames>Ian</forenames></author><author><keyname>Cottrell</keyname><forenames>Garrison</forenames></author><author><keyname>Raffel</keyname><forenames>Colin</forenames></author></authors><title>Imperceptible, Robust, and Targeted Adversarial Examples for Automatic
  Speech Recognition</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>International Conference on Machine Learning (ICML), 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial examples are inputs to machine learning models designed by an
adversary to cause an incorrect output. So far, adversarial examples have been
studied most extensively in the image domain. In this domain, adversarial
examples can be constructed by imperceptibly modifying images to cause
misclassification, and are practical in the physical world. In contrast,
current targeted adversarial examples applied to speech recognition systems
have neither of these properties: humans can easily identify the adversarial
perturbations, and they are not effective when played over-the-air. This paper
makes advances on both of these fronts. First, we develop effectively
imperceptible audio adversarial examples (verified through a human study) by
leveraging the psychoacoustic principle of auditory masking, while retaining
100% targeted success rate on arbitrary full-sentence targets. Next, we make
progress towards physical-world over-the-air audio adversarial examples by
constructing perturbations which remain effective even after applying realistic
simulated environmental distortions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10417</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10417</id><created>2019-03-25</created><authors><author><keyname>Singh</keyname><forenames>Ravinder</forenames></author><author><keyname>O'Farrell</keyname><forenames>Timothy</forenames></author><author><keyname>Bui</keyname><forenames>Thai C.</forenames></author><author><keyname>Biagi</keyname><forenames>Mauro</forenames></author><author><keyname>David</keyname><forenames>John P. R.</forenames></author></authors><title>Performance Evaluation of Frequency Domain Equalisation Based Colour
  Shift Keying Modulation Schemes Over Diffuse Optical Wireless Channels</title><categories>eess.SP</categories><comments>6 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-colour light-emitting diode (LED) based visible light communication
(VLC) benefits from wavelength diversity while providing indoor illumination.
Colour shift keying (CSK) is a well-researched IEEE standardised multi-colour
VLC modulation technique. This paper presents an investigation into the
performance of tri-chromatic LED (TLED) CSK standardised in IEEE 802.15.7 and a
quad-chromatic LED (QLED) CSK, over a range of diffuse optical wireless
channels, and proposes the use of frequency domain equalisation (FDE) at the
receiver to combat multipath dispersion. The investigation results show that
the FDE enables the higher order CSK modulation modes to achieve a bit error
rate (BER) of 10\textsuperscript{-6} for finite amounts of optical power while
operating over highly dispersive channels and hence provide data links of up to
85.33 and 256 Mbit/s through TLED and QLED CSK, respectively, for a system
bandwidth of 24 MHz. The overall optical power requirements of the CSK schemes
can be reduce by up to 12.6 dB with the use of FDE at the cost of a small
overhead due to cyclic prefix. The optical channel model used in this
investigation includes the cross-talk and insertion losses caused by the
optical properties of commercially available system front end devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10459</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10459</id><created>2019-03-25</created><authors><author><keyname>Dai</keyname><forenames>Sida</forenames></author><author><keyname>Kurras</keyname><forenames>Martin</forenames></author></authors><title>Spatial Consistency Evaluation Based on Massive SIMO Measurements</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, the spatial consistency of wireless massive
single-input-multiple-output channels in a cellular small cell scenario is
evaluated based on measurements taken in Berlin city. The evaluation is done by
computing the similarity of covariance matrices over the distance. As
similarity measure the correlation matrix distance is used. A classification of
the measurements tracks based on the shape of the curves into four different
categories is done. The results in this paper indicate that spatial consistency
is a highly deterministic property in the sense that it depends strongly on the
individual environment and not so much on large scale parameters. Therefore, we
conclude that spatial consistency is not sufficiently modelled by the current
3rd Generation Partnership Project feature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10482</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10482</id><created>2019-03-25</created><updated>2019-07-13</updated><authors><author><keyname>Yazdani</keyname><forenames>Hassan</forenames></author><author><keyname>Vosoughi</keyname><forenames>Azadeh</forenames></author><author><keyname>Gong</keyname><forenames>Xun</forenames></author></authors><title>Beam Selection and Discrete Power Allocation in Opportunistic Cognitive
  Radio Systems with Limited Feedback Using ESPAR Antennas</title><categories>eess.SP</categories><comments>This paper has been submitted to IEEE Transactions on Cognitive
  Communications and Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an opportunistic cognitive radio (CR) system consisting of a
primary user (PU), secondary transmitter (SUtx), and secondary receiver (SUrx),
where SUtx is equipped with an electrically steerable parasitic array radiator
(ESPAR) antenna with beam steering capability for sensing and communication,
and there is a limited feedback channel from SUrx to SUtx. Taking a holistic
approach, we develop a framework for integrated sector-based spectrum sensing
and sector-based data communication. Upon sensing the channel busy, SUtx
determines the beam corresponding to PU's orientation. Upon sensing the channel
idle, SUtx transmits data to SUrx, using the selected beam corresponding to the
strongest channel between SUtx and SUrx. We formulate a constrained
optimization problem, where SUtx-SUrx link ergodic capacity is maximized,
subject to average transmit power and interference constraints, and the
optimization variables are sensing duration, thresholds of channel quantizer at
SUrx, and transmit power levels at SUtx. Since this problem is non-convex we
develop a suboptimal computationally efficient iterative algorithm to find the
solution. Our numerical results quantify the capacity improvement provided by
the ESPAR antenna and demonstrate that our CR system yields lower outage and
symbol error probabilities, compared with a CR system that its SUtx has an
omni-directional antenna.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10534</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10534</id><created>2019-03-25</created><authors><author><keyname>Raposo</keyname><forenames>Francisco Afonso</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author></authors><title>Learning Embodied Semantics via Music and Dance Semiotic Correlations</title><categories>cs.CV cs.LG cs.SD eess.AS</categories><comments>24 pages, 1 figure, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Music semantics is embodied, in the sense that meaning is biologically
mediated by and grounded in the human body and brain. This embodied cognition
perspective also explains why music structures modulate kinetic and
somatosensory perception. We leverage this aspect of cognition, by considering
dance as a proxy for music perception, in a statistical computational model
that learns semiotic correlations between music audio and dance video. We
evaluate the ability of this model to effectively capture underlying semantics
in a cross-modal retrieval task. Quantitative results, validated with
statistical significance testing, strengthen the body of evidence for embodied
cognition in music and show the model can recommend music audio for dance video
queries and vice-versa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10593</identifier>
 <datestamp>2020-02-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10593</id><created>2019-03-25</created><updated>2020-02-12</updated><authors><author><keyname>Flamant</keyname><forenames>Julien</forenames></author><author><keyname>Miron</keyname><forenames>Sebastian</forenames></author><author><keyname>Brie</keyname><forenames>David</forenames></author></authors><title>Quaternion Non-negative Matrix Factorization: definition, uniqueness and
  algorithm</title><categories>eess.SP</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces quaternion non-negative matrix factorization (QNMF),
which generalizes the usual non-negative matrix factorization (NMF) to the case
of polarized signals. Polarization information is represented by Stokes
parameters, a set of 4 energetic parameters widely used in polarimetric
imaging. QNMF relies on two key ingredients: (i) the algebraic representation
of Stokes parameters thanks to quaternions and (ii) the exploitation of
physical constraints on Stokes parameters. These constraints generalize
non-negativity to the case of polarized signals, encoding positive
semi-definiteness of the covariance matrix associated which each source.
Uniqueness conditions for QNMF are presented. Remarkably, they encompass known
sufficient uniqueness conditions from NMF. Meanwhile, QNMF further relaxes NMF
uniqueness conditions requiring sources to exhibit a certain zero-pattern, by
leveraging the complete polarization information. We introduce a simple yet
efficient algorithm called quaternion alternating least squares (QALS) to solve
the QNMF problem in practice. Closed-form quaternion updates are obtained using
the recently introduced generalized HR calculus. Numerical experiments on
synthetic data demonstrate the relevance of the approach. QNMF defines a
promising generic low-rank approximation tool to handle polarization, notably
for blind source separation problems arising in imaging applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10611</identifier>
 <datestamp>2019-09-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10611</id><created>2019-03-25</created><updated>2019-09-17</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author></authors><title>Making Cell-Free Massive MIMO Competitive With MMSE Processing and
  Centralized Implementation</title><categories>cs.IT eess.SP math.IT</categories><comments>14 pages, 6 figures, To appear in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell-free Massive MIMO is considered as a promising technology for satisfying
the increasing number of users and high rate expectations in beyond-5G
networks. The key idea is to let many distributed access points (APs)
communicate with all users in the network, possibly by using joint coherent
signal processing. The aim of this paper is to provide the first comprehensive
analysis of this technology under different degrees of cooperation among the
APs. Particularly, the uplink spectral efficiencies of four different cell-free
implementations are analyzed, with spatially correlated fading and arbitrary
linear processing. It turns out that it is possible to outperform conventional
Cellular Massive MIMO and small cell networks by a wide margin, but only using
global or local minimum mean-square error (MMSE) combining. This is in sharp
contrast to the existing literature, which advocates for maximum-ratio
combining. Also, we show that a centralized implementation with optimal MMSE
processing not only maximizes the SE but largely reduces the fronthaul
signaling compared to the standard distributed approach. This makes it the
preferred way to operate Cell-free Massive MIMO networks. Non-linear decoding
is also investigated and shown to bring negligible improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10703</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10703</id><created>2019-03-26</created><authors><author><keyname>Wyse</keyname><forenames>Lonce</forenames></author><author><keyname>Huzaifah</keyname><forenames>Muhammad</forenames></author></authors><title>Conditioning a Recurrent Neural Network to synthesize musical instrument
  transients</title><categories>cs.SD cs.LG eess.AS</categories><comments>Sound and Music Computing Conference. Malaga, Spain, May 2019</comments><acm-class>H.5.5</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A recurrent Neural Network (RNN) is trained to predict sound samples based on
audio input augmented by control parameter information for pitch, volume, and
instrument identification. During the generative phase following training,
audio input is taken from the output of the previous time step, and the
parameters are externally controlled allowing the network to be played as a
musical instrument. Building on an architecture developed in previous work, we
focus on the learning and synthesis of transients - the temporal response of
the network during the short time (tens of milliseconds) following the onset
and offset of a control signal. We find that the network learns the particular
transient characteristics of two different synthetic instruments, and
furthermore shows some ability to interpolate between the characteristics of
the instruments used in training in response to novel parameter settings. We
also study the behaviour of the units in hidden layers of the RNN using various
visualisation techniques and find a variety of volume-specific response
characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10713</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10713</id><created>2019-03-26</created><updated>2019-03-27</updated><authors><author><keyname>Thakur</keyname><forenames>Anshul</forenames></author><author><keyname>Thapar</keyname><forenames>Daksh</forenames></author><author><keyname>Rajan</keyname><forenames>Padmanabhan</forenames></author><author><keyname>Nigam</keyname><forenames>Aditya</forenames></author></authors><title>Multiscale CNN based Deep Metric Learning for Bioacoustic
  Classification: Overcoming Training Data Scarcity Using Dynamic Triplet Loss</title><categories>eess.AS cs.LG cs.SD</categories><comments>Under Review at JASA. Primitive version of paper. We are still
  working on getting better performances out of the comparative methods</comments><doi>10.1121/1.5118245</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes multiscale convolutional neural network (CNN)-based deep
metric learning for bioacoustic classification, under low training data
conditions. The proposed CNN is characterized by the utilization of four
different filter sizes at each level to analyze input feature maps. This
multiscale nature helps in describing different bioacoustic events effectively:
smaller filters help in learning the finer details of bioacoustic events,
whereas, larger filters help in analyzing a larger context leading to global
details. A dynamic triplet loss is employed in the proposed CNN architecture to
learn a transformation from the input space to the embedding space, where
classification is performed. The triplet loss helps in learning this
transformation by analyzing three examples, referred to as triplets, at a time
where intra-class distance is minimized while maximizing the inter-class
separation by a dynamically increasing margin. The number of possible triplets
increases cubically with the dataset size, making triplet loss more suitable
than the softmax cross-entropy loss in low training data conditions.
Experiments on three different publicly available datasets show that the
proposed framework performs better than existing bioacoustic classification
frameworks. Experimental results also confirm the superiority of the triplet
loss over the cross-entropy loss in low training data conditions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10729</identifier>
 <datestamp>2020-02-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10729</id><created>2019-03-26</created><updated>2019-06-19</updated><authors><author><keyname>Chandna</keyname><forenames>Pritish</forenames></author><author><keyname>Blaauw</keyname><forenames>Merlijn</forenames></author><author><keyname>Bonada</keyname><forenames>Jordi</forenames></author><author><keyname>Gomez</keyname><forenames>Emilia</forenames></author></authors><title>WGANSing: A Multi-Voice Singing Voice Synthesizer Based on the
  Wasserstein-GAN</title><categories>cs.SD eess.AS</categories><journal-ref>2019 27th European Signal Processing Conference (EUSIPCO)</journal-ref><doi>10.23919/EUSIPCO.2019.8903099</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deep neural network based singing voice synthesizer, inspired by
the Deep Convolutions Generative Adversarial Networks (DCGAN) architecture and
optimized using the Wasserstein-GAN algorithm. We use vocoder parameters for
acoustic modelling, to separate the influence of pitch and timbre. This
facilitates the modelling of the large variability of pitch in the singing
voice. Our network takes a block of consecutive frame-wise linguistic and
fundamental frequency features, along with global singer identity as input and
outputs vocoder features, corresponding to the block of features. This
block-wise approach, along with the training methodology allows us to model
temporal dependencies within the features of the input block. For inference,
sequential blocks are concatenated using an overlap-add procedure. We show that
the performance of our model is competitive with regards to the
state-of-the-art and the original sample using objective metrics and a
subjective listening test. We also present examples of the synthesis on a
supplementary website and the source code via GitHub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10825</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10825</id><created>2019-03-26</created><authors><author><keyname>Demarchou</keyname><forenames>Eleni</forenames></author><author><keyname>Psomas</keyname><forenames>Constantinos</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Asynchronous Ad Hoc Networks with Wireless Powered Cognitive
  Communications</title><categories>eess.SP</categories><comments>IEEE Transactions on Cognitive Communications and Networking
  (accepted for publication)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the recent years, the proliferation of smart devices and their
applications has led to a rapid evolution of the concept of the Internet of
Things (IoT), advancing large scale machine type networks which are
characterized by sporadic transmissions of short packets. In contrast to
typical communication models and in order to capture a realistic IoT
environment, we study an asynchronous channel access performed by a primary ad
hoc network underlaid with a cognitive secondary wireless-powered ad hoc
network. Specifically, we consider that the primary transmitters are connected
to the power grid and employ asynchronous transmissions. On the other hand, the
cognitive secondary transmitters have radio frequency energy harvesting
capabilities, and their asynchronous channel access is established based on
certain energy and interference based criteria. We model this sporadic channel
traffic with time-space Poisson point processes and by using tools from
stochastic geometry, we provide an analytical framework for the performance of
this asynchronous system. In particular, we provide closed-form expressions for
the information coverage probability and the spatial throughput for both
networks and we derive the meta distribution of the
signal-to-interference-plus-noise ratio. Finally, we present numerical results
and provide important insights behind the main system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10839</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10839</id><created>2019-03-26</created><authors><author><keyname>Schreiber</keyname><forenames>Hendrik</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Meinard</forenames></author></authors><title>Musical Tempo and Key Estimation using Convolutional Neural Networks
  with Directional Filters</title><categories>cs.SD cs.LG eess.AS</categories><comments>Sound &amp; Music Computing Conference (SMC), M\'alaga, Spain, May 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this article we explore how the different semantics of spectrograms' time
and frequency axes can be exploited for musical tempo and key estimation using
Convolutional Neural Networks (CNN). By addressing both tasks with the same
network architectures ranging from shallow, domain-specific approaches to deep
variants with directional filters, we show that axis-aligned architectures
perform similarly well as common VGG-style networks developed for computer
vision, while being less vulnerable to confounding factors and requiring fewer
model parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10841</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10841</id><created>2019-03-26</created><updated>2019-04-01</updated><authors><author><keyname>Li&#xdf;ner</keyname><forenames>Julian</forenames></author><author><keyname>Fritzen</keyname><forenames>Felix</forenames></author></authors><title>Data-Driven Microstructure Property Relations</title><categories>cs.CE cs.LG eess.IV</categories><comments>23 pages, 2 tables, 11 figures - EDIT 2019/04/01: recompiled in the
  proper (A4) page format</comments><msc-class>74-04, 74A40, 74E30, 74Q05, 74S30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An image based prediction of the effective heat conductivity for highly
heterogeneous microstructured materials is presented. The synthetic materials
under consideration show different inclusion morphology, orientation, volume
fraction and topology. The prediction of the effective property is made
exclusively based on image data with the main emphasis being put on the 2-point
spatial correlation function. This task is implemented using both unsupervised
and supervised machine learning methods. First, a snapshot proper orthogonal
decomposition (POD) is used to analyze big sets of random microstructures and
thereafter compress significant characteristics of the microstructure into a
low-dimensional feature vector. In order to manage the related amount of data
and computations, three different incremental snapshot POD methods are
proposed. In the second step, the obtained feature vector is used to predict
the effective material property by using feed forward neural networks.
Numerical examples regarding the incremental basis identification and the
prediction accuracy of the approach are presented. A Python code illustrating
the application of the surrogate is freely available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10876</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10876</id><created>2019-03-22</created><authors><author><keyname>Raj</keyname><forenames>A. Govinda</forenames></author><author><keyname>McClellan</keyname><forenames>J. H.</forenames></author></authors><title>Super-Resolution DOA Estimation for Arbitrary Array Geometries Using a
  Single Noisy Snapshot</title><categories>eess.SP cs.IT math.IT</categories><comments>To appear in Proc. ICASSP 2019, May 12-17, 2019, Brighton, UK. arXiv
  admin note: substantial text overlap with arXiv:1810.00017</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of search-free DOA estimation from a single noisy
snapshot for sensor arrays of arbitrary geometry, by extending a method of
gridless super-resolution beamforming to arbitrary arrays with noisy
measurements. The primal atomic norm minimization problem is converted to a
dual problem in which the periodic dual function is represented with a
trigonometric polynomial using truncated Fourier series. The number of terms
required for accurate representation depends linearly on the distance of the
farthest sensor from a reference. The dual problem is then expressed as a
semidefinite program and solved in polynomial time. DOA estimates are obtained
via polynomial rooting followed by a LASSO based approach to remove extraneous
roots arising in root finding from noisy data, and then source amplitudes are
recovered by least squares. Simulations using circular and random planar arrays
show high resolution DOA estimation in white and colored noise scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10899</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10899</id><created>2019-03-26</created><authors><author><keyname>Schmidt</keyname><forenames>Jorge F.</forenames></author><author><keyname>Schilcher</keyname><forenames>Udo</forenames></author><author><keyname>Atiq</keyname><forenames>Mahin K.</forenames></author><author><keyname>Bettstetter</keyname><forenames>Christian</forenames></author></authors><title>Interference Prediction in Wireless Networks: Stochastic Geometry meets
  Recursive Filtering</title><categories>cs.LG cs.NI eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes and evaluates a technique to predict the level of
interference in wireless networks. We design a recursive predictor that
computes future interference values at a given location by filtering measured
interference at this location. The parametrization of the predictor is done
offline by translating the autocorrelation of interference into an
autoregressive moving average (ARMA) representation. This ARMA model is
inserted into a steady-state Kalman filter enabling nodes to predict with low
computational effort. Results show good performance in terms of accuracy
between predicted and true values for relevant time horizons. Although the
predictor is parametrized for the case of Poisson networks, Rayleigh fading,
and fixed message lengths, a sensitivity analysis shows that it also works well
in more general network scenarios. Numerical examples for underlay
device-to-device communications and a common wireless sensor technology
illustrate its broad applicability. The predictor can be applied as part of
interference management to improve medium access, scheduling, and resource
allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10923</identifier>
 <datestamp>2019-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10923</id><created>2019-03-26</created><updated>2019-10-15</updated><authors><author><keyname>Alsulami</keyname><forenames>Osama Zwaid</forenames></author><author><keyname>Alresheedi</keyname><forenames>Mohammed T.</forenames></author><author><keyname>Elmirghani</keyname><forenames>Jaafar M. H.</forenames></author></authors><title>Transmitter Diversity with Beam Steering</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing high data rates is one of the drivers in visible light
communication (VLC) systems. This paper introduces a VLC system that employs
transmitters diversity with beam steering to provide high data rates. In this
work, red, yellow, green, and blue (RYGB) laser diodes (LD) are used as
transmitters to obtain a high modulation bandwidth. Two types of RYGB LDs units
are used in this paper: transmitters diversity (TD) RYGB LDs light unit (for
illumination and communication) and RYGB LDs light unit (for illumination). In
addition, a seven branch angle diversity receiver (ADR) is used where we study
the delay spread and SNR. The proposed system was compared to the normal VLC
system. A data rate up to 22.8 Gb/s was achieved using simple on-off-keying
(OOK) modulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.10925</identifier>
 <datestamp>2019-03-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.10925</id><created>2019-03-26</created><authors><author><keyname>Alsulami</keyname><forenames>Osama Zwaid</forenames></author><author><keyname>Musa</keyname><forenames>Mohamed O. I.</forenames></author><author><keyname>Alresheedi</keyname><forenames>Mohammed T.</forenames></author><author><keyname>Elmirghani</keyname><forenames>Jaafar M. H.</forenames></author></authors><title>Visible Light Optical Data Centre Links</title><categories>eess.SP</categories><comments>arXiv admin note: text overlap with arXiv:1903.10923</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing high data rates is one of the big concerns in visible light
communication (VLC) systems. This paper introduces a data centre design that
use a VLC system for downlink communication. In this work, RYGB laser diodes
(LD) are used as transmitters to obtain a high modulation bandwidth. Three
types of receivers, wide field of view receiver (WFOVR), 3 branches angle
diversity receiver (ADR) and 50 pixels imaging receiver (ImR) are used to
examine delay spread and SNR. The proposed system achieved data rates up to
14.2 Gbps using simple on-off-keying (OOK) modulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11058</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11058</id><created>2019-03-25</created><authors><author><keyname>Hojjatinia</keyname><forenames>Sarah</forenames></author><author><keyname>Lagoa</keyname><forenames>Constantino M.</forenames></author></authors><title>Identification of Markov Jump Autoregressive Processes from Large Noisy
  Data Sets</title><categories>eess.SP cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1804.07411</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel methodology for the identification of switching
dynamics for switched autoregressive linear models. Switching behavior is
assumed to follow a Markov model. The system's outputs are contaminated by
possibly large values of measurement noise. Although the procedure provided can
handle other noise distributions, for simplicity, it is assumed that the
distribution is Normal with unknown variance. Given noisy input-output data, we
aim at identifying switched system coefficients, parameters of the noise
distribution, dynamics of switching and probability transition matrix of
Markovian model. System dynamics are estimated using previous results which
exploit algebraic constraints that system trajectories have to satisfy.
Switching dynamics are computed with solving a maximum likelihood estimation
problem. The efficiency of proposed approach is shown with several academic
examples. Although the noise to output ratio can be high, the method is shown
to be extremely effective in the situations where a large number of
measurements is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11101</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11101</id><created>2019-03-26</created><authors><author><keyname>Dunnmon</keyname><forenames>Jared</forenames></author><author><keyname>Ratner</keyname><forenames>Alexander</forenames></author><author><keyname>Khandwala</keyname><forenames>Nishith</forenames></author><author><keyname>Saab</keyname><forenames>Khaled</forenames></author><author><keyname>Markert</keyname><forenames>Matthew</forenames></author><author><keyname>Sagreiya</keyname><forenames>Hersh</forenames></author><author><keyname>Goldman</keyname><forenames>Roger</forenames></author><author><keyname>Lee-Messer</keyname><forenames>Christopher</forenames></author><author><keyname>Lungren</keyname><forenames>Matthew</forenames></author><author><keyname>Rubin</keyname><forenames>Daniel</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Cross-Modal Data Programming Enables Rapid Medical Machine Learning</title><categories>cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Labeling training datasets has become a key barrier to building medical
machine learning models. One strategy is to generate training labels
programmatically, for example by applying natural language processing pipelines
to text reports associated with imaging studies. We propose cross-modal data
programming, which generalizes this intuitive strategy in a
theoretically-grounded way that enables simpler, clinician-driven input,
reduces required labeling time, and improves with additional unlabeled data. In
this approach, clinicians generate training labels for models defined over a
target modality (e.g. images or time series) by writing rules over an auxiliary
modality (e.g. text reports). The resulting technical challenge consists of
estimating the accuracies and correlations of these rules; we extend a recent
unsupervised generative modeling technique to handle this cross-modal setting
in a provably consistent way. Across four applications in radiography, computed
tomography, and electroencephalography, and using only several hours of
clinician time, our approach matches or exceeds the efficacy of
physician-months of hand-labeling with statistical significance, demonstrating
a fundamentally faster and more flexible way of building machine learning
models in medicine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11131</identifier>
 <datestamp>2019-06-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11131</id><created>2019-03-26</created><updated>2019-06-25</updated><authors><author><keyname>Wolter</keyname><forenames>Franz-Erich</forenames></author><author><keyname>Berger</keyname><forenames>Benjamin</forenames></author></authors><title>Differential Geometric Foundations for Power Flow Computations</title><categories>math.DG cs.CE cs.CG eess.SP</categories><msc-class>53B21 (Primary), 97R30, 70G55, 53C22, 65D99 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to systematically and comprehensively initiate a foundation
for using concepts from computational differential geometry as instruments for
power flow computing and research. At this point we focus our discussion on the
static case, with power flow equations given by quadratic functions defined on
voltage space with values in power space; both spaces have real Euclidean
coordinates. Central issue is a differential geometric analysis of the power
flow solution space boundary (SSB) both in voltage and in power space. We
present different methods for computing tangent vectors, tangent planes and
normals of the SSB and the normals' derivatives. Using the latter we compute
normal and principal curvatures. All this is needed for tracing the orthogonal
projection of curves in voltage and power space onto the SSB for points on the
SSB cosest to given points on the curves, thus obtaining estimates for the
distance to the SSB. Furthermore, we present a new high precision continuation
method for power flow solutions. We also compute geodesics on the SSB or an
implicitly defined submanfold thereof and, used to define geodesic coordinates
together with their Jacobians on the manifolds. These computations might be the
most innovative and most significant contribution of this paper, because this
concept provides a comprehensive coordinate system for sub many folds defined
by implicit equations. Therefore while moving on geodesics described by the
geodesic coordinates of the sub manifold at hand we get, via systematic
navigation guided by geodesic coordinates, access to all feasible operation
points of the system. We propose some applications and show some properties of
the Jacobian of the power flow map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11143</identifier>
 <datestamp>2019-07-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11143</id><created>2019-03-26</created><updated>2019-03-28</updated><authors><author><keyname>Dumphart</keyname><forenames>Gregor</forenames></author><author><keyname>Schulten</keyname><forenames>Henry</forenames></author><author><keyname>Bhatia</keyname><forenames>Bharat</forenames></author><author><keyname>Sulser</keyname><forenames>Christoph</forenames></author><author><keyname>Wittneben</keyname><forenames>Armin</forenames></author></authors><title>Practical Accuracy Limits of Radiation-Aware Magneto-Inductive 3D
  Localization</title><categories>eess.SP stat.AP</categories><comments>To appear at the IEEE ICC 2019 Workshops. This work has been
  submitted to the IEEE for possible publication. Copyright may be transferred
  without notice, after which this version may no longer be accessible</comments><doi>10.1109/ICCW.2019.8757178</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key motivation for the low-frequency magnetic localization approach is
that magnetic near-fields are well predictable by a free-space model, which
should enable accurate localization. Yet, limited accuracy has been reported
for practical systems and it is unclear whether the inaccuracies are caused by
field distortion due to nearby conductors, unconsidered radiative propagation,
or measurement noise. Hence, we investigate the practical performance limits by
means of a calibrated magnetoinductive system which localizes an active
single-coil agent with arbitrary orientation, using 4 mW transmit power at 500
kHz. The system uses eight single-coil anchors around a 3m x 3m area in an
office room. We base the location estimation on a complex baseband model which
comprises both reactive and radiative propagation. The link coefficients, which
serve as input data for location estimation, are measured with a multiport
network analyzer while the agent is moved with a positioner device. This
establishes a reliable ground truth for calibration and evaluation. The system
achieves a median position error of 3.2 cm and a 90th percentile of 8.3 cm.
After investigating the model error we conjecture that field distortion due to
conducting building structures is the main cause of the performance bottleneck.
The results are complemented with predictions on the achievable accuracy in
more suitable circumstances using the Cram\'er-Rao lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11171</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11171</id><created>2019-03-26</created><updated>2019-04-04</updated><authors><author><keyname>Shad</keyname><forenames>Saeideh</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author></authors><title>A Compact and High Gain Dielectric-Loaded 60GHz Multi-Stepped Waveguide
  Antenna Array</title><categories>eess.SP</categories><comments>This paper has been accepted accepted for presentation at the 2019
  IEEE International Symposium on Antennas and Propagation and USNC-URSI Radio
  Science Meeting in Atlanta, Georgia, 7-12 July 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a wideband high-gain 2 ? 2-element subarray is presented for
60 GHz band applications. The antenna is fed with waveguide-fed cavity backed
configuration and designed entirely via simple rectangular apertures. To
improve radiation pattern characteristics and reduce the antenna size,
stepped-radiating apertures loaded with a solid dielectric material. A standard
WR-15 rectangular waveguide is designed to excite the antenna at the input port
over the operation frequency. The most significant advantage of using this
design is its efficient radiation patterns, ability to decrease complexity and
cost of fabrication. Simulated results demonstrate a maximum gain of about 19.5
dB, and the sidelobe level (SLL) of the antenna is less than -19 dB in E- and
H-planes radiation patterns over the frequency range from 57.5 to 62.5 GHz. In
contrast to previous works, the proposed antenna is much more simple to use in
antenna array applications. Reduction in the number of radiating apertures and
compact feeding networks will lower significantly the size and complexity of a
large array with higher gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11175</identifier>
 <datestamp>2019-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11175</id><created>2019-03-27</created><authors><author><keyname>Yu</keyname><forenames>Wen-Kai</forenames></author></authors><title>Super sub-Nyquist single-pixel imaging by means of cake-cutting Hadamard
  basis sort</title><categories>eess.IV physics.optics</categories><comments>24 pages, 7 figures</comments><journal-ref>Sensors, 19, 4122 (2019)</journal-ref><doi>10.3390/s19194122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-pixel imaging via compressed sensing can reconstruct high-quality
images from a few linear random measurements of an object/scene known a priori
to be sparse or compressive, by using a point/bucket detector without spatial
resolution. Nevertheless, it still faces a harsh trade-off among the
acquisition time, the spatial resolution and the signal-to-noise ratio. Here we
present a new compressive imaging approach with use of a strategy called
cake-cutting which optimally reorders the deterministic Hadamard basis. By this
means, the number of measurements can be dramatically reduced by more than two
orders of magnitude. Furthermore, by exploiting the structured characteristic
of the Hadamard matrix, we can accelerate the computational process and
simultaneously reduce the memory consumption of storing the matrix. The
proposed method is capable of recovering an image of the object, of pixel size
$1024\times1024$, with a sampling ratio of even 0.2%, thereby realizing super
sub-Nyquist sampling and significantly reducing the acquisition time. Moreover,
through the differential modulation/measurements, we demonstrate this method
with a single-photon single-pixel camera under low light condition and retrieve
clear images through partially obscuring scenes. This described practical
method complements the single-pixel imaging approaches and can be applied to a
variety of fields, such as video, night vision goggles and automatic drive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11176</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11176</id><created>2019-03-26</created><authors><author><keyname>Agrawal</keyname><forenames>Taruna</forenames></author><author><keyname>Gupta</keyname><forenames>Rahul</forenames></author><author><keyname>Narayanan</keyname><forenames>Shrikanth</forenames></author></authors><title>On evaluating CNN representations for low resource medical image
  classification</title><categories>eess.IV cs.LG stat.ML</categories><comments>Accepted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) have revolutionized performances in
several machine learning tasks such as image classification, object tracking,
and keyword spotting. However, given that they contain a large number of
parameters, their direct applicability into low resource tasks is not
straightforward. In this work, we experiment with an application of CNN models
to gastrointestinal landmark classification with only a few thousands of
training samples through transfer learning. As in a standard transfer learning
approach, we train CNNs on a large external corpus, followed by representation
extraction for the medical images. Finally, a classifier is trained on these
CNN representations. However, given that several variants of CNNs exist, the
choice of CNN is not obvious. To address this, we develop a novel metric that
can be used to predict test performances, given CNN representations on the
training set. Not only we demonstrate the superiority of the CNN based transfer
learning approach against an assembly of knowledge driven features, but the
proposed metric also carries an 87% correlation with the test set performances
as obtained using various CNN representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11177</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11177</id><created>2019-03-26</created><authors><author><keyname>Shad</keyname><forenames>Saeideh</forenames></author><author><keyname>Kausar</keyname><forenames>Shafaq</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author></authors><title>Waveguide-Fed Lens Based Beam-Steering Antenna For 5G Wireless
  Communications</title><categories>eess.SP</categories><comments>This paper has been accepted for presentation at the 2019 IEEE
  International Symposium on Antennas and Propagation and USNC-URSI Radio
  Science Meeting in Atlanta, Georgia, 7-12 July 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a two-dimensional cylindrical Lens antenna based on the
parallel plate technique is designed. It supports beam-steering capability of
58 degree at 28 GHz. The antenna is composed of low loss rectangular waveguide
antennas, which are positioned around a homogeneous cylindrical Teflon lens in
the air region of two conducting parallel plates. The Beam scanning can be
achieved by switching between the antenna elements. The main advantages of our
design include its relative simplicity, ease of fabrication, and high-power
handling capability. Compared to previous works including a curvature
optimization for the plate separation of the parallel plates, the proposed
antenna has a constant distance between plates. At the 28 GHz, the maximum
simulated gain value is about 19 dB. Furthermore, the designed antenna only
deviates about 0.4 dB over the 58 degree scan range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11179</identifier>
 <datestamp>2019-05-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11179</id><created>2019-03-26</created><updated>2019-05-12</updated><authors><author><keyname>Stankovic</keyname><forenames>Ljubisa</forenames></author><author><keyname>Mandic</keyname><forenames>Danilo</forenames></author><author><keyname>Dakovic</keyname><forenames>Milos</forenames></author><author><keyname>Kisil</keyname><forenames>Ilya</forenames></author><author><keyname>Sejdic</keyname><forenames>Ervin</forenames></author><author><keyname>Constantinides</keyname><forenames>Anthony G.</forenames></author></authors><title>An Example-Driven Introduction to Data Analytics on Graphs</title><categories>eess.SP</categories><comments>10 pages, 3 figures, 5 boxes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are irregular structures which naturally account for data integrity,
however, traditional approaches have been established outside Signal
Processing, and largely focus on analyzing the underlying graphs rather than
signals on graphs. Given the rapidly increasing availability of multisensor and
multinode measurements, likely recorded on irregular or ad-hoc grids, it would
be extremely advantageous to analyze such structured data as graph signals and
thus benefit from the ability of graphs to incorporate spatial awareness of the
sensing locations, sensor importance, and local versus global sensor
association. The aim of this lecture note is therefore to establish a common
language between graph signals, defined on irregular signal domains, and some
of the most fundamental paradigms in DSP, such as spectral analysis of
multichannel signals, system transfer function, digital filter design,
parameter estimation, and optimal filter design. This is achieved through a
physically meaningful and intuitive real-world example of geographically
distributed multisensor temperature estimation. A similar spatial multisensor
arrangement is already widely used in Signal Processing curricula to introduce
minimum variance estimators and Kalman filters \cite{HM}, and by adopting this
framework we facilitate a seamless integration of graph theory into the
curriculum of existing DSP courses. By bridging the gap between standard
approaches and graph signal processing, we also show that standard methods can
be thought of as special cases of their graph counterparts, evaluated on linear
graphs. It is hoped that our approach would not only help to demystify graph
theoretic approaches in education and research but it would also empower
practitioners to explore a whole host of otherwise prohibitive modern
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11210</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11210</id><created>2019-03-26</created><updated>2019-03-27</updated><authors><author><keyname>Malik</keyname><forenames>Junaid</forenames></author><author><keyname>Kiranyaz</keyname><forenames>Serkan</forenames></author><author><keyname>Kunhoth</keyname><forenames>Suchitra</forenames></author><author><keyname>Ince</keyname><forenames>Turker</forenames></author><author><keyname>Al-Maadeed</keyname><forenames>Somaya</forenames></author><author><keyname>Hamila</keyname><forenames>Ridha</forenames></author><author><keyname>Gabbouj</keyname><forenames>Moncef</forenames></author></authors><title>Colorectal cancer diagnosis from histology images: A comparative study</title><categories>cs.CV cs.LG eess.IV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Computer-aided diagnosis (CAD) based on histopathological imaging has
progressed rapidly in recent years with the rise of machine learning based
methodologies. Traditional approaches consist of training a classification
model using features extracted from the images, based on textures or
morphological properties. Recently, deep-learning based methods have been
applied directly to the raw (unprocessed) data. However, their usability is
impacted by the paucity of annotated data in the biomedical sector. In order to
leverage the learning capabilities of deep Convolutional Neural Nets (CNNs)
within the confines of limited labelled data, in this study we shall
investigate the transfer learning approaches that aim to apply the knowledge
gained from solving a source (e.g., non-medical) problem, to learn better
predictive models for the target (e.g., biomedical) task. As an alternative, we
shall further propose a new adaptive and compact CNN based architecture that
can be trained from scratch even on scarce and low-resolution data. Moreover,
we conduct quantitative comparative evaluations among the traditional methods,
transfer learning-based methods and the proposed adaptive approach for the
particular task of cancer detection and identification from scarce and
low-resolution histology images. Over the largest benchmark dataset formed for
this purpose, the proposed adaptive approach achieved a higher cancer detection
accuracy with a significant gap, whereas the deep CNNs with transfer learning
achieved a superior cancer identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11356</identifier>
 <datestamp>2020-01-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11356</id><created>2019-03-27</created><updated>2020-01-11</updated><authors><author><keyname>Song</keyname><forenames>Anna</forenames></author><author><keyname>Uhlmann</keyname><forenames>Virginie</forenames></author><author><keyname>Fageot</keyname><forenames>Julien</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author></authors><title>Dictionary Learning for Two-Dimensional Kendall Shapes</title><categories>eess.IV</categories><comments>33 pages, 13 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We propose a novel sparse dictionary learning method for planar shapes in the
sense of Kendall, namely configurations of landmarks in the plane considered up
to similitudes. Our shape dictionary method provides a good trade-off between
algorithmic simplicity and faithfulness with respect to the nonlinear geometric
structure of Kendall's shape space. Remarkably, it boils down to a classical
dictionary learning formulation modified using complex weights. Existing
dictionary learning methods extended to nonlinear spaces either map the
manifold to a reproducing kernel Hilbert space or to a tangent space. The first
approach is unnecessarily heavy in the case of Kendall's shape space and causes
the geometrical understanding of shapes to be lost, while the second one
induces distortions and theoretical complexity. Our approach does not suffer
from these drawbacks. Instead of embedding the shape space into a linear space,
we rely on the hyperplane of centered configurations, including pre-shapes from
which shapes are defined as rotation orbits. In this linear space, the
dictionary atoms are scaled and rotated using complex weights before summation.
Furthermore, our formulation is more general than Kendall's original one: it
applies to discretely-defined configurations of landmarks as well as
continuously-defined interpolating curves. We implemented our algorithm by
adapting the method of optimal directions combined to a Cholesky-optimized
order recursive matching pursuit. An interesting feature of our shape
dictionary is that it produces visually realistic atoms, while guaranteeing
reconstruction accuracy. Its efficiency can mostly be attributed to a clear
formulation of the framework with complex numbers. We illustrate the strong
potential of our approach for the characterization of datasets of shapes up to
similitudes and the analysis of patterns in deforming 2D shapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11365</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11365</id><created>2019-03-27</created><authors><author><keyname>Alonzo</keyname><forenames>Mario</forenames></author><author><keyname>Buzzi</keyname><forenames>Stefano</forenames></author><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>D'Elia</keyname><forenames>Ciro</forenames></author></authors><title>Energy-Efficient Power Control in Cell-Free and User-Centric Massive
  MIMO at Millimeter Wave</title><categories>eess.SP cs.IT math.IT</categories><comments>To appear on the IEEE Transactions on Green Communications and
  Networking; originally submitted on April 24, 2018 and finally accepted for
  publication on March 24, 2019</comments><journal-ref>IEEE Transactions on Green Communications and Networking, 2019</journal-ref><doi>10.1109/TGCN.2019.2908228</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a cell-free massive MIMO architecture a very large number of distributed
access points simultaneously and jointly serves a much smaller number of mobile
stations; a variant of the cell-free technique is the user-centric approach,
wherein each access point just serves a reduced set of mobile stations. This
paper introduces and analyzes the cell-free and user-centric architectures at
millimeter wave frequencies, considering a training-based channel estimation
phase, and the downlink and uplink data transmission phases. First of all, a
multiuser clustered millimeter wave channel model is introduced in order to
account for the correlation among the channels of nearby users; second, an
uplink multiuser channel estimation scheme is described along with
low-complexity hybrid analog/digital beamforming architectures. Third, the
non-convex problem of power allocation for downlink global energy efficiency
maximization is addressed. Interestingly, in the proposed schemes no channel
estimation is needed at the mobile stations, and the beamforming schemes used
at the mobile stations are channel-independent and have a very simple
structure. Numerical results show the benefits granted by the power control
procedure, that the considered architectures are effective, and permit
assessing the loss incurred by the use of the hybrid beamformers and by the
channel estimation errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11381</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11381</id><created>2019-03-25</created><authors><author><keyname>Hosseini</keyname><forenames>Morteza</forenames></author><author><keyname>Paneliya</keyname><forenames>Hirenkumar</forenames></author><author><keyname>Kallakuri</keyname><forenames>Uttej</forenames></author><author><keyname>Khatwani</keyname><forenames>Mohit</forenames></author><author><keyname>Mohsenin</keyname><forenames>Tinoosh</forenames></author></authors><title>Minimizing Classification Energy of Binarized Neural Network Inference
  for Wearable Devices</title><categories>eess.SP</categories><comments>Accepted and presented at 20th International Symposium on Quality
  Electronic Design (ISQED 2019) on March 7th, 2019, Santa Clara, CA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a low-power hardware for efficient deployment of
binarized neural networks (BNNs) that have been trained for physiological
datasets. BNNs constrain weights and feature-map to 1 bit, can pack in as many
1-bit weights as the width of a memory entry provides, and can execute multiple
multiply-accumulate (MAC) operations with one fused bit-wise xnor and
population-count instruction over aligned packed entries. Our proposed hardware
is scalable with the number of processing engines (PEs) and the memory width,
both of which adjustable for the most energy efficient configuration given an
application. We implement two real case studies including Physical Activity
Monitoring and Stress Detection on our platform, and for each case study on the
target platform, we seek the optimal PE and memory configurations. Our
implementation results indicate that a configuration with a good choice of
memory width and number of PEs can be optimized up to 4x and 2.5x in energy
consumption respectively on Artix-7 FPGA and on 65nm CMOS ASIC implementation.
We also show that, generally, wider memories make more efficient BNN processing
hardware. To further reduce the energy, we introduce Pool-Skipping technique
that can skip at least 25% of the operations that are accompanied by a Max-Pool
layer in BNNs, leading to a total of 22% operation reduction in the Stress
Detection case study. Compared to the related works using the same case studies
on the same target platform and with the same classification accuracy, our
hardware is respectively 4.5x and 250x more energy efficient for the Stress
Detection on FPGA and Physical Activity Monitoring on ASIC, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11385</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11385</id><created>2019-03-13</created><authors><author><keyname>Ma</keyname><forenames>Shuai</forenames></author><author><keyname>Dai</keyname><forenames>Jiahui</forenames></author><author><keyname>Lu</keyname><forenames>Songtao</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Zhang</keyname><forenames>Han</forenames></author><author><keyname>Du</keyname><forenames>Chun</forenames></author><author><keyname>Li</keyname><forenames>Shiyin</forenames></author></authors><title>Signal Demodulation with Machine Learning Methods for Physical Layer
  Visible Light Communications: Prototype Platform, Open Dataset and Algorithms</title><categories>eess.SP cs.LG stat.ML</categories><doi>10.1109/ACCESS.2019.2903375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the design and implementation of machine
learning (ML) based demodulation methods in the physical layer of visible light
communication (VLC) systems. We build a flexible hardware prototype of an
end-to-end VLC system, from which the received signals are collected as the
real data. The dataset is available online, which contains eight types of
modulated signals. Then, we propose three ML demodulators based on
convolutional neural network (CNN), deep belief network (DBN), and adaptive
boosting (AdaBoost), respectively. Specifically, the CNN based demodulator
converts the modulated signals to images and recognizes the signals by the
image classification. The proposed DBN based demodulator contains three
restricted Boltzmann machines (RBMs) to extract the modulation features. The
AdaBoost method includes a strong classifier that is constructed by the weak
classifiers with the k-nearest neighbor (KNN) algorithm. These three
demodulators are trained and tested by our online open dataset. Experimental
results show that the demodulation accuracy of the three data-driven
demodulators drops as the transmission distance increases. A higher modulation
order negatively influences the accuracy for a given transmission distance.
Among the three ML methods, the AdaBoost modulator achieves the best
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11386</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11386</id><created>2019-03-04</created><authors><author><keyname>Kostallari</keyname><forenames>Krist</forenames><affiliation>LVA, INRS</affiliation></author><author><keyname>Parizet</keyname><forenames>Etienne</forenames><affiliation>LVA</affiliation></author><author><keyname>Chevret</keyname><forenames>Patrick</forenames><affiliation>INRS</affiliation></author><author><keyname>Amato</keyname><forenames>Jean-No&#xeb;l</forenames><affiliation>INRS</affiliation></author><author><keyname>Galy</keyname><forenames>Edith</forenames><affiliation>PsyCL&#xc9;</affiliation></author></authors><title>Irrelevant speech effect in open plan offices: A laboratory study</title><categories>eess.AS</categories><proxy>ccsd</proxy><journal-ref>24th INTERNATIONAL CONGRESS ON SOUND AND VIBRATION, Jul 2017,
  London, United Kingdom</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It seems now accepted that speech noise in open plan offices is the main
source of discomfort for employees. This work follows a series of studies
conducted at INRS France and INSA Lyon based on Hongisto's theoretical model
(2005) linking the Decrease in Performance (DP) and the Speech Transmission
Index (STI). This model predicts that for STI values between 0.7 and 1, which
means a speech signal close to 100% of intelligibility, the DP remains constant
at about 7%. The experiment that we carried out aimed to gather more
information about the relation between DP and STI, varying the STI value up to
0.9. Fifty-five subjects between 25-59 years old participated in the
experiment. First, some psychological parameters were observed in order to
better characterize the inter-subjects variability. Then, subjects performed a
Working-Memory (WM) task in silence and in four different sound conditions (STI
from 0.25 to 0.9). This task was customized by an initial measure of mnemonic
span so that two different cognitive loads (low/high) were equally defined for
each subject around their span value. Subjects also subjectively evaluated
their mental load and discomfort at the end of each WM task, for each noise
condition. Results show a significant effect of the STI on the DP, the mental
load and the discomfort. Furthermore, a significant correlation was found
between the age of subjects and their performance during the WM task. This
result was confirmed by a cluster analysis that enabled us to separate the
subjects on two different groups, one group of younger and more efficient
subjects and one group of older and less efficient subjects. General results
did not show any increase of DP for the highest STI values, so the &quot;plateau&quot;
hypothesis of Hongisto's model cannot be rejected on the basis of this
experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11387</identifier>
 <datestamp>2019-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11387</id><created>2018-12-29</created><updated>2019-10-17</updated><authors><author><keyname>Ehrenborg</keyname><forenames>Casimir</forenames></author><author><keyname>Gustafsson</keyname><forenames>Mats</forenames></author></authors><title>Physical bounds and radiation modes for MIMO antennas</title><categories>eess.SP physics.class-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern antenna design for communication systems revolves around two extremes:
devices, where only a small region is dedicated to antenna design, and base
stations, where design space is not shared with other components. Both imply
different restrictions on what performance is realizable. In this paper
properties of both ends of the spectrum in terms of MIMO performance is
investigated. For electrically small antennas the size restriction dominates
the performance parameters. The regions dedicated to antenna design induce
currents on the rest of the device. Here a method for studying fundamental
bound on spectral efficiency of such configurations is presented. This bound is
also studied for $N$-degree MIMO systems. For electrically large structures the
number of degrees of freedom available per unit area is investigated for
different shapes. Both of these are achieved by formulating a convex
optimization problem for maximum spectral efficiency in the current density on
the antenna. A computationally efficient solution for this problem is
formulated and investigated in relation to constraining parameters, such as
size and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11394</identifier>
 <datestamp>2019-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11394</id><created>2019-03-25</created><updated>2019-09-20</updated><authors><author><keyname>Purohit</keyname><forenames>Kuldeep</forenames></author><author><keyname>Rajagopalan</keyname><forenames>A. N.</forenames></author></authors><title>Region-Adaptive Dense Network for Efficient Motion Deblurring</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of dynamic scene deblurring in the
presence of motion blur. Restoration of images affected by severe blur
necessitates a network design with a large receptive field, which existing
networks attempt to achieve through simple increment in the number of generic
convolution layers, kernel-size, or the scales at which the image is processed.
However, these techniques ignore the non-uniform nature of blur, and they come
at the expense of an increase in model size and inference time. We present a
new architecture composed of region adaptive dense deformable modules that
implicitly discover the spatially varying shifts responsible for non-uniform
blur in the input image and learn to modulate the filters. This capability is
complemented by a self-attentive module which captures non-local spatial
relationships among the intermediate features and enhances the
spatially-varying processing capability. We incorporate these modules into a
densely connected encoder-decoder design which utilizes pre-trained Densenet
filters to further improve the performance. Our network facilitates
interpretable modeling of the spatially-varying deblurring process while
dispensing with multi-scale processing and large filters entirely. Extensive
comparisons with prior art on benchmark dynamic scene deblurring datasets
clearly demonstrate the superiority of the proposed networks via significant
improvements in accuracy and speed, enabling almost real-time deblurring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11399</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11399</id><created>2019-03-26</created><authors><author><keyname>Virkkunen</keyname><forenames>Iikka</forenames></author><author><keyname>Koskinen</keyname><forenames>Tuomas</forenames></author><author><keyname>Jessen-Juhler</keyname><forenames>Oskari</forenames></author><author><keyname>Rinta-Aho</keyname><forenames>Jari</forenames></author></authors><title>Augmented Ultrasonic Data for Machine Learning</title><categories>eess.SP cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flaw detection in non-destructive testing, especially in complex signals like
ultrasonic data, has thus far relied heavily on the expertise and judgement of
trained human inspectors. While automated systems have been used for a long
time, these have mostly been limited to using simple decision automation, such
as signal amplitude threshold. The recent advances in various machine learning
algorithms have solved many similarly difficult classification problems, that
have previously been considered intractable. For non-destructive testing,
encouraging results have already been reported in the open literature, but the
use of machine learning is still very limited in NDT applications in the field.
Key issue hindering their use, is the limited availability of representative
flawed data-sets to be used for training. In the present paper, we develop
modern, very deep convolutional network to detect flaws from phased-array
ultrasonic data. We make extensive use of data augmentation to enhance the
initially limited raw data and to aid learning. The data augmentation utilizes
virtual flaws - a technique, that has successfully been used in training human
inspectors and is soon to be used in nuclear inspection qualification. The
results from the machine learning classifier are compared to human performance.
We show, that using sophisticated data augmentation, modern deep learning
networks can be trained to achieve superhuman performance by significant
margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11431</identifier>
 <datestamp>2019-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11431</id><created>2019-03-24</created><updated>2019-11-05</updated><authors><author><keyname>Wen</keyname><forenames>Bihan</forenames></author><author><keyname>Ravishankar</keyname><forenames>Saiprasad</forenames></author><author><keyname>Pfister</keyname><forenames>Luke</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Transform Learning for Magnetic Resonance Image Reconstruction: From
  Model-based Learning to Building Neural Networks</title><categories>eess.IV cs.LG stat.ML</categories><comments>Accepted to IEEE Signal Processing Magazine, Special Issue on
  Computational MRI: Compressed Sensing and Beyond</comments><doi>10.1109/MSP.2019.2951469</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonance imaging (MRI) is widely used in clinical practice, but it
has been traditionally limited by its slow data acquisition. Recent advances in
compressed sensing (CS) techniques for MRI reduce acquisition time while
maintaining high image quality. Whereas classical CS assumes the images are
sparse in known analytical dictionaries or transform domains, methods using
learned image models for reconstruction have become popular. The model could be
pre-learned from datasets, or learned simultaneously with the reconstruction,
i.e., blind CS (BCS). Besides the well-known synthesis dictionary model, recent
advances in transform learning (TL) provide an efficient alternative framework
for sparse modeling in MRI. TL-based methods enjoy numerous advantages
including exact sparse coding, transform update, and clustering solutions,
cheap computation, and convergence guarantees, and provide high-quality results
in MRI compared to popular competing methods. This paper provides a review of
some recent works in MRI reconstruction from limited data, with focus on the
recent TL-based methods. A unified framework for incorporating various TL-based
models is presented. We discuss the connections between transform learning and
convolutional or filter bank models and corresponding multi-layer extensions,
with connections to deep learning. Finally, we discuss recent trends in MRI,
open problems, and future directions for the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11432</identifier>
 <datestamp>2019-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11432</id><created>2019-03-27</created><authors><author><keyname>Yu</keyname><forenames>Wen-Kai</forenames></author><author><keyname>Liu</keyname><forenames>Yi-Ming</forenames></author></authors><title>Single-pixel imaging with origami pattern construction</title><categories>eess.IV physics.optics</categories><comments>12 pages, 6 figures</comments><journal-ref>Sensors, 19(23), 5135 (2019)</journal-ref><doi>10.3390/s19235135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-pixel compressive imaging can recover images from a small amount of
measurements, offering many benefits especially for the scenes where the array
detection is unavailable. However, the widely used random patterns fail to
explore internal relations between the patterns and the image reconstruction.
Here we propose a single-pixel imaging method based on origami pattern
construction with a better imaging quality, but with less uncertainty of the
pattern sequence. It can decrease the sampling ratio even to 0.5\%, really
realizing super sub-Nyquist sampling. The experimental realization of this
approach is a big step forward toward the real-time compressive video
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11570</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11570</id><created>2019-03-27</created><authors><author><keyname>Tits</keyname><forenames>No&#xe9;</forenames></author><author><keyname>Wang</keyname><forenames>Fengna</forenames></author><author><keyname>Haddad</keyname><forenames>Kevin El</forenames></author><author><keyname>Pagel</keyname><forenames>Vincent</forenames></author><author><keyname>Dutoit</keyname><forenames>Thierry</forenames></author></authors><title>Visualization and Interpretation of Latent Spaces for Controlling
  Expressive Speech Synthesis through Audio Analysis</title><categories>cs.CL cs.AI cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of Text-to-Speech has experienced huge improvements last years
benefiting from deep learning techniques. Producing realistic speech becomes
possible now. As a consequence, the research on the control of the
expressiveness, allowing to generate speech in different styles or manners, has
attracted increasing attention lately. Systems able to control style have been
developed and show impressive results. However the control parameters often
consist of latent variables and remain complex to interpret. In this paper, we
analyze and compare different latent spaces and obtain an interpretation of
their influence on expressive speech. This will enable the possibility to build
controllable speech synthesis systems with an understandable behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11574</identifier>
 <datestamp>2019-03-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11574</id><created>2019-03-27</created><authors><author><keyname>Farah</keyname><forenames>Joumana</forenames></author><author><keyname>Akiki</keyname><forenames>Jacques</forenames></author><author><keyname>Simon</keyname><forenames>Eric Pierre</forenames></author></authors><title>Energy-efficient techniques for combating the influence of reactive
  jamming using Non-Orthogonal Multiple Access and Distributed Antenna Systems</title><categories>eess.SP</categories><comments>Accepted conference paper</comments><journal-ref>Wireless Telecommunications Symposium (WTS 2019)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to propose new approaches for maximizing the energy
efficiency of downlink 5G mobile communication systems, in the presence of a
reactive jammer. The concepts of non-orthogonal multiple access (NOMA) and
distributed antenna systems (DAS) are exploited to devise joint subband, power
and antenna assignment techniques, so as to guarantee a certain quality of
service (QoS) to users. Also, the scheduler relies on jamming statistics,
observed at the end of each timeslot, to perform resource allocation based on
the prediction of the jammer behavior over the next timeslot. A particular care
is given, in the proposed techniques, to maintain a moderate complexity at the
receiver level, and to limit the number of active RRHs (remote radio heads) in
the cell. Simulation results show that a proper combination of NOMA with DAS
can allow a significant enhancement of the system robustness to jamming, with
respect to centralized antenna systems and orthogonal multiple access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11593</identifier>
 <datestamp>2019-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11593</id><created>2019-03-26</created><updated>2019-11-08</updated><authors><author><keyname>Baek</keyname><forenames>Stephen</forenames></author><author><keyname>He</keyname><forenames>Yusen</forenames></author><author><keyname>Allen</keyname><forenames>Bryan G.</forenames></author><author><keyname>Buatti</keyname><forenames>John M.</forenames></author><author><keyname>Smith</keyname><forenames>Brian J.</forenames></author><author><keyname>Tong</keyname><forenames>Ling</forenames></author><author><keyname>Sun</keyname><forenames>Zhiyu</forenames></author><author><keyname>Wu</keyname><forenames>Jia</forenames></author><author><keyname>Diehn</keyname><forenames>Maximilian</forenames></author><author><keyname>Loo</keyname><forenames>Billy W.</forenames></author><author><keyname>Plichta</keyname><forenames>Kristin A.</forenames></author><author><keyname>Seyedin</keyname><forenames>Steven N.</forenames></author><author><keyname>Gannon</keyname><forenames>Maggie</forenames></author><author><keyname>Cabel</keyname><forenames>Katherine R.</forenames></author><author><keyname>Kim</keyname><forenames>Yusung</forenames></author><author><keyname>Wu</keyname><forenames>Xiaodong</forenames></author></authors><title>Deep segmentation networks predict survival of non-small cell lung
  cancer</title><categories>eess.IV cs.AI cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-small-cell lung cancer (NSCLC) represents approximately 80-85% of lung
cancer diagnoses and is the leading cause of cancer-related death worldwide.
Recent studies indicate that image-based radiomics features from positron
emission tomography-computed tomography (PET/CT) images have predictive power
on NSCLC outcomes. To this end, easily calculated functional features such as
the maximum and the mean of standard uptake value (SUV) and total lesion
glycolysis (TLG) are most commonly used for NSCLC prognostication, but their
prognostic value remains controversial. Meanwhile, convolutional neural
networks (CNN) are rapidly emerging as a new premise for cancer image analysis,
with significantly enhanced predictive power compared to other hand-crafted
radiomics features. Here we show that CNN trained to perform the tumor
segmentation task, with no other information than physician contours, identify
a rich set of survival-related image features with remarkable prognostic value.
In a retrospective study on 96 NSCLC patients before stereotactic-body
radiotherapy (SBRT), we found that the CNN segmentation algorithm (U-Net)
trained for tumor segmentation in PET/CT images, contained features having
strong correlation with 2- and 5-year overall and disease-specific survivals.
The U-net algorithm has not seen any other clinical information (e.g. survival,
age, smoking history) than the images and the corresponding tumor contours
provided by physicians. Furthermore, through visualization of the U-Net, we
also found convincing evidence that the regions of progression appear to match
with the regions where the U-Net features identified patterns that predicted
higher likelihood of death. We anticipate our findings will be a starting point
for more sophisticated non-intrusive patient specific cancer prognosis
determination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11672</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11672</id><created>2019-03-27</created><authors><author><keyname>Jaiswal</keyname><forenames>Mimansa</forenames></author><author><keyname>Aldeneh</keyname><forenames>Zakaria</forenames></author><author><keyname>Bara</keyname><forenames>Cristian-Paul</forenames></author><author><keyname>Luo</keyname><forenames>Yuanhang</forenames></author><author><keyname>Burzo</keyname><forenames>Mihai</forenames></author><author><keyname>Mihalcea</keyname><forenames>Rada</forenames></author><author><keyname>Provost</keyname><forenames>Emily Mower</forenames></author></authors><title>MuSE-ing on the Impact of Utterance Ordering On Crowdsourced Emotion
  Annotations</title><categories>cs.SD cs.HC cs.LG eess.AS</categories><comments>5 pages, ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotion recognition algorithms rely on data annotated with high quality
labels. However, emotion expression and perception are inherently subjective.
There is generally not a single annotation that can be unambiguously declared
&quot;correct&quot;. As a result, annotations are colored by the manner in which they
were collected. In this paper, we conduct crowdsourcing experiments to
investigate this impact on both the annotations themselves and on the
performance of these algorithms. We focus on one critical question: the effect
of context. We present a new emotion dataset, Multimodal Stressed Emotion
(MuSE), and annotate the dataset using two conditions: randomized, in which
annotators are presented with clips in random order, and contextualized, in
which annotators are presented with clips in order. We find that contextual
labeling schemes result in annotations that are more similar to a speaker's own
self-reported labels and that labels generated from randomized schemes are most
easily predictable by automated systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11673</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11673</id><created>2019-03-27</created><authors><author><keyname>Ozdenizci</keyname><forenames>Ozan</forenames></author><author><keyname>Wang</keyname><forenames>Ye</forenames></author><author><keyname>Koike-Akino</keyname><forenames>Toshiaki</forenames></author><author><keyname>Erdogmus</keyname><forenames>Deniz</forenames></author></authors><title>Adversarial Deep Learning in EEG Biometrics</title><categories>cs.LG eess.SP stat.ML</categories><comments>Accepted for publication by IEEE Signal Processing Letters</comments><journal-ref>IEEE Signal Processing Letters, 2019</journal-ref><doi>10.1109/LSP.2019.2906826</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning methods for person identification based on
electroencephalographic (EEG) brain activity encounters the problem of
exploiting the temporally correlated structures or recording session specific
variability within EEG. Furthermore, recent methods have mostly trained and
evaluated based on single session EEG data. We address this problem from an
invariant representation learning perspective. We propose an adversarial
inference approach to extend such deep learning models to learn
session-invariant person-discriminative representations that can provide
robustness in terms of longitudinal usability. Using adversarial learning
within a deep convolutional network, we empirically assess and show
improvements with our approach based on longitudinally collected EEG data for
person identification from half-second EEG epochs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11696</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11696</id><created>2019-03-27</created><authors><author><keyname>Peeters</keyname><forenames>Carel F. W.</forenames></author><author><keyname>&#xdc;belh&#xf6;r</keyname><forenames>Caroline</forenames></author><author><keyname>Mes</keyname><forenames>Steven W.</forenames></author><author><keyname>Martens</keyname><forenames>Roland</forenames></author><author><keyname>Koopman</keyname><forenames>Thomas</forenames></author><author><keyname>de Graaf</keyname><forenames>Pim</forenames></author><author><keyname>van Velden</keyname><forenames>Floris H. P.</forenames></author><author><keyname>Boellaard</keyname><forenames>Ronald</forenames></author><author><keyname>Castelijns</keyname><forenames>Jonas A.</forenames></author><author><keyname>Beest</keyname><forenames>Dennis E. te</forenames></author><author><keyname>Heymans</keyname><forenames>Martijn W.</forenames></author><author><keyname>van de Wiel</keyname><forenames>Mark A.</forenames></author></authors><title>Stable prediction with radiomics data</title><categories>stat.ML cs.LG eess.IV q-bio.QM stat.AP stat.ME</categories><comments>52 pages: 14 pages Main Text and 38 pages of Supplementary Material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Radiomics refers to the high-throughput mining of quantitative
features from radiographic images. It is a promising field in that it may
provide a non-invasive solution for screening and classification. Standard
machine learning classification and feature selection techniques, however, tend
to display inferior performance in terms of (the stability of) predictive
performance. This is due to the heavy multicollinearity present in radiomic
data. We set out to provide an easy-to-use approach that deals with this
problem.
  Results: We developed a four-step approach that projects the original
high-dimensional feature space onto a lower-dimensional latent-feature space,
while retaining most of the covariation in the data. It consists of (i)
penalized maximum likelihood estimation of a redundancy filtered correlation
matrix. The resulting matrix (ii) is the input for a maximum likelihood factor
analysis procedure. This two-stage maximum-likelihood approach can be used to
(iii) produce a compact set of stable features that (iv) can be directly used
in any (regression-based) classifier or predictor. It outperforms other
classification (and feature selection) techniques in both external and internal
validation settings regarding survival in squamous cell cancers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11703</identifier>
 <datestamp>2019-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11703</id><created>2019-03-27</created><updated>2019-10-22</updated><authors><author><keyname>Hoang</keyname><forenames>Minh Tu</forenames></author><author><keyname>Yuen</keyname><forenames>Brosnan</forenames></author><author><keyname>Dong</keyname><forenames>Xiaodai</forenames></author><author><keyname>Lu</keyname><forenames>Tao</forenames></author><author><keyname>Westendorp</keyname><forenames>Robert</forenames></author><author><keyname>Reddy</keyname><forenames>Kishore</forenames></author></authors><title>Recurrent Neural Networks For Accurate RSSI Indoor Localization</title><categories>eess.SP cs.LG stat.ML</categories><comments>Received signal strength indicator (RSSI), WiFi indoor localization,
  recurrent neuron network (RNN), long shortterm memory (LSTM),
  fingerprint-based localization</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes recurrent neuron networks (RNNs) for a fingerprinting
indoor localization using WiFi. Instead of locating user's position one at a
time as in the cases of conventional algorithms, our RNN solution aims at
trajectory positioning and takes into account the relation among the received
signal strength indicator (RSSI) measurements in a trajectory. Furthermore, a
weighted average filter is proposed for both input RSSI data and sequential
output locations to enhance the accuracy among the temporal fluctuations of
RSSI. The results using different types of RNN including vanilla RNN, long
short-term memory (LSTM), gated recurrent unit (GRU) and bidirectional LSTM
(BiLSTM) are presented. On-site experiments demonstrate that the proposed
structure achieves an average localization error of $0.75$ m with $80\%$ of the
errors under $1$ m, which outperforms the conventional KNN algorithms and
probabilistic algorithms by approximately $30\%$ under the same test
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11726</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11726</id><created>2019-03-27</created><authors><author><keyname>Zhang</keyname><forenames>Zhenwei</forenames></author><author><keyname>Sejdic</keyname><forenames>Ervin</forenames></author></authors><title>Radiological images and machine learning: trends, perspectives, and
  prospects</title><categories>eess.IV cs.LG</categories><comments>13 figures</comments><journal-ref>Computers in Biology and Medicine (2019)</journal-ref><doi>10.1016/j.compbiomed.2019.02.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of machine learning to radiological images is an increasingly
active research area that is expected to grow in the next five to ten years.
Recent advances in machine learning have the potential to recognize and
classify complex patterns from different radiological imaging modalities such
as x-rays, computed tomography, magnetic resonance imaging and positron
emission tomography imaging. In many applications, machine learning based
systems have shown comparable performance to human decision-making. The
applications of machine learning are the key ingredients of future clinical
decision making and monitoring systems. This review covers the fundamental
concepts behind various machine learning techniques and their applications in
several radiological imaging areas, such as medical image segmentation, brain
function studies and neurological disease diagnosis, as well as computer-aided
systems, image registration, and content-based image retrieval systems.
Synchronistically, we will briefly discuss current challenges and future
directions regarding the application of machine learning in radiological
imaging. By giving insight on how take advantage of machine learning powered
applications, we expect that clinicians can prevent and diagnose diseases more
accurately and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11791</identifier>
 <datestamp>2019-12-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11791</id><created>2019-03-28</created><updated>2019-11-27</updated><authors><author><keyname>He</keyname><forenames>Ke-Xin</forenames></author><author><keyname>Shen</keyname><forenames>Yu-Han</forenames></author><author><keyname>Zhang</keyname><forenames>Wei-Qiang</forenames></author></authors><title>Hierarchical Pooling Structure for Weakly Labeled Sound Event Detection</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sound event detection with weakly labeled data is considered as a problem of
multi-instance learning. And the choice of pooling function is the key to
solving this problem. In this paper, we proposed a hierarchical pooling
structure to improve the performance of weakly labeled sound event detection
system. Proposed pooling structure has made remarkable improvements on three
types of pooling function without adding any parameters. Moreover, our system
has achieved competitive performance on Task 4 of Detection and Classification
of Acoustic Scenes and Events (DCASE) 2017 Challenge using hierarchical pooling
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11827</identifier>
 <datestamp>2019-07-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11827</id><created>2019-03-28</created><authors><author><keyname>Coluccia</keyname><forenames>Angelo</forenames></author><author><keyname>Fascista</keyname><forenames>Alessio</forenames></author><author><keyname>Ricci</keyname><forenames>Giuseppe</forenames></author></authors><title>A novel approach to robust radar detection of range-spread targets</title><categories>eess.SP</categories><comments>28 pages, 8 figures</comments><doi>10.1016/j.sigpro.2019.07.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach to robust radar detection of
range-spread targets embedded in Gaussian noise with unknown covariance matrix.
The idea is to model the useful target echo in each range cell as the sum of a
coherent signal plus a random component that makes the signal-plus-noise
hypothesis more plausible in presence of mismatches. Moreover, an unknown power
of the random components, to be estimated from the observables, is inserted to
optimize the performance when the mismatch is absent. The generalized
likelihood ratio test (GLRT) for the problem at hand is considered. In
addition, a new parametric detector that encompasses the GLRT as a special case
is also introduced and assessed. The performance assessment shows the
effectiveness of the idea also in comparison to natural competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11849</identifier>
 <datestamp>2019-07-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11849</id><created>2019-03-28</created><authors><author><keyname>Brambilla</keyname><forenames>Mattia</forenames></author><author><keyname>Nicoli</keyname><forenames>Monica</forenames></author><author><keyname>Savaresi</keyname><forenames>Sergio</forenames></author><author><keyname>Spagnolini</keyname><forenames>Umberto</forenames></author></authors><title>Inertial Sensor Aided mmWave Beam Tracking to Support Cooperative
  Autonomous Driving</title><categories>eess.SP cs.IT math.IT</categories><comments>Accepted at IEEE ICC 2019</comments><doi>10.1109/ICCW.2019.8756931</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an inertial sensor aided technique for beam alignment and
tracking in massive multiple-input multiple-output (MIMO) vehicle-to-vehicle
(V2V) communications based on millimeter waves (mmWave). Since directional
communications in vehicular scenarios are severely hindered by beam pointing
issues, a beam alignment procedure has to be periodically carried out to
guarantee the communication reliability. When dealing with massive MIMO links,
the beam sweeping approach is known to be time consuming and often unfeasible
due to latency constraints. To speed up the process, we propose a method that
exploits a-priori information on array dynamics provided by an inertial sensor
on transceivers to assist the beam alignment procedure. The proposed inertial
sensor aided technique allows a continuous tracking of the beam while
transmitting, avoiding frequent realignment phases. Numerical results based on
real measurements of on-transceiver accelerometers demonstrate a significant
gain in terms of V2V communication throughput with respect to conventional beam
alignment protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.11996</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.11996</id><created>2019-03-28</created><updated>2019-04-02</updated><authors><author><keyname>Saeed</keyname><forenames>Nasir</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>Towards the Internet of X-things: New Possibilities for Underwater,
  Underground, and Outer Space Exploration</title><categories>eess.SP cs.NI</categories><comments>Accepted in IEEE ComSoc News (CTN)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid growth of the world's population demands more natural resources,
food, and space. World-renowned physicist Stephan Hawking has argued that soon
we will require another world to live on because we are running out of space
and natural resources. This ever-increasing demand for resources and space
needs novel technologies to explore natural resources, produce more crops, and
explore outer space. Internet of X-things (X-IoT) is an enabling technology to
overcome all of the above issues. In this article, we have presented an
overview of a unified framework of X-IoT. The framework of X-IoT consists of
three major categories. The first one is the Internet of underwater things
(IoUT) for smart oceans. The second category is the Internet of underground
things (IoUGT) for smart agriculture, seismic monitoring, and Oil/Gas fields.
The third category is the Internet of space things (IoST) for outer space
exploration, to provide global coverage, and to enable inter-satellite
communications. Through this framework, we get to know what has been done since
recently and how the technical challenges across the broad spectrum of emerging
use cases under the water, underground and over the space are converging toward
future solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12029</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12029</id><created>2019-03-28</created><authors><author><keyname>Khaki</keyname><forenames>Bahman</forenames></author><author><keyname>Das</keyname><forenames>Pritam</forenames></author></authors><title>Sizing and Placement of Battery Energy Storage Systems and Wind Turbines
  by Minimizing Costs and System Losses</title><categories>eess.SP</categories><comments>10 page , 8 figure, 7 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic and intermittent output power of wind turbines (WT) is one
major inconsistency of WTs. Battery Energy Storage Systems (BESSs) are a
suitable solution to mitigate this intermittency which use to smoothen the
output power injected to the grid by such intermittent sources. This paper
proposes a new optimization formulation using genetic algorithm to simultaneous
sizing and placement of BESSs and WTs which result in finding best location and
size (capacity) of WTs and BESSs in power system by minimizing total system
loss (active and reactive loss) and Costs of WTs and BESSs which improves
demand bus voltage profiles. The result of optimization problem is best buses
to locate WTs and BESSs and the size (installable active and reactive power) of
them. The case studies performed on IEEE 33 bus system, validates the
suitability of the formulation for loss minimization and bus voltage profiles
improvement in the test system in presence of WT and BESS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12053</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12053</id><created>2019-03-27</created><authors><author><keyname>Ota</keyname><forenames>Sadao</forenames></author><author><keyname>Horisaki</keyname><forenames>Ryoichi</forenames></author><author><keyname>Kawamura</keyname><forenames>Yoko</forenames></author><author><keyname>Sato</keyname><forenames>Issei</forenames></author><author><keyname>Noji</keyname><forenames>Hiroyuki</forenames></author></authors><title>Imaging cytometry without image reconstruction (ghost cytometry)</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imaging and analysis of many single cells hold great potential in our
understanding of heterogeneous and complex life systems and in enabling
biomedical applications. We here introduce a recently realized image-free
&quot;imaging&quot; cytometry technology, which we call ghost cytometry. While a
compressive ghost imaging technique utilizing object's motion relative to a
projected static light pattern allows recovery of their images, a key of this
ghost cytometry is to achieve ultrafast cell classification by directly
applying machine learning methods to the compressive imaging signals in a
temporal domain. We show the applicability of our method in the analysis of
flowing objects based on the reconstructed images as well as in that based on
the imaging waveform without image production.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12058</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12058</id><created>2019-03-28</created><updated>2019-04-04</updated><authors><author><keyname>You</keyname><forenames>Lanhua</forenames></author><author><keyname>Guo</keyname><forenames>Wu</forenames></author><author><keyname>Dai</keyname><forenames>Lirong</forenames></author><author><keyname>Du</keyname><forenames>Jun</forenames></author></authors><title>Multi-Task Learning with High-Order Statistics for X-vector based
  Text-Independent Speaker Verification</title><categories>eess.AS cs.SD</categories><comments>5 pages,2 figures, submitted to INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The x-vector based deep neural network (DNN) embedding systems have
demonstrated effectiveness for text-independent speaker verification. This
paper presents a multi-task learning architecture for training the speaker
embedding DNN with the primary task of classifying the target speakers, and the
auxiliary task of reconstructing the first- and higher-order statistics of the
original input utterance. The proposed training strategy aggregates both the
supervised and unsupervised learning into one framework to make the speaker
embeddings more discriminative and robust. Experiments are carried out using
the NIST SRE16 evaluation dataset and the VOiCES dataset. The results
demonstrate that our proposed method outperforms the original x-vector approach
with very low additional complexity added.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12087</identifier>
 <datestamp>2019-07-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12087</id><created>2019-03-28</created><updated>2019-06-27</updated><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Skoglund</keyname><forenames>Jan</forenames></author></authors><title>A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet</title><categories>eess.AS cs.LG cs.SD</categories><comments>Accepted for Interspeech 2019, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural speech synthesis algorithms are a promising new approach for coding
speech at very low bitrate. They have so far demonstrated quality that far
exceeds traditional vocoders, at the cost of very high complexity. In this
work, we present a low-bitrate neural vocoder based on the LPCNet model. The
use of linear prediction and sparse recurrent networks makes it possible to
achieve real-time operation on general-purpose hardware. We demonstrate that
LPCNet operating at 1.6 kb/s achieves significantly higher quality than MELP
and that uncompressed LPCNet can exceed the quality of a waveform codec
operating at low bitrate. This opens the way for new codec designs based on
neural synthesis models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12089</identifier>
 <datestamp>2019-07-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12089</id><created>2019-03-28</created><updated>2019-07-24</updated><authors><author><keyname>Drumetz</keyname><forenames>Lucas</forenames></author><author><keyname>Chanussot</keyname><forenames>Jocelyn</forenames></author><author><keyname>Jutten</keyname><forenames>Christian</forenames></author></authors><title>Spectral Unmixing: A Derivation of the Extended Linear Mixing Model from
  the Hapke Model</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In hyperspectral imaging, spectral unmixing aims at decomposing the image
into a set of reference spectral signatures corresponding to the materials
present in the observed scene and their relative proportions in every pixel.
While a linear mixing model was used for a long time, the complex nature of the
physical mixing processes, led to shift the community's attention towards
nonlinear models and algorithms accounting for the variability of the
endmembers. Such intra class variations are due to local changes in the
physico-chemical composition of the materials, and to illumination changes. In
the physical remote sensing community, a popular model accounting for
illumination variability is the radiative transfer model proposed by Hapke. It
is however too complex to be directly used in hyperspectral unmixing in a
tractable way. Instead, the Extended Linear Mixing Model (ELMM) allows to
easily unmix hyperspectral data accounting for changing illumination
conditions. In this letter, we show that the ELMM can be obtained from the
Hapke model by successive simplifiying physical assumptions, thus theoretically
confirming its relevance to handle illumination induced variability in the
unmixing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12092</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12092</id><created>2019-03-28</created><updated>2019-04-04</updated><authors><author><keyname>You</keyname><forenames>Lanhua</forenames></author><author><keyname>Guo</keyname><forenames>Wu</forenames></author><author><keyname>Dai</keyname><forenames>Lirong</forenames></author><author><keyname>Du</keyname><forenames>Jun</forenames></author></authors><title>Deep Neural Network Embeddings with Gating Mechanisms for
  Text-Independent Speaker Verification</title><categories>eess.AS cs.LG cs.SD</categories><comments>5 pages, 3 figures, submitted to INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, gating mechanisms are applied in deep neural network (DNN)
training for x-vector-based text-independent speaker verification. First, a
gated convolution neural network (GCNN) is employed for modeling the
frame-level embedding layers. Compared with the time-delay DNN (TDNN), the GCNN
can obtain more expressive frame-level representations through carefully
designed memory cell and gating mechanisms. Moreover, we propose a novel
gated-attention statistics pooling strategy in which the attention scores are
shared with the output gate. The gated-attention statistics pooling combines
both gating and attention mechanisms into one framework; therefore, we can
capture more useful information in the temporal pooling layer. Experiments are
carried out using the NIST SRE16 and SRE18 evaluation datasets. The results
demonstrate the effectiveness of the GCNN and show that the proposed
gated-attention statistics pooling can further improve the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12094</identifier>
 <datestamp>2019-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12094</id><created>2019-03-28</created><updated>2019-11-03</updated><authors><author><keyname>Gideon</keyname><forenames>John</forenames></author><author><keyname>McInnis</keyname><forenames>Melvin G</forenames></author><author><keyname>Provost</keyname><forenames>Emily Mower</forenames></author></authors><title>Improving Cross-Corpus Speech Emotion Recognition with Adversarial
  Discriminative Domain Generalization (ADDoG)</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><doi>10.1109/TAFFC.2019.2916092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic speech emotion recognition provides computers with critical context
to enable user understanding. While methods trained and tested within the same
dataset have been shown successful, they often fail when applied to unseen
datasets. To address this, recent work has focused on adversarial methods to
find more generalized representations of emotional speech. However, many of
these methods have issues converging, and only involve datasets collected in
laboratory conditions. In this paper, we introduce Adversarial Discriminative
Domain Generalization (ADDoG), which follows an easier to train &quot;meet in the
middle&quot; approach. The model iteratively moves representations learned for each
dataset closer to one another, improving cross-dataset generalization. We also
introduce Multiclass ADDoG, or MADDoG, which is able to extend the proposed
method to more than two datasets, simultaneously. Our results show consistent
convergence for the introduced methods, with significantly improved results
when not using labels from the target dataset. We also show how, in most cases,
ADDoG and MADDoG can be used to improve upon baseline state-of-the-art methods
when target dataset labels are added and in-the-wild data are considered. Even
though our experiments focus on cross-corpus speech emotion, these methods
could be used to remove unwanted factors of variation in other settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12149</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12149</id><created>2019-03-28</created><authors><author><keyname>Gadgil</keyname><forenames>Shubhada</forenames></author><author><keyname>Ranjan</keyname><forenames>Shashi</forenames></author><author><keyname>Karandikar</keyname><forenames>Abhay</forenames></author></authors><title>Performance and Energy Conservation of 3GPP IFOM Protocol for Dual
  Connectivity in Heterogeneous LTE-WLAN Network</title><categories>eess.SP</categories><comments>12 pages, 15 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the 5th Generation (5G) networks, Third Generation Partnership Project
(3GPP) is considering standardization of various solutions for traffic
aggregation using licensed and unlicensed spectrum, to meet the rising data
demands. IP Flow Mobility (IFOM) is a multi access connectivity
solution/protocol standardized by the Internet Engineering Task force (IETF)
and 3GPP in Release 10. It enables concurrent access for an User Equipment (UE)
to Heterogeneous Networks (HetNets) such as Long Term Evolution (LTE) and IEEE
802.11 Wireless Local Area Network (WLAN). IFOM enabled UEs have multiple
interfaces to connect to HetNets. They can have concurrent flows with different
traffic types over these networks and can seamlessly switch the flows from one
network to the other. In this paper, we focus on two objectives. First is to
investigate the performance parameters e.g. throughput, latency, tunnelling
overhead, packet loss, energy cost etc. of IFOM enabled UEs (IeUs) in HetNets
of LTE and WLAN. We have proposed a novel mechanism to maximize the throughput
of IeUs achieving a significant throughput gain with low latency for the IeUs.
We have explored further and observed a throughput energy trade off for low
data rate flows. To address this, we also propose a smart energy efficient and
throughput optimization algorithm for the IeUs, resulting in a substantial
reduction in energy cost, while maintaining the high throughput at lower
latency and satisfying the Quality of Service (QoS) requirements of the IeUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12202</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12202</id><created>2019-03-28</created><authors><author><keyname>Carpio</keyname><forenames>A.</forenames></author><author><keyname>Dimiduk</keyname><forenames>T. G.</forenames></author><author><keyname>Louer</keyname><forenames>F. Le</forenames></author><author><keyname>Rapun</keyname><forenames>M. L.</forenames></author></authors><title>When topological derivatives met regularized Gauss-Newton iterations in
  holographic 3D imaging</title><categories>physics.optics eess.IV</categories><journal-ref>Journal of Computational Physics 388, 224-251, 2019</journal-ref><doi>10.1016/j.jcp.2019.03.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an automatic algorithm for 3D inverse electromagnetic scattering
based on the combination of topological derivatives and regularized
Gauss-Newton iterations. The algorithm is adapted to decoding digital
holograms. A hologram is a two-dimensional light interference pattern that
encodes information about three-dimensional shapes and their optical
properties. The formation of the hologram is modeled using Maxwell theory for
light scattering by particles. We then seek shapes optimizing error functionals
which measure the deviation from the recorded holograms. Their topological
derivatives provide initial guesses of the objects. Next, we correct these
predictions by regularized Gauss-Newton techniques. In contrast to standard
Gauss-Newton methods, in our implementation the number of objects can be
automatically updated during the iterative procedure by new topological
derivative computations. We show that the combined use of topological
derivative based optimization and iteratively regularized Gauss-Newton methods
produces fast and accurate descriptions of the geometry of objects formed by
multiple components with nanoscale resolution, even for a small number of
detectors and non convex components aligned in the incidence direction. The
method could be applied in general imaging set-ups involving other waves
(microwave imaging, elastography...) provided closed-form expressions for the
topological and Frechet derivatives are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12209</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12209</id><created>2019-03-28</created><authors><author><keyname>Dardikman</keyname><forenames>Gili</forenames></author><author><keyname>Shaked</keyname><forenames>Natan T.</forenames></author></authors><title>Is multiplexed off-axis holography for quantitative phase imaging more
  spatial bandwidth-efficient than on-axis holography?</title><categories>physics.optics eess.IV</categories><journal-ref>J. Opt. Soc. Am. A 36, A1-A11 (2019)</journal-ref><doi>10.1364/JOSAA.36.0000A1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital holographic microcopy is a thriving imaging modality that attracts
considerable research interest due to its ability to not only create excellent
label-free contrast, but also supply valuable physical information regarding
the density and dimensions of the sample with nanometer-scale axial
sensitivity. Three basic holographic recording geometries currently exist,
including on-axis, off-axis and slightly off-axis holography, each of them
enabling a variety of architectures in terms of bandwidth use and compression
capacity. Specifically, off-axis holography and slightly off-axis holography
allow spatial hologram multiplexing, enabling compressing more information into
the same digital hologram. In this paper, we define an efficiency score used to
analyze the various possible architectures, and compare the signal-to-noise
ratio and mean squared error obtained using each of them, determining the
optimal holographic method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12210</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12210</id><created>2019-03-26</created><authors><author><keyname>Ly</keyname><forenames>Tiffany T.</forenames></author><author><keyname>Batabyal</keyname><forenames>Tamal</forenames></author><author><keyname>Thompson</keyname><forenames>Jeremy</forenames></author><author><keyname>Harris</keyname><forenames>Tajie</forenames></author><author><keyname>Weller</keyname><forenames>Daniel S.</forenames></author><author><keyname>Acton</keyname><forenames>Scott T.</forenames></author></authors><title>Hieroglyph: Hierarchical Glia Graph Skeletonization and Matching</title><categories>eess.IV</categories><comments>submitted to IEEE International Conference on Image Processing, 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Automatic 3D reconstruction of glia morphology is a powerful tool necessary
for investigating the role of microglia in neurological disorders in the
central nervous system. Current glia skeleton reconstruction techniques fail to
capture an accurate tracing of the processes over time, useful for the study of
the microglia motility and morphology in the brain during healthy and diseased
states. We propose Hieroglyph, a fully automatic temporal 3D skeleton
reconstruction algorithm for glia imaged via 3D multiphoton microscopy.
Hieroglyph yielded a 21% performance increase compared to state of the art
automatic skeleton reconstruction methods and outperforms the state of the art
in different measures of consistency on datasets of 3D images of microglia. The
results from this method provide a 3D graph and digital reconstruction of glia
useful for a myriad of morphological analyses that could impact studies in
brain immunology and disease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12235</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12235</id><created>2019-03-28</created><updated>2019-04-05</updated><authors><author><keyname>Ozdenizci</keyname><forenames>Ozan</forenames></author><author><keyname>Erdogmus</keyname><forenames>Deniz</forenames></author></authors><title>Information Theoretic Feature Transformation Learning for Brain
  Interfaces</title><categories>cs.LG cs.HC cs.IT eess.SP math.IT stat.ML</categories><comments>Accepted for publication by IEEE Transactions on Biomedical
  Engineering</comments><journal-ref>IEEE Transactions on Biomedical Engineering, 2019</journal-ref><doi>10.1109/TBME.2019.2908099</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: A variety of pattern analysis techniques for model training in
brain interfaces exploit neural feature dimensionality reduction based on
feature ranking and selection heuristics. In the light of broad evidence
demonstrating the potential sub-optimality of ranking based feature selection
by any criterion, we propose to extend this focus with an information theoretic
learning driven feature transformation concept. Methods: We present a maximum
mutual information linear transformation (MMI-LinT), and a nonlinear
transformation (MMI-NonLinT) framework derived by a general definition of the
feature transformation learning problem. Empirical assessments are performed
based on electroencephalographic (EEG) data recorded during a four class motor
imagery brain-computer interface (BCI) task. Exploiting state-of-the-art
methods for initial feature vector construction, we compare the proposed
approaches with conventional feature selection based dimensionality reduction
techniques which are widely used in brain interfaces. Furthermore, for the
multi-class problem, we present and exploit a hierarchical graphical model
based BCI decoding system. Results: Both binary and multi-class decoding
analyses demonstrate significantly better performances with the proposed
methods. Conclusion: Information theoretic feature transformations are capable
of tackling potential confounders of conventional approaches in various
settings. Significance: We argue that this concept provides significant
insights to extend the focus on feature selection heuristics to a broader
definition of feature transformation learning in brain interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12248</identifier>
 <datestamp>2019-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12248</id><created>2019-03-28</created><updated>2019-09-07</updated><authors><author><keyname>P.</keyname><forenames>Prathosh A.</forenames></author><author><keyname>Srivastava</keyname><forenames>Varun</forenames></author><author><keyname>Mishra</keyname><forenames>Mayank</forenames></author></authors><title>Adversarial Approximate Inference for Speech to Electroglottograph
  Conversion</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>Submitted to IEEE/ACM Transactions on Audio, Speech and Language
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech produced by human vocal apparatus conveys substantial non-semantic
information including the gender of the speaker, voice quality, affective
state, abnormalities in the vocal apparatus etc. Such information is attributed
to the properties of the voice source signal, which is usually estimated from
the speech signal. However, most of the source estimation techniques depend
heavily on the goodness of the model assumptions and are prone to noise. A
popular alternative is to indirectly obtain the source information through the
Electroglottographic (EGG) signal that measures the electrical admittance
around the vocal folds using dedicated hardware. In this paper, we address the
problem of estimating the EGG signal directly from the speech signal, devoid of
any hardware. Sampling from the intractable conditional distribution of the EGG
signal given the speech signal is accomplished through optimization of an
evidence lower bound. This is constructed via minimization of the KL-divergence
between the true and the approximated posteriors of a latent variable learned
using a deep neural auto-encoder that serves an informative prior. We
demonstrate the efficacy of the method at generating the EGG signal by
conducting several experiments on datasets comprising multiple speakers, voice
qualities, noise settings and speech pathologies. The proposed method is
evaluated on many benchmark metrics and is found to agree with the gold
standard while proving better than the state-of-the-art algorithms on a few
tasks such as epoch extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12259</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12259</id><created>2019-03-10</created><authors><author><keyname>Mousaei</keyname><forenames>Mohammadreza</forenames></author></authors><title>Efficient Use of Spectral Resources in Wireless Communication Using
  Training Data Optimization</title><categories>eess.SP cs.IT math.IT</categories><comments>Masters Thesis</comments><journal-ref>University of Illinois at Chicago Dissertations and Theses
  library, 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication applications has acquired a vastly increasing range
over the past decade. This rapidly increasing demand implies limitations on
utilizing wireless resources. One of the most important resources in wireless
communication is frequency spectrum. This thesis provides different solutions
towards increasing the spectral efficiency. The first solution provided in this
thesis is to use a more accurate optimization metric: maximal acheivable rate
(compared to traditional metric: ergodic capacity) to optimize training data
size in wireless communication. Training data symbols are previously known
symbols to the receiver inserted in data packets which are used by receiver to
acquire channel state information (CSI). Optimizing training data size with
respect to our proposed tight optimization metric, we could achieve higher
rates especially for short packet and ultra reliable applications. Our second
proposed solution to increase spectral efficiency is to design a multifunction
communication and sensing platform utilizing a special training sequence
design. We proposed a platform where two training sequences are designed, one
for the base-station and the other for the user. By designing these two
training sequence such that they are uncorrelated to each other, the base
station will be able to distinguish between the two training sequence. Having
one of the sequences especially designed for radar purposes (by designing it
such that it has an impulse-like autocorrelation), the system will be able to
sense the environment, transmit and receive the communication data
simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12260</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12260</id><created>2019-03-27</created><updated>2019-04-26</updated><authors><author><keyname>Pilori</keyname><forenames>Dario</forenames></author></authors><title>Advanced Digital Signal Processing Techniques for High-Speed Optical
  Communications Links</title><categories>eess.SP cs.IT math.IT</categories><comments>Thesis discussed on 22 March 2019 for the conferral of the PhD degree
  in Electrical, Electronics and Communications Engineering at the Doctoral
  School of Politecnico di Torino, Italy</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The main topic of this thesis is the application of advanced Digital Signal
Processing (DSP) techniques to high data-rate optical links. This thesis is
divided in two parts: Direct-Detection systems, and Coherent systems. In the
first part, it is proposed a novel bi-directional architecture for &lt;2km
Intra-DC link and a detailed analysis of self-coherent single-sideband
transmission for &lt;80km dispersion-uncompensated Inter-DC link. The second part
instead focuses on Probabilistic Constellation Shaping techniques, and their
impact on long-haul optical communication links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12289</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12289</id><created>2019-03-28</created><authors><author><keyname>Romanov</keyname><forenames>Elad</forenames></author><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author></authors><title>Above the Nyquist Rate, Modulo Folding Does Not Hurt</title><categories>cs.IT eess.SP math.IT</categories><doi>10.1109/LSP.2019.2923835</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a continuous-time bandlimited signal
from the discrete-time signal obtained from sampling it every $T_s$ seconds and
reducing the result modulo $\Delta$, for some $\Delta&gt;0$. For $\Delta=\infty$
the celebrated Shannon-Nyquist sampling theorem guarantees that perfect
recovery is possible provided that the sampling rate $1/T_s$ exceeds the
so-called Nyquist rate. Recent work by Bhandari et al. has shown that for any
$\Delta&gt;0$ perfect reconstruction is still possible if the sampling rate
exceeds the Nyquist rate by a factor of $\pi e$. In this letter we improve upon
this result and show that for finite energy signals, perfect recovery is
possible for any $\Delta&gt;0$ and any sampling rate above the Nyquist rate. Thus,
modulo folding does not degrade the signal, provided that the sampling rate
exceeds the Nyquist rate. This claim is proved by establishing a connection
between the recovery problem of a discrete-time signal from its modulo reduced
version and the problem of predicting the next sample of a discrete-time signal
from its past, and leveraging the fact that for a bandlimited signal the
prediction error can be made arbitrarily small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12316</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12316</id><created>2019-03-28</created><updated>2019-04-09</updated><authors><author><keyname>Zhao</keyname><forenames>Yi</forenames></author><author><keyname>Ando</keyname><forenames>Atsushi</forenames></author><author><keyname>Takaki</keyname><forenames>Shinji</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Kobashikawa</keyname><forenames>Satoshi</forenames></author></authors><title>Does the Lombard Effect Improve Emotional Communication in Noise? -
  Analysis of Emotional Speech Acted in Noise -</title><categories>eess.AS cs.SD</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speakers usually adjust their way of talking in noisy environments
involuntarily for effective communication. This adaptation is known as the
Lombard effect. Although speech accompanying the Lombard effect can improve the
intelligibility of a speaker's voice, the changes in acoustic features (e.g.
fundamental frequency, speech intensity, and spectral tilt) caused by the
Lombard effect may also affect the listener's judgment of emotional content. To
the best of our knowledge, there is no published study on the influence of the
Lombard effect in emotional speech. Therefore, we recorded parallel emotional
speech waveforms uttered by 12 speakers under both quiet and noisy conditions
in a professional recording studio in order to explore how the Lombard effect
interacts with emotional speech. By analyzing confusion matrices and acoustic
features, we aim to answer the following questions: 1) Can speakers express
their emotions correctly even under adverse conditions? 2) Can listeners
recognize the emotion contained in speech signals even under noise? 3) How does
emotional speech uttered in noise differ from emotional speech uttered in quiet
conditions in terms of acoustic characteristic?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12319</identifier>
 <datestamp>2019-07-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12319</id><created>2019-03-28</created><updated>2019-07-17</updated><authors><author><keyname>Niu</keyname><forenames>Haiqiang</forenames></author><author><keyname>Gong</keyname><forenames>Zaixiao</forenames></author><author><keyname>Ozanich</keyname><forenames>Emma</forenames></author><author><keyname>Gerstoft</keyname><forenames>Peter</forenames></author><author><keyname>Wang</keyname><forenames>Haibin</forenames></author><author><keyname>Li</keyname><forenames>Zhenglin</forenames></author></authors><title>Deep-learning source localization using multi-frequency magnitude-only
  data</title><categories>physics.ao-ph eess.SP</categories><comments>It has been published on the Journal of the Acoustical Society of
  America</comments><journal-ref>J. Acoust. Soc. Am. 146(1), 211-222 (2019)</journal-ref><doi>10.1121/1.5116016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deep learning approach based on big data is proposed to locate broadband
acoustic sources using a single hydrophone in ocean waveguides with uncertain
bottom parameters. Several 50-layer residual neural networks, trained on a huge
number of sound field replicas generated by an acoustic propagation model, are
used to handle the bottom uncertainty in source localization. A two-step
training strategy is presented to improve the training of the deep models.
First, the range is discretized in a coarse (5 km) grid. Subsequently, the
source range within the selected interval and source depth are discretized on a
finer (0.1 km and 2 m) grid. The deep learning methods were demonstrated for
simulated magnitude-only multi-frequency data in uncertain environments.
Experimental data from the China Yellow Sea also validated the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12331</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12331</id><created>2019-03-28</created><updated>2019-04-04</updated><authors><author><keyname>Zong</keyname><forenames>Weiwei</forenames></author><author><keyname>Lee</keyname><forenames>Joon</forenames></author><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Carver</keyname><forenames>Eric</forenames></author><author><keyname>Feldman</keyname><forenames>Aharon</forenames></author><author><keyname>Janic</keyname><forenames>Branislava</forenames></author><author><keyname>Elshaikh</keyname><forenames>Mohamed</forenames></author><author><keyname>Pantelic</keyname><forenames>Milan</forenames></author><author><keyname>Hearshen</keyname><forenames>David</forenames></author><author><keyname>Chetty</keyname><forenames>Indrin</forenames></author><author><keyname>Movsas</keyname><forenames>Benjamin</forenames></author><author><keyname>Wen</keyname><forenames>Ning</forenames></author></authors><title>A Deep Dive into Understanding Tumor Foci Classification using
  Multiparametric MRI Based on Convolutional Neural Network</title><categories>cs.CV eess.IV q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data scarcity has refrained deep learning models from making greater progress
in prostate images analysis using multiparametric MRI. In this paper, an
efficient convolutional neural network (CNN) was developed to classify lesion
malignancy for prostate cancer patients, based on which model interpretation
was systematically analyzed to bridge the gap between natural images and MR
images, which is the first one of its kind in the literature. The problem of
small sample size was addressed and successfully tackled by feeding the
intermediate features into a traditional classification algorithm known as
weighted extreme learning machine, with imbalanced distribution among output
categories taken into consideration. Model trained on public data set was able
to generalize to data from an independent institution to make accurate
prediction. The generated saliency map was found to overlay well with the
lesion and could benefit clinicians for diagnosing purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12384</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12384</id><created>2019-03-29</created><authors><author><keyname>Heinecke</keyname><forenames>Andreas</forenames></author><author><keyname>Hwang</keyname><forenames>Wen-Liang</forenames></author></authors><title>Deep Representation with ReLU Neural Networks</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider deep feedforward neural networks with rectified linear units from
a signal processing perspective. In this view, such representations mark the
transition from using a single (data-driven) linear representation to utilizing
a large collection of affine linear representations tailored to particular
regions of the signal space. This paper provides a precise description of the
individual affine linear representations and corresponding domain regions that
the (data-driven) neural network associates to each signal of the input space.
In particular, we describe atomic decompositions of the representations and,
based on estimating their Lipschitz regularity, suggest some conditions that
can stabilize learning independent of the network depth. Such an analysis may
promote further theoretical insight from both the signal processing and machine
learning communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12389</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12389</id><created>2019-03-29</created><updated>2019-04-07</updated><authors><author><keyname>Zhang</keyname><forenames>Mingyang</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Fang</keyname><forenames>Fuming</forenames></author><author><keyname>Li</keyname><forenames>Haizhou</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author></authors><title>Joint training framework for text-to-speech and voice conversion using
  multi-source Tacotron and WaveNet</title><categories>eess.AS cs.CL cs.SD stat.ML</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigated the training of a shared model for both text-to-speech (TTS)
and voice conversion (VC) tasks. We propose using an extended model
architecture of Tacotron, that is a multi-source sequence-to-sequence model
with a dual attention mechanism as the shared model for both the TTS and VC
tasks. This model can accomplish these two different tasks respectively
according to the type of input. An end-to-end speech synthesis task is
conducted when the model is given text as the input while a
sequence-to-sequence voice conversion task is conducted when it is given the
speech of a source speaker as the input. Waveform signals are generated by
using WaveNet, which is conditioned by using a predicted mel-spectrogram. We
propose jointly training a shared model as a decoder for a target speaker that
supports multiple sources. Listening experiments show that our proposed
multi-source encoder-decoder model can efficiently achieve both the TTS and VC
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12392</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12392</id><created>2019-03-29</created><updated>2019-04-07</updated><authors><author><keyname>Takaki</keyname><forenames>Shinji</forenames></author><author><keyname>Kameoka</keyname><forenames>Hirokazu</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author></authors><title>Training a Neural Speech Waveform Model using Spectral Losses of
  Short-Time Fourier Transform and Continuous Wavelet Transform</title><categories>eess.AS cs.CL cs.SD stat.ML</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we proposed short-time Fourier transform (STFT)-based loss
functions for training a neural speech waveform model. In this paper, we
generalize the above framework and propose a training scheme for such models
based on spectral amplitude and phase losses obtained by either STFT or
continuous wavelet transform (CWT), or both of them. Since CWT is capable of
having time and frequency resolutions different from those of STFT and is cable
of considering those closer to human auditory scales, the proposed loss
functions could provide complementary information on speech signals.
Experimental results showed that it is possible to train a high-quality model
by using the proposed CWT spectral loss and is as good as one using STFT-based
loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12422</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12422</id><created>2019-03-29</created><authors><author><keyname>Zhang</keyname><forenames>Zixing</forenames></author><author><keyname>Han</keyname><forenames>Jing</forenames></author><author><keyname>Qian</keyname><forenames>Kun</forenames></author><author><keyname>Janott</keyname><forenames>Christoph</forenames></author><author><keyname>Guo</keyname><forenames>Yanan</forenames></author><author><keyname>Schuller</keyname><forenames>Bjoern</forenames></author></authors><title>Snore-GANs: Improving Automatic Snore Sound Classification with
  Synthesized Data</title><categories>cs.LG cs.SD eess.AS</categories><comments>accepted by IEEE JBHI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the frontier issues that severely hamper the development of automatic
snore sound classification (ASSC) associates to the lack of sufficient
supervised training data. To cope with this problem, we propose a novel data
augmentation approach based on semi-supervised conditional Generative
Adversarial Networks (scGANs), which aims to automatically learn a mapping
strategy from a random noise space to original data distribution. The proposed
approach has the capability of well synthesizing 'realistic' high-dimensional
data, while requiring no additional annotation process. To handle the mode
collapse problem of GANs, we further introduce an ensemble strategy to enhance
the diversity of the generated data. The systematic experiments conducted on a
widely used Munich-Passau snore sound corpus demonstrate that the scGANs-based
systems can remarkably outperform other classic data augmentation systems, and
are also competitive to other recently reported systems for ASSC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12428</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12428</id><created>2019-03-29</created><authors><author><keyname>You</keyname><forenames>Lanhua</forenames></author><author><keyname>Gu</keyname><forenames>Bin</forenames></author><author><keyname>Guo</keyname><forenames>Wu</forenames></author></authors><title>USTCSpeech System for VOiCES from a Distance Challenge 2019</title><categories>eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document describes the speaker verification systems developed in the
Speech lab at the University of Science and Technology of China (USTC) for the
VOiCES from a Distance Challenge 2019. We develop the system for the Fixed
Condition on two public corpus, VoxCeleb and SITW. The frameworks of our
systems are based on the mainstream ivector/PLDA and x-vector/PLDA algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12441</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12441</id><created>2019-03-29</created><authors><author><keyname>Souto</keyname><forenames>Nuno</forenames></author><author><keyname>Silva</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Pavia</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Ribeiro</keyname><forenames>Marco</forenames></author></authors><title>An Alternating Direction Algorithm for Hybrid Precoding and Combining in
  Millimeter Wave MIMO Systems</title><categories>eess.SP</categories><comments>preprint submitted to Elsevier Physical Communication</comments><doi>10.1016/j.phycom.2019.03.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter-wave (mmWave) technology is one of the most promising candidates
for future wireless communication systems as it can offer large underutilized
bandwidths and eases the implementation of large antenna arrays which are
required to help overcome the severe signal attenuation that occurs at these
frequencies. To reduce the high cost and power consumption of a fully digital
mmWave precoder and combiner, hybrid analog/digital designs based on analog
phase shifters are often adopted. In this work we derive an iterative algorithm
for the hybrid precoding and combining design for spatial multiplexing in
mmWave massive multiple-input multiple-output (MIMO) systems. To cope with the
difficulty of handling the hardware constraint imposed by the analog phase
shifters we use the alternating direction method of the multipliers (ADMM) to
split the hybrid design problem into a sequence of smaller subproblems. This
results in an iterative algorithm where the design of the analog
precoder/combiner consists of a closed form solution followed by a simple
projection over the set of matrices with equal magnitude elements. It is
initially developed for the fully-connected structure and then extended to the
partially-connected architecture which allows simpler hardware implementation.
Furthermore, to cope with the more likely wideband scenarios where the channel
is frequency selective, we also extend the algorithm to an orthogonal frequency
division multiplexing (OFDM) based mmWave system. Simulation results in
different scenarios show that the proposed design algorithms are capable of
achieving performances close to the optimal fully digital solution and can work
with a broad range of configuration of antennas, RF chains and data streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12443</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12443</id><created>2019-03-29</created><updated>2019-04-26</updated><authors><author><keyname>Souto</keyname><forenames>Nuno</forenames></author><author><keyname>Correia</keyname><forenames>Am&#xe9;rico</forenames></author></authors><title>Frequency Domain Equalization for Single and Multiuser Generalized
  Spatial Modulation Systems in Time Dispersive Channels</title><categories>eess.SP</categories><comments>to be submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, a low-complexity iterative detector with frequency domain
equalization is proposed for generalized spatial modulation (GSM) aided single
carrier (SC) transmissions operating in frequency selective channels. The
detector comprises three main separate tasks namely, multiple-input
multiple-output (MIMO) equalization, active antenna detection and symbol wise
demodulation. This approach makes the detector suitable for a broad range of
MIMO configurations, which includes single-user and multiuser scenarios, as
well as arbitrary signal constellations. Simulation results show that the
receiver can cope with the intersymbol interference induced by severe time
dispersive channels and operate in difficult underdetermined scenarios where
the total number of transmitter antennas is substantially larger than the
number of receiver antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12536</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12536</id><created>2019-03-29</created><authors><author><keyname>Ravichandran</keyname><forenames>Vignesh</forenames></author><author><keyname>Murugesan</keyname><forenames>Balamurali</forenames></author><author><keyname>Shankaranarayana</keyname><forenames>Sharath M</forenames></author><author><keyname>Ram</keyname><forenames>Keerthi</forenames></author><author><keyname>P</keyname><forenames>Preejith S.</forenames></author><author><keyname>Joseph</keyname><forenames>Jayaraj</forenames></author><author><keyname>Sivaprakasam</keyname><forenames>Mohanasankar</forenames></author></authors><title>Deep Network for Capacitive ECG Denoising</title><categories>cs.LG eess.SP stat.ML</categories><comments>Accepted IEEE MEMEA 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous monitoring of cardiac health under free living condition is
crucial to provide effective care for patients undergoing post operative
recovery and individuals with high cardiac risk like the elderly. Capacitive
Electrocardiogram (cECG) is one such technology which allows comfortable and
long term monitoring through its ability to measure biopotential in conditions
without having skin contact. cECG monitoring can be done using many household
objects like chairs, beds and even car seats allowing for seamless monitoring
of individuals. This method is unfortunately highly susceptible to motion
artifacts which greatly limits its usage in clinical practice. The current use
of cECG systems has been limited to performing rhythmic analysis. In this paper
we propose a novel end-to-end deep learning architecture to perform the task of
denoising capacitive ECG. The proposed network is trained using motion
corrupted three channel cECG and a reference LEAD I ECG collected on
individuals while driving a car. Further, we also propose a novel joint loss
function to apply loss on both signal and frequency domain. We conduct
extensive rhythmic analysis on the model predictions and the ground truth. We
further evaluate the signal denoising using Mean Square Error(MSE) and Cross
Correlation between model predictions and ground truth. We report MSE of 0.167
and Cross Correlation of 0.476. The reported results highlight the feasibility
of performing morphological analysis using the filtered cECG. The proposed
approach can allow for continuous and comprehensive monitoring of the
individuals in free living conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12539</identifier>
 <datestamp>2019-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12539</id><created>2019-03-29</created><authors><author><keyname>Garcia</keyname><forenames>Joaquim Dias</forenames></author><author><keyname>Chabar</keyname><forenames>Raphael</forenames></author></authors><title>Modelling power markets with multi-stage stochastic Nash equilibria</title><categories>math.OC cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modelling of modern power markets requires the representation of the
following main features: (i) a stochastic dynamic decision process, with
uncertainties related to renewable production and fuel costs, among others; and
(ii) a game-theoretic framework that represents the strategic behavior of
multiple agents, for example in daily price bids. These features can be in
theory represented as a stochastic dynamic programming recursion, where we have
a Nash equilibrium for multiple agents. However, the resulting problem is very
challenging to solve. This work presents an iterative process to solve the
above problem for realistic power systems. The proposed algorithm is consist of
a fixed point algorithm, in which, each step is solved via stochastic dual
dynamic programming method. The application of the proposed algorithm are
illustrated in case studies with the Panama systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12546</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12546</id><created>2019-03-29</created><authors><author><keyname>Jeon</keyname><forenames>Yo-Seb</forenames></author><author><keyname>Lee</keyname><forenames>Namyoon</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Robust Data Detection for MIMO Systems with One-Bit ADCs: A
  Reinforcement Learning Approach</title><categories>eess.SP cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of one-bit analog-to-digital converters (ADCs) at a receiver is a
power-efficient solution for future wireless systems operating with a large
signal bandwidth and/or a massive number of receive radio frequency chains.
This solution, however, induces a high channel estimation error and therefore
makes it difficult to perform the optimal data detection that requires perfect
knowledge of likelihood functions at the receiver. In this paper, we propose a
likelihood function learning method for multiple-input multiple-output (MIMO)
systems with one-bit ADCs using a reinforcement learning approach. The key idea
is to exploit input-output samples obtained from data detection, to compensate
the mismatch in the likelihood function. The underlying difficulty of this idea
is a label uncertainty in the samples caused by a data detection error. To
resolve this problem, we define a Markov decision process (MDP) to maximize the
accuracy of the likelihood function learned from the samples. We then develop a
reinforcement learning algorithm that efficiently finds the optimal policy by
approximating the transition function and the optimal state of the MDP.
Simulation results demonstrate that the proposed method provides significant
performance gains for the optimal data detection methods that suffer from the
mismatch in the likelihood function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12565</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12565</id><created>2019-03-28</created><authors><author><keyname>Zhang</keyname><forenames>He</forenames></author><author><keyname>Bian</keyname><forenames>Zichao</forenames></author><author><keyname>Jiang</keyname><forenames>Shaowei</forenames></author><author><keyname>Liu</keyname><forenames>Jian</forenames></author><author><keyname>Song</keyname><forenames>Pengming</forenames></author><author><keyname>Zheng</keyname><forenames>Guoan</forenames></author></authors><title>Field-portable quantitative lensless microscopy based on translated
  speckle illumination and sub-sampled ptychographic phase retrieval</title><categories>eess.IV</categories><journal-ref>Optics Letters, 44(8), 1976-1979, (2019)</journal-ref><doi>10.1364/OL.44.001976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report a compact, cost-effective and field-portable lensless imaging
platform for quantitative microscopy. In this platform, the object is placed on
top of an image sensor chip without using any lens. We use a low-cost galvo
scanner to rapidly scan an unknown laser speckle pattern on the object. To
address the positioning repeatability and accuracy issues, we directly recover
the positional shifts of the speckle pattern based on the phase correlation of
the captured images. To bypass the resolution limit set by the imager pixel
size, we employ a sub-sampled ptychographic phase retrieval process to recover
the complex object. We validate our approach using a resolution target, a phase
target, and a biological sample. Our results show that accurate, high-quality
complex images can be obtained from a lensless dataset with as few as ~10
images. We also demonstrate the reported approach to achieve a 6.4 mm by 4.6 mm
field of view and a half pitch resolution of 1 miron. The reported approach may
provide a quantitative lensless imaging strategy for addressing point-of-care,
global-health, and telemedicine related challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12575</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12575</id><created>2019-03-29</created><updated>2019-11-05</updated><authors><author><keyname>Ruiz</keyname><forenames>Luana</forenames></author><author><keyname>Gama</keyname><forenames>Fernando</forenames></author><author><keyname>Marques</keyname><forenames>Antonio G.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Invariance-Preserving Localized Activation Functions for Graph Neural
  Networks</title><categories>eess.SP cs.LG cs.NE</categories><comments>Accepted at TSP</comments><doi>10.1109/TSP.2019.2955832</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph signals are signals with an irregular structure that can be described
by a graph. Graph neural networks (GNNs) are information processing
architectures tailored to these graph signals and made of stacked layers that
compose graph convolutional filters with nonlinear activation functions. Graph
convolutions endow GNNs with invariance to permutations of the graph nodes'
labels. In this paper, we consider the design of trainable nonlinear activation
functions that take into consideration the structure of the graph. This is
accomplished by using graph median filters and graph max filters, which mimic
linear graph convolutions and are shown to retain the permutation invariance of
GNNs. We also discuss modifications to the backpropagation algorithm necessary
to train local activation functions. The advantages of localized activation
function architectures are demonstrated in four numerical experiments: source
localization on synthetic graphs, authorship attribution of 19th century
novels, movie recommender systems and scientific article classification. In all
cases, localized activation functions are shown to improve model capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12589</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12589</id><created>2019-03-29</created><authors><author><keyname>Maschietti</keyname><forenames>Flavio</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author></authors><title>Coordinated Beam Selection in Millimeter Wave Multi-User MIMO Using
  Out-of-Band Information</title><categories>eess.SP cs.IT math.IT</categories><comments>16 pages, 4 figures. The short version of this paper has been
  accepted to IEEE ICC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using out-of-band (OOB) side-information has recently been shown to
accelerate beam selection in single-user millimeter wave (mmWave) massive MIMO
communications. In this paper, we propose a novel OOB-aided beam selection
framework for a mmWave uplink multi-user system. In particular, we exploit
spatial information extracted from lower (sub-6 GHz) bands in order to assist
with an inter-user coordination scheme at mmWave bands. To enforce
coordination, we propose an exchange protocol exploiting device-to-device
communications, where low-rate beam-related information is exchanged between
the mobile terminals. The decentralized coordination mechanism allows the
suppression of the so-called co-beam interference which would otherwise lead to
irreducible interference at the base station side, thereby triggering
substantial spectral efficiency gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1903.12644</identifier>
 <datestamp>2019-04-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1903.12644</id><created>2019-03-20</created><authors><author><keyname>Jantunen</keyname><forenames>Erkki</forenames></author><author><keyname>Zurutuza</keyname><forenames>Urko</forenames></author><author><keyname>Ferreira</keyname><forenames>Luis Lino</forenames></author><author><keyname>Varga</keyname><forenames>Pal</forenames></author></authors><title>Optimising maintenance: What are the expectations for Cyber Physical
  Systems</title><categories>eess.SP cs.SY</categories><comments>2016 3rd International Workshop on Emerging Ideas and Trends in
  Engineering of Cyber-Physical Systems (EITEC)</comments><doi>10.1109/EITEC.2016.7503697</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for maintenance is based on the wear of components of machinery. If
this need can be defined reliably beforehand so that no unpredicted failures
take place then the maintenance actions can be carried out economically with
mini-mum disturbances to production. There are two basic challenges in solving
the above. First understanding the development of wear and failures, and second
managing the measurement and diagnosis of such parameters that can reveal the
development of wear. In principle the development of wear and failures can be
predicted through monitoring time, load or wear as such. Moni-toring time is
not very efficient, as there are only limited numbers of components that suffer
from aging which as such is the result of chemical wear i.e. changes in the
material. In most cases the loading of components influences their wear. In
principle the loading can be stable or varying in nature. Of these two cases
the varying load case is much more challenging than the stable one. The
monitoring of wear can be done either directly e.g. optical methods or
indirectly e.g. vibration. Monitoring actual wear is naturally the most
reliable approach, but it often means that additional investments are needed.
The paper discusses how the monitoring of wear and need for maintenance can be
done based on the use of Cyber Physical Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00055</identifier>
 <datestamp>2019-12-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00055</id><created>2019-03-29</created><updated>2019-12-21</updated><authors><author><keyname>Trowitzsch</keyname><forenames>Ivo</forenames></author><author><keyname>Schymura</keyname><forenames>Christopher</forenames></author><author><keyname>Kolossa</keyname><forenames>Dorothea</forenames></author><author><keyname>Obermayer</keyname><forenames>Klaus</forenames></author></authors><title>Joining Sound Event Detection and Localization Through Spatial
  Segregation</title><categories>cs.SD eess.AS</categories><comments>Accepted for publication in IEEE/ACM Transactions on Audio, Speech,
  and Language Processing</comments><doi>10.1109/TASLP.2019.2958408</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identification and localization of sounds are both integral parts of
computational auditory scene analysis. Although each can be solved separately,
the goal of forming coherent auditory objects and achieving a comprehensive
spatial scene understanding suggests pursuing a joint solution of the two
problems. This work presents an approach that robustly binds localization with
the detection of sound events in a binaural robotic system. Both tasks are
joined through the use of spatial stream segregation which produces
probabilistic time-frequency masks for individual sources attributable to
separate locations, enabling segregated sound event detection operating on
these streams. We use simulations of a comprehensive suite of test scenes with
multiple co-occurring sound sources, and propose performance measures for
systematic investigation of the impact of scene complexity on this segregated
detection of sound types. Analyzing the effect of spatial scene arrangement, we
show how a robot could facilitate high performance through optimal head
rotation. Furthermore, we investigate the performance of segregated detection
given possible localization error as well as error in the estimation of number
of active sources. Our analysis demonstrates that the proposed approach is an
effective method to obtain joint sound event location and type information
under a wide range of conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00063</identifier>
 <datestamp>2019-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00063</id><created>2019-03-29</created><updated>2019-09-09</updated><authors><author><keyname>Zhang</keyname><forenames>Jingyang</forenames></author><author><keyname>Ding</keyname><forenames>Wenhao</forenames></author><author><keyname>Kang</keyname><forenames>Jintao</forenames></author><author><keyname>He</keyname><forenames>Liang</forenames></author></authors><title>Multi-Scale Time-Frequency Attention for Acoustic Event Detection</title><categories>cs.SD eess.AS</categories><comments>Accepted by Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most attention-based methods only concentrate along the time axis, which is
insufficient for Acoustic Event Detection (AED). Meanwhile, previous methods
for AED rarely considered that target events possess distinct temporal and
frequential scales. In this work, we propose a Multi-Scale Time-Frequency
Attention (MTFA) module for AED. MTFA gathers information at multiple
resolutions to generate a time-frequency attention mask which tells the model
where to focus along both time and frequency axis. With MTFA, the model could
capture the characteristics of target events with different scales. We
demonstrate the proposed method on Task 2 of Detection and Classification of
Acoustic Scenes and Events (DCASE) 2017 Challenge. Our method achieves
competitive results on both development dataset and evaluation dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00095</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00095</id><created>2019-03-29</created><authors><author><keyname>Mohammadian</keyname><forenames>Amirhossein</forenames></author><author><keyname>Tellambura</keyname><forenames>Chintha</forenames></author></authors><title>Full-Duplex GFDM Radio Transceivers in the Presence of Phase Noise, CFO
  and IQ Imbalance</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the performance of a full-duplex (FD) generalized
frequency division multiplexing (GFDM) transceiver in the presence of radio
frequency (RF) impairments including phase noise, carrier frequency offset
(CFO) and in-phase (I) and quadrature (Q) imbalance. We study analog and
digital self-interference (SI) cancellation and develop a complementary SI
suppression method. Closed-form solutions for the residual SI power and the
desired signal power and signal-to-interference ratio (SIR) are provided.
Simulation results show that the RF impairments degrade SI cancellation and FD
GFDM is more sensitive to them compares to FD orthogonal frequency division
multiplexing (OFDM). Hence, we propose an FD GFDM receiver filter for
maximizing the SIR. Significantly, it achieves 25 dB higher SIR than FD OFDM
transceiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00138</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00138</id><created>2019-03-29</created><updated>2019-04-11</updated><authors><author><keyname>Rajput</keyname><forenames>K. S.</forenames></author><author><keyname>Wibowo</keyname><forenames>S.</forenames></author><author><keyname>Hao</keyname><forenames>C.</forenames></author><author><keyname>Majmudar</keyname><forenames>M.</forenames></author></authors><title>On Arrhythmia Detection by Deep Learning and Multidimensional
  Representation</title><categories>stat.ML cs.LG eess.SP</categories><comments>draft paper; prepared for journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An electrocardiogram (ECG) is a time-series signal that is represented by
one-dimensional (1-D) data. Higher dimensional representation contains more
information that is accessible for feature extraction. Hidden variables such as
frequency relation and morphology of segment is not directly accessible in the
time domain. In this paper, 1-D time series data is converted into
multi-dimensional representation in the form of multichannel 2-D images.
Following that, deep learning was used to train a deep neural network based
classifier to detect arrhythmias. The results of simulation on testing database
demonstrate the effectiveness of the proposed methodology by showing an
outstanding classification performance compared to other existing methods and
hand-crafted annotations made by certified cardiologists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00150</identifier>
 <datestamp>2019-04-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00150</id><created>2019-03-30</created><updated>2019-04-16</updated><authors><author><keyname>Verma</keyname><forenames>Gaurav</forenames></author><author><keyname>Dhekane</keyname><forenames>Eeshan Gunesh</forenames></author><author><keyname>Guha</keyname><forenames>Tanaya</forenames></author></authors><title>Learning Affective Correspondence between Music and Image</title><categories>cs.MM cs.LG cs.SD eess.AS</categories><comments>5 pages, International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the problem of learning affective correspondence between audio
(music) and visual data (images). For this task, a music clip and an image are
considered similar (having true correspondence) if they have similar emotion
content. In order to estimate this crossmodal, emotion-centric similarity, we
propose a deep neural network architecture that learns to project the data from
the two modalities to a common representation space, and performs a binary
classification task of predicting the affective correspondence (true or false).
To facilitate the current study, we construct a large scale database containing
more than $3,500$ music clips and $85,000$ images with three emotion classes
(positive, neutral, negative). The proposed approach achieves $61.67\%$
accuracy for the affective correspondence prediction task on this database,
outperforming two relevant and competitive baselines. We also demonstrate that
our network learns modality-specific representations of emotion (without
explicitly being trained with emotion labels), which are useful for emotion
recognition in individual modalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00169</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00169</id><created>2019-03-30</created><authors><author><keyname>Li</keyname><forenames>Xiaolong</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author><author><keyname>Zhang</keyname><forenames>Tianxian</forenames></author><author><keyname>Yi</keyname><forenames>Wei</forenames></author><author><keyname>Cui</keyname><forenames>Guolong</forenames></author><author><keyname>Kong</keyname><forenames>Lingjiang</forenames></author></authors><title>WRFRFT-based Coherent Detection and Parameter Estimation of Radar Moving
  Target With Unknown Entry/Departure Time</title><categories>eess.SP</categories><comments>30 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A moving target may enter a radar coverage area unannounced and leave after
an unspecified period, which implies that the target's entry time and departure
time are unknown. In the absence of these time information, target detection
and parameter estimation (DAPE) will be severely impacted. In this paper, we
consider the coherent detection and parameters estimation problem for a radar
moving target with unknown entry time and departure time (that is, the time
when the target appears-in/leaves the radar detection field is unknown),
involving across range cell (ARC) and Doppler spread (DS) effects within the
observation period. A new algorithm, known as window Radon Fractional Fourier
transform (WRFRFT) is proposed to detect and estimate the target's time
parameters (i.e., entry time and departure time) and motion parameters (i.e.,
range, velocity and acceleration). The observation values of a maneuvering
target are first intercepted and extracted by the window function and searching
along the motion trajectory. Then these values are fractional Fourier
transformed and well accumulated in the WRFRFT domain, where the DAPE of target
could be accomplished thereafter. Experiments with simulated and real radar
data sets prove its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00202</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00202</id><created>2019-03-30</created><authors><author><keyname>Swietojanski</keyname><forenames>Pawel</forenames></author><author><keyname>Miksik</keyname><forenames>Ondrej</forenames></author></authors><title>Static Visual Spatial Priors for DoA Estimation</title><categories>cs.SD eess.AS</categories><comments>6 pages, 6 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As we interact with the world, for example when we communicate with our
colleagues in a large open space or meeting room, we continuously analyse the
surrounding environment and, in particular, localise and recognise acoustic
events. While we largely take such abilities for granted, they represent a
challenging problem for current robots or smart voice assistants as they can be
easily fooled by high degree of sound interference in acoustically complex
environments. Preventing such failures when using solely audio data is
challenging, if not impossible since the algorithms need to take into account
wider context and often understand the scene on a semantic level. In this
paper, we propose what to our knowledge is the first multi-modal direction of
arrival (DoA) of sound, which uses static visual spatial prior providing an
auxiliary information about the environment to suppress some of the false DoA
detections. We validate our approach on a newly collected real-world dataset,
and show that our approach consistently improves over classic DoA baselines
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00228</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00228</id><created>2019-03-30</created><authors><author><keyname>Basumallik</keyname><forenames>Sagnik</forenames></author></authors><title>Voltage Quality Time Series Classification using Convolutional Neural
  Network</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the effectiveness of convolutional neural network (CNN)
to classify power quality problems. These problems arise mainly due to increase
in use of non-linear loads, operation of devices like adjustable speed drives
and power factor correction capacitors, which is a growing concern both for
utilities and customers. This work uses the advances in supervised learning to
classify different power quality time-series waveforms such as voltage sag,
swell, interruption, harmonics, transients and flicker. CNN results in a very
high classification accuracy compared to other traditional and machine learning
methods in presence of noise. This process can be employed by utilities as well
as customers to understand the cause and mitigate voltage quality problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00239</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00239</id><created>2019-03-30</created><authors><author><keyname>Hofer</keyname><forenames>L. R.</forenames></author><author><keyname>Jones</keyname><forenames>L. W.</forenames></author><author><keyname>Goedert</keyname><forenames>J. L.</forenames></author><author><keyname>Dragone</keyname><forenames>R. V.</forenames></author></authors><title>Hermite-Gaussian Mode Detection via Convolution Neural Networks</title><categories>eess.IV physics.optics</categories><doi>10.1364/JOSAA.36.000936</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hermite-Gaussian (HG) laser modes are a complete set of solutions to the
free-space paraxial wave equation in Cartesian coordinates and represent a
close approximation to physically-realizable laser cavity modes. Additionally,
HG modes can be mode-multiplexed to significantly increase the information
capacity of optical communication systems due to their orthogonality. Since,
both cavity tuning and optical communication applications benefit from a
machine vision determination of HG modes, convolution neural networks were
implemented to detect the lowest twenty-one unique HG modes with an accuracy
greater than 99%. As the effectiveness of a CNN is dependent on the diversity
of its training data, extensive simulated and experimental datasets were
created for training, validation and testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00309</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00309</id><created>2019-03-30</created><authors><author><keyname>Lu</keyname><forenames>Yang</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Optimal Number of Measurements in a Linear System with Quadratically
  Decreasing SNR</title><categories>eess.SP</categories><comments>12 pages, accepted by IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2019.2910451</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the design of a linear sensing system with a fixed energy budget
assuming that the sampling noise is the dominant noise source. The energy
constraint implies that the signal energy per measurement decreases linearly
with the number of measurements. When the maximum sampling rate of the sampling
circuit is chosen to match the designed sampling rate, the assumption on the
noise implies that its variance increases approximately linearly with the
sampling rate (number of measurements). Therefore, the overall SNR per
measurement decreases quadratically in the number of measurements. Our analysis
shows that, in this setting there is an optimal number of measurements. This is
in contrast to the standard case, where noise variance remains unchanged with
sampling rate, in which case more measurements imply better performance. Our
results are based on a state evolution technique of the well-known approximate
message passing algorithm. We consider both the sparse (e.g. Bernoulli-Gaussian
and least-favorable distributions) and the non-sparse (e.g. Gaussian
distribution) settings in both the real and complex domains. Numerical results
corroborate our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00341</identifier>
 <datestamp>2019-08-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00341</id><created>2019-03-31</created><updated>2019-07-03</updated><authors><author><keyname>Walker</keyname><forenames>Michael R.</forenames><suffix>II</suffix></author><author><keyname>O'Sullivan</keyname><forenames>Joseph A.</forenames></author></authors><title>The Broken Ray Transform: Additional Properties and New Inversion
  Formula</title><categories>eess.SP cs.NA math.NA</categories><doi>10.1088/1361-6420/ab355f</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The significance of the broken ray transform (BRT) is due to its occurrence
in a number of modalities spanning optical, x-ray, and nuclear imaging. When
data are indexed by the scatter location, the BRT is both linear and shift
invariant. Analyzing the BRT as a linear system provides a new perspective on
the inverse problem. In this framework we contrast prior inversion formulas and
identify numerical issues. This has practical benefits as well. We clarify the
extent of data required for global reconstruction by decomposing the BRT as a
linear combination of cone beam transforms. Additionally we leverage the two
dimensional Fourier transform to derive new inversion formulas that are
computationally efficient for arbitrary scatter angles. Results of numerical
simulations are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00361</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00361</id><created>2019-03-31</created><authors><author><keyname>Qin</keyname><forenames>Ying</forenames></author><author><keyname>Wu</keyname><forenames>Yuzhong</forenames></author><author><keyname>Lee</keyname><forenames>Tan</forenames></author><author><keyname>Kong</keyname><forenames>Anthony Pak Hin</forenames></author></authors><title>An End-to-End Approach to Automatic Speech Assessment for
  Cantonese-speaking People with Aphasia</title><categories>eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional automatic assessment of pathological speech usually follows two
main steps: (1) extraction of pathology-specific features; (2) classification
or regression on extracted features. Given the great variety of speech and
language disorders, feature design is never a straightforward task, and yet it
is most crucial to the performance of assessment. This paper presents an
end-to-end approach to automatic speech assessment for Cantonese-speaking
People With Aphasia (PWA). The assessment is formulated as a binary
classification task to discriminate PWA with high scores of subjective
assessment from those with low scores. The sequence-to-one Recurrent Neural
Network with Gated Recurrent Unit (GRU-RNN) and Convolutional Neural Network
(CNN) models are applied to realize the end-to-end mapping from fundamental
speech features to the classification result. The pathology-specific features
used for assessment can be learned implicitly by the neural network model.
Class Activation Mapping (CAM) method is utilized to visualize how those
features contribute to the assessment result. Our experimental results show
that the end-to-end approach outperforms the conventional two-step approach in
the classification task, and confirm that the CNN model is able to learn
impairment-related features that are similar to human-designed features. The
experimental results also suggest that CNN model performs better than
sequence-to-one GRU-RNN model in this specific task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00458</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00458</id><created>2019-03-31</created><authors><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Biswas</keyname><forenames>Sudip</forenames></author><author><keyname>Singh</keyname><forenames>Keshav</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharmalingam</forenames></author></authors><title>Analysis of Cached-Enabled Hybrid Millimter Wave &amp; Sub-6 GHz Massive
  MIMO Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on edge caching in mm/{\mu}Wave hybrid wireless networks,
in which all mmWave SBSs and {\mu}Wave MBSs are capable of storing contents to
alleviate the traffic burden on the backhaul link that connect the BSs and the
core network to retrieve the non-cached contents. The main aim of this work is
to address the effect of capacity-limited backhaul on the average success
probability (ASP) of file delivery and latency. In particular, we consider a
more practical mmWave hybrid beamforming in small cells and massive MIMO
communication in macro cells. Based on stochastic geometry and a simple
retransmission protocol, we derive the association probabilities by which the
ASP of file delivery and latency are derived. Taking no caching event as the
benchmark, we evaluate these QoS performance metrics under MC and UC placement
policies. The theoretical results demonstrate that backhaul capacity indeed has
a significant impact on network performance especially under weak backhaul
capacity. Besides, we also show the tradeoff among cache size, retransmission
attempts, ASP of file delivery, and latency. The interplay shows that cache
size and retransmission under different caching placement schemes alleviates
the backhaul requirements. Simulation results are present to valid our
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00520</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00520</id><created>2019-03-29</created><authors><author><keyname>Routray</keyname><forenames>Sudhir K.</forenames></author><author><keyname>Hussein</keyname><forenames>Habib Mohammed</forenames></author></authors><title>Satellite Based IoT Networks for Emerging Applications</title><categories>eess.SP cs.NI</categories><comments>5 pages, Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of things (IoT) has been proven to be a ubiquitous technology for
several existing and new applications. It can provide both accuracy and
sustainability in the emerging services and applications. IoT has several
advantages and it can provide different levels of coverage in different
geographical locations and contexts. There are several applications in which
the wide coverage, low power and reliability of the communication have very
high priorities. Conglomeration of IoT and satellite networks can make it very
attractive for these applications. In this article, we present the general
features, motivation and deployment issues of satellite based IoT networks. We
present the applications where these satellite based IoT networks can play
important roles. We also present the hybridization of narrowband IoT and
satellite networks which can provide a long term sustainable network solution
for several applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00562</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00562</id><created>2019-04-01</created><authors><author><keyname>Sun</keyname><forenames>Jinguang</forenames></author><author><keyname>Wang</keyname><forenames>Wanli</forenames></author><author><keyname>Wei</keyname><forenames>Xian</forenames></author><author><keyname>Fang</keyname><forenames>Li</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoliang</forenames></author><author><keyname>Xu</keyname><forenames>Yusheng</forenames></author><author><keyname>Yu</keyname><forenames>Hui</forenames></author><author><keyname>Yao</keyname><forenames>Wei</forenames></author></authors><title>Deep Clustering With Intra-class Distance Constraint for Hyperspectral
  Images</title><categories>cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high dimensionality of hyperspectral images often results in the
degradation of clustering performance. Due to the powerful ability of deep
feature extraction and non-linear feature representation, the clustering
algorithm based on deep learning has become a hot research topic in the field
of hyperspectral remote sensing. However, most deep clustering algorithms for
hyperspectral images utilize deep neural networks as feature extractor without
considering prior knowledge constraints that are suitable for clustering. To
solve this problem, we propose an intra-class distance constrained deep
clustering algorithm for high-dimensional hyperspectral images. The proposed
algorithm constrains the feature mapping procedure of the auto-encoder network
by intra-class distance so that raw images are transformed from the original
high-dimensional space to the low-dimensional feature space that is more
conducive to clustering. Furthermore, the related learning process is treated
as a joint optimization problem of deep feature extraction and clustering.
Experimental results demonstrate the intense competitiveness of the proposed
algorithm in comparison with state-of-the-art clustering methods of
hyperspectral images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00583</identifier>
 <datestamp>2019-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00583</id><created>2019-04-01</created><authors><author><keyname>Chi</keyname><forenames>Jing</forenames></author><author><keyname>Li</keyname><forenames>Xiaolei</forenames></author><author><keyname>Wang</keyname><forenames>Haozhong</forenames></author><author><keyname>Gao</keyname><forenames>Dazhi</forenames></author><author><keyname>Gerstoft</keyname><forenames>Peter</forenames></author></authors><title>Sound source ranging using a feed-forward neural network with
  fitting-based early stopping</title><categories>cs.LG cs.SD eess.SP physics.comp-ph stat.ML</categories><doi>10.1121/1.5126115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a feed-forward neural network (FNN) is trained for source ranging in an
ocean waveguide, it is difficult evaluating the range accuracy of the FNN on
unlabeled test data. A fitting-based early stopping (FEAST) method is
introduced to evaluate the range error of the FNN on test data where the
distance of source is unknown. Based on FEAST, when the evaluated range error
of the FNN reaches the minimum on test data, stopping training, which will help
to improve the ranging accuracy of the FNN on the test data. The FEAST is
demonstrated on simulated and experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00640</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00640</id><created>2019-04-01</created><authors><author><keyname>Zhang</keyname><forenames>Jin</forenames></author><author><keyname>Lu</keyname><forenames>Weibing</forenames></author><author><keyname>Liu</keyname><forenames>Zhenguo</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Wu</keyname><forenames>Bian</forenames></author><author><keyname>Liu</keyname><forenames>Qifeng</forenames></author></authors><title>A Low Profile Tunable Microwave Absorber based on Graphene Sandwich
  Structure and High Impedance Surface</title><categories>physics.app-ph eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a low profile dynamically tunable microwave absorber is
proposed, which consists of high impedance surface (HIS) and graphene sandwich
structure (GSS). We theoretically demonstrate and experimentally verify that
the proposed absorber can provide a dynamically tunable reflection range from
larger than -3dB to less than -30 dB (corresponding to the absorption range
from 50% to 99.9%) at opera-ting frequency of 11.2GHz by external bias voltage.
The entire thickness of this absorber is only 2.8mm, nearly one tenth of
working wavelength. In addition, a modified equivalent circuit model is
proposed to explicate its absorption mechanism. At last, we fabricate a
prototype absorber, and measure its absorption in anechoic chamber. The
experimental results agree well with the full wave simulation results. This
work may provide a reference for design and fabrication of dynamically tunable
microwave absorber based on large-scale graphene and may promote the actual
applications of graphene at microwave frequency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00708</identifier>
 <datestamp>2019-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00708</id><created>2019-04-01</created><updated>2019-10-08</updated><authors><author><keyname>Thormann</keyname><forenames>Kolja</forenames></author><author><keyname>Baum</keyname><forenames>Marcus</forenames></author></authors><title>Optimal Fusion of Elliptic Extended Target Estimates based on the
  Wasserstein Distance</title><categories>eess.SP cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the fusion of multiple estimates of a spatially extended
object, where the object extent is modeled as an ellipse parameterized by the
orientation and semiaxes lengths. For this purpose, we propose a novel
systematic approach that employs a distance measure for ellipses, i.e., the
Gaussian Wasserstein distance, as a cost function. We derive an explicit
approximate expression for the Minimum Mean Gaussian Wasserstein distance
(MMGW) estimate. Based on the concept of a MMGW estimator, we develop efficient
methods for the fusion of extended target estimates. The proposed fusion
methods are evaluated in a simulated experiment and the benefits of the novel
methods are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00715</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00715</id><created>2019-04-01</created><authors><author><keyname>Jin</keyname><forenames>Di</forenames></author><author><keyname>Yin</keyname><forenames>Feng</forenames></author><author><keyname>Fritsche</keyname><forenames>Carsten</forenames></author><author><keyname>Gustafsson</keyname><forenames>Fredrik</forenames></author><author><keyname>Zoubir</keyname><forenames>Abdelhak M.</forenames></author></authors><title>Bayesian Cooperative Localization Using Received Signal Strength With
  Unknown Path Loss Exponent: Message Passing Approaches</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian framework for the received-signal-strength-based
cooperative localization problem with unknown path loss exponent. Our purpose
is to infer the marginal posterior of each unknown parameter: the position or
the path loss exponent. This probabilistic inference problem is solved using
message passing algorithms that update messages and beliefs iteratively. To
enable the numerical tractability, we combine the variable discretization and
Monte-Carlo-based numerical approximation schemes. To further improve
computational efficiency, we develop an auxiliary importance sampler that
updates the beliefs with the help of an auxiliary variable. To sample from a
normalized likelihood function, which is an important ingredient of the
proposed auxiliary importance sampler, we develop a stochastic sampling
strategy that mathematically interprets and corrects an existing heuristic
strategy. The proposed message passing algorithms are analyzed systematically
in terms of computational complexity, demonstrating the computational
efficiency of the proposed auxiliary importance sampler. Various simulations
are conducted to validate the overall good performance of the proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00750</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00750</id><created>2019-02-19</created><authors><author><keyname>Lin</keyname><forenames>Qi</forenames><affiliation>UNSW Australia</affiliation><affiliation>Data61, CSIRO, Australia</affiliation></author><author><keyname>Xu</keyname><forenames>Weitao</forenames><affiliation>UNSW Australia</affiliation></author><author><keyname>Liu</keyname><forenames>Jun</forenames><affiliation>UNSW Australia</affiliation></author><author><keyname>Khamis</keyname><forenames>Abdelwahed</forenames><affiliation>UNSW Australia</affiliation><affiliation>Data61, CSIRO, Australia</affiliation></author><author><keyname>Hu</keyname><forenames>Wen</forenames><affiliation>UNSW Australia</affiliation><affiliation>Data61, CSIRO, Australia</affiliation></author><author><keyname>Hassan</keyname><forenames>Mahbub</forenames><affiliation>UNSW Australia</affiliation><affiliation>Data61, CSIRO, Australia</affiliation></author><author><keyname>Seneviratne</keyname><forenames>Aruna</forenames><affiliation>UNSW Australia</affiliation><affiliation>Data61, CSIRO, Australia</affiliation></author></authors><title>H2B: Heartbeat-based Secret Key Generation Using Piezo Vibration Sensors</title><categories>cs.CR eess.SP</categories><doi>10.1145/3302506.3310406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Heartbeats-2-Bits (H2B), which is a system for securely pairing
wearable devices by generating a shared secret key from the skin vibrations
caused by heartbeat. This work is motivated by potential power saving
opportunity arising from the fact that heartbeat intervals can be detected
energy-efficiently using inexpensive and power-efficient piezo sensors, which
obviates the need to employ complex heartbeat monitors such as
Electrocardiogram or Photoplethysmogram. Indeed, our experiments show that
piezo sensors can measure heartbeat intervals on many different body locations
including chest, wrist, waist, neck and ankle. Unfortunately, we also discover
that the heartbeat interval signal captured by piezo vibration sensors has low
Signal-to-Noise Ratio (SNR) because they are not designed as precision
heartbeat monitors, which becomes the key challenge for H2B. To overcome this
problem, we first apply a quantile function-based quantization method to fully
extract the useful entropy from the noisy piezo measurements. We then propose a
novel Compressive Sensing-based reconciliation method to correct the high bit
mismatch rates between the two independently generated keys caused by low SNR.
We prototype H2B using off-the-shelf piezo sensors and evaluate its performance
on a dataset collected from different body positions of 23 participants. Our
results show that H2B has an overwhelming pairing success rate of 95.6%. We
also analyze and demonstrate H2B's robustness against three types of attacks.
Finally, our power measurements show that H2B is very power-efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00757</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00757</id><created>2019-03-21</created><authors><author><keyname>Rosen</keyname><forenames>Eitan</forenames></author><author><keyname>Shkolnisky</keyname><forenames>Yoel</forenames></author></authors><title>Common lines ab-initio reconstruction of $D_2$-symmetric molecules</title><categories>eess.IV math.NA physics.comp-ph</categories><msc-class>92-08, 62-07, 62P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryo-electron microscopy is a state-of-the-art method for determining
high-resolution three-dimensional models of molecules, from their
two-dimensional projection images taken by an electron microscope. A crucial
step in this method is to determine a low-resolution model of the molecule
using only the given projection images, without using any three-dimensional
information, such as an assumed reference model. For molecules without
symmetry, this is often done by exploiting common lines between pairs of
images. Common lines algorithms have been recently devised for molecules with
cyclic symmetry, but no such algorithms exist for molecules with dihedral
symmetry. In this work, we present a common lines algorithm for determining the
structure of molecules with $D_{2}$ symmetry. The algorithm exploits the common
lines between all pairs of images simultaneously, as well as common lines
within each image. We demonstrate the applicability of our algorithm using
experimental cryo-electron microscopy data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00764</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00764</id><created>2019-03-19</created><authors><author><keyname>Bulbul</keyname><forenames>Mohammad Farhad</forenames></author><author><keyname>Islam</keyname><forenames>Saiful</forenames></author><author><keyname>Ali</keyname><forenames>Hazrat</forenames></author></authors><title>3D human action analysis and recognition through GLAC descriptor on 2D
  motion and static posture images</title><categories>cs.CV cs.LG eess.IV stat.ML</categories><comments>Multimed Tools Appl (2019)</comments><doi>10.1007/s11042-019-7365-2</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we present an approach for identification of actions within
depth action videos. First, we process the video to get motion history images
(MHIs) and static history images (SHIs) corresponding to an action video based
on the use of 3D Motion Trail Model (3DMTM). We then characterize the action
video by extracting the Gradient Local Auto-Correlations (GLAC) features from
the SHIs and the MHIs. The two sets of features i.e., GLAC features from MHIs
and GLAC features from SHIs are concatenated to obtain a representation vector
for action. Finally, we perform the classification on all the action samples by
using the l2-regularized Collaborative Representation Classifier (l2-CRC) to
recognize different human actions in an effective way. We perform evaluation of
the proposed method on three action datasets, MSR-Action3D, DHA and UTD-MHAD.
Through experimental results, we observe that the proposed method performs
superior to other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00771</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00771</id><created>2019-04-01</created><updated>2019-04-07</updated><authors><author><keyname>Luong</keyname><forenames>Hieu-Thi</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Nishizawa</keyname><forenames>Nobuyuki</forenames></author></authors><title>Training Multi-Speaker Neural Text-to-Speech Systems using
  Speaker-Imbalanced Speech Corpora</title><categories>eess.AS cs.CL cs.SD stat.ML</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When the available data of a target speaker is insufficient to train a high
quality speaker-dependent neural text-to-speech (TTS) system, we can combine
data from multiple speakers and train a multi-speaker TTS model instead. Many
studies have shown that neural multi-speaker TTS model trained with a small
amount data from multiple speakers combined can generate synthetic speech with
better quality and stability than a speaker-dependent one. However when the
amount of data from each speaker is highly unbalanced, the best approach to
make use of the excessive data remains unknown. Our experiments showed that
simply combining all available data from every speaker to train a multi-speaker
model produces better than or at least similar performance to its
speaker-dependent counterpart. Moreover by using an ensemble multi-speaker
model, in which each subsystem is trained on a subset of available data, we can
further improve the quality of the synthetic speech especially for
underrepresented speakers whose training data is limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00798</identifier>
 <datestamp>2020-01-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00798</id><created>2019-03-28</created><updated>2020-01-22</updated><authors><author><keyname>Rottenberg</keyname><forenames>Francois</forenames></author><author><keyname>Choi</keyname><forenames>Thomas</forenames></author><author><keyname>Luo</keyname><forenames>Peng</forenames></author><author><keyname>Zhang</keyname><forenames>Jianzhong</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Performance Analysis of Channel Extrapolation in FDD Massive MIMO
  Systems</title><categories>eess.SP</categories><comments>arXiv admin note: substantial text overlap with arXiv:1902.06844</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel estimation for the downlink of frequency division duplex (FDD)
massive MIMO systems is well known to generate a large overhead as the amount
of training generally scales with the number of transmit antennas in a MIMO
system. In this paper, we consider the solution of extrapolating the channel
frequency response from uplink pilot estimates to the downlink frequency band,
which completely removes the training overhead. We first show that conventional
estimators fail to achieve reasonable accuracy. We propose instead to use
high-resolution channel estimation. We derive theoretical lower bounds (LB) for
the mean squared error (MSE) of the extrapolated channel. Assuming that the
paths are well separated, the LB is simplified in an expression that gives
considerable physical insight. It is then shown that the MSE is inversely
proportional to the number of receive antennas while the extrapolation
performance penalty scales with the square of the ratio of the frequency offset
and the training bandwidth. The channel extrapolation performance is validated
through numeric simulations and experimental measurements taken in an anechoic
chamber. Our main conclusion is that channel extrapolation is a viable solution
for FDD massive MIMO systems if accurate system calibration is performed and
favorable propagation conditions are present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00810</identifier>
 <datestamp>2019-07-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00810</id><created>2019-03-29</created><updated>2019-06-28</updated><authors><author><keyname>Scholler</keyname><forenames>Jules</forenames></author></authors><title>Motion artifact removal and signal enhancement to achieve in vivo
  dynamic full field OCT</title><categories>eess.IV physics.med-ph</categories><comments>10 pages, 6 figures</comments><journal-ref>Opt. Express 27, 19562-19572 (2019)</journal-ref><doi>10.1364/OE.27.019562</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a filtering procedure based on singular value decomposition to
remove artifacts arising from sample motion during dynamic full field OCT
acquisitions. The presented method succeeded in removing artifacts created by
environmental noise from data acquired in a clinical setting, including in vivo
data. Moreover, we report on a new method based on using the cumulative sum to
compute dynamic images from raw signals, leading to a higher signal to noise
ratio, and thus enabling dynamic imaging deeper in tissues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00811</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00811</id><created>2019-04-01</created><authors><author><keyname>1</keyname><forenames>Mansourah K. A. Aljohani</forenames></author><author><keyname>Musa1</keyname><forenames>Mohamed O. I.</forenames></author><author><keyname>Alresheedi2</keyname><forenames>Mohammed T.</forenames></author><author><keyname>H</keyname><forenames>Jaafar M.</forenames></author></authors><title>WDM NOMA VLC Systems</title><categories>eess.SP</categories><comments>5 pages,3 figures, conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wavelength division multiplexing (WDM) and non-orthogonal multiple access
(NOMA) are employed in a visible light communication (VLC) system to increase
the capacity of the system. The system is evaluated using two different
scenarios focusing on power and data rate, where the first scenario is based on
fair power allocation while the second scenario provides equal power
allocation. Data rate variation is evaluated as a function of the users
positioning and mobility. In both scenarios the proposed NOMA-WDM system
achieved higher data rate NOMA system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00839</identifier>
 <datestamp>2020-02-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00839</id><created>2019-03-29</created><authors><author><keyname>Dov</keyname><forenames>David</forenames></author><author><keyname>Kovalsky</keyname><forenames>Shahar</forenames></author><author><keyname>Cohen</keyname><forenames>Jonathan</forenames></author><author><keyname>Range</keyname><forenames>Danielle</forenames></author><author><keyname>Henao</keyname><forenames>Ricardo</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Thyroid Cancer Malignancy Prediction From Whole Slide Cytopathology
  Images</title><categories>cs.CV cs.AI eess.IV</categories><journal-ref>Proceedings of Machine Learning Research, 2019, Vol. 106</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider preoperative prediction of thyroid cancer based on
ultra-high-resolution whole-slide cytopathology images. Inspired by how human
experts perform diagnosis, our approach first identifies and classifies
diagnostic image regions containing informative thyroid cells, which only
comprise a tiny fraction of the entire image. These local estimates are then
aggregated into a single prediction of thyroid malignancy. Several unique
characteristics of thyroid cytopathology guide our deep-learning-based
approach. While our method is closely related to multiple-instance learning, it
deviates from these methods by using a supervised procedure to extract
diagnostically relevant regions. Moreover, we propose to simultaneously predict
thyroid malignancy, as well as a diagnostic score assigned by a human expert,
which further allows us to devise an improved training strategy. Experimental
results show that the proposed algorithm achieves performance comparable to
human experts, and demonstrate the potential of using the algorithm for
screening and as an assistive tool for the improved diagnosis of indeterminate
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00864</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00864</id><created>2019-04-01</created><authors><author><keyname>Kim</keyname><forenames>Kyung-Su</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author></authors><title>Tree Search Network for Sparse Regression</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical sparse regression problem of recovering a sparse
signal $x_0$ given a measurement vector $y = \Phi x_0+w$. We propose a tree
search algorithm driven by the deep neural network for sparse regression (TSN).
TSN improves the signal reconstruction performance of the deep neural network
designed for sparse regression by performing a tree search with pruning. It is
observed in both noiseless and noisy cases, TSN recovers synthetic and real
signals with lower complexity than a conventional tree search and is superior
to existing algorithms by a large margin for various types of the sensing
matrix $\Phi$, widely used in sparse regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00865</identifier>
 <datestamp>2019-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00865</id><created>2019-04-01</created><updated>2019-10-01</updated><authors><author><keyname>Guedj</keyname><forenames>Benjamin</forenames></author><author><keyname>Rengot</keyname><forenames>Juliette</forenames></author></authors><title>Non-linear aggregation of filters to improve image denoising</title><categories>stat.ML cs.CV cs.LG eess.IV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We introduce a novel aggregation method to efficiently perform image
denoising. Preliminary filters are aggregated in a non-linear fashion, using a
new metric of pixel proximity based on how the pool of filters reaches a
consensus. We provide a theoretical bound to support our aggregation scheme,
its numerical performance is illustrated and we show that the aggregate
significantly outperforms each of the preliminary filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00869</identifier>
 <datestamp>2019-05-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00869</id><created>2019-04-01</created><updated>2019-05-15</updated><authors><author><keyname>Yu</keyname><forenames>Wangyang</forenames></author><author><keyname>Kleijn</keyname><forenames>W. Bastiaan</forenames></author></authors><title>Room Geometry Estimation from Room Impulse Responses using Convolutional
  Neural Networks</title><categories>eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new method to estimate the geometry of a room given room
impulse responses. The method utilises convolutional neural networks to
estimate the room geometry and uses the mean square error as the loss function.
In contrast to existing methods, we do not require the position or distance of
sources or receivers in the room. The method can be used with only a single
room impulse response between one source and one receiver for room geometry
estimation. The proposed estimation method can achieve an average of six
centimetre accuracy. In addition, the proposed method is shown to be
computationally efficient compared to state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00920</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00920</id><created>2019-04-01</created><updated>2019-04-12</updated><authors><author><keyname>Heineken</keyname><forenames>Sigrid B.</forenames></author><author><keyname>Morillas</keyname><forenames>Patricia M.</forenames></author><author><keyname>Tarazaga</keyname><forenames>Pablo</forenames></author></authors><title>Balanced frames: a useful tool in signal processing with good properties</title><categories>math.FA eess.SP math.CA</categories><comments>Changes in the presentation of the paper</comments><msc-class>Primary 42C15, Secondary 15A03, 15A60, 94A05, 94A12, 94A13</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So far there has not been paid attention in the literature to frames that are
balanced, i.e. those frames which sum is zero. In this paper we consider
balanced frames, and in particular balanced unit norm tight frames in finite
dimensional Hilbert spaces. Unit norm tight frames play a central role in frame
theory and its applications. Here we discover the various advantages of
balanced unit norm tight frames in signal processing and show that they turn
out to perform better than the non balanced ones. They give an exact
reconstruction in the presence of systematic errors in the transmitted
coefficients, and are optimal when these coefficients are corrupted with noises
that can have non-zero mean. Moreover, using balanced frames we can easily know
that the transmitted coefficients were perturbed, and we also have an
indication of the source of the error. We analyze several properties of these
types of frames. In particular, we define an equivalence relation in the set of
the dual frames of a balanced frame, and use it to show that we can obtain
easily all the duals from the balanced ones. We study the problems of finding
the nearest balanced frame in the $\ell^{1}$ and $\ell^{2}$ norms to a given
frame, characterizing completely its existence and giving its expression.
Finally, we present many examples and methods for constructing balanced unit
norm tight frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.00928</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.00928</id><created>2019-04-01</created><authors><author><keyname>Maschietti</keyname><forenames>Flavio</forenames></author><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>Exploring the Trade-Off between Privacy and Coordination in Millimeter
  Wave Spectrum Sharing</title><categories>eess.SP cs.IT math.IT</categories><comments>16 pages, 4 figures. The short version of this paper has been
  accepted to IEEE ICC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The synergetic gains of spectrum sharing and millimeter wave communication
networks have recently attracted attention, owing to the interference canceling
benefits of highly-directional beamforming in such systems. In principle,
fine-tuned coordinated scheduling and beamforming can drastically reduce
cross-operator interference. However, this goes at the expense of the exchange
of global channel state information, which is not realistic in particular when
considering inter-operator coordination. Indeed, such an exchange of
information is expensive in terms of backhaul infrastructure, and besides, it
raises sensitive privacy issues between otherwise competing operators. In this
paper, we expose the existence of a trade-off between coordination and privacy.
We propose an algorithm capable of balancing spectrum sharing performance with
privacy preservation based on the sharing of a low-rate beam-related
information. Such information is subject to a data obfuscation mechanism
borrowed from the digital security literature so as to control the privacy,
measured in terms of information-theoretical equivocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01010</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01010</id><created>2019-04-01</created><authors><author><keyname>Ma</keyname><forenames>Shuang</forenames></author><author><keyname>Liu</keyname><forenames>Zhentao</forenames></author><author><keyname>Wang</keyname><forenames>Chenglong</forenames></author><author><keyname>Hu</keyname><forenames>Chenyu</forenames></author><author><keyname>Li</keyname><forenames>Enrong</forenames></author><author><keyname>Gong</keyname><forenames>Wenlin</forenames></author><author><keyname>Tong</keyname><forenames>Zhishen</forenames></author><author><keyname>Wu</keyname><forenames>Jianrong</forenames></author><author><keyname>Shen</keyname><forenames>Xia</forenames></author><author><keyname>Han</keyname><forenames>Shensheng</forenames></author></authors><title>Ghost imaging LiDAR via sparsity constraints using push-broom scanning</title><categories>eess.IV physics.optics</categories><doi>10.1364/OE.27.013219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ghost imaging LiDAR via sparsity constraints using push-broom scanning is
proposed. It can image the stationary target scene continuously along the
scanning direction by taking advantage of the relative movement between the
platform and the target scene. Compared to conventional ghost imaging LiDAR
that requires multiple speckle patterns staring the target, ghost imaging LiDAR
via sparsity constraints using push-broom scanning not only simplifies the
imaging system, but also reduces the sampling number. Numerical simulations and
experiments have demonstrated its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01014</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01014</id><created>2019-04-01</created><authors><author><keyname>Peeples</keyname><forenames>Joshua</forenames></author><author><keyname>Cook</keyname><forenames>Matthew</forenames></author><author><keyname>Suen</keyname><forenames>Daniel</forenames></author><author><keyname>Zare</keyname><forenames>Alina</forenames></author><author><keyname>Keller</keyname><forenames>James</forenames></author></authors><title>Comparison of Possibilistic Fuzzy Local Information C-Means and
  Possibilistic K-Nearest Neighbors for Synthetic Aperture Sonar Image
  Segmentation</title><categories>eess.IV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthetic aperture sonar (SAS) imagery can generate high resolution images of
the seafloor. Thus, segmentation algorithms can be used to partition the images
into different seafloor environments. In this paper, we compare two
possibilistic segmentation approaches. Possibilistic approaches allow for the
ability to detect novel or outlier environments as well as well known classes.
The Possibilistic Fuzzy Local Information C-Means (PFLICM) algorithm has been
previously applied to segment SAS imagery. Additionally, the Possibilistic
K-Nearest Neighbors (PKNN) algorithm has been used in other domains such as
landmine detection and hyperspectral imagery. In this paper, we compare the
segmentation performance of a semi-supervised approach using PFLICM and a
supervised method using Possibilistic K-NN. We include final segmentation
results on multiple SAS images and a quantitative assessment of each algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01071</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01071</id><created>2019-04-01</created><authors><author><keyname>Servin</keyname><forenames>Manuel</forenames></author><author><keyname>Padilla</keyname><forenames>Moises</forenames></author><author><keyname>Garnica</keyname><forenames>Guillermo</forenames></author><author><keyname>Paez</keyname><forenames>Gonzalo</forenames></author></authors><title>Fourier spectra for nonuniform phase-shifting algorithms based on
  principal component analysis</title><categories>eess.SP physics.optics</categories><comments>10 pages, 8 figures</comments><doi>10.1364/OE.27.025861</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an error-free, nonuniform phase-stepping algorithm (nPSA) based on
principal component analysis (PCA). PCA-based algorithms typically give
phase-demodulation errors when applied to nonuniform phase-shifted
interferograms. We present a straightforward way to correct those PCA
phase-demodulation errors. We give mathematical formulas to fully analyze
PCA-based nPSA (PCA-nPSA). These formulas give a) the PCA-nPSA frequency
transfer function (FTF), b) its corrected Lissajous figure, c) the corrected
PCA-nPSA formula, d) its harmonic robustness, and e) its signal-to-noise-ratio
(SNR). We show that the PCA-nPSA can be seen as a linear quadrature filter, and
as consequence, one can find its FTF. Using the FTF, we show why plain PCA
often fails to demodulate nonuniform phase-shifted fringes. Previous works on
PCA-nPSA (without FTF), give specific numerical/experimental fringe data to
&quot;visually demonstrate&quot; that their new nPSA works better than competitors. This
often leads to biased/favorable fringe pattern selections which &quot;visually
demonstrate&quot; the superior performance of their new nPSA. This biasing is herein
totally avoided because we provide figures-of-merit formulas based on linear
systems and stochastic process theories. However, and for illustrative purposes
only, we provide specific fringe data phase-demodulation, including
comprehensive analysis and comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01112</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01112</id><created>2019-04-01</created><authors><author><keyname>Knoll</keyname><forenames>Florian</forenames></author><author><keyname>Hammernik</keyname><forenames>Kerstin</forenames></author><author><keyname>Zhang</keyname><forenames>Chi</forenames></author><author><keyname>Moeller</keyname><forenames>Steen</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author><author><keyname>Sodickson</keyname><forenames>Daniel K.</forenames></author><author><keyname>Akcakaya</keyname><forenames>Mehmet</forenames></author></authors><title>Deep Learning Methods for Parallel Magnetic Resonance Image
  Reconstruction</title><categories>eess.SP cs.CV cs.LG eess.IV</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the success of deep learning in a wide range of applications,
neural network-based machine learning techniques have received interest as a
means of accelerating magnetic resonance imaging (MRI). A number of ideas
inspired by deep learning techniques from computer vision and image processing
have been successfully applied to non-linear image reconstruction in the spirit
of compressed sensing for both low dose computed tomography and accelerated
MRI. The additional integration of multi-coil information to recover missing
k-space lines in the MRI reconstruction process, is still studied less
frequently, even though it is the de-facto standard for currently used
accelerated MR acquisitions. This manuscript provides an overview of the recent
machine learning approaches that have been proposed specifically for improving
parallel imaging. A general background introduction to parallel MRI is given
that is structured around the classical view of image space and k-space based
methods. Both linear and non-linear methods are covered, followed by a
discussion of recent efforts to further improve parallel imaging using machine
learning, and specifically using artificial neural networks. Image-domain based
techniques that introduce improved regularizers are covered as well as k-space
based methods, where the focus is on better interpolation strategies using
neural networks. Issues and open problems are discussed as well as recent
efforts for producing open datasets and benchmarks for the community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01120</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01120</id><created>2019-04-01</created><authors><author><keyname>Lai</keyname><forenames>Cheng-I</forenames></author><author><keyname>Chen</keyname><forenames>Nanxin</forenames></author><author><keyname>Villalba</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Dehak</keyname><forenames>Najim</forenames></author></authors><title>ASSERT: Anti-Spoofing with Squeeze-Excitation and Residual neTworks</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present JHU's system submission to the ASVspoof 2019 Challenge:
Anti-Spoofing with Squeeze-Excitation and Residual neTworks (ASSERT).
Anti-spoofing has gathered more and more attention since the inauguration of
the ASVspoof Challenges, and ASVspoof 2019 dedicates to address attacks from
all three major types: text-to-speech, voice conversion, and replay. Built upon
previous research work on Deep Neural Network (DNN), ASSERT is a pipeline for
DNN-based approach to anti-spoofing. ASSERT has four components: feature
engineering, DNN models, network optimization and system combination, where the
DNN models are variants of squeeze-excitation and residual networks. We
conducted an ablation study of the effectiveness of each component on the
ASVspoof 2019 corpus, and experimental results showed that ASSERT obtained more
than 93% and 17% relative improvements over the baseline systems in the two
sub-challenges in ASVspooof 2019, ranking ASSERT one of the top performing
systems. Code and pretrained models will be made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01156</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01156</id><created>2019-04-01</created><authors><author><keyname>Kargas</keyname><forenames>Nikos</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Nicholas D.</forenames></author></authors><title>Learning Mixtures of Smooth Product Distributions: Identifiability and
  Algorithm</title><categories>eess.SP cs.LG stat.ML</categories><comments>accepted to appear in AISTATS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning a mixture model of non-parametric product
distributions. The problem of learning a mixture model is that of finding the
component distributions along with the mixing weights using observed samples
generated from the mixture. The problem is well-studied in the parametric
setting, i.e., when the component distributions are members of a parametric
family -- such as Gaussian distributions. In this work, we focus on
multivariate mixtures of non-parametric product distributions and propose a
two-stage approach which recovers the component distributions of the mixture
under a smoothness condition. Our approach builds upon the identifiability
properties of the canonical polyadic (low-rank) decomposition of tensors, in
tandem with Fourier and Shannon-Nyquist sampling staples from signal
processing. We demonstrate the effectiveness of the approach on synthetic and
real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01167</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01167</id><created>2019-04-01</created><authors><author><keyname>Kim</keyname><forenames>Dongsun</forenames></author><author><keyname>Lee</keyname><forenames>Jemin</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>Multi-layer Unmanned Aerial Vehicle Networks: Modeling and Performance
  Analysis</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since various types of unmanned aerial vehicles (UAVs) with different
hardware capabilities are introduced, we establish a foundation for the
multi-layer aerial network (MAN). First, the MAN is modeled as K layer ANs, and
each layer has UAVs with different densities, floating altitudes, and
transmission power. To make the framework applicable for various scenarios in
MAN, we consider the transmitter- and the receiver-oriented node association
rules as well as the air-to-ground and air-to-air channel models, which form
line of sight links with a location-dependent probability. We then newly
analyze the association probability, the main link distance distribution,
successful transmission probability (STP), and area spectral efficiency (ASE)
of MAN. The upper bounds of the optimal densities that maximize STP and ASE are
also provided. Finally, in the numerical results, we show the optimal UAV
densities of an AN that maximize the ASE and the STP decrease with the altitude
of the network. We also show that when the total UAV density is fixed for two
layer AN, the use of single layer in higher(lower) altitude only for all UAVs
can achieve better performance for low(high) total density case, otherwise,
distributing UAVs in two layers, i.e., MAN, achieves better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01269</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01269</id><created>2019-04-02</created><authors><author><keyname>Imoscopi</keyname><forenames>Stefano</forenames></author><author><keyname>Grancharov</keyname><forenames>Volodya</forenames></author><author><keyname>Sverrisson</keyname><forenames>Sigurdur</forenames></author><author><keyname>Karlsson</keyname><forenames>Erlendur</forenames></author><author><keyname>Pobloth</keyname><forenames>Harald</forenames></author></authors><title>Experiments on Open-Set Speaker Identification with Discriminatively
  Trained Neural Networks</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a study on discriminative artificial neural network
classifiers in the context of open-set speaker identification. Both 2-class and
multi-class architectures are tested against the conventional Gaussian mixture
model based classifier on enrolled speaker sets of different sizes. The
performance evaluation shows that the multi-class neural network system has
superior performance for large population sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01340</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01340</id><created>2019-04-02</created><authors><author><keyname>Drude</keyname><forenames>Lukas</forenames></author><author><keyname>Hasenklever</keyname><forenames>Daniel</forenames></author><author><keyname>Haeb-Umbach</keyname><forenames>Reinhold</forenames></author></authors><title>Unsupervised training of a deep clustering model for multichannel blind
  source separation</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a training scheme to train neural network-based source separation
algorithms from scratch when parallel clean data is unavailable. In particular,
we demonstrate that an unsupervised spatial clustering algorithm is sufficient
to guide the training of a deep clustering system. We argue that previous work
on deep clustering requires strong supervision and elaborate on why this is a
limitation. We demonstrate that (a) the single-channel deep clustering system
trained according to the proposed scheme alone is able to achieve a similar
performance as the multi-channel teacher in terms of word error rates and (b)
initializing the spatial clustering approach with the deep clustering result
yields a relative word error rate reduction of 26 % over the unsupervised
teacher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01499</identifier>
 <datestamp>2019-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01499</id><created>2019-04-02</created><authors><author><keyname>Liu</keyname><forenames>Fengjiao</forenames></author><author><keyname>Morse</keyname><forenames>A. Stephen</forenames></author></authors><title>On the Existence of a Fixed Spectrum for a Multi-channel Linear System:
  A Matroid Theory Approach</title><categories>cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditions for the existence of a fixed spectrum \{i.e., the set of fixed
modes\} for a multi-channel linear system have been known for a long time. The
aim of this paper is to reestablish one of these conditions using a new and
transparent approach based on matroid theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01504</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01504</id><created>2019-03-08</created><authors><author><keyname>Lynggaard</keyname><forenames>Per</forenames></author></authors><title>Smart Home Wireless Sensor Nodes Addressing the Challenges using Smart
  Objects and Artificial Intelligence</title><categories>cs.NI eess.SP</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Smart homes are further development of intelligent buildings and home
automation, where context awareness and autonomous behaviour are added. They
are based on a combination of the Internet and emerging technologies like
wireless sensor nodes. These wireless sensor nodes are challenging because they
consume battery power, they use network bandwidth, and they produce wireless
interferences. Currently, different methods exist for handling these
challenges. These methods are, however, based on adjusting the transmitter
frequency and using duty-cycling in combination with sleep mode approaches.
This paper introduces an approach that considerably lowers the wireless sensor
node power consumption and the amount of transmitted sensor events. It uses
smart objects that include artificial intelligence to efficiently process the
sensor event on location and thereby saves the costly wireless transportation
of these events. In this paper it has been shown that this approach provides
huge savings of power consumption and network load, which in turn reduces the
interference level
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01505</identifier>
 <datestamp>2020-02-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01505</id><created>2019-04-02</created><updated>2020-02-05</updated><authors><author><keyname>Liu</keyname><forenames>Fengjiao</forenames></author><author><keyname>Morse</keyname><forenames>A. Stephen</forenames></author></authors><title>Structural Completeness of a Multi-channel Linear System with Dependent
  Parameters</title><categories>eess.SP cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that the &quot;fixed spectrum&quot; {i.e., the set of fixed modes} of
a multi-channel linear system plays a central role in the stabilization of such
a system with decentralized control. A parameterized multi-channel linear
system is said to be &quot;structurally complete&quot; if it has no fixed spectrum for
almost all parameter values. Necessary and sufficient algebraic conditions are
presented for a multi-channel linear system with dependent parameters to be
structurally complete. An equivalent graphical condition is also given for a
certain type of parameterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01509</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01509</id><created>2019-04-02</created><authors><author><keyname>Yan</keyname><forenames>Yanfu</forenames></author><author><keyname>Lu</keyname><forenames>Ke</forenames></author><author><keyname>Xue</keyname><forenames>Jian</forenames></author><author><keyname>Gao</keyname><forenames>Pengcheng</forenames></author><author><keyname>Lyu</keyname><forenames>Jiayi</forenames></author></authors><title>FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D
  Facial Animation</title><categories>cs.LG cs.CV cs.GR eess.IV stat.ML</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial expression analysis based on machine learning requires large number of
well-annotated data to reflect different changes in facial motion. Publicly
available datasets truly help to accelerate research in this area by providing
a benchmark resource, but all of these datasets, to the best of our knowledge,
are limited to rough annotations for action units, including only their
absence, presence, or a five-level intensity according to the Facial Action
Coding System. To meet the need for videos labeled in great detail, we present
a well-annotated dataset named FEAFA for Facial Expression Analysis and 3D
Facial Animation. One hundred and twenty-two participants, including children,
young adults and elderly people, were recorded in real-world conditions. In
addition, 99,356 frames were manually labeled using Expression Quantitative
Tool developed by us to quantify 9 symmetrical FACS action units, 10
asymmetrical (unilateral) FACS action units, 2 symmetrical FACS action
descriptors and 2 asymmetrical FACS action descriptors, and each action unit or
action descriptor is well-annotated with a floating point number between 0 and
1. To provide a baseline for use in future research, a benchmark for the
regression of action unit values based on Convolutional Neural Networks are
presented. We also demonstrate the potential of our FEAFA dataset for 3D facial
animation. Almost all state-of-the-art algorithms for facial animation are
achieved based on 3D face reconstruction. We hence propose a novel method that
drives virtual characters only based on action unit value regression of the 2D
video frames of source actors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01520</identifier>
 <datestamp>2019-04-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01520</id><created>2019-03-25</created><authors><author><keyname>Tsompanas</keyname><forenames>Michail-Antisthenis</forenames></author><author><keyname>Fullarton</keyname><forenames>Claire</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Belousov-Zhabotinsky liquid marbles in robot control</title><categories>cs.RO eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to control the movement of a wheeled robot using on-board liquid
marbles made of Belousov-Zhabotinsky solution droplets coated with polyethylene
powder. Two stainless steel, iridium coated electrodes were inserted in a
marble and the electrical potential recorded was used to control the robot's
motor. We stimulated the marble with a laser beam. It responded to the
stimulation by pronounced changes in the electrical potential output. The
electrical output was detected by robot. The robot was changing its trajectory
in response to the stimulation. The results open new horizons for applications
for oscillatory chemical reactions in robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01537</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01537</id><created>2019-04-02</created><authors><author><keyname>Maiti</keyname><forenames>Soumi</forenames></author><author><keyname>Mandel</keyname><forenames>Michael I</forenames></author></authors><title>Speech denoising by parametric resynthesis</title><categories>eess.AS cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes the use of clean speech vocoder parameters as the target
for a neural network performing speech enhancement. These parameters have been
designed for text-to-speech synthesis so that they both produce high-quality
resyntheses and also are straightforward to model with neural networks, but
have not been utilized in speech enhancement until now. In comparison to a
matched text-to-speech system that is given the ground truth transcripts of the
noisy speech, our model is able to produce more natural speech because it has
access to the true prosody in the noisy speech. In comparison to two denoising
systems, the oracle Wiener mask and a DNN-based mask predictor, our model
equals the oracle Wiener mask in subjective quality and intelligibility and
surpasses the realistic system. A vocoder-based upper bound shows that there is
still room for improvement with this approach beyond the oracle Wiener mask. We
test speaker-dependence with two speakers and show that a single model can be
used for multiple speakers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01574</identifier>
 <datestamp>2019-08-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01574</id><created>2019-04-01</created><updated>2019-08-13</updated><authors><author><keyname>Kofler</keyname><forenames>Andreas</forenames></author><author><keyname>Dewey</keyname><forenames>Marc</forenames></author><author><keyname>Schaeffter</keyname><forenames>Tobias</forenames></author><author><keyname>Wald</keyname><forenames>Christian</forenames></author><author><keyname>Kolbitsch</keyname><forenames>Christoph</forenames></author></authors><title>Spatio-Temporal Deep Learning-Based Undersampling Artefact Reduction for
  2D Radial Cine MRI with Limited Data</title><categories>eess.IV cs.LG stat.ML</categories><comments>To be published in IEEE Transactions on Medical Imaging</comments><doi>10.1109/TMI.2019.2930318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we reduce undersampling artefacts in two-dimensional ($2D$)
golden-angle radial cine cardiac MRI by applying a modified version of the
U-net. We train the network on $2D$ spatio-temporal slices which are previously
extracted from the image sequences. We compare our approach to two $2D$ and a
$3D$ Deep Learning-based post processing methods and to three iterative
reconstruction methods for dynamic cardiac MRI. Our method outperforms the $2D$
spatially trained U-net and the $2D$ spatio-temporal U-net. Compared to the
$3D$ spatio-temporal U-net, our method delivers comparable results, but with
shorter training times and less training data. Compared to the Compressed
Sensing-based methods $kt$-FOCUSS and a total variation regularised
reconstruction approach, our method improves image quality with respect to all
reported metrics. Further, it achieves competitive results when compared to an
iterative reconstruction method based on adaptive regularization with
Dictionary Learning and total variation, while only requiring a small fraction
of the computational time. A persistent homology analysis demonstrates that the
data manifold of the spatio-temporal domain has a lower complexity than the
spatial domain and therefore, the learning of a projection-like mapping is
facilitated. Even when trained on only one single subject without
data-augmentation, our approach yields results which are similar to the ones
obtained on a large training dataset. This makes the method particularly
suitable for training a network on limited training data. Finally, in contrast
to the spatial $2D$ U-net, our proposed method is shown to be naturally robust
with respect to image rotation in image space and almost achieves
rotation-equivariance where neither data-augmentation nor a particular network
design are required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01575</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01575</id><created>2019-04-01</created><authors><author><keyname>Lai</keyname><forenames>Cheng-I</forenames></author></authors><title>Contrastive Predictive Coding Based Feature for Automatic Speaker
  Verification</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis describes our ongoing work on Contrastive Predictive Coding (CPC)
features for speaker verification. CPC is a recently proposed representation
learning framework based on predictive coding and noise contrastive estimation.
We focus on incorporating CPC features into the standard automatic speaker
verification systems, and we present our methods, experiments, and analysis.
This thesis also details necessary background knowledge in past and recent work
on automatic speaker verification systems, conventional speech features, and
the motivation and techniques behind CPC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01624</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01624</id><created>2019-04-02</created><authors><author><keyname>Parthasarathi</keyname><forenames>Sree Hari Krishnan</forenames></author><author><keyname>Strom</keyname><forenames>Nikko</forenames></author></authors><title>Lessons from Building Acoustic Models with a Million Hours of Speech</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>&quot;Copyright 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works.&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a report of our lessons learned building acoustic models from 1
Million hours of unlabeled speech, while labeled speech is restricted to 7,000
hours. We employ student/teacher training on unlabeled data, helping scale out
target generation in comparison to confidence model based methods, which
require a decoder and a confidence model. To optimize storage and to
parallelize target generation, we store high valued logits from the teacher
model. Introducing the notion of scheduled learning, we interleave learning on
unlabeled and labeled data. To scale distributed training across a large number
of GPUs, we use BMUF with 64 GPUs, while performing sequence training only on
labeled data with gradient threshold compression SGD using 16 GPUs. Our
experiments show that extremely large amounts of data are indeed useful; with
little hyper-parameter tuning, we obtain relative WER improvements in the 10 to
20% range, with higher gains in noisier conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01627</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01627</id><created>2019-03-29</created><authors><author><keyname>Artner</keyname><forenames>Gerald</forenames></author></authors><title>Channel Static Antennas</title><categories>eess.SP physics.app-ph</categories><comments>submitted to IEEE Trans Ant Propag</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possibility to keep wireless communication channels static is
investigated. When an antenna is moved away from its position, this will in
general cause the channel to change. Considerations suggest that wireless
communication channels can be kept static by performing a counter-movement of
the antenna to keep it in its original position relative to outside observers.
Feasibility is shown for a platform moving in straight motion over a finite
distance. The channel is kept static by countering the platform's movement with
physical movement of the antenna in the opposite direction. The experiment is
conducted with a quarter-wavelength monopole antenna in the gigahertz range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01638</identifier>
 <datestamp>2019-07-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01638</id><created>2019-04-02</created><authors><author><keyname>Yao</keyname><forenames>Li</forenames></author><author><keyname>Prosky</keyname><forenames>Jordan</forenames></author><author><keyname>Covington</keyname><forenames>Ben</forenames></author><author><keyname>Lyman</keyname><forenames>Kevin</forenames></author></authors><title>A Strong Baseline for Domain Adaptation and Generalization in Medical
  Imaging</title><categories>cs.CV cs.AI eess.IV stat.ML</categories><comments>Extended abstract of a journal submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work provides a strong baseline for the problem of multi-source
multi-target domain adaptation and generalization in medical imaging. Using a
diverse collection of ten chest X-ray datasets, we empirically demonstrate the
benefits of training medical imaging deep learning models on varied patient
populations for generalization to out-of-sample domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01696</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01696</id><created>2019-04-02</created><authors><author><keyname>Sokolov</keyname><forenames>Volodymyr Yu.</forenames></author></authors><title>Comparison of Possible Approaches for the Development of Low-Budget
  Spectrum Analyzers for Sensor Networks in the Range of 2.4-2.5 GHz</title><categories>eess.SP cs.NI</categories><comments>in Ukrainian</comments><acm-class>C.2.1; I.2.9</acm-class><journal-ref>Cybersecurity: Education, Science, Technique (ISSN: 2663-4023),
  no. 2, 2018</journal-ref><doi>10.28925/2663-4023.2018.2.3146</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The article deals with the development, implementation and research of the
spectrum analyzers that can be used in sensor networks and Internet systems of
things. As an operating frequency range, 2.4-2.5 GHz ISM is selected. At the
stage of hardware selection, a comparative analysis of existing available
microcontrollers for the analysis of the spectrum, the choice of hardware
interfaces, the ordering of the required modules and electrical components, as
well as the input control is carried out. During development, several variants
of spectrum analyzers on the basis of microcontroller and TI Chipcon CC2500
microcontrollers with USB interfaces, as well as Cypress CYWUSB6935 modules
with LPT and USB interfaces, have been implemented. At the development stage,
the development of the printed circuit board, its fabrication, component
assembly, microcontroller programming, the verification of the assembly's
robustness, making corrections, connecting to a personal computer and assembly
in the case have been carried out. An analysis of existing software for
collecting information on the state of the wireless broadcast is also
conducted. According to the results of comparative experiments of various
collections of spectrum analyzers, spectrographs for different types of signals
were obtained. On these typical spectrographs a comparative analysis of the
work of various prototypes was conducted. The offered approaches to building
sensors on the basis of spectrum analyzers allow to create low-power modules
for embedding in existing wireless information networks of enterprises for
prevention of inter-channel interference and ensuring the integrity of data
transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01740</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01740</id><created>2019-04-02</created><updated>2019-04-04</updated><authors><author><keyname>Hernandez-Ortega</keyname><forenames>Javier</forenames></author><author><keyname>Galbally</keyname><forenames>Javier</forenames></author><author><keyname>Fierrez</keyname><forenames>Julian</forenames></author><author><keyname>Haraksim</keyname><forenames>Rudolf</forenames></author><author><keyname>Beslay</keyname><forenames>Laurent</forenames></author></authors><title>FaceQnet: Quality Assessment for Face Recognition based on Deep Learning</title><categories>cs.CV cs.LG eess.IV</categories><comments>Preprint version of a paper accepted at ICB 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a Quality Assessment approach for face recognition
based on deep learning. The method consists of a Convolutional Neural Network,
FaceQnet, that is used to predict the suitability of a specific input image for
face recognition purposes. The training of FaceQnet is done using the VGGFace2
database. We employ the BioLab-ICAO framework for labeling the VGGFace2 images
with quality information related to their ICAO compliance level. The
groundtruth quality labels are obtained using FaceNet to generate comparison
scores. We employ the groundtruth data to fine-tune a ResNet-based CNN, making
it capable of returning a numerical quality measure for each input image.
Finally, we verify if the FaceQnet scores are suitable to predict the expected
performance when employing a specific image for face recognition with a COTS
face recognition system. Several conclusions can be drawn from this work, most
notably: 1) we managed to employ an existing ICAO compliance framework and a
pretrained CNN to automatically label data with quality information, 2) we
trained FaceQnet for quality estimation by fine-tuning a pre-trained face
recognition network (ResNet-50), and 3) we have shown that the predictions from
FaceQnet are highly correlated with the face recognition accuracy of a
state-of-the-art commercial system not used during development. FaceQnet is
publicly available in GitHub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01760</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01760</id><created>2019-04-03</created><authors><author><keyname>Chan</keyname><forenames>Raymond</forenames></author><author><keyname>Yang</keyname><forenames>Hongfei</forenames></author><author><keyname>Zeng</keyname><forenames>Tieyong</forenames></author></authors><title>Total Variation and Tight Frame Image Segmentation with Intensity
  Inhomogeneity</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation is an important task in the domain of computer vision and
medical imaging. In natural and medical images, intensity inhomogeneity, i.e.
the varying image intensity, occurs often and it poses considerable challenges
for image segmentation. In this paper, we propose an efficient variational
method for segmenting images with intensity inhomogeneity. The method is
inspired by previous works on two-stage segmentation and variational Retinex.
Our method consists of two stages. In the first stage, we decouple the image
into reflection and illumination parts by solving a convex energy minimization
model with either total variation or tight-frame regularisation. In the second
stage, we segment the original image by thresholding on the reflection part,
and the inhomogeneous intensity is estimated by the smoothly varying
illumination part. We adopt a primal dual algorithm to solve the convex model
in the first stage, and the convergence is guaranteed. Numerical experiments
clearly show that our method is robust and efficient to segment both natural
and medical images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01775</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01775</id><created>2019-04-03</created><authors><author><keyname>Somandepalli</keyname><forenames>Krishna</forenames></author><author><keyname>Kumar</keyname><forenames>Naveen</forenames></author><author><keyname>Travadi</keyname><forenames>Ruchir</forenames></author><author><keyname>Narayanan</keyname><forenames>Shrikanth</forenames></author></authors><title>Multimodal Representation Learning using Deep Multiset Canonical
  Correlation</title><categories>cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Deep Multiset Canonical Correlation Analysis (dMCCA) as an
extension to representation learning using CCA when the underlying signal is
observed across multiple (more than two) modalities. We use deep learning
framework to learn non-linear transformations from different modalities to a
shared subspace such that the representations maximize the ratio of between-
and within-modality covariance of the observations. Unlike linear discriminant
analysis, we do not need class information to learn these representations, and
we show that this model can be trained for complex data using mini-batches.
Using synthetic data experiments, we show that dMCCA can effectively recover
the common signal across the different modalities corrupted by multiplicative
and additive noise. We also analyze the sensitivity of our model to recover the
correlated components with respect to mini-batch size and dimension of the
embeddings. Performance evaluation on noisy handwritten datasets shows that our
model outperforms other CCA-based approaches and is comparable to deep neural
network models trained end-to-end on this dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01864</identifier>
 <datestamp>2019-06-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01864</id><created>2019-04-03</created><authors><author><keyname>Zaman</keyname><forenames>Bakht</forenames></author><author><keyname>Ramos</keyname><forenames>Luis Miguel Lopez</forenames></author><author><keyname>Romero</keyname><forenames>Daniel</forenames></author><author><keyname>Beferull-Lozano</keyname><forenames>Baltasar</forenames></author></authors><title>Online Topology Identification from Vector Autoregressive Time Series</title><categories>eess.SP stat.ML</categories><comments>13 pages, 6 figures, submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to their capacity to condense the spatiotemporal structure of a data set
in a format amenable for human interpretation, forecasting, and anomaly
detection, causality graphs are routinely estimated in social sciences, natural
sciences, and engineering. A popular approach to mathematically formalize
causality is based on vector autoregressive (VAR) models, which constitutes an
alternative to the well-known but usually intractable Granger causality.
Relying on such a VAR causality notion, this paper develops two algorithms with
complementary benefits to track time-varying causality graphs in an online
fashion. Despite using data in a sequential fashion, both algorithms are shown
to asymptotically attain the same average performance as a batch estimator with
all data available at once. Moreover, their constant complexity per update
renders these algorithms appealing for big-data scenarios. Theoretical and
experimental performance analysis support the merits of the proposed
algorithms. Remarkably, no probabilistic models or stationarity assumptions
need to be introduced, which endows the developed algorithms with considerable
generality
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01885</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01885</id><created>2019-04-03</created><authors><author><keyname>Aldababsa</keyname><forenames>Mahmoud</forenames></author><author><keyname>Kucur</keyname><forenames>O&#x11f;uz</forenames></author></authors><title>Majority Based TAS/MRC Scheme in Downlink NOMA Network with Channel
  Estimation Errors and Feedback Delay</title><categories>eess.SP</categories><comments>10 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The antenna selection (AS) in non-orthogonal multiple access (NOMA) networks
is still a challenging problem since finding optimal AS solution may not be
available for all channel realizations and has quite computational complexity
when it exists. For this reason, in this paper, we develop a new suboptimal
solution, majority based transmit antenna selection (TAS-maj), with significant
reduction in computational complexity. The TAS-maj basically selects the
transmit antenna with the majority. It is more efficient when compared to
previously proposed suboptimal AS algorithms, namely max-max-max (A^3) and
max-min-max (AIA) because these schemes are merely interested in optimizing the
performance of the strongest and weakest users, respectively at the price of
worse performance for the remaining users. On the other hand, the TAS-maj
scheme yields better performance for more than half of mobile users in the NOMA
networks. In this paper, we consider a multiple-input multiple-output
communication system, where all the nodes are equipped with multi-antenna.
Besides the TAS-maj is employed at the base station, a maximal ratio combining
(MRC) is also employed at each mobile user in order to achieve superior
performance. The impact of the channel estimation errors (CEEs) and feedback
delay (FD) on the performance of the TAS-maj/MRC scheme is studied in the NOMA
network over Nakagami-m fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01916</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01916</id><created>2019-04-03</created><authors><author><keyname>Vecchiotti</keyname><forenames>Paolo</forenames></author><author><keyname>Ma</keyname><forenames>Ning</forenames></author><author><keyname>Squartini</keyname><forenames>Stefano</forenames></author><author><keyname>Brown</keyname><forenames>Guy J.</forenames></author></authors><title>End-to-end Binaural Sound Localisation from the Raw Waveform</title><categories>cs.SD eess.AS</categories><comments>Accepted by ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel end-to-end binaural sound localisation approach is proposed which
estimates the azimuth of a sound source directly from the waveform. Instead of
employing hand-crafted features commonly employed for binaural sound
localisation, such as the interaural time and level difference, our end-to-end
system approach uses a convolutional neural network (CNN) to extract specific
features from the waveform that are suitable for localisation. Two systems are
proposed which differ in the initial frequency analysis stage. The first system
is auditory-inspired and makes use of a gammatone filtering layer, while the
second system is fully data-driven and exploits a trainable convolutional layer
to perform frequency analysis. In both systems, a set of dedicated
convolutional kernels are then employed to search for specific localisation
cues, which are coupled with a localisation stage using fully connected layers.
Localisation experiments using binaural simulation in both anechoic and
reverberant environments show that the proposed systems outperform a
state-of-the-art deep neural network system. Furthermore, our investigation of
the frequency analysis stage in the second system suggests that the CNN is able
to exploit different frequency bands for localisation according to the
characteristics of the reverberant environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.01949</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.01949</id><created>2019-04-01</created><authors><author><keyname>Ribeiro</keyname><forenames>Ant&#xf4;nio H.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Manoel Horta</forenames></author><author><keyname>Paix&#xe3;o</keyname><forenames>Gabriela M. M.</forenames></author><author><keyname>Oliveira</keyname><forenames>Derick M.</forenames></author><author><keyname>Gomes</keyname><forenames>Paulo R.</forenames></author><author><keyname>Canazart</keyname><forenames>J&#xe9;ssica A.</forenames></author><author><keyname>Ferreira</keyname><forenames>Milton P. S.</forenames></author><author><keyname>Andersson</keyname><forenames>Carl R.</forenames></author><author><keyname>Macfarlane</keyname><forenames>Peter W.</forenames></author><author><keyname>Meira</keyname><forenames>Wagner</forenames><suffix>Jr.</suffix></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Antonio Luiz P.</forenames></author></authors><title>Automatic Diagnosis of the Short-Duration 12-Lead ECG using a Deep
  Neural Network: the CODE Study</title><categories>cs.LG eess.SP stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1811.12194</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Deep Neural Network (DNN) model for predicting electrocardiogram
(ECG) abnormalities in short-duration 12-lead ECG recordings. The analysis of
the digital ECG obtained in a clinical setting can provide a full evaluation of
the cardiac electrical activity and have not been studied in an end-to-end
machine learning scenario. Using the database of the Telehealth Network of
Minas Gerais, under the scope of the CODE (Clinical Outcomes in Digital
Electrocardiology) study, we built a novel dataset with more than 2 million ECG
tracings, orders of magnitude larger than those used in previous studies.
Moreover, our dataset is more realistic, as it consists of 12-lead ECGs
recorded during standard in-clinic exams. Using this data, we trained a
residual neural network with 9 convolutional layers to map ECG signals with a
duration of 7 to 10 seconds into 6 different classes of ECG abnormalities.
High-performance measures were obtained for all ECG abnormalities, with F1
scores above $80\%$ and specificity indexes over $99\%$. We compare the
performance with cardiology and emergency resident medical doctors as well as
medical students and, considering the F1 score, the DNN matches or outperforms
the medical residents and students for all abnormalities. These results
indicate that end-to-end automatic ECG analysis based on DNNs, previously used
only in a single-lead setup, generalizes well to the 12-lead ECG. This is an
important result in that it takes this technology much closer to standard
clinical practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02080</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02080</id><created>2019-03-27</created><authors><author><keyname>Kozarcanin</keyname><forenames>Smail</forenames></author><author><keyname>Andresen</keyname><forenames>Gorm Bruun</forenames></author><author><keyname>Staffell</keyname><forenames>Iain</forenames></author></authors><title>Estimating country-specific space heating threshold temperatures from
  national consumption data</title><categories>physics.soc-ph eess.SP physics.app-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space heating in buildings is becoming a key element of sector-coupled energy
system research. Data availability limits efforts to model the buildings
sector, because heat consumption is not directly metered in most countries.
Space heating is often related to weather through the proxy of heating
degree-days using a specific heating threshold temperature, but methods vary
between studies. This study estimates country-specific heating threshold
temperatures using widely and publicly available consumption and weather data.
This allows for national climate and culture-specific human behaviour to be
captured in energy systems modelling. National electricity and gas consumption
data are related to degree-days through linear models, and Akaike's Information
Criteria is used to define the summer season in each country, when space
heating is not required. We find that the heating threshold temperatures
computed using daily, weekly and monthly aggregated consumption data are
statistically indifferent. In general, threshold temperatures for gas heating
centre around 15.0 +/- 1.7 degree C (daily averaged temperature), while heating
by electricity averages to 13.4 +/- 2.4 degree C. We find no evidence of space
heating during June, July and August, even if heating degree-days are present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02096</identifier>
 <datestamp>2019-12-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02096</id><created>2019-04-03</created><updated>2019-12-02</updated><authors><author><keyname>Yamamoto</keyname><forenames>Katsuhiko</forenames></author><author><keyname>Irino</keyname><forenames>Toshio</forenames></author><author><keyname>Araki</keyname><forenames>Shoko</forenames></author><author><keyname>Kinoshita</keyname><forenames>Keisuke</forenames></author><author><keyname>Nakatani</keyname><forenames>Tomohiro</forenames></author></authors><title>GEDI: Gammachirp Envelope Distortion Index for Predicting
  Intelligibility of Enhanced Speech</title><categories>cs.SD eess.AS</categories><comments>Preprint, 35 pages, 6 tables, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we propose a new concept, the gammachirp envelope distortion
index (GEDI), based on the signal-to-distortion ratio in the auditory envelope,
SDRenv to predict the intelligibility of speech enhanced by nonlinear
algorithms. The objective of GEDI is to calculate the distortion between
enhanced and clean-speech representations in the domain of a temporal envelope
extracted by the gammachirp auditory filterbank and modulation filterbank. We
also extend GEDI with multi-resolution analysis (mr-GEDI) to predict the speech
intelligibility of sounds under non-stationary noise conditions. We evaluate
GEDI in terms of speech intelligibility predictions of speech sounds enhanced
by a classic spectral subtraction and a Wiener filtering method. The
predictions are compared with human results for various signal-to-noise ratio
conditions with additive pink and babble noises. The results showed that
mr-GEDI predicted the intelligibility curves better than short-time objective
intelligibility (STOI) measure, extended-STOI (ESTOI) measure, and hearing-aid
speech perception index (HASPI) under pink-noise conditions, and better than
HASPI under babble-noise conditions. The mr-GEDI method does not present an
overestimation tendency and is considered a more conservative approach than
STOI and ESTOI. Therefore, the evaluation with mr-GEDI may provide additional
information in the development of speech enhancement algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02147</identifier>
 <datestamp>2019-04-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02147</id><created>2019-03-31</created><authors><author><keyname>Nguyen</keyname><forenames>Thai-Son</forenames></author><author><keyname>Stueker</keyname><forenames>Sebastian</forenames></author><author><keyname>Waibel</keyname><forenames>Alex</forenames></author></authors><title>Learning Shared Encoding Representation for End-to-End Speech
  Recognition Models</title><categories>eess.AS cs.LG cs.SD</categories><comments>arXiv admin note: substantial text overlap with arXiv:1902.01951</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we learn a shared encoding representation for a multi-task
neural network model optimized with connectionist temporal classification (CTC)
and conventional framewise cross-entropy training criteria. Our experiments
show that the multi-task training not only tackles the complexity of optimizing
CTC models such as acoustic-to-word but also results in significant improvement
compared to the plain-task training with an optimal setup. Furthermore, we
propose to use the encoding representation learned by the multi-task network to
initialize the encoder of attention-based models. Thereby, we train a deep
attention-based end-to-end model with 10 long short-term memory (LSTM) layers
of encoder which produces 12.2\% and 22.6\% word-error-rate on Switchboard and
CallHome subsets of the Hub5 2000 evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02243</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02243</id><created>2019-04-03</created><authors><author><keyname>Storey</keyname><forenames>Emily E</forenames></author><author><keyname>Helmy</keyname><forenames>Amr S.</forenames></author></authors><title>Optimized Preprocessing and Machine Learning for Quantitative Raman
  Spectroscopy in Biology</title><categories>eess.SP cs.LG q-bio.QM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Raman spectroscopy's capability to provide meaningful composition predictions
is heavily reliant on a pre-processing step to remove insignificant spectral
variation. This is crucial in biofluid analysis. Widespread adoption of
diagnostics using Raman requires a robust model which can withstand routine
spectra discrepancies due to unavoidable variations such as age, diet, and
medical background. A wealth of pre-processing methods are available, and it is
often up to trial-and-error or user experience to select the method which gives
the best results. This process can be incredibly time consuming and
inconsistent for multiple operators.
  In this study we detail a method to analyze the statistical variability
within a set of training spectra and determine suitability to form a robust
model. This allows us to selectively qualify or exclude a pre-processing
method, predetermine robustness, and simultaneously identify the number of
components which will form the best predictive model. We demonstrate the
ability of this technique to improve predictive models of two artificial
biological fluids.
  Raman spectroscopy is ideal for noninvasive, nondestructive analysis. Routine
health monitoring which maximizes comfort is increasingly crucial, particularly
in epidemic-level diabetes diagnoses. High variability in spectra of biological
samples can hinder Raman's adoption for these methods. Our technique allows the
decision of optimal pre-treatment method to be determined for the operator;
model performance is no longer a function of user experience. We foresee this
statistical technique being an instrumental element to widening the adoption of
Raman as a monitoring tool in a field of biofluid analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02334</identifier>
 <datestamp>2019-05-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02334</id><created>2019-04-03</created><authors><author><keyname>Scheibler</keyname><forenames>Robin</forenames></author><author><keyname>Ono</keyname><forenames>Nobutaka</forenames></author></authors><title>Multi-modal Blind Source Separation with Microphones and Blinkies</title><categories>cs.SD eess.AS</categories><comments>Accepted at IEEE ICASSP 2019, Brighton, UK. 5 pages. 3 figures</comments><doi>10.1109/ICASSP.2019.8682594</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a blind source separation algorithm that jointly exploits
measurements by a conventional microphone array and an ad hoc array of low-rate
sound power sensors called blinkies. While providing less information than
microphones, blinkies circumvent some difficulties of microphone arrays in
terms of manufacturing, synchronization, and deployment. The algorithm is
derived from a joint probabilistic model of the microphone and sound power
measurements. We assume the separated sources to follow a time-varying
spherical Gaussian distribution, and the non-negative power measurement
space-time matrix to have a low-rank structure. We show that alternating
updates similar to those of independent vector analysis and Itakura-Saito
non-negative matrix factorization decrease the negative log-likelihood of the
joint distribution. The proposed algorithm is validated via numerical
experiments. Its median separation performance is found to be up to 8 dB more
than that of independent vector analysis, with significantly reduced
variability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02436</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02436</id><created>2019-04-04</created><authors><author><keyname>McKinley</keyname><forenames>Richard</forenames></author><author><keyname>Rebsamen</keyname><forenames>Michael</forenames></author><author><keyname>Meier</keyname><forenames>Raphael</forenames></author><author><keyname>Reyes</keyname><forenames>Mauricio</forenames></author><author><keyname>Rummel</keyname><forenames>Christian</forenames></author><author><keyname>Wiest</keyname><forenames>Roland</forenames></author></authors><title>Few-shot brain segmentation from weakly labeled data with deep
  heteroscedastic multi-task networks</title><categories>cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In applications of supervised learning applied to medical image segmentation,
the need for large amounts of labeled data typically goes unquestioned. In
particular, in the case of brain anatomy segmentation, hundreds or thousands of
weakly-labeled volumes are often used as training data. In this paper, we first
observe that for many brain structures, a small number of training examples,
(n=9), weakly labeled using Freesurfer 6.0, plus simple data augmentation,
suffice as training data to achieve high performance, achieving an overall mean
Dice coefficient of $0.84 \pm 0.12$ compared to Freesurfer over 28 brain
structures in T1-weighted images of $\approx 4000$ 9-10 year-olds from the
Adolescent Brain Cognitive Development study. We then examine two varieties of
heteroscedastic network as a method for improving classification results. An
existing proposal by Kendall and Gal, which uses Monte-Carlo inference to learn
to predict the variance of each prediction, yields an overall mean Dice of
$0.85 \pm 0.14$ and showed statistically significant improvements over 25 brain
structures. Meanwhile a novel heteroscedastic network which directly learns the
probability that an example has been mislabeled yielded an overall mean Dice of
$0.87 \pm 0.11$ and showed statistically significant improvements over all but
one of the brain structures considered. The loss function associated to this
network can be interpreted as performing a form of learned label smoothing,
where labels are only smoothed where they are judged to be uncertain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02500</identifier>
 <datestamp>2020-01-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02500</id><created>2019-04-04</created><updated>2020-01-28</updated><authors><author><keyname>Harel</keyname><forenames>Nadav</forenames></author><author><keyname>Routtenberg</keyname><forenames>Tirza</forenames></author></authors><title>Low-Complexity Methods for Estimation After Parameter Selection</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical inference of multiple parameters often involves a preliminary
parameter selection stage. The selection stage has an impact on subsequent
estimation, for example by introducing a selection bias. The post-selection
maximum likelihood (PSML) estimator is shown to reduce the selection bias and
the post-selection mean-squared-error (PSMSE) compared with conventional
estimators, such as the maximum likelihood (ML) estimator.
  However, the computational complexity of the PSML is usually high due to the
multi-dimensional exhaustive search for a global maximum of the post-selection
log-likelihood (PSLL) function. Moreover, the PSLL involves the probability of
selection that, in general, does not have an analytical form. In this paper, we
develop new low-complexity post-selection estimation methods for a two-stage
estimation after parameter selection architecture.
  The methods are based on implementing the iterative maximization by parts
(MBP) approach, which is based on the decomposition of the PSLL function into
&quot;easily-optimized&quot; and complicated parts.
  The proposed second-best PSML method applies the MBP-PSML algorithm with a
pairwise probability of selection between the two highest-ranked parameters
w.r.t. the selection rule.
  The proposed SA-PSML method is based on using stochastic approximation (SA)
and Monte Carlo integrations to obtain a non-parametric estimation of the
gradient of the probability of selection and then applying the MBP-PSML
algorithm on this approximation. For low-complexity performance analysis, we
develop the empirical post-selection Cramer-Rao-type lower bound.
  Simulations demonstrate that the proposed post-selection estimation methods
are tractable and reduce both the bias and the PSMSE, compared with the ML
estimator, while only requiring moderate computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02529</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02529</id><created>2019-02-19</created><authors><author><keyname>Ito</keyname><forenames>Yuji</forenames></author><author><keyname>Tadokoro</keyname><forenames>Yukihiro</forenames></author></authors><title>Simple Design on Nanoscale Receivers Using CNT Cantilevers</title><categories>eess.SP physics.app-ph</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nanoscale receiver utilizing the cantilever of a carbon nanotube has been
developed to detect phase information included in transmitted signals. The
existing receiver consists of a phase detector and demodulator which employ a
reference wave and carrier signal, respectively. This paper presents a design
method to simplify the receiver in structure with enhancing the performance for
the phase detection. The reference wave or carrier signal is not needed in the
receiver via the proposed design method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02531</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02531</id><created>2019-03-31</created><authors><author><keyname>Sen</keyname><forenames>Arnesh</forenames></author><author><keyname>Das</keyname><forenames>Aishik</forenames></author><author><keyname>Das</keyname><forenames>Jayoti</forenames></author></authors><title>Software Based Pole-Zero Extraction Technique for nth Order Analog
  Filters</title><categories>eess.SP</categories><report-no>03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The positions of poles and zeros of a filter circuit indicates how the system
behaves with variation in frequency as well as it results a tool to utilize the
system gain by setting the frequency range in a complex process. The existing
solutions to extract the poles and zeros often include too complex calculations
to use it frequently. In this paper we propose a compact tool to detect the
position of poles and zeros of any practically designed analog filter of any
order without going through the working principle of the circuit as well as by
using this technique calculation of transfer function is does not need to do
derive which often becomes hectic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02532</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02532</id><created>2019-04-03</created><authors><author><keyname>Mursia</keyname><forenames>Placido</forenames></author><author><keyname>Atzeni</keyname><forenames>Italo</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Kobayashi</keyname><forenames>Mari</forenames></author></authors><title>D2D-Aided Multi-Antenna Multicasting</title><categories>eess.SP</categories><comments>6 pages, 5 figures, IEEE International Conference on Communications
  (ICC), 20-24 May 2019, Shanghai, China</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicast services, whereby a common valuable message needs to reach a whole
population of user equipments (UEs), are gaining attention on account of new
applications such as vehicular networks. As it proves challenging to guarantee
decodability by every UE in a large population, service reliability is indeed
the Achilles' heel of multicast transmissions. To circumvent this problem, a
two-phase protocol capitalizing on device-to-device (D2D) links between UEs has
been proposed, which overcomes the vanishing behavior of the multicast rate. In
this paper, we revisit such D2D-aided protocol in the new light of precoding
capabilities at the base station (BS). We obtain an enhanced scheme that aims
at selecting a subset of UEs who cooperate to spread the common message across
the rest of the network via D2D retransmissions. With the objective of
maximizing the multicast rate under some outage constraint, we propose an
algorithm with provable convergence that jointly identifies the most pertinent
relaying UEs and optimizes the precoding strategy at the BS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02570</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02570</id><created>2019-04-03</created><authors><author><keyname>Jayarajah</keyname><forenames>Kasthuri</forenames></author><author><keyname>Subbaraju</keyname><forenames>Vigneshwaran</forenames></author><author><keyname>Athaide</keyname><forenames>Noel</forenames></author><author><keyname>Meegahapola</keyname><forenames>Lakmal</forenames></author><author><keyname>Tan</keyname><forenames>Andrew</forenames></author><author><keyname>Misra</keyname><forenames>Archan</forenames></author></authors><title>Can multimodal sensing detect and localize transient events?</title><categories>eess.SP</categories><journal-ref>SPIE Defense+Security 2018</journal-ref><doi>10.1117/12.2322858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increased focus on making cities &quot;smarter&quot;, we see an upsurge in
investment in sensing technologies embedded in the urban infrastructure. The
deployment of GPS sensors aboard taxis and buses, smartcards replacing paper
tickets, and other similar initiatives have led to an abundance of data on
human mobility, generated at scale and available real-time. Further still,
users of social media platforms such as Twitter and LBSNs continue to
voluntarily share multimedia content revealing in-situ information on their
respective localities. The availability of such longitudinal multimodal data
not only allows for both the characterization of the dynamics of the city, but
also, in detecting anomalies, resulting from events (e.g., concerts) that
disrupt such dynamics, transiently. In this work, we investigate the
capabilities of such urban sensor modalities, both physical and social, in
detecting a variety of local events of varying intensities (e.g., concerts)
using statistical outlier detection techniques. We look at loading levels on
arriving bus stops, telecommunication records, and taxi trips, accrued via the
public APIs made available through the local transport authorities from
Singapore and New York City, and Twitter/Foursquare check-ins collected during
the same period, and evaluate against a set of events assimilated from multiple
event websites. In particular, we report on our early findings on (1) the
spatial impact evident via each modality (i.e., how far from the event venue is
the anomaly still present), and (2) the utility in combining decisions from the
collection of sensors using rudimentary fusion techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02583</identifier>
 <datestamp>2019-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02583</id><created>2019-04-04</created><updated>2019-09-19</updated><authors><author><keyname>Tachella</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Altmann</keyname><forenames>Yoann</forenames></author><author><keyname>M&#xe1;rquez</keyname><forenames>Miguel</forenames></author><author><keyname>Arguello-Fuentes</keyname><forenames>Henry</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author><author><keyname>McLaughlin</keyname><forenames>Stephen</forenames></author></authors><title>Bayesian 3D Reconstruction of Subsampled Multispectral Single-photon
  Lidar Signals</title><categories>eess.SP</categories><comments>code: https://gitlab.com/Tachella/musapop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Light detection and ranging (Lidar) single-photon devices capture range and
intensity information from a 3D scene. This modality enables long range 3D
reconstruction with high range precision and low laser power. A multispectral
single-photon Lidar system provides additional spectral diversity, allowing the
discrimination of different materials. However, the main drawback of such
systems can be the long acquisition time needed to collect enough photons in
each spectral band. In this work, we tackle this problem in two ways: first, we
propose a Bayesian 3D reconstruction algorithm that is able to find multiple
surfaces per pixel, using few photons, i.e., shorter acquisitions. In contrast
to previous algorithms, the novel method processes the jointly all the spectral
bands, obtaining better reconstructions using less photon detections. The
proposed model promotes spatial correlation between neighbouring points within
a given surface using spatial point processes. Secondly, we account for
different spatial and spectral subsampling schemes, which reduce the total
number of measurements, without significant degradation of the reconstruction
performance. In this way, the total acquisition time, memory requirements and
computational time can be significantly reduced. The experiments performed
using both synthetic and real single-photon Lidar data demonstrate the
advantages of tailored sampling schemes over random alternatives. Furthermore,
the proposed algorithm yields better estimates than other existing methods for
multi-surface reconstruction using multispectral Lidar data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02608</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02608</id><created>2019-04-04</created><authors><author><keyname>Song</keyname><forenames>Qiang</forenames></author><author><keyname>Pigeon</keyname><forenames>Yoran Eli</forenames></author><author><keyname>Heggarty</keyname><forenames>Kevin</forenames></author></authors><title>LED illumination faceted Fresnel DOEs generating perceived virtual 3D
  vision</title><categories>physics.optics eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach for the optimization and synthesis of a phase-only faceted
Fresnel type diffractive optical element (FDOE) generating 3D virtual images is
proposed. The FDOE is a transmissive Fresnel type DOE array, which produces the
perception of a customized floating 3D virtual image behind the FDOE when
illuminated with a divergent monochromatic Light Emitter Diode (LED) source.
Each DOE unit of the FDOE is optimized with a modified iterative Fourier
transform algorithm (M-IFTA). Each unit of the FDOE locally deflects the
incident light to the same position to form a designated view of the target
plane. The FDOE is fabricated using our home-built parallel write
photo-lithography machine. Numerical simulations and optical experiments are
performed to verify the proposed design method. Photos of the generated image
are also presented. This work can find applications in optical security,
anti-counterfeiting, and holographic display.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02644</identifier>
 <datestamp>2019-12-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02644</id><created>2019-04-04</created><updated>2019-04-05</updated><authors><author><keyname>Gordon</keyname><forenames>George S. D.</forenames></author><author><keyname>Gataric</keyname><forenames>Milana</forenames></author><author><keyname>Ramos</keyname><forenames>Alberto Gil C. P.</forenames></author><author><keyname>Mouthaan</keyname><forenames>Ralf</forenames></author><author><keyname>Williams</keyname><forenames>Calum</forenames></author><author><keyname>Yoon</keyname><forenames>Jonghee</forenames></author><author><keyname>Wilkinson</keyname><forenames>Timothy D.</forenames></author><author><keyname>Bohndiek</keyname><forenames>Sarah E.</forenames></author></authors><title>Characterising optical fibre transmission matrices using metasurface
  reflector stacks for lensless imaging without distal access</title><categories>physics.optics eess.IV</categories><comments>Main text: 38 pages, 9 Figures, Appendices: 26 pages, 6 Figures.
  Corrected author affiliation</comments><journal-ref>Phys. Rev. X 9, 041050 (2019)</journal-ref><doi>10.1103/PhysRevX.9.041050</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ability to form images through hair-thin optical fibres promises to open
up new applications from biomedical imaging to industrial inspection.
Unfortunately, deployment has been limited because small changes in mechanical
deformation (e.g. bending) and temperature can completely scramble optical
information, distorting images. Since such changes are dynamic, correcting them
requires measurement of the fibre transmission matrix (TM) in situ immediately
before imaging. TM calibration typically requires access to both the proximal
and distal facets of the fibre simultaneously, which is not feasible during
most realistic usage scenarios without compromising the thin form factor with
bulky distal optics. Here, we introduce a new approach to determine the TM of
multi-mode fibre (MMF) or multi-core fibre (MCF) in a reflection-mode
configuration without access to the distal facet. A thin stack of structured
metasurface reflectors is used at the distal facet to introduce
wavelength-dependent, spatially heterogeneous reflectance profiles. We derive a
first-order fibre model that compensates these wavelength-dependent changes in
the TM and show that, consequently, the reflected data at 3 wavelengths can be
used to unambiguously reconstruct the full TM by an iterative optimisation
algorithm. We then present a method for sample illumination and imaging
following TM reconstruction. Unlike previous approaches, our method does not
require the TM to be unitary making it applicable to physically realistic fibre
systems. We demonstrate TM reconstruction and imaging first using simulated
non-unitary fibres and noisy reflection matrices, then using much larger
experimentally-measured TMs of a densely-packed MCF, and finally on an
experimentally-measured multi-wavelength set of TMs recorded from a MMF. Our
findings pave the way for online transmission matrix calibration in situ in
hair-thin optical fibres
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02675</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02675</id><created>2019-04-04</created><authors><author><keyname>Jionghao</keyname><forenames>Wu</forenames></author></authors><title>UU-Nets Connecting Discriminator and Generator for Image to Image
  Translation</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Adversarial generative model have successfully manifest itself in image
synthesis. However, the performance deteriorate and unstable, because
discriminator is far stable than generator, and it is hard to control the game
between the two modules. Various methods have been introduced to tackle the
problem such as WGAN, Relativistic GAN and their successors by adding or
restricting the loss function, which certainly help balance the min-max game,
but they all focused on the loss function ignoring the intrinsic structure
limitation. We present a UU-Net architecture inspired by U-net bridging the
encoder and the decoder, UU-Net composed by two U-Net liked modules
respectively served as generator and discriminator. Because the modules in
U-net are symmetrical, therefore it shares weights easily between all four
components. Thanks to UU-net's modules identical and symmetric property, we
could not only carried the features from inner generator's encoder to its
decoder, but also to the discriminator's encoder and decoder. By this design,
it give us more control and condition flexibility to intervene the process
between the generator and the discriminator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02740</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02740</id><created>2019-04-04</created><authors><author><keyname>Viswanath</keyname><forenames>Sanjay</forenames></author><author><keyname>Arigovindan</keyname><forenames>Muthuvel</forenames></author></authors><title>Generalized Multi-Order Total Variation for Signal Restoration</title><categories>eess.SP</categories><comments>Draft: 14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Total Variation (TV) based regularization has been widely applied in
restoration problems due to its simple derivative filters based formulation and
robust performance. While first order TV suffers from staircase effect, second
order TV promotes piece-wise linear reconstructions. Generalized Multi-Order
Total Variation (GMO-TV) is proposed as a novel regularization method which
incorporates a new multivariate Laplacian prior on signal derivatives in a
non-quadratic regularization functional, that utilizes subtle
inter-relationship between multiple order derivatives. We also propose a
computational framework to automatically determine the weight parameters
associated with these derivative orders, rather than treating them as user
parameters. Using simulation results on ECG and EEG signals, we show that
GMO-TV performs better than related regularization functionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02790</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02790</id><created>2019-04-04</created><authors><author><keyname>Prateek</keyname><forenames>Nishant</forenames></author><author><keyname>&#x141;ajszczak</keyname><forenames>Mateusz</forenames></author><author><keyname>Barra-Chicote</keyname><forenames>Roberto</forenames></author><author><keyname>Drugman</keyname><forenames>Thomas</forenames></author><author><keyname>Lorenzo-Trueba</keyname><forenames>Jaime</forenames></author><author><keyname>Merritt</keyname><forenames>Thomas</forenames></author><author><keyname>Ronanki</keyname><forenames>Srikanth</forenames></author><author><keyname>Wood</keyname><forenames>Trevor</forenames></author></authors><title>In Other News: A Bi-style Text-to-speech Model for Synthesizing
  Newscaster Voice with Limited Data</title><categories>cs.CL cs.LG eess.AS</categories><comments>Accepted at NAACL-HLT 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural text-to-speech synthesis (NTTS) models have shown significant progress
in generating high-quality speech, however they require a large quantity of
training data. This makes creating models for multiple styles expensive and
time-consuming. In this paper different styles of speech are analysed based on
prosodic variations, from this a model is proposed to synthesise speech in the
style of a newscaster, with just a few hours of supplementary data. We pose the
problem of synthesising in a target style using limited data as that of
creating a bi-style model that can synthesise both neutral-style and
newscaster-style speech via a one-hot vector which factorises the two styles.
We also propose conditioning the model on contextual word embeddings, and
extensively evaluate it against neutral NTTS, and neutral concatenative-based
synthesis. This model closes the gap in perceived style-appropriateness between
natural recordings for newscaster-style of speech, and neutral speech synthesis
by approximately two-thirds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02816</identifier>
 <datestamp>2019-08-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02816</id><created>2019-04-04</created><updated>2019-08-15</updated><authors><author><keyname>Ravishankar</keyname><forenames>Saiprasad</forenames></author><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author><author><keyname>Fessler</keyname><forenames>Jeffrey A.</forenames></author></authors><title>Image Reconstruction: From Sparsity to Data-adaptive Methods and Machine
  Learning</title><categories>eess.IV cs.LG stat.ML</categories><comments>To appear in the Proceedings of the IEEE, Special Issue on Biomedical
  Imaging and Analysis in the Age of Sparsity, Big Data, and Deep Learning</comments><doi>10.1109/JPROC.2019.2936204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of medical image reconstruction has seen roughly four types of
methods. The first type tended to be analytical methods, such as filtered
back-projection (FBP) for X-ray computed tomography (CT) and the inverse
Fourier transform for magnetic resonance imaging (MRI), based on simple
mathematical models for the imaging systems. These methods are typically fast,
but have suboptimal properties such as poor resolution-noise trade-off for CT.
A second type is iterative reconstruction methods based on more complete models
for the imaging system physics and, where appropriate, models for the sensor
statistics. These iterative methods improved image quality by reducing noise
and artifacts. The FDA-approved methods among these have been based on
relatively simple regularization models. A third type of methods has been
designed to accommodate modified data acquisition methods, such as reduced
sampling in MRI and CT to reduce scan time or radiation dose. These methods
typically involve mathematical image models involving assumptions such as
sparsity or low-rank. A fourth type of methods replaces mathematically designed
models of signals and systems with data-driven or adaptive models inspired by
the field of machine learning. This paper focuses on the two most recent trends
in medical image reconstruction: methods based on sparsity or low-rank models,
and data-driven methods based on machine learning techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02828</identifier>
 <datestamp>2019-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02828</id><created>2019-04-03</created><updated>2019-10-15</updated><authors><author><keyname>Alsulami</keyname><forenames>Osama Zwaid</forenames></author><author><keyname>Alresheedi</keyname><forenames>Mohammed T.</forenames></author><author><keyname>Elmirghani</keyname><forenames>Jaafar M. H.</forenames></author></authors><title>Infrared uplink design for visible light communication (VLC) systems
  with beam steering</title><categories>eess.SP</categories><comments>arXiv admin note: substantial text overlap with arXiv:1903.10923,
  arXiv:1903.10925</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing uplink high data rate is one of the big concerns in visible light
communication (VLC) systems. This paper introduces an uplink VLC system based
on an infrared transmitter with beam steering to provide high data rates. In
this work, a 4 branches angle diversity receiver (ADR) is used and the
resultant delay spread and SNR are examined. The proposed system achieved data
rates up to 3.57 Gb/s using simple on-off-keying (OOK).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02836</identifier>
 <datestamp>2019-08-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02836</id><created>2019-04-04</created><updated>2019-08-14</updated><authors><author><keyname>Hafiz</keyname><forenames>Faeza</forenames></author><author><keyname>Shahnaz</keyname><forenames>Celia</forenames></author></authors><title>A Machine Learning Based Classification Approach for Power Quality
  Disturbances Exploiting Higher Order Statistics in the EMD Domain</title><categories>eess.SP</categories><comments>8 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to propose a new approach for the pattern
recognition of power quality (PQ) disturbances based on Empirical mode
decomposition (EMD) and $k$ Nearest Neighbor ($k$-NN) classifier. Since EMD
decomposes a signal into intrinsic mode functions (IMF) in time-domain with
same length of the original signal, it preserves the information that is hidden
in Fourier domain or in wavelet coefficients. In this proposed method, power
signals are decomposed into IMFs in EMD domain. Due to the presence of
non-linearity and noise on the original signal, it is hard to analyze them by
second order statistics. Thus, an effective feature set is developed
considering higher order statistics (HOS) like variance, skewness, and kurtosis
from the decomposed first three IMFs. This feature vector is fed into different
classifiers like $k$-NN, probabilistic neural network (PNN), and radial basis
function (RBF). Among all the classifiers, $k$-NN showed higher classification
accuracy and robustness both in training and testing to detect the PQ
disturbance events. Simulation results evaluated that the proposed HOS-EMD
based method along with $k$-NN classifier outperformed in terms of
classification accuracy and computational efficiency in comparison to the other
state-of-art methods both in clean and noisy environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02837</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02837</id><created>2019-04-04</created><authors><author><keyname>Marzi</keyname><forenames>Zhinus</forenames></author><author><keyname>Madhow</keyname><forenames>Upamanyu</forenames></author></authors><title>Interference management and capacity analysis for mm-wave picocells in
  urban canyons</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter (mm) wave picocellular networks are a promising approach for
delivering the 1000-fold capacity increase required to keep up with projected
demand for wireless data: the available bandwidth is orders of magnitude larger
than that in existing cellular systems, and the small carrier wavelength
enables the realization of highly directive antenna arrays in compact form
factor, thus drastically increasing spatial reuse. In this paper, we carry out
an interference analysis for mm-wave picocells in an urban canyon with a dense
deployment of base stations. Each base station sector can serve multiple
simultaneous users, which implies that both intra- and inter-cell interference
must be managed. We propose a \textit{cross-layer} approach to interference
management based on (i) suppressing interference at the physical layer and (ii)
managing the residual interference at the medium access control layer. We
provide an estimate of network capacity and establish that 1000-fold increase
relative to conventional LTE cellular networks is indeed feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02843</identifier>
 <datestamp>2019-07-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02843</id><created>2019-04-04</created><updated>2019-07-15</updated><authors><author><keyname>Khan</keyname><forenames>Shujaat</forenames></author><author><keyname>Huh</keyname><forenames>Jaeyoung</forenames></author><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author></authors><title>Deep Learning-based Universal Beamformer for Ultrasound Imaging</title><categories>eess.IV cs.CV cs.LG stat.ML</categories><comments>Accepted for MICCAI 2019. arXiv admin note: substantial text overlap
  with arXiv:1901.01706</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In ultrasound (US) imaging, individual channel RF measurements are
back-propagated and accumulated to form an image after applying specific
delays. While this time reversal is usually implemented using a hardware- or
software-based delay-and-sum (DAS) beamformer, the performance of DAS decreases
rapidly in situations where data acquisition is not ideal. Herein, for the
first time, we demonstrate that a single data-driven adaptive beamformer
designed as a deep neural network can generate high quality images robustly for
various detector channel configurations and subsampling rates. The proposed
deep beamformer is evaluated for two distinct acquisition schemes: focused
ultrasound imaging and planewave imaging. Experimental results showed that the
proposed deep beamformer exhibit significant performance gain for both focused
and planar imaging schemes, in terms of contrast-to-noise ratio and structural
similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02852</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02852</id><created>2019-04-04</created><authors><author><keyname>Narisetty</keyname><forenames>Chaitanya</forenames></author><author><keyname>Komatsu</keyname><forenames>Tatsuya</forenames></author><author><keyname>Kondo</keyname><forenames>Reishi</forenames></author></authors><title>Modelling of Sound Events with Hidden Imbalances Based on Clustering and
  Separate Sub-Dictionary Learning</title><categories>eess.AS cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an effective modelling of sound event spectra with a
hidden data-size-imbalance, for improved Acoustic Event Detection (AED). The
proposed method models each event as an aggregated representation of a few
latent factors, while conventional approaches try to find acoustic elements
directly from the event spectra. In the method, all the latent factors across
all events are assigned comparable importance and complexity to overcome the
hidden imbalance of data-sizes in event spectra. To extract latent factors in
each event, the proposed method employs clustering and performs non-negative
matrix factorization to each latent factor, and learns its acoustic elements as
a sub-dictionary. Separate sub-dictionary learning effectively models the
acoustic elements with limited data-sizes and avoids over-fitting due to hidden
imbalances in training data. For the task of polyphonic sound event detection
from DCASE 2013 challenge, an AED based on the proposed modelling achieves a
detection F-measure of 46.5%, a significant improvement of more than 19% as
compared to the existing state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02882</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02882</id><created>2019-04-05</created><authors><author><keyname>Zen</keyname><forenames>Heiga</forenames></author><author><keyname>Dang</keyname><forenames>Viet</forenames></author><author><keyname>Clark</keyname><forenames>Rob</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Weiss</keyname><forenames>Ron J.</forenames></author><author><keyname>Jia</keyname><forenames>Ye</forenames></author><author><keyname>Chen</keyname><forenames>Zhifeng</forenames></author><author><keyname>Wu</keyname><forenames>Yonghui</forenames></author></authors><title>LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech</title><categories>cs.SD eess.AS</categories><comments>Submitted for Interspeech 2019, 7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new speech corpus called &quot;LibriTTS&quot; designed for
text-to-speech use. It is derived from the original audio and text materials of
the LibriSpeech corpus, which has been used for training and evaluating
automatic speech recognition systems. The new corpus inherits desired
properties of the LibriSpeech corpus while addressing a number of issues which
make LibriSpeech less than ideal for text-to-speech work. The released corpus
consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers
and the corresponding texts. Experimental results show that neural end-to-end
TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion
scores in naturalness in five out of six evaluation speakers. The corpus is
freely available for download from http://www.openslr.org/60/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02892</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02892</id><created>2019-04-05</created><updated>2019-04-08</updated><authors><author><keyname>Tanaka</keyname><forenames>Kou</forenames></author><author><keyname>Kameoka</keyname><forenames>Hirokazu</forenames></author><author><keyname>Kaneko</keyname><forenames>Takuhiro</forenames></author><author><keyname>Hojo</keyname><forenames>Nobukatsu</forenames></author></authors><title>WaveCycleGAN2: Time-domain Neural Post-filter for Speech Waveform
  Generation</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Submitted to INTERSPEECH2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WaveCycleGAN has recently been proposed to bridge the gap between natural and
synthesized speech waveforms in statistical parametric speech synthesis and
provides fast inference with a moving average model rather than an
autoregressive model and high-quality speech synthesis with the adversarial
training. However, the human ear can still distinguish the processed speech
waveforms from natural ones. One possible cause of this distinguishability is
the aliasing observed in the processed speech waveform via down/up-sampling
modules. To solve the aliasing and provide higher quality speech synthesis, we
propose WaveCycleGAN2, which 1) uses generators without down/up-sampling
modules and 2) combines discriminators of the waveform domain and acoustic
parameter domain. The results show that the proposed method 1) alleviates the
aliasing well, 2) is useful for both speech waveforms generated by
analysis-and-synthesis and statistical parametric speech synthesis, and 3)
achieves a mean opinion score comparable to those of natural speech and speech
synthesized by WaveNet (open WaveNet) and WaveGlow while processing speech
samples at a rate of more than 150 kHz on an NVIDIA Tesla P100.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.02992</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.02992</id><created>2019-04-05</created><authors><author><keyname>Romero</keyname><forenames>Hector E.</forenames></author><author><keyname>Ma</keyname><forenames>Ning</forenames></author><author><keyname>Brown</keyname><forenames>Guy J.</forenames></author><author><keyname>Beeston</keyname><forenames>Amy V.</forenames></author><author><keyname>Hasan</keyname><forenames>Madina</forenames></author></authors><title>Deep Learning Features for Robust Detection of Acoustic Events in
  Sleep-Disordered Breathing</title><categories>eess.AS cs.SD</categories><comments>Accepted by IEEE ICASSP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sleep-disordered breathing (SDB) is a serious and prevalent condition, and
acoustic analysis via consumer devices (e.g. smartphones) offers a low-cost
solution to screening for it. We present a novel approach for the acoustic
identification of SDB sounds, such as snoring, using bottleneck features
learned from a corpus of whole-night sound recordings. Two types of bottleneck
features are described, obtained by applying a deep autoencoder to the output
of an auditory model or a short-term autocorrelation analysis. We investigate
two architectures for snore sound detection: a tandem system and a hybrid
system. In both cases, a `language model' (LM) was incorporated to exploit
information about the sequence of different SDB events. Our results show that
the proposed bottleneck features give better performance than conventional
mel-frequency cepstral coefficients, and that the tandem system outperforms the
hybrid system given the limited amount of labelled training data available. The
LM made a small improvement to the performance of both classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03001</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03001</id><created>2019-04-05</created><authors><author><keyname>Ma</keyname><forenames>Ning</forenames></author><author><keyname>May</keyname><forenames>Tobias</forenames></author><author><keyname>Brown</keyname><forenames>Guy J.</forenames></author></authors><title>Exploiting Deep Neural Networks and Head Movements for Robust Binaural
  Localisation of Multiple Sources in Reverberant Environments</title><categories>eess.AS cs.SD</categories><comments>10 pages</comments><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  vol. 25, no. 12, pp. 2444-2453, 2017</journal-ref><doi>10.1109/TASLP.2017.2750760</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel machine-hearing system that exploits deep neural
networks (DNNs) and head movements for robust binaural localisation of multiple
sources in reverberant environments. DNNs are used to learn the relationship
between the source azimuth and binaural cues, consisting of the complete
cross-correlation function (CCF) and interaural level differences (ILDs). In
contrast to many previous binaural hearing systems, the proposed approach is
not restricted to localisation of sound sources in the frontal hemifield. Due
to the similarity of binaural cues in the frontal and rear hemifields,
front-back confusions often occur. To address this, a head movement strategy is
incorporated in the localisation model to help reduce the front-back errors.
The proposed DNN system is compared to a Gaussian mixture model (GMM) based
system that employs interaural time differences (ITDs) and ILDs as localisation
features. Our experiments show that the DNN is able to exploit information in
the CCF that is not available in the ITD cue, which together with head
movements substantially improves localisation accuracies under challenging
acoustic scenarios in which multiple talkers and room reverberation are
present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03006</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03006</id><created>2019-04-05</created><authors><author><keyname>Ma</keyname><forenames>Ning</forenames></author><author><keyname>Gonzalez</keyname><forenames>Jose A.</forenames></author><author><keyname>Brown</keyname><forenames>Guy J.</forenames></author></authors><title>Robust Binaural Localization of a Target Sound Source by Combining
  Spectral Source Models and Deep Neural Networks</title><categories>eess.AS cs.SD</categories><comments>10 pages</comments><journal-ref>IEEE/ACM Transactions on Audio Speech and Language Processing,
  vol. 26, no. 11, pp. 2122-2131, 2018</journal-ref><doi>10.1109/TASLP.2018.2855960</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite there being clear evidence for top-down (e.g., attentional) effects
in biological spatial hearing, relatively few machine hearing systems exploit
top-down model-based knowledge in sound localisation. This paper addresses this
issue by proposing a novel framework for binaural sound localisation that
combines model-based information about the spectral characteristics of sound
sources and deep neural networks (DNNs). A target source model and a background
source model are first estimated during a training phase using spectral
features extracted from sound signals in isolation. When the identity of the
background source is not available, a universal background model can be used.
During testing, the source models are used jointly to explain the mixed
observations and improve the localisation process by selectively weighting
source azimuth posteriors output by a DNN-based localisation system. To address
the possible mismatch between training and testing, a model adaptation process
is further employed on-the-fly during testing, which adapts the background
model parameters directly from the noisy observations in an iterative manner.
The proposed system therefore combines model-based and data-driven information
flow within a single computational framework. The evaluation task involved
localisation of a target speech source in the presence of an interfering source
and room reverberation. Our experiments show that by exploiting model-based
information in this way, sound localisation performance can be improved
substantially under various noisy and reverberant conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03024</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03024</id><created>2019-03-07</created><authors><author><keyname>Dimopoulou</keyname><forenames>Melpomeni</forenames></author><author><keyname>Antonini</keyname><forenames>Marc</forenames></author><author><keyname>Barbry</keyname><forenames>Pascal</forenames></author><author><keyname>Appuswamy</keyname><forenames>Raja</forenames></author></authors><title>A biologically constrained encoding solution for long-term storage of
  images onto synthetic DNA</title><categories>eess.IV cs.MM q-bio.GN</categories><comments>Submitted to EUSIPCO 2019 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Living in the age of the digital media explosion, the amount of data that is
being stored increases dramatically. However, even if existing storage systems
suggest efficiency in capacity, they are lacking in durability. Hard disks,
flash, tape or even optical storage have limited lifespan in the range of 5 to
20 years. Interestingly, recent studies have proven that it was possible to use
synthetic DNA for the storage of digital data, introducing a strong candidate
to achieve data longevity. The DNA's biological properties allows the storage
of a great amount of information into an extraordinary small volume while also
promising efficient storage for centuries or even longer with no loss of
information. However, encoding digital data onto DNA is not obvious, because
when decoding, we have to face the problem of sequencing noise robustness.
Furthermore, synthesizing DNA is an expensive process and thus, controlling the
compression ratio by optimizing the rate-distortion trade-off is an important
challenge we have to deal with. This work proposes a coding solution for the
storage of digital images onto synthetic DNA. We developed a new encoding
algorithm which generates a DNA code robust to biological errors coming from
the synthesis and the sequencing processes. Furthermore, thanks to an optimized
allocation process the solution is able to control the compression ratio and
thus the length of the synthesized DNA strand. Results show an improvement in
terms of coding potential compared to previous state-of-the-art works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03065</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03065</id><created>2019-04-05</created><updated>2019-09-01</updated><authors><author><keyname>Takahashi</keyname><forenames>Naoya</forenames></author><author><keyname>Parthasaarathy</keyname><forenames>Sudarsanam</forenames></author><author><keyname>Goswami</keyname><forenames>Nabarun</forenames></author><author><keyname>Mitsufuji</keyname><forenames>Yuki</forenames></author></authors><title>Recursive speech separation for unknown number of speakers</title><categories>cs.SD eess.AS</categories><comments>Interspeech 2019 (oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a method of single-channel speaker-independent
multi-speaker speech separation for an unknown number of speakers. As opposed
to previous works, in which the number of speakers is assumed to be known in
advance and speech separation models are specific for the number of speakers,
our proposed method can be applied to cases with different numbers of speakers
using a single model by recursively separating a speaker. To make the
separation model recursively applicable, we propose one-and-rest permutation
invariant training (OR-PIT). Evaluation on WSJ0-2mix and WSJ0-3mix datasets
show that our proposed method achieves state-of-the-art results for two- and
three-speaker mixtures with a single model. Moreover, the same model can
separate four-speaker mixture, which was never seen during the training. We
further propose the detection of the number of speakers in a mixture during
recursive separation and show that this approach can more accurately estimate
the number of speakers than detection in advance by using a deep neural network
based classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03130</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03130</id><created>2019-04-05</created><authors><author><keyname>Wood</keyname><forenames>Sean U. N.</forenames></author><author><keyname>Rouat</keyname><forenames>Jean</forenames></author></authors><title>Unsupervised Low Latency Speech Enhancement with RT-GCC-NMF</title><categories>eess.AS cs.SD</categories><comments>Accepted for publication in the IEEE JSTSP Special Issue on Data
  Science: Machine Learning for Audio Signal Processing</comments><doi>10.1109/JSTSP.2019.2909193</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present RT-GCC-NMF: a real-time (RT), two-channel blind
speech enhancement algorithm that combines the non-negative matrix
factorization (NMF) dictionary learning algorithm with the generalized
cross-correlation (GCC) spatial localization method. Using a pre-learned
universal NMF dictionary, RT-GCC-NMF operates in a frame-by-frame fashion by
associating individual dictionary atoms to target speech or background
interference based on their estimated time-delay of arrivals (TDOA). We
evaluate RT-GCC-NMF on two-channel mixtures of speech and real-world noise from
the Signal Separation and Evaluation Campaign (SiSEC). We demonstrate that this
approach generalizes to new speakers, acoustic environments, and recording
setups from very little training data, and outperforms all but one of the
algorithms from the SiSEC challenge in terms of overall Perceptual Evaluation
methods for Audio Source Separation (PEASS) scores and compares favourably to
the ideal binary mask baseline. Over a wide range of input SNRs, we show that
this approach simultaneously improves the PEASS and signal to noise ratio
(SNR)-based Blind Source Separation (BSS) Eval objective quality metrics as
well as the short-time objective intelligibility (STOI) and extended STOI
(ESTOI) objective speech intelligibility metrics. A flexible, soft masking
function in the space of NMF activation coefficients offers real-time control
of the trade-off between interference suppression and target speaker fidelity.
Finally, we use an asymmetric short-time Fourier transform (STFT) to reduce the
inherent algorithmic latency of RT-GCC-NMF from 64 ms to 2 ms with no loss in
performance. We demonstrate that latencies within the tolerable range for
hearing aids are possible on current hardware platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03221</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03221</id><created>2019-04-05</created><authors><author><keyname>Ganjalizadeh</keyname><forenames>Milad</forenames></author><author><keyname>Di Marco</keyname><forenames>Piergiuseppe</forenames></author></authors><title>The Derivation of Failure Event Correlation Based on Shadowing
  Cross-Correlation</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document we derive the mapping between the failure event correlation
and shadowing cross-correlation in dual connectivity architectures. In this
case, we assume that a single UE is connected to two gNBs (next generation
NodeB).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03240</identifier>
 <datestamp>2019-06-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03240</id><created>2019-04-05</created><updated>2019-06-18</updated><authors><author><keyname>Chung</keyname><forenames>Yu-An</forenames></author><author><keyname>Hsu</keyname><forenames>Wei-Ning</forenames></author><author><keyname>Tang</keyname><forenames>Hao</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>An Unsupervised Autoregressive Model for Speech Representation Learning</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Accepted to Interspeech 2019. Code available at:
  https://github.com/iamyuanchung/Autoregressive-Predictive-Coding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel unsupervised autoregressive neural model for
learning generic speech representations. In contrast to other speech
representation learning methods that aim to remove noise or speaker
variabilities, ours is designed to preserve information for a wide range of
downstream tasks. In addition, the proposed model does not require any phonetic
or word boundary labels, allowing the model to benefit from large quantities of
unlabeled data. Speech representations learned by our model significantly
improve performance on both phone classification and speaker verification over
the surface features and other supervised and unsupervised approaches. Further
analysis shows that different levels of speech information are captured by our
model at different layers. In particular, the lower layers tend to be more
discriminative for speakers, while the upper layers provide more phonetic
content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03283</identifier>
 <datestamp>2019-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03283</id><created>2019-04-05</created><updated>2019-10-28</updated><authors><author><keyname>Hao</keyname><forenames>Peng</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author></authors><title>Integrating PHY Security Into NDN-IoT Networks By Exploiting MEC:
  Authentication Efficiency, Robustness, and Accuracy Enhancement</title><categories>eess.SP</categories><journal-ref>IEEE Transactions on Signal and Information Processing over
  Networks, Vol. 5, no. 4, 792-806, 2019</journal-ref><doi>10.1109/TSIPN.2019.2932678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent literature has demonstrated the improved data discovery and delivery
efficiency gained through applying named data networking (NDN) to a variety of
information-centric Internet of things (IoT) applications. However, from a data
security perspective, the development of NDN-IoT raises several new
authentication challenges. In particular, NDN-IoT authentication may require
per-packet-level signatures, thus imposing intolerably high computational and
time costs on the resource-poor IoT end devices. This paper proposes an
effective solution by seamlessly integrating the lightweight and unforgeable
physical-layer identity (PHY-ID) into the existing NDN signature scheme for the
mobile edge computing (MEC)-enabled NDN-IoT networks. The PHY-ID generation
exploits the inherent signal-level device-specific radio-frequency
imperfections of IoT devices, including the in-phase/quadrature-phase
imbalance, and thereby avoids adding any implementation complexity to the
constrained IoT devices. We derive the offline maximum entropy-based
quantization rule and propose an online two-step authentication scheme to
improve the accuracy of the authentication decision making. Consequently, a
cooperative MEC device can securely execute the costly signing task on behalf
of the authenticated IoT device in an optimal manner. The evaluation results
demonstrate 1) elevated authentication time efficiency, 2) robustness to
several impersonation attacks including the replay attack and the
computation-based spoofing attack, and 3) increased differentiation rate and
correct authentication probability by applying our integration design in
MEC-enabled NDN-IoT networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03288</identifier>
 <datestamp>2019-08-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03288</id><created>2019-04-05</created><updated>2019-08-26</updated><authors><author><keyname>Li</keyname><forenames>Jason</forenames></author><author><keyname>Lavrukhin</keyname><forenames>Vitaly</forenames></author><author><keyname>Ginsburg</keyname><forenames>Boris</forenames></author><author><keyname>Leary</keyname><forenames>Ryan</forenames></author><author><keyname>Kuchaiev</keyname><forenames>Oleksii</forenames></author><author><keyname>Cohen</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Nguyen</keyname><forenames>Huyen</forenames></author><author><keyname>Gadde</keyname><forenames>Ravi Teja</forenames></author></authors><title>Jasper: An End-to-End Convolutional Neural Acoustic Model</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>Accepted to INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we report state-of-the-art results on LibriSpeech among
end-to-end speech recognition models without any external training data. Our
model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout,
and residual connections. To improve training, we further introduce a new
layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that
the proposed deep architecture performs as well or better than more complex
choices. Our deepest Jasper variant uses 54 convolutional layers. With this
architecture, we achieve 2.95% WER using a beam-search decoder with an external
neural language model and 3.86% WER with a greedy decoder on LibriSpeech
test-clean. We also report competitive results on the Wall Street Journal and
the Hub5'00 conversational evaluation datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03299</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03299</id><created>2019-04-05</created><authors><author><keyname>Doroshewitz</keyname><forenames>John</forenames></author><author><keyname>Oakley</keyname><forenames>Christopher</forenames></author><author><keyname>Gjokaj</keyname><forenames>Vincens</forenames></author><author><keyname>Nanzer</keyname><forenames>Jeffrey</forenames></author></authors><title>A Multiband Patch Antenna with Co-Located Phase Centers for
  High-Accuracy Inter-Node Ranging in Distributed Antenna Arrays</title><categories>eess.SP</categories><comments>4 pages, 6 figures (2 figures have a and b, 8 total pictures), 3
  tables, IEEE Journal format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A unique three-band patch antenna for high accuracy ranging between nodes in
open-loop coherent distributed antenna arrays is presented. Open-loop coherent
distributed operations require accurate relative position knowledge between
nodes to enable distributed beamforming operations. For the highest accuracy,
the inter-node ranging will typically occur at frequencies higher than the
frequency of the signal transmitted by the distributed array (the coherent
action signal) to enable wider bandwidth signals to be used, however it is
important that the inter-node ranging measure the distance between the antennas
transmitting the coherent action signal. In this work, a three-band antenna is
presented that is designed to support distributed beamforming at 1.88 GHz and
high-accuracy ranging using a sparse, two-tone waveform operating near 9.5 and
10.5 GHz. The two-tone waveform is supported by a slotted patch antenna
surrounded by a larger patch antenna supporting the 1.88 GHz array signal. The
antennas are concentrically designed to ensure that the phase centers of the
ranging antenna and the coherent action antenna are closely aligned. Simulated
and measured performance shows phase center displacement of approximately
lambda/10 relative to the coherent action signal, while maintaining S11 below
-10 dB at each band.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03309</identifier>
 <datestamp>2019-10-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03309</id><created>2019-04-05</created><updated>2019-10-11</updated><authors><author><keyname>Karaku&#x15f;</keyname><forenames>Oktay</forenames></author><author><keyname>Rizaev</keyname><forenames>Igor</forenames></author><author><keyname>Achim</keyname><forenames>Alin</forenames></author></authors><title>Ship Wake Detection in SAR Images via Sparse Regularisation</title><categories>eess.SP</categories><comments>18 pages</comments><doi>10.1109/TGRS.2019.2947360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to analyse synthetic aperture radar (SAR) images of the sea surface,
ship wake detection is essential for extracting information on the wake
generating vessels. One possibility is to assume a linear model for wakes, in
which case detection approaches are based on transforms such as Radon and
Hough. These express the bright (dark) lines as peak (trough) points in the
transform domain. In this paper, ship wake detection is posed as an inverse
problem, which the associated cost function including a sparsity enforcing
penalty, i.e. the generalized minimax concave (GMC) function. Despite being a
non-convex regularizer, the GMC penalty enforces the overall cost function to
be convex. The proposed solution is based on a Bayesian formulation, whereby
the point estimates are recovered using maximum a posteriori (MAP) estimation.
To quantify the performance of the proposed method, various types of SAR images
are used, corresponding to TerraSAR-X, COSMO-SkyMed, Sentinel-1, and ALOS2. The
performance of various priors in solving the proposed inverse problem is first
studied by investigating the GMC along with the L1, Lp, nuclear and total
variation (TV) norms. We show that the GMC achieves the best results and we
subsequently study the merits of the corresponding method in comparison to two
state-of-the-art approaches for ship wake detection. The results show that our
proposed technique offers the best performance by achieving 80% success rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03365</identifier>
 <datestamp>2019-07-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03365</id><created>2019-04-06</created><updated>2019-07-02</updated><authors><author><keyname>Gong</keyname><forenames>Yuan</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Huber</keyname><forenames>Jacob</forenames></author><author><keyname>MacKnight</keyname><forenames>Mitchell</forenames></author><author><keyname>Poellabauer</keyname><forenames>Christian</forenames></author></authors><title>ReMASC: Realistic Replay Attack Corpus for Voice Controlled Systems</title><categories>cs.CR cs.LG cs.SD eess.AS</categories><comments>To appear in Interspeech 2019. Data set available at
  https://github.com/YuanGongND/ReMASC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new database of voice recordings with the goal of
supporting research on vulnerabilities and protection of voice-controlled
systems (VCSs). In contrast to prior efforts, the proposed database contains
both genuine voice commands and replayed recordings of such commands, collected
in realistic VCSs usage scenarios and using modern voice assistant development
kits. Specifically, the database contains recordings from four systems (each
with a different microphone array) in a variety of environmental conditions
with different forms of background noise and relative positions between speaker
and device. To the best of our knowledge, this is the first publicly available
database that has been specifically designed for the protection of
state-of-the-art voice-controlled systems against various replay attacks in
various conditions and environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03406</identifier>
 <datestamp>2019-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03406</id><created>2019-04-06</created><updated>2019-10-05</updated><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Hoydis</keyname><forenames>Jakob</forenames></author></authors><title>Towards Massive MIMO 2.0: Understanding spatial correlation,
  interference suppression, and pilot contamination</title><categories>eess.SP cs.IT math.IT</categories><comments>26 pages, 16 figures, IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the seminal paper by Marzetta from 2010, Massive MIMO has changed from
being a theoretical concept with an infinite number of antennas to a practical
technology. The key concepts are adopted in 5G and base stations (BSs) with
$M=64$ full-digital transceivers have been commercially deployed in sub-6\,GHz
bands. The fast progress was enabled by many solid research contributions of
which the vast majority assume spatially uncorrelated channels and signal
processing schemes developed for single-cell operation. These assumptions make
the performance analysis and optimization of Massive MIMO tractable but have
three major caveats: 1) practical channels are spatially correlated; 2) large
performance gains can be obtained by multicell processing, without BS
cooperation; 3) the interference caused by pilot contamination creates a finite
capacity limit, as $M\to\infty$. There is a thin line of papers that avoided
these caveats, but the results are easily missed. Hence, this tutorial article
explains the importance of considering spatial channel correlation and using
signal processing schemes designed for multicell networks. We present recent
results on the fundamental limits of Massive MIMO, which are not determined by
pilot contamination but the ability to acquire channel statistics. These
results will guide the journey towards the next level of Massive MIMO, which we
call ``Massive MIMO 2.0''.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03411</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03411</id><created>2019-04-06</created><authors><author><keyname>Stoica</keyname><forenames>Razvan-Andrei</forenames></author><author><keyname>de Abreu</keyname><forenames>Giuseppe Thadeu Freitas</forenames></author><author><keyname>Iimori</keyname><forenames>Hiroki</forenames></author></authors><title>A Frame-Theoretic Scheme for Robust Millimeter Wave Channel Estimation</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We propose a new scheme for the robust estimation of the millimeter wave
(mmWave) channel. Our approach is based on a sparse formulation of the channel
estimation problem coupled with a frame theoretic representation of the sensing
dictionary. To clarify, under this approach, the combined effect of transmit
precoders and receive beamformers is modeled by a single frame, whose design is
optimized to improve the accuracy of the sparse reconstruction problem to which
the channel estimation problem is ultimately reduced. The optimized sensing
dictionary frame is then decomposed via a Kronecker decomposition back into the
precoding and beamforming vectors used by the transmitter and receiver.
Simulation results illustrate the significant gain in estimation accuracy
obtained over state of the art alternatives. As a bonus, the work offers new
insights onto the sparse mmWave-multiple-input multiple-output (MIMO) channel
estimation problem by casting the trade-off between correlation and variation
range in terms of frame coherence and tightness
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03416</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03416</id><created>2019-04-06</created><authors><author><keyname>Pascual</keyname><forenames>Santiago</forenames></author><author><keyname>Ravanelli</keyname><forenames>Mirco</forenames></author><author><keyname>Serr&#xe0;</keyname><forenames>Joan</forenames></author><author><keyname>Bonafonte</keyname><forenames>Antonio</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Learning Problem-agnostic Speech Representations from Multiple
  Self-supervised Tasks</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning good representations without supervision is still an open issue in
machine learning, and is particularly challenging for speech signals, which are
often characterized by long sequences with a complex hierarchical structure.
Some recent works, however, have shown that it is possible to derive useful
speech representations by employing a self-supervised encoder-discriminator
approach. This paper proposes an improved self-supervised method, where a
single neural encoder is followed by multiple workers that jointly solve
different self-supervised tasks. The needed consensus across different tasks
naturally imposes meaningful constraints to the encoder, contributing to
discover general representations and to minimize the risk of learning
superficial ones. Experiments show that the proposed approach can learn
transferable, robust, and problem-agnostic features that carry on relevant
information from the speech signal, such as speaker identity, phonemes, and
even higher-level features such as emotional cues. In addition, a number of
design choices make the encoder easily exportable, facilitating its direct
usage or adaptation to different problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03418</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03418</id><created>2019-04-06</created><authors><author><keyname>Pascual</keyname><forenames>Santiago</forenames></author><author><keyname>Serr&#xe0;</keyname><forenames>Joan</forenames></author><author><keyname>Bonafonte</keyname><forenames>Antonio</forenames></author></authors><title>Towards Generalized Speech Enhancement with Generative Adversarial
  Networks</title><categories>cs.SD cs.LG eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The speech enhancement task usually consists of removing additive noise or
reverberation that partially mask spoken utterances, affecting their
intelligibility. However, little attention is drawn to other, perhaps more
aggressive signal distortions like clipping, chunk elimination, or
frequency-band removal. Such distortions can have a large impact not only on
intelligibility, but also on naturalness or even speaker identity, and require
of careful signal reconstruction. In this work, we give full consideration to
this generalized speech enhancement task, and show it can be tackled with a
time-domain generative adversarial network (GAN). In particular, we extend a
previous GAN-based speech enhancement system to deal with mixtures of four
types of aggressive distortions. Firstly, we propose the addition of an
adversarial acoustic regression loss that promotes a richer feature extraction
at the discriminator. Secondly, we also make use of a two-step adversarial
training schedule, acting as a warm up-and-fine-tune sequence. Both objective
and subjective evaluations show that these two additions bring improved speech
reconstructions that better match the original speaker identity and
naturalness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03446</identifier>
 <datestamp>2019-08-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03446</id><created>2019-04-06</created><updated>2019-08-11</updated><authors><author><keyname>Sun</keyname><forenames>Hao</forenames></author><author><keyname>Tan</keyname><forenames>Xu</forenames></author><author><keyname>Gan</keyname><forenames>Jun-Wei</forenames></author><author><keyname>Liu</keyname><forenames>Hongzhi</forenames></author><author><keyname>Zhao</keyname><forenames>Sheng</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>5 pages, accepted by interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grapheme-to-phoneme (G2P) conversion is an important task in automatic speech
recognition and text-to-speech systems. Recently, G2P conversion is viewed as a
sequence to sequence task and modeled by RNN or CNN based encoder-decoder
framework. However, previous works do not consider the practical issues when
deploying G2P model in the production system, such as how to leverage
additional unlabeled data to boost the accuracy, as well as reduce model size
for online deployment. In this work, we propose token-level ensemble
distillation for G2P conversion, which can (1) boost the accuracy by distilling
the knowledge from additional unlabeled data, and (2) reduce the model size but
maintain the high accuracy, both of which are very practical and helpful in the
online production system. We use token-level knowledge distillation, which
results in better accuracy than the sequence-level counterpart. What is more,
we adopt the Transformer instead of RNN or CNN based models to further boost
the accuracy of G2P conversion. Experiments on the publicly available CMUDict
dataset and an internal English dataset demonstrate the effectiveness of our
proposed method. Particularly, our method achieves 19.88% WER on CMUDict
dataset, outperforming the previous works by more than 4.22% WER, and setting
the new state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03458</identifier>
 <datestamp>2019-07-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03458</id><created>2019-04-06</created><updated>2019-06-30</updated><authors><author><keyname>Zhao</keyname><forenames>Wenjing</forenames></author><author><keyname>Wang</keyname><forenames>Gongpu</forenames></author><author><keyname>Atapattu</keyname><forenames>Saman</forenames></author><author><keyname>He</keyname><forenames>Ruisi</forenames></author><author><keyname>Liang</keyname><forenames>Ying-Chang</forenames></author></authors><title>Channel Estimation for Ambient Backscatter Communication Systems with
  Massive-Antenna Reader</title><categories>eess.SP cs.IT math.IT</categories><comments>5 figures, submitted to IEEE Transactions on Vehicular Technology, 29
  March, 2019</comments><journal-ref>IEEE Transactions on Vehicular Technology,2019 June</journal-ref><doi>10.1109/TVT.2019.2925212</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ambient backscatter, an emerging green communication technology, has aroused
great interest from both academia and industry. One open problem for ambient
backscatter communication (AmBC) systems is channel estimation for a
massive-antenna reader. In this paper, we focus on channel estimation problem
in AmBC systems with uniform linear array (ULA) at the reader which consists of
large number of antennas. We first design a two-step method to jointly estimate
channel gains and direction of arrivals (DoAs), and then refine the estimates
through angular rotation. Additionally, Cramer-Rao lower bounds (CRLBs) are
derived for both the modulus of the channel gain and the DoA estimates.
Simulations are then provided to validate the analysis, and to show the
efficiency of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03476</identifier>
 <datestamp>2019-06-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03476</id><created>2019-04-06</created><updated>2019-06-09</updated><authors><author><keyname>Kong</keyname><forenames>Qiuqiang</forenames></author><author><keyname>Cao</keyname><forenames>Yin</forenames></author><author><keyname>Iqbal</keyname><forenames>Turab</forenames></author><author><keyname>Xu</keyname><forenames>Yong</forenames></author><author><keyname>Wang</keyname><forenames>Wenwu</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Cross-task learning for audio tagging, sound event detection and spatial
  localization: DCASE 2019 baseline systems</title><categories>cs.SD eess.AS</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019
challenge focuses on audio tagging, sound event detection and spatial
localisation. DCASE 2019 consists of five tasks: 1) acoustic scene
classification, 2) audio tagging with noisy labels and minimal supervision, 3)
sound event localisation and detection, 4) sound event detection in domestic
environments, and 5) urban sound tagging. In this paper, we propose generic
cross-task baseline systems based on convolutional neural networks (CNNs). The
motivation is to investigate the performance of a variety of models across
several audio recognition tasks without exploiting the specific characteristics
of the tasks. We looked at CNNs with 5, 9, and 13 layers, and found that the
optimal architecture is task-dependent. For the systems we considered, we found
that the 9-layer CNN with average pooling after convolutional layers is a good
model for a majority of the DCASE 2019 tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03479</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03479</id><created>2019-04-06</created><authors><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>He</keyname><forenames>Liang</forenames></author><author><keyname>Liu</keyname><forenames>Jia</forenames></author></authors><title>Large Margin Softmax Loss for Speaker Verification</title><categories>cs.SD eess.AS</categories><comments>submitted to Interspeech 2019. The code and models have been released</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In neural network based speaker verification, speaker embedding is expected
to be discriminative between speakers while the intra-speaker distance should
remain small. A variety of loss functions have been proposed to achieve this
goal. In this paper, we investigate the large margin softmax loss with
different configurations in speaker verification. Ring loss and minimum
hyperspherical energy criterion are introduced to further improve the
performance. Results on VoxCeleb show that our best system outperforms the
baseline approach by 15\% in EER, and by 13\%, 33\% in minDCF08 and minDCF10,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03522</identifier>
 <datestamp>2019-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03522</id><created>2019-04-06</created><updated>2019-10-26</updated><authors><author><keyname>Leshem</keyname><forenames>Roee Levy</forenames></author><author><keyname>Giryes</keyname><forenames>Raja</forenames></author></authors><title>Taco-VC: A Single Speaker Tacotron based Voice Conversion with Limited
  Data</title><categories>cs.SD cs.LG eess.AS</categories><comments>Submitted to ICASSP 2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Taco-VC, a novel architecture for voice conversion (VC)
based on the Tacotron synthesizer, which is a sequence-to-sequence with
attention model. The training of multi-speaker voice conversion systems
requires a large amount of resources, both in training and corpus size. Taco-VC
is implemented using a single speaker Tacotron synthesizer based on Phonetic
Posteriorgrams (PPGs) and a single speaker Wavenet vocoder conditioned on Mel
Spectrograms. To enhance the converted speech quality, the outputs of the
Tacotron are passed through a novel speech-enhancement network, which is
composed of a combination of phoneme recognition and Tacotron networks. Our
system is trained just with a mid-size, single speaker corpus, and adapted to
new speakers using only few minutes of training data. Using public mid-size
datasets, our method outperforms the baseline in the VCC 2018 SPOKE task, and
achieves competitive results compared to multi-speaker networks trained on
private large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03530</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03530</id><created>2019-04-06</created><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Gurram</keyname><forenames>Prudhvi</forenames></author><author><keyname>Whipps</keyname><forenames>Gene</forenames></author></authors><title>A Bayesian Theory of Change Detection in Statistically Periodic Random
  Processes</title><categories>eess.SP cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of stochastic processes called independent and periodically
identically distributed (i.p.i.d.) processes is defined to capture periodically
varying statistical behavior. A novel Bayesian theory is developed for
detecting a change in the distribution of an i.p.i.d. process. It is shown that
the Bayesian change point problem can be expressed as a problem of optimal
control of a Markov decision process (MDP) with periodic transition and cost
structures. Optimal control theory is developed for periodic MDPs for
discounted and undiscounted total cost criteria. A fixed-point equation is
obtained that is satisfied by the optimal cost function. It is shown that the
optimal policy for the MDP is nonstationary but periodic in nature. A value
iteration algorithm is obtained to compute the optimal cost function. The
results from the MDP theory are then applied to detect changes in i.p.i.d.
processes. It is shown that while the optimal change point algorithm is a
stopping rule based on a periodic sequence of thresholds, a single-threshold
policy is asymptotically optimal, as the probability of false alarm goes to
zero. Numerical results are provided to demonstrate that the asymptotically
optimal policy is not strictly optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03543</identifier>
 <datestamp>2019-07-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03543</id><created>2019-04-06</created><updated>2019-06-28</updated><authors><author><keyname>Phan</keyname><forenames>Huy</forenames></author><author><keyname>Ch&#xe9;n</keyname><forenames>Oliver Y.</forenames></author><author><keyname>Pham</keyname><forenames>Lam</forenames></author><author><keyname>Koch</keyname><forenames>Philipp</forenames></author><author><keyname>De Vos</keyname><forenames>Maarten</forenames></author><author><keyname>McLoughlin</keyname><forenames>Ian</forenames></author><author><keyname>Mertins</keyname><forenames>Alfred</forenames></author></authors><title>Spatio-Temporal Attention Pooling for Audio Scene Classification</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>To appear at the 20th Annual Conference of the International Speech
  Communication Association (INTERSPEECH 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustic scenes are rich and redundant in their content. In this work, we
present a spatio-temporal attention pooling layer coupled with a convolutional
recurrent neural network to learn from patterns that are discriminative while
suppressing those that are irrelevant for acoustic scene classification. The
convolutional layers in this network learn invariant features from
time-frequency input. The bidirectional recurrent layers are then able to
encode the temporal dynamics of the resulting convolutional features.
Afterwards, a two-dimensional attention mask is formed via the outer product of
the spatial and temporal attention vectors learned from two designated
attention layers to weigh and pool the recurrent output into a final feature
vector for classification. The network is trained with between-class examples
generated from between-class data augmentation. Experiments demonstrate that
the proposed method not only outperforms a strong convolutional neural network
baseline but also sets new state-of-the-art performance on the LITIS Rouen
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03575</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03575</id><created>2019-04-06</created><authors><author><keyname>Zou</keyname><forenames>Difan</forenames></author><author><keyname>Xu</keyname><forenames>Zhengyuan</forenames></author><author><keyname>Gong</keyname><forenames>Chen</forenames></author></authors><title>Two Dimension Intensity Distribution of Ultraviolet Scattering
  Communication</title><categories>eess.SP</categories><comments>Work was done when Difan Zou was in USTC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a ultraviolet (UV) scattering communication system where the
position of the transmitter is fixed and the receiver can move around on the
ground. To obtain the link gain effectively and economically, we propose an
algorithm based on one-dimensional (1D) numerical integration and an off-line
data library. Moreover, we analyze the 2D scattering intensity distributions
for both LED and laser, and observe that the contours can be well fitted by
elliptic models. The relationships between the characteristics of fitting
ellipses and the source parameters are provided by numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03576</identifier>
 <datestamp>2019-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03576</id><created>2019-04-06</created><updated>2019-07-01</updated><authors><author><keyname>Shivakumar</keyname><forenames>Prashanth Gurunath</forenames></author><author><keyname>Yang</keyname><forenames>Mu</forenames></author><author><keyname>Georgiou</keyname><forenames>Panayiotis</forenames></author></authors><title>Spoken Language Intent Detection using Confusion2Vec</title><categories>cs.CL cs.SD eess.AS</categories><report-no>2226</report-no><journal-ref>Proceedings of Interspeech 2019</journal-ref><doi>10.21437/Interspeech.2019-2226</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decoding speaker's intent is a crucial part of spoken language understanding
(SLU). The presence of noise or errors in the text transcriptions, in real life
scenarios make the task more challenging. In this paper, we address the spoken
language intent detection under noisy conditions imposed by automatic speech
recognition (ASR) systems. We propose to employ confusion2vec word feature
representation to compensate for the errors made by ASR and to increase the
robustness of the SLU system. The confusion2vec, motivated from human speech
production and perception, models acoustic relationships between words in
addition to the semantic and syntactic relations of words in human language. We
hypothesize that ASR often makes errors relating to acoustically similar words,
and the confusion2vec with inherent model of acoustic relationships between
words is able to compensate for the errors. We demonstrate through experiments
on the ATIS benchmark dataset, the robustness of the proposed model to achieve
state-of-the-art results under noisy ASR conditions. Our system reduces
classification error rate (CER) by 20.84% and improves robustness by 37.48%
(lower CER degradation) relative to the previous state-of-the-art going from
clean to noisy transcripts. Improvements are also demonstrated when training
the intent detection models on noisy transcripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03601</identifier>
 <datestamp>2019-07-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03601</id><created>2019-04-07</created><updated>2019-07-05</updated><authors><author><keyname>Shon</keyname><forenames>Suwon</forenames></author><author><keyname>Tang</keyname><forenames>Hao</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>VoiceID Loss: Speech Enhancement for Speaker Verification</title><categories>eess.AS cs.SD</categories><comments>interspeech 2019; demo link :
  https://people.csail.mit.edu/swshon/supplement/voiceid-loss</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose VoiceID loss, a novel loss function for training a
speech enhancement model to improve the robustness of speaker verification. In
contrast to the commonly used loss functions for speech enhancement such as the
L2 loss, the VoiceID loss is based on the feedback from a speaker verification
model to generate a ratio mask. The generated ratio mask is multiplied
pointwise with the original spectrogram to filter out unnecessary components
for speaker verification. In the experiments, we observed that the enhancement
network, after training with the VoiceID loss, is able to ignore a substantial
amount of time-frequency bins, such as those dominated by noise, for
verification. The resulting model consistently improves the speaker
verification system on both clean and noisy conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03617</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03617</id><created>2019-04-07</created><authors><author><keyname>Zhang</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Lantian</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author></authors><title>VAE-based regularization for deep speaker embedding</title><categories>cs.SD cs.LG eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep speaker embedding has achieved state-of-the-art performance in speaker
recognition. A potential problem of these embedded vectors (called `x-vectors')
are not Gaussian, causing performance degradation with the famous PLDA back-end
scoring. In this paper, we propose a regularization approach based on
Variational Auto-Encoder (VAE). This model transforms x-vectors to a latent
space where mapped latent codes are more Gaussian, hence more suitable for PLDA
scoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03643</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03643</id><created>2019-04-07</created><authors><author><keyname>Kim</keyname><forenames>Donghoh</forenames></author><author><keyname>Choi</keyname><forenames>Guebin</forenames></author><author><keyname>Oh</keyname><forenames>Hee-Seok</forenames></author></authors><title>Ensemble Patch Transformation: A New Tool for Signal Decomposition</title><categories>eess.SP stat.ME</categories><comments>32 pages with 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of signal decomposition and data
visualization. For this purpose, we introduce a new multiscale transform,
termed `ensemble patch transformation' that enhances identification of local
characteristics embedded in a signal and provides multiscale visualization
according to different levels; hence, it is useful for data analysis and signal
decomposition. In literature, there are data-adaptive decomposition methods
such as empirical mode decomposition (EMD) by Huang et al. (1998). Along the
same line of EMD, we propose a new decomposition algorithm that extracts
meaningful components from a signal that belongs to a large class of signals,
compared to the previous methods. Some theoretical properties of the proposed
algorithm are investigated. To evaluate the proposed method, we analyze several
synthetic examples and a real-world signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03670</identifier>
 <datestamp>2019-07-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03670</id><created>2019-04-07</created><updated>2019-07-25</updated><authors><author><keyname>Lugosch</keyname><forenames>Loren</forenames></author><author><keyname>Ravanelli</keyname><forenames>Mirco</forenames></author><author><keyname>Ignoto</keyname><forenames>Patrick</forenames></author><author><keyname>Tomar</keyname><forenames>Vikrant Singh</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Speech Model Pre-training for End-to-End Spoken Language Understanding</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>Accepted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whereas conventional spoken language understanding (SLU) systems map speech
to text, and then text to intent, end-to-end SLU systems map speech directly to
intent through a single trainable model. Achieving high accuracy with these
end-to-end models without a large amount of training data is difficult. We
propose a method to reduce the data requirements of end-to-end SLU in which the
model is first pre-trained to predict words and phonemes, thus learning good
features for SLU. We introduce a new SLU dataset, Fluent Speech Commands, and
show that our method improves performance both when the full dataset is used
for training and when only a small subset is used. We also describe preliminary
experiments to gauge the model's ability to generalize to new phrases not heard
during training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03710</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03710</id><created>2019-04-07</created><updated>2019-04-15</updated><authors><author><keyname>Purohit</keyname><forenames>Kuldeep</forenames></author><author><keyname>Vasu</keyname><forenames>Subeesh</forenames></author><author><keyname>Rao</keyname><forenames>M. Purnachandra</forenames></author><author><keyname>Rajagopalan</keyname><forenames>A. N.</forenames></author></authors><title>Planar Geometry and Latest Scene Recovery from a Single Motion Blurred
  Image</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing works on motion deblurring either ignore the effects of
depth-dependent blur or work with the assumption of a multi-layered scene
wherein each layer is modeled in the form of fronto-parallel plane. In this
work, we consider the case of 3D scenes with piecewise planar structure i.e., a
scene that can be modeled as a combination of multiple planes with arbitrary
orientations. We first propose an approach for estimation of normal of a planar
scene from a single motion blurred observation. We then develop an algorithm
for automatic recovery of a number of planes, the parameters corresponding to
each plane, and camera motion from a single motion blurred image of a
multiplanar 3D scene. Finally, we propose a first-of-its-kind approach to
recover the planar geometry and latent image of the scene by adopting an
alternating minimization framework built on our findings. Experiments on
synthetic and real data reveal that our proposed method achieves
state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03760</identifier>
 <datestamp>2019-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03760</id><created>2019-04-07</created><updated>2019-09-22</updated><authors><author><keyname>Wu</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Shi-Xiong</forenames></author><author><keyname>Chen</keyname><forenames>Lian-Wu</forenames></author><author><keyname>Yu</keyname><forenames>Meng</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>Dong</forenames></author></authors><title>Time Domain Audio Visual Speech Separation</title><categories>eess.AS cs.SD</categories><comments>Accepted to ASRU 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio-visual multi-modal modeling has been demonstrated to be effective in
many speech related tasks, such as speech recognition and speech enhancement.
This paper introduces a new time-domain audio-visual architecture for target
speaker extraction from monaural mixtures. The architecture generalizes the
previous TasNet (time-domain speech separation network) to enable multi-modal
learning and at meanwhile it extends the classical audio-visual speech
separation from frequency-domain to time-domain. The main components of
proposed architecture include an audio encoder, a video encoder that extracts
lip embedding from video streams, a multi-modal separation network and an audio
decoder. Experiments on simulated mixtures based on recently released LRS2
dataset show that our method can bring 3dB+ and 4dB+ Si-SNR improvements on
two- and three-speaker cases respectively, compared to audio-only TasNet and
frequency-domain audio-visual networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03765</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03765</id><created>2019-04-07</created><updated>2019-04-10</updated><authors><author><keyname>Diab</keyname><forenames>Hassan</forenames><affiliation>American University of Beirut</affiliation></author><author><keyname>Damaj</keyname><forenames>Issam</forenames><affiliation>American University of Beirut</affiliation></author><author><keyname>Kurdahi</keyname><forenames>Fadi</forenames><affiliation>University of California at Irvine</affiliation></author></authors><title>Optimizing FIR Filter Mapping on the Morphosys Reconfigurable System</title><categories>eess.SP</categories><comments>10 Pages, 8 Figures, 6 Tables</comments><acm-class>B.4.4; B.5.1; B.5.2; B.6.3</acm-class><journal-ref>Intl.Jrnl. of Par. &amp; Dist. Sys. &amp; Nets. ACTA. 5(2002) 108-115</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an optimized mapping of the FIR filter algorithm that
enhances the rate of a reconfigurable computer over a basic mapping previously
proposed [1]. It also presents a new interconnection scheme in the
reconfigurable part of MorphoSys, a reconfigurable computing system [2].
Reconfigurable computing (RC) is introduced, followed by the MorphoSys RC
system. Two optimized FIR mappings are then presented which deliver enhanced
speed. A spreadsheet model will detail the modification and the improvement.
The speedup achieved is also explained as well as the advantages in the mapping
of the application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03774</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03774</id><created>2019-04-07</created><authors><author><keyname>Safi</keyname><forenames>Hossein</forenames></author><author><keyname>Dargahi</keyname><forenames>Akbar</forenames></author><author><keyname>Cheng</keyname><forenames>Julian</forenames></author></authors><title>Spatial Beam Tracking and Data Detection for an FSO Link to a UAV in the
  Presence of Hovering Fluctuations</title><categories>eess.SP math.OC</categories><comments>27 pages, 8 figures, submitted to IEEE Transaction on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in small-scale unmanned aerial vehicles (UAVs) have opened up
new horizons for establishing UAV-based free-space optical (FSO) links.
However, FSO technology requires precise beam alignment while random
fluctuations of hovering UAVs can induce beam misalignment and angle-of-arrival
(AoA) fluctuations. For an FSO link to a UAV, we consider a quadrant detector
array for optical beam tracking and study the effect of random hovering
fluctuations of the UAV on the performance of the tracking method, and based on
the degree of instabilities for the UAV, the optimum size of the detectors for
minimizing the tracking error is found. Furthermore, for optimal detection of
On - Off keying symbols, the receiver requires instantaneous channel fading
coefficients. We propose a blind method to estimate the channel coefficients,
i.e., without using any pilot symbols, to increase link bandwidth efficiency.
To evaluate the performance of the considered system, closed-form expressions
of tracking error and bit-error rate are derived. Moreover, Monte-Carlo
simulation is carried out to corroborate the accuracy of the derived analytical
expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03787</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03787</id><created>2019-04-07</created><authors><author><keyname>Narisetty</keyname><forenames>Chaitanya</forenames></author><author><keyname>Komatsu</keyname><forenames>Tatsuya</forenames></author><author><keyname>Kondo</keyname><forenames>Reishi</forenames></author></authors><title>Bayesian Non-Parametric Multi-Source Modelling Based Determined Blind
  Source Separation</title><categories>cs.SD cs.LG eess.AS</categories><comments>5 pages, 2 figures. Accepted at ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a determined blind source separation method using
Bayesian non-parametric modelling of sources. Conventionally source signals are
separated from a given set of mixture signals by modelling them using
non-negative matrix factorization (NMF). However in NMF, a latent variable
signifying model complexity must be appropriately specified to avoid
over-fitting or under-fitting. As real-world sources can be of varying and
unknown complexities, we propose a Bayesian non-parametric framework which is
invariant to such latent variables. We show that our proposed method adapts to
different source complexities, while conventional methods require parameter
tuning for optimal separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03792</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03792</id><created>2019-04-07</created><authors><author><keyname>Wu</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Shi-Xiong</forenames></author><author><keyname>Chen</keyname><forenames>Lian-Wu</forenames></author><author><keyname>Yu</keyname><forenames>Meng</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>Dong</forenames></author></authors><title>Improved Speaker-Dependent Separation for CHiME-5 Challenge</title><categories>eess.AS cs.SD</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper summarizes several follow-up contributions for improving our
submitted NWPU speaker-dependent system for CHiME-5 challenge, which aims to
solve the problem of multi-channel, highly-overlapped conversational speech
recognition in a dinner party scenario with reverberations and non-stationary
noises. We adopt a speaker-aware training method by using i-vector as the
target speaker information for multi-talker speech separation. With only one
unified separation model for all speakers, we achieve a 10\% absolute
improvement in terms of word error rate (WER) over the previous baseline of
80.28\% on the development set by leveraging our newly proposed data processing
techniques and beamforming approach. With our improved back-end acoustic model,
we further reduce WER to 60.15\% which surpasses the result of our submitted
CHiME-5 challenge system without applying any fusion techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03814</identifier>
 <datestamp>2019-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03814</id><created>2019-04-07</created><updated>2019-11-18</updated><authors><author><keyname>Choi</keyname><forenames>Seungwoo</forenames></author><author><keyname>Seo</keyname><forenames>Seokjun</forenames></author><author><keyname>Shin</keyname><forenames>Beomjun</forenames></author><author><keyname>Byun</keyname><forenames>Hyeongmin</forenames></author><author><keyname>Kersner</keyname><forenames>Martin</forenames></author><author><keyname>Kim</keyname><forenames>Beomsu</forenames></author><author><keyname>Kim</keyname><forenames>Dongyoung</forenames></author><author><keyname>Ha</keyname><forenames>Sungjoo</forenames></author></authors><title>Temporal Convolution for Real-time Keyword Spotting on Mobile Devices</title><categories>cs.SD cs.LG cs.NE eess.AS</categories><comments>In INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyword spotting (KWS) plays a critical role in enabling speech-based user
interactions on smart devices. Recent developments in the field of deep
learning have led to wide adoption of convolutional neural networks (CNNs) in
KWS systems due to their exceptional accuracy and robustness. The main
challenge faced by KWS systems is the trade-off between high accuracy and low
latency. Unfortunately, there has been little quantitative analysis of the
actual latency of KWS models on mobile devices. This is especially concerning
since conventional convolution-based KWS approaches are known to require a
large number of operations to attain an adequate level of performance. In this
paper, we propose a temporal convolution for real-time KWS on mobile devices.
Unlike most of the 2D convolution-based KWS approaches that require a deep
architecture to fully capture both low- and high-frequency domains, we exploit
temporal convolutions with a compact ResNet architecture. In Google Speech
Command Dataset, we achieve more than \textbf{385x} speedup on Google Pixel 1
and surpass the accuracy compared to the state-of-the-art model. In addition,
we release the implementation of the proposed and the baseline models including
an end-to-end pipeline for training models and evaluating them on mobile
devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03829</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03829</id><created>2019-04-08</created><authors><author><keyname>Kong</keyname><forenames>Yehao</forenames></author><author><keyname>Zhang</keyname><forenames>Jiliang</forenames></author></authors><title>Adversarial Audio: A New Information Hiding Method and Backdoor for
  DNN-based Speech Recognition Models</title><categories>cs.CR cs.SD eess.AS</categories><comments>Submitted to RAID2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio is an important medium in people's daily life, hidden information can
be embedded into audio for covert communication. Current audio information
hiding techniques can be roughly classed into time domain-based and transform
domain-based techniques. Time domain-based techniques have large hiding
capacity but low imperceptibility. Transform domain-based techniques have
better imperceptibility, but the hiding capacity is poor. This paper proposes a
new audio information hiding technique which shows high hiding capacity and
good imperceptibility. The proposed audio information hiding method takes the
original audio signal as input and obtains the audio signal embedded with
hidden information (called stego audio) through the training of our private
automatic speech recognition (ASR) model. Without knowing the internal
parameters and structure of the private model, the hidden information can be
extracted by the private model but cannot be extracted by public models. We use
four other ASR models to extract the hidden information on the stego audios to
evaluate the security of the private model. The experimental results show that
the proposed audio information hiding technique has a high hiding capacity of
48 cps with good imperceptibility and high security. In addition, our proposed
adversarial audio can be used to activate an intrinsic backdoor of DNN-based
ASR models, which brings a serious threat to intelligent speakers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03833</identifier>
 <datestamp>2019-07-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03833</id><created>2019-04-08</created><updated>2019-07-03</updated><authors><author><keyname>Latif</keyname><forenames>Siddique</forenames></author><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Khalifa</keyname><forenames>Sara</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Epps</keyname><forenames>Julien</forenames></author></authors><title>Direct Modelling of Speech Emotion from Raw Speech</title><categories>cs.SD eess.AS</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech emotion recognition is a challenging task and heavily depends on
hand-engineered acoustic features, which are typically crafted to echo human
perception of speech signals. However, a filter bank that is designed from
perceptual evidence is not always guaranteed to be the best in a statistical
modelling framework where the end goal is for example emotion classification.
This has fuelled the emerging trend of learning representations from raw speech
especially using deep learning neural networks. In particular, a combination of
Convolution Neural Networks (CNNs) and Long Short Term Memory (LSTM) have
gained great traction for the intrinsic property of LSTM in learning contextual
information crucial for emotion recognition; and CNNs been used for its ability
to overcome the scalability problem of regular neural networks. In this paper,
we show that there are still opportunities to improve the performance of
emotion recognition from the raw speech by exploiting the properties of CNN in
modelling contextual information. We propose the use of parallel convolutional
layers to harness multiple temporal resolutions in the feature extraction block
that is jointly trained with the LSTM based classification network for the
emotion recognition task. Our results suggest that the proposed model can reach
the performance of CNN trained with hand-engineered features from both IEMOCAP
and MSP-IMPROV datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03834</identifier>
 <datestamp>2019-06-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03834</id><created>2019-04-08</created><updated>2019-06-06</updated><authors><author><keyname>Greaves-Tunnell</keyname><forenames>Alexander</forenames></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames></author></authors><title>A Statistical Investigation of Long Memory in Language and Music</title><categories>stat.ML cs.LG cs.SD eess.AS</categories><comments>29 pages; expanded supplement, added details in background and
  methods per reviewer feedback, included additional references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation and learning of long-range dependencies is a central challenge
confronted in modern applications of machine learning to sequence data. Yet
despite the prominence of this issue, the basic problem of measuring long-range
dependence, either in a given data source or as represented in a trained deep
model, remains largely limited to heuristic tools. We contribute a statistical
framework for investigating long-range dependence in current applications of
deep sequence modeling, drawing on the well-developed theory of long memory
stochastic processes. This framework yields testable implications concerning
the relationship between long memory in real-world data and its learned
representation in a deep learning architecture, which are explored through a
semiparametric framework adapted to the high-dimensional setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03841</identifier>
 <datestamp>2020-01-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03841</id><created>2019-04-08</created><updated>2020-01-26</updated><authors><author><keyname>Dinkel</keyname><forenames>Heinrich</forenames></author><author><keyname>Yu</keyname><forenames>Kai</forenames></author></authors><title>Duration robust weakly supervised sound event detection</title><categories>cs.SD eess.AS</categories><comments>Accepted by ICASSP2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task 4 of the DCASE2018 challenge demonstrated that substantially more
research is needed for a real-world application of sound event detection.
Analyzing the challenge results it can be seen that most successful models are
biased towards predicting long (e.g., over 5s) clips. This work aims to
investigate the performance impact of fixed-sized window median filter
post-processing and advocate the use of double thresholding as a more robust
and predictable post-processing method. Further, four different temporal
subsampling methods within the CRNN framework are proposed: mean-max,
alpha-mean-max, Lp-norm and convolutional. We show that for this task
subsampling the temporal resolution by a neural network enhances the F1 score
as well as its robustness towards short, sporadic sound events. Our best single
model achieves 30.1% F1 on the evaluation set and the best fusion model 32.5%,
while being robust to event length variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03851</identifier>
 <datestamp>2020-01-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03851</id><created>2019-04-08</created><updated>2020-01-03</updated><authors><author><keyname>Huang</keyname><forenames>Chao</forenames></author><author><keyname>Liu</keyname><forenames>Haojie</forenames></author><author><keyname>Chen</keyname><forenames>Tong</forenames></author><author><keyname>Shen</keyname><forenames>Qiu</forenames></author><author><keyname>Ma</keyname><forenames>Zhan</forenames></author></authors><title>Extreme Image Coding via Multiscale Autoencoders With Generative
  Adversarial Optimization</title><categories>eess.IV cs.LG</categories><comments>Accepted to IEEE VCIP 2019 as an oral presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a MultiScale AutoEncoder(MSAE) based extreme image compression
framework to offer visually pleasing reconstruction at a very low bitrate. Our
method leverages the &quot;priors&quot; at different resolution scale to improve the
compression efficiency, and also employs the generative adversarial
network(GAN) with multiscale discriminators to perform the end-to-end trainable
rate-distortion optimization. We compare the perceptual quality of our
reconstructions with traditional compression algorithms using High-Efficiency
Video Coding(HEVC) based Intra Profile and JPEG2000 on the public Cityscapes
and ADE20K datasets, demonstrating the significant subjective quality
improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03864</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03864</id><created>2019-04-08</created><authors><author><keyname>Lee</keyname><forenames>Harim</forenames></author><author><keyname>Yang</keyname><forenames>Hyun Jong</forenames></author></authors><title>Analysis of Channel Model for LAA-WLAN Coexistence with different OFDM
  Parameters between LAA and Wi-Fi</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the channel model for the asynchronous Wi-Fi and LAA signals is
investigated, also taking into consideration the impairment of the OFDM
parameters between LAA and Wi-Fi. Even for the same bandwidth, e.g., 20 MHz
channel, OFDM parameters of LAA-LTE and Wi-Fi are different [1], [2]. In
addition, the legacy Wi-Fi devices cannot receive or transmit synchronously
with LAA-LTE. Thus, the orthogonality between the subcarriers is broken if
LAA-LTE's signals are received at Wi-Fi devices and vice versa. In order to
facilitate an analysis of the throughput, we should derive the distortion of
LAA-LTE's signals received at Wi-Fi devices and the distortion of Wi-Fi's
signals received at LAA-LTE devices due to the difference in OFDM parameters of
LAA-LTE and Wi-Fi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03876</identifier>
 <datestamp>2019-07-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03876</id><created>2019-04-08</created><updated>2019-07-02</updated><authors><author><keyname>Ondel</keyname><forenames>Lucas</forenames></author><author><keyname>Vydana</keyname><forenames>Hari Krishna</forenames></author><author><keyname>Burget</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>&#x10c;ernock&#xfd;</keyname><forenames>Jan</forenames></author></authors><title>Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Accepted to Interspeech 2019 * corrected typos * Recalculated the
  segmentation using +-2 frames tolerance to comply with other publications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work tackles the problem of learning a set of language specific acoustic
units from unlabeled speech recordings given a set of labeled recordings from
other languages. Our approach may be described by the following two steps
procedure: first the model learns the notion of acoustic units from the
labelled data and then the model uses its knowledge to find new acoustic units
on the target language. We implement this process with the Bayesian Subspace
Hidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model
(SGMM) where each low dimensional embedding represents an acoustic unit rather
than just a HMM's state. The subspace is trained on 3 languages from the
GlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on
the TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that
this approach significantly outperforms previous HMM based acoustic units
discovery systems and compares favorably with the Variational Auto Encoder-HMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03888</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03888</id><created>2019-04-08</created><authors><author><keyname>Drumetz</keyname><forenames>Lucas</forenames></author><author><keyname>Chanussot</keyname><forenames>Jocelyn</forenames></author><author><keyname>Jutten</keyname><forenames>Christian</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Iwasaki</keyname><forenames>Akira</forenames></author></authors><title>Spectral Variability Aware Blind Hyperspectral Image Unmixing Based on
  Convex Geometry</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral image unmixing has proven to be a useful technique to interpret
hyperspectral data, and is a prolific research topic in the community. Most of
the approaches used to perform linear unmixing are based on convex geometry
concepts, because of the strong geometrical structure of the linear mixing
model. However, two main phenomena lead to question this model, namely
nonlinearities and the spectral variability of the materials. Many algorithms
based on convex geometry are still used when considering these two limitations
of the linear model. A natural question is to wonder to what extent these
concepts and tools (Intrinsic Dimensionality estimation, endmember extraction
algorithms, pixel purity) can be safely used in these different scenarios. In
this paper, we analyze them with a focus on endmember variability, assuming
that the linear model holds. In the light of this analysis, we propose an
integrated unmixing chain which tries to adress the shortcomings of the
classical tools used in the linear case, based on our previously proposed
extended linear mixing model. We show the interest of the proposed approach on
simulated and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03908</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03908</id><created>2019-04-08</created><authors><author><keyname>Bazrafkan</keyname><forenames>Shabab</forenames></author><author><keyname>Van Nieuwenhove</keyname><forenames>Vincent</forenames></author><author><keyname>Soons</keyname><forenames>Joris</forenames></author><author><keyname>De Beenhouwer</keyname><forenames>Jan</forenames></author><author><keyname>Sijbers</keyname><forenames>Jan</forenames></author></authors><title>Deep Learning Based Computed Tomography Whys and Wherefores</title><categories>eess.IV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an article about the Computed Tomography (CT) and how Deep Learning
influences CT reconstruction pipeline, especially in low dose scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.03976</identifier>
 <datestamp>2019-06-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.03976</id><created>2019-04-08</created><updated>2019-06-26</updated><authors><author><keyname>Juvela</keyname><forenames>Lauri</forenames></author><author><keyname>Bollepalli</keyname><forenames>Bajibabu</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Alku</keyname><forenames>Paavo</forenames></author></authors><title>GELP: GAN-Excited Linear Prediction for Speech Synthesis from
  Mel-spectrogram</title><categories>eess.AS cs.LG cs.SD</categories><comments>Interspeech 2019 accepted version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in neural network -based text-to-speech have reached human
level naturalness in synthetic speech. The present sequence-to-sequence models
can directly map text to mel-spectrogram acoustic features, which are
convenient for modeling, but present additional challenges for vocoding (i.e.,
waveform generation from the acoustic features). High-quality synthesis can be
achieved with neural vocoders, such as WaveNet, but such autoregressive models
suffer from slow sequential inference. Meanwhile, their existing parallel
inference counterparts are difficult to train and require increasingly large
model sizes. In this paper, we propose an alternative training strategy for a
parallel neural vocoder utilizing generative adversarial networks, and
integrate a linear predictive synthesis filter into the model. Results show
that the proposed model achieves significant improvement in inference speed,
while outperforming a WaveNet in copy-synthesis quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04083</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04083</id><created>2019-04-08</created><authors><author><keyname>Buchner</keyname><forenames>Herbert</forenames></author><author><keyname>Petersen</keyname><forenames>Eike</forenames></author><author><keyname>Eger</keyname><forenames>Marcus</forenames></author><author><keyname>Rostalski</keyname><forenames>Philipp</forenames></author></authors><title>Convolutive Blind Source Separation on Surface EMG Signals for
  Respiratory Diagnostics and Medical Ventilation Control</title><categories>eess.SP cs.SY stat.AP</categories><journal-ref>2016 38th Annual International Conference of the IEEE Engineering
  in Medicine and Biology Society (EMBC)</journal-ref><doi>10.1109/EMBC.2016.7591513</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The electromyogram (EMG) is an important tool for assessing the activity of a
muscle and thus also a valuable measure for the diagnosis and control of
respiratory support. In this article we propose convolutive blind source
separation (BSS) as an effective tool to pre-process surface electromyogram
(sEMG) data of the human respiratory muscles. Specifically, the problem of
discriminating between inspiratory, expiratory and cardiac muscle activity is
addressed, which currently poses a major obstacle for the clinical use of sEMG
for adaptive ventilation control. It is shown that using the investigated
broadband algorithm, a clear separation of these components can be achieved.
The algorithm is based on a generic framework for BSS that utilizes multiple
statistical signal characteristics. Apart from a four-channel FIR structure,
there are no further restrictive assumptions on the demixing system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04100</identifier>
 <datestamp>2019-08-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04100</id><created>2019-04-08</created><updated>2019-08-23</updated><authors><author><keyname>Chen</keyname><forenames>Kuan-Yu</forenames></author><author><keyname>Tsai</keyname><forenames>Che-Ping</forenames></author><author><keyname>Liu</keyname><forenames>Da-Rong</forenames></author><author><keyname>Lee</keyname><forenames>Hung-Yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>Completely Unsupervised Speech Recognition By A Generative Adversarial
  Network Harmonized With Iteratively Refined Hidden Markov Models</title><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted by Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Producing a large annotated speech corpus for training ASR systems remains
difficult for more than 95% of languages all over the world which are
low-resourced, but collecting a relatively big unlabeled data set for such
languages is more achievable. This is why some initial effort have been
reported on completely unsupervised speech recognition learned from unlabeled
data only, although with relatively high error rates. In this paper, we develop
a Generative Adversarial Network (GAN) to achieve this purpose, in which a
Generator and a Discriminator learn from each other iteratively to improve the
performance. We further use a set of Hidden Markov Models (HMMs) iteratively
refined from the machine generated labels to work in harmony with the GAN. The
initial experiments on TIMIT data set achieve an phone error rate of 33.1%,
which is 8.5% lower than the previous state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04156</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04156</id><created>2019-04-04</created><authors><author><keyname>Pal</keyname><forenames>Monalisa</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Sanghamitra</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Saugat</forenames></author></authors><title>A Many Objective Optimization Approach for Transfer Learning in EEG
  Classification</title><categories>eess.SP cs.HC cs.LG cs.NE</categories><comments>Pre-submission work</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In Brain-Computer Interfacing (BCI), due to inter-subject non-stationarities
of electroencephalogram (EEG), classifiers are trained and tested using EEG
from the same subject. When physical disabilities bottleneck the natural
modality of performing a task, acquisition of ample training data is difficult
which practically obstructs classifier training. Previous works have tackled
this problem by generalizing the feature space amongst multiple subjects
including the test subject. This work aims at knowledge transfer to classify
EEG of the target subject using a classifier trained with the EEG of another
unit source subject. A many-objective optimization framework is proposed where
optimal weights are obtained for projecting features in another dimension such
that single source-trained target EEG classification performance is maximized
with the modified features. To validate the approach, motor imagery tasks from
the BCI Competition III Dataset IVa are classified using power spectral density
based features and linear support vector machine. Several performance metrics,
improvement in accuracy, sensitivity to the dimension of the projected space,
assess the efficacy of the proposed approach. Addressing single-source training
promotes independent living of differently-abled individuals by reducing
assistance from others. The proposed approach eliminates the requirement of EEG
from multiple source subjects and is applicable to any existing feature
extractors and classifiers. Source code is available at
http://worksupplements.droppages.com/tlbci.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04161</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04161</id><created>2019-04-08</created><authors><author><keyname>Narayanaswamy</keyname><forenames>Vivek Sivaraman</forenames></author><author><keyname>Katoch</keyname><forenames>Sameeksha</forenames></author><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author><author><keyname>Song</keyname><forenames>Huan</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Audio Source Separation via Multi-Scale Learning with Dilated Dense
  U-Nets</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern audio source separation techniques rely on optimizing sequence model
architectures such as, 1D-CNNs, on mixture recordings to generalize well to
unseen mixtures. Specifically, recent focus is on time-domain based
architectures such as Wave-U-Net which exploit temporal context by extracting
multi-scale features. However, the optimality of the feature extraction process
in these architectures has not been well investigated. In this paper, we
examine and recommend critical architectural changes that forge an optimal
multi-scale feature extraction process. To this end, we replace regular $1-$D
convolutions with adaptive dilated convolutions that have innate capability of
capturing increased context by using large temporal receptive fields. We also
investigate the impact of dense connections on the extraction process that
encourage feature reuse and better gradient flow. The dense connections between
the downsampling and upsampling paths of a U-Net architecture capture
multi-resolution information leading to improved temporal modelling. We
evaluate the proposed approaches on the MUSDB test dataset. In addition to
providing an improved performance over the state-of-the-art, we also provide
insights on the impact of different architectural choices on complex
data-driven solutions for source separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04169</identifier>
 <datestamp>2019-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04169</id><created>2019-04-08</created><updated>2019-10-29</updated><authors><author><keyname>Biadsy</keyname><forenames>Fadi</forenames></author><author><keyname>Weiss</keyname><forenames>Ron J.</forenames></author><author><keyname>Moreno</keyname><forenames>Pedro J.</forenames></author><author><keyname>Kanevsky</keyname><forenames>Dimitri</forenames></author><author><keyname>Jia</keyname><forenames>Ye</forenames></author></authors><title>Parrotron: An End-to-End Speech-to-Speech Conversion Model and its
  Applications to Hearing-Impaired Speech and Speech Separation</title><categories>eess.AS cs.SD</categories><comments>5 pages, submitted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Parrotron, an end-to-end-trained speech-to-speech conversion
model that maps an input spectrogram directly to another spectrogram, without
utilizing any intermediate discrete representation. The network is composed of
an encoder, spectrogram and phoneme decoders, followed by a vocoder to
synthesize a time-domain waveform. We demonstrate that this model can be
trained to normalize speech from any speaker regardless of accent, prosody, and
background noise, into the voice of a single canonical target speaker with a
fixed accent and consistent articulation and prosody. We further show that this
normalization model can be adapted to normalize highly atypical speech from a
deaf speaker, resulting in significant improvements in intelligibility and
naturalness, measured via a speech recognizer and listening tests. Finally,
demonstrating the utility of this model on other speech tasks, we show that the
same model architecture can be trained to perform a speech separation task
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04175</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04175</id><created>2019-04-08</created><authors><author><keyname>Kellman</keyname><forenames>Michael</forenames></author><author><keyname>Bostan</keyname><forenames>Emrah</forenames></author><author><keyname>Chen</keyname><forenames>Michael</forenames></author><author><keyname>Waller</keyname><forenames>Laura</forenames></author></authors><title>Data-Driven Design for Fourier Ptychographic Microscopy</title><categories>eess.SP cs.CV eess.IV</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fourier Ptychographic Microscopy (FPM) is a computational imaging method that
is able to super-resolve features beyond the diffraction-limit set by the
objective lens of a traditional microscope. This is accomplished by using
synthetic aperture and phase retrieval algorithms to combine many measurements
captured by an LED array microscope with programmable source patterns. FPM
provides simultaneous large field-of-view and high resolution imaging, but at
the cost of reduced temporal resolution, thereby limiting live cell
applications. In this work, we learn LED source pattern designs that compress
the many required measurements into only a few, with negligible loss in
reconstruction quality or resolution. This is accomplished by recasting the
super-resolution reconstruction as a Physics-based Neural Network and learning
the experimental design to optimize the network's overall performance.
Specifically, we learn LED patterns for different applications (e.g. amplitude
contrast and quantitative phase imaging) and show that the designs we learn
through simulation generalize well in the experimental setting. Further, we
discuss a context-specific loss function, practical memory limitations, and
interpretability of our learned designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04184</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04184</id><created>2019-04-08</created><authors><author><keyname>Fortunati</keyname><forenames>Stefano</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Greco</keyname><forenames>Maria Sabrina</forenames></author><author><keyname>Gini</keyname><forenames>Fulvio</forenames></author></authors><title>Scaling up MIMO radar for target detection</title><categories>eess.SP cs.IT math.IT</categories><comments>8 pages, 2 figures. This is the extended version (with all the proofs
  in Appendices) of a paper that will be presented at ICASSP 2019, 12-17 May,
  2019, Brighton, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on target detection in a colocated MIMO radar system.
Instead of exploiting the classical temporal domain, we propose to explore the
spatial dimension (i.e., number of antennas $M$) to derive asymptotic results
for the detector. Specifically, we assume no a priori knowledge of the
statistics of the autoregressive data generating process and propose to use a
mispecified Wald-type detector, which is shown to have an asymptotic
$\chi$-squared distribution as $M\to\infty$. Closed-form expressions for the
probabilities of false alarm and detection are derived. Numerical results are
used to validate the asymptotic analysis in the finite system regime. It turns
out that, for the considered scenario, the asymptotic performance is closely
matched already for $M\ge 50$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04221</identifier>
 <datestamp>2019-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04221</id><created>2019-04-08</created><updated>2019-11-25</updated><authors><author><keyname>Esmaeilpour</keyname><forenames>Mohammad</forenames></author><author><keyname>Cardinal</keyname><forenames>Patrick</forenames></author><author><keyname>Koerich</keyname><forenames>Alessandro Lameiras</forenames></author></authors><title>Unsupervised Feature Learning for Environmental Sound Classification
  Using Weighted Cycle-Consistent Generative Adversarial Network</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Paper Accepted for Publication in Elsevier Applied Soft Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a novel environmental sound classification approach
incorporating unsupervised feature learning from codebook via spherical
$K$-Means++ algorithm and a new architecture for high-level data augmentation.
The audio signal is transformed into a 2D representation using a discrete
wavelet transform (DWT). The DWT spectrograms are then augmented by a novel
architecture for cycle-consistent generative adversarial network. This
high-level augmentation bootstraps generated spectrograms in both intra and
inter class manners by translating structural features from sample to sample. A
codebook is built by coding the DWT spectrograms with the speeded-up robust
feature detector (SURF) and the K-Means++ algorithm. The Random Forest is our
final learning algorithm which learns the environmental sound classification
task from the clustered codewords in the codebook. Experimental results in four
benchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and
DCASE-2017) have shown that the proposed classification approach outperforms
the state-of-the-art classifiers in the scope, including advanced and dense
convolutional neural networks such as AlexNet and GoogLeNet, improving the
classification rate between 3.51% and 14.34%, depending on the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04235</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04235</id><created>2019-04-05</created><authors><author><keyname>Novotny</keyname><forenames>Ondrej</forenames></author><author><keyname>Plchot</keyname><forenames>Oldrich</forenames></author><author><keyname>Glembek</keyname><forenames>Ondrej</forenames></author><author><keyname>Burget</keyname><forenames>Lukas</forenames></author></authors><title>Factorization of Discriminatively Trained i-vector Extractor for Speaker
  Recognition</title><categories>eess.AS cs.SD</categories><comments>Submitted to Interspeech 2019, Graz, Austria. arXiv admin note:
  substantial text overlap with arXiv:1810.13183</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we continue in our research on i-vector extractor for speaker
verification (SV) and we optimize its architecture for fast and effective
discriminative training. We were motivated by computational and memory
requirements caused by the large number of parameters of the original
generative i-vector model. Our aim is to preserve the power of the original
generative model, and at the same time focus the model towards extraction of
speaker-related information. We show that it is possible to represent a
standard generative i-vector extractor by a model with significantly less
parameters and obtain similar performance on SV tasks. We can further refine
this compact model by discriminative training and obtain i-vectors that lead to
better performance on various SV benchmarks representing different acoustic
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04239</identifier>
 <datestamp>2019-08-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04239</id><created>2019-04-06</created><updated>2019-08-13</updated><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Gurram</keyname><forenames>Prudhvi</forenames></author><author><keyname>Whipps</keyname><forenames>Gene</forenames></author></authors><title>Minimax-Optimal Algorithms for Detecting Changes in Statistically
  Periodic Random Processes</title><categories>eess.SP cs.IT math.IT math.ST stat.TH</categories><comments>arXiv admin note: text overlap with arXiv:1810.12760,
  arXiv:1807.06945</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theory and algorithms are developed for detecting changes in the distribution
of statistically periodic random processes. The statistical periodicity is
modeled using independent and periodically identically distributed processes, a
new class of stochastic processes proposed by us. An algorithm is developed
that is minimax asymptotically optimal as the false alarm rate goes to zero.
Algorithms are also developed for the cases when the post-change distribution
is not known or when there are multiple streams of observations. The modeling
is inspired by real datasets encountered in cyber-physical systems, biology,
and medicine. The developed algorithms are applied to sequences of Instagram
counts collected around a 5K run in New York City to detect the run.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04240</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04240</id><created>2019-04-07</created><authors><author><keyname>Shon</keyname><forenames>Suwon</forenames></author><author><keyname>Dehak</keyname><forenames>Najim</forenames></author><author><keyname>Reynolds</keyname><forenames>Douglas</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>MCE 2018: The 1st Multi-target Speaker Detection and Identification
  Challenge Evaluation</title><categories>eess.AS cs.SD</categories><comments>http://mce.csail.mit.edu . arXiv admin note: text overlap with
  arXiv:1807.06663</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Multi-target Challenge aims to assess how well current speech technology
is able to determine whether or not a recorded utterance was spoken by one of a
large number of blacklisted speakers. It is a form of multi-target speaker
detection based on real-world telephone conversations. Data recordings are
generated from call center customer-agent conversations. The task is to measure
how accurately one can detect 1) whether a test recording is spoken by a
blacklisted speaker, and 2) which specific blacklisted speaker was talking.
This paper outlines the challenge and provides its baselines, results, and
discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04283</identifier>
 <datestamp>2019-12-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04283</id><created>2019-04-08</created><updated>2019-12-23</updated><authors><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author><author><keyname>Da Costa</keyname><forenames>Maxime Ferreira</forenames></author></authors><title>Harnessing Sparsity over the Continuum: Atomic Norm Minimization for
  Super Resolution</title><categories>eess.SP cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex optimization recently emerges as a compelling framework for performing
super resolution, garnering significant attention from multiple communities
spanning signal processing, applied mathematics, and optimization. This article
offers a friendly exposition to atomic norm minimization as a canonical convex
approach to solve super resolution problems. The mathematical foundations and
performances guarantees of this approach are presented, and its application in
super resolution image reconstruction for single-molecule fluorescence
microscopy are highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04294</identifier>
 <datestamp>2019-07-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04294</id><created>2019-04-08</created><updated>2019-07-21</updated><authors><author><keyname>Wang</keyname><forenames>Xiaofei</forenames></author><author><keyname>Yang</keyname><forenames>Jinyi</forenames></author><author><keyname>Li</keyname><forenames>Ruizhi</forenames></author><author><keyname>Sadhu</keyname><forenames>Samik</forenames></author><author><keyname>Hermansky</keyname><forenames>Hynek</forenames></author></authors><title>Exploring Methods for the Automatic Detection of Errors in Manual
  Transcription</title><categories>cs.CL cs.SD eess.AS</categories><comments>Submitted in Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality of data plays an important role in most deep learning tasks. In the
speech community, transcription of speech recording is indispensable. Since the
transcription is usually generated artificially, automatically finding errors
in manual transcriptions not only saves time and labors but benefits the
performance of tasks that need the training process. Inspired by the success of
hybrid automatic speech recognition using both language model and acoustic
model, two approaches of automatic error detection in the transcriptions have
been explored in this work. Previous study using a biased language model
approach, relying on a strong transcription-dependent language model, has been
reviewed. In this work, we propose a novel acoustic model based approach,
focusing on the phonetic sequence of speech. Both methods have been evaluated
on a completely real dataset, which was originally transcribed with errors and
strictly corrected manually afterwards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04352</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04352</id><created>2019-04-08</created><authors><author><keyname>Saha</keyname><forenames>Pramit</forenames></author><author><keyname>Fels</keyname><forenames>Sidney</forenames></author></authors><title>Hierarchical Deep Feature Learning For Decoding Imagined Speech From EEG</title><categories>cs.LG eess.IV stat.ML</categories><comments>Accepted in AAAI 2019 under Student Abstract and Poster Program</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a mixed deep neural network strategy, incorporating parallel
combination of Convolutional (CNN) and Recurrent Neural Networks (RNN),
cascaded with deep autoencoders and fully connected layers towards automatic
identification of imagined speech from EEG. Instead of utilizing raw EEG
channel data, we compute the joint variability of the channels in the form of a
covariance matrix that provide spatio-temporal representations of EEG. The
networks are trained hierarchically and the extracted features are passed onto
the next network hierarchy until the final classification. Using a publicly
available EEG based speech imagery database we demonstrate around 23.45%
improvement of accuracy over the baseline method. Our approach demonstrates the
promise of a mixed DNN approach for complex spatial-temporal classification
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04358</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04358</id><created>2019-04-08</created><authors><author><keyname>Saha</keyname><forenames>Pramit</forenames></author><author><keyname>Abdul-Mageed</keyname><forenames>Muhammad</forenames></author><author><keyname>Fels</keyname><forenames>Sidney</forenames></author></authors><title>Deep Learning the EEG Manifold for Phonological Categorization from
  Active Thoughts</title><categories>cs.LG cs.CL cs.SD eess.AS stat.ML</categories><comments>Accepted for publication in IEEE ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an
alternative vocal communication pathway for people with speaking disabilities.
As a step towards full decoding of imagined speech from active thoughts, we
present a BCI system for subject-independent classification of phonological
categories exploiting a novel deep learning based hierarchical feature
extraction scheme. To better capture the complex representation of
high-dimensional electroencephalography (EEG) data, we compute the joint
variability of EEG electrodes into a channel cross-covariance matrix. We then
extract the spatio-temporal information encoded within the matrix using a mixed
deep neural network strategy. Our model framework is composed of a
convolutional neural network (CNN), a long-short term network (LSTM), and a
deep autoencoder. We train the individual networks hierarchically, feeding
their combined outputs in a final gradient boosting classification step. Our
best models achieve an average accuracy of 77.9% across five different binary
classification tasks, providing a significant 22.5% improvement over previous
methods. As we also show visually, our work demonstrates that the speech
imagery EEG possesses significant discriminative information about the intended
articulatory movements responsible for natural speech synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04364</identifier>
 <datestamp>2019-09-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04364</id><created>2019-04-08</created><updated>2019-09-18</updated><authors><author><keyname>Okawa</keyname><forenames>Masaki</forenames></author><author><keyname>Saito</keyname><forenames>Takuya</forenames></author><author><keyname>Sawada</keyname><forenames>Naoki</forenames></author><author><keyname>Nishizaki</keyname><forenames>Hiromitsu</forenames></author></authors><title>Audio Classification of Bit-Representation Waveform</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>Accepted at INTERSPEECH2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigated the waveform representation for audio signal
classification. Recently, many studies on audio waveform classification such as
acoustic event detection and music genre classification have been published.
Most studies on audio waveform classification have proposed the use of a deep
learning (neural network) framework. Generally, a frequency analysis method
such as Fourier transform is applied to extract the frequency or spectral
information from the input audio waveform before inputting the raw audio
waveform into the neural network. In contrast to these previous studies, in
this paper, we propose a novel waveform representation method, in which audio
waveforms are represented as a bit sequence, for audio classification. In our
experiment, we compare the proposed bit representation waveform, which is
directly given to a neural network, to other representations of audio waveforms
such as a raw audio waveform and a power spectrum with two classification
tasks: one is an acoustic event classification task and the other is a
sound/music classification task. The experimental results showed that the bit
representation waveform achieved the best classification performance for both
the tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04376</identifier>
 <datestamp>2019-07-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04376</id><created>2019-04-08</created><updated>2019-07-05</updated><authors><author><keyname>Rodrigues</keyname><forenames>Victor Croisfelt</forenames></author><author><keyname>Filho</keyname><forenames>Jose Carlos Marinello</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>Randomized Kaczmarz Algorithm for Massive MIMO Systems with Channel
  Estimation and Spatial Correlation</title><categories>eess.SP stat.AP</categories><comments>36 pages, 5 figures, full paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To exploit the benefits of massive multiple-input multiple-output (M-MIMO)
technology in scenarios where base stations (BSs) need to be cheap and equipped
with simple hardware, the computational complexity of classical signal
processing schemes for spatial multiplexing of users shall be reduced. This
calls for suboptimal designs that perform well the combining/precoding steps
and simultaneously achieve low computational complexities. An approach based on
the iterative Kaczmarz algorithm (KA) has been recently investigated, assuring
well execution without the knowledge of second order moments of the wireless
channels in the BS, and with easiness, since no tuning parameters, besides the
number of iterations, are required. In fact, the randomized version of KA (rKA)
has been used in this context due to global convergence properties. Herein,
modifications are proposed on this first rKA-based attempt, aiming to improve
its performance-complexity trade-off solution for M-MIMO systems. We observe
that long-term channel effects degrade the rate of convergence of the rKA-based
schemes. This issue is then tackled herein by means of a hybrid rKA
initialization proposal that lands within the region of convexity of the
algorithm and assures fairness to the communication system. The effectiveness
of our proposal is illustrated through numerical results which bring more
realistic system conditions in terms of channel estimation and spatial
correlation than those used so far. We also characterize the computational
complexity of the proposed rKA scheme, deriving upper bounds for the number of
iterations. A case study focused on a dense urban application scenario is used
to gather new insights on the feasibility of the proposed scheme to cope with
the inserted BS constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04395</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04395</id><created>2019-04-08</created><updated>2019-04-20</updated><authors><author><keyname>Gao</keyname><forenames>Dawei</forenames></author><author><keyname>Guo</keyname><forenames>Qinghua</forenames></author><author><keyname>Tong</keyname><forenames>Jun</forenames></author><author><keyname>Wu</keyname><forenames>Nan</forenames></author><author><keyname>Xi</keyname><forenames>Jiangtao</forenames></author><author><keyname>Yu</keyname><forenames>Yanguang</forenames></author></authors><title>Extreme Learning Machine Based Non-Iterative and Iterative Nonlinearity
  Mitigation for LED Communications</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns receiver design for light emitting diode (LED)
communications where the LED nonlinearity can severely degrade the performance
of communications. We propose extreme learning machine (ELM) based
non-iterative receivers and iterative receivers to effectively handle the LED
nonlinearity and memory effects. For the iterative receiver design, we also
develop a data-aided receiver, where data is used as virtual training sequence
in ELM training. It is shown that the ELM based receivers significantly
outperform conventional polynomial based receivers; iterative receivers can
achieve huge performance gain compared to non-iterative receivers; and the
data-aided receiver can reduce training overhead considerably. This work can
also be extended to radio frequency communications, e.g., to deal with the
nonlinearity of power amplifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04427</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04427</id><created>2019-04-08</created><authors><author><keyname>Duan</keyname><forenames>Chaojing</forenames></author><author><keyname>Chen</keyname><forenames>Siheng</forenames></author><author><keyname>Kovacevic</keyname><forenames>Jelena</forenames></author></authors><title>3D Point Cloud Denoising via Deep Neural Network based Local Surface
  Estimation</title><categories>cs.CV eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a neural-network-based architecture for 3D point cloud denoising
called neural projection denoising (NPD). In our previous work, we proposed a
two-stage denoising algorithm, which first estimates reference planes and
follows by projecting noisy points to estimated reference planes. Since the
estimated reference planes are inevitably noisy, multi-projection is applied to
stabilize the denoising performance. NPD algorithm uses a neural network to
estimate reference planes for points in noisy point clouds. With more accurate
estimations of reference planes, we are able to achieve better denoising
performances with only one-time projection. To the best of our knowledge, NPD
is the first work to denoise 3D point clouds with deep learning techniques. To
conduct the experiments, we sample 40000 point clouds from the 3D data in
ShapeNet to train a network and sample 350 point clouds from the 3D data in
ModelNet10 to test. Experimental results show that our algorithm can estimate
normal vectors of points in noisy point clouds. Comparing to five competitive
methods, the proposed algorithm achieves better denoising performance and
produces much smaller variances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04465</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04465</id><created>2019-04-09</created><authors><author><keyname>Zhang</keyname><forenames>Zhaorong</forenames></author><author><keyname>Fu</keyname><forenames>Minyue</forenames></author></authors><title>Convergence of Message-Passing for Distributed Convex Optimisation with
  Scaled Diagonal Dominance</title><categories>math.OC eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the convergence properties the well-known message-passing
algorithm for convex optimisation. Under the assumption of pairwise
separability and scaled diagonal dominance, asymptotic convergence is
established and a simple bound for the convergence rate is provided for
message-passing. In comparison with previous results, our results do not
require the given convex program to have known convex pairwise components and
that our bound for the convergence rate is tighter and simpler. When
specialised to quadratic optimisation, we generalise known results by providing
a very simple bound for the convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04472</identifier>
 <datestamp>2019-08-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04472</id><created>2019-04-09</created><updated>2019-08-27</updated><authors><author><keyname>Yamamoto</keyname><forenames>Ryuichi</forenames></author><author><keyname>Song</keyname><forenames>Eunwoo</forenames></author><author><keyname>Kim</keyname><forenames>Jae-Min</forenames></author></authors><title>Probability density distillation with generative adversarial networks
  for high-quality parallel waveform generation</title><categories>eess.AS cs.SD</categories><comments>Accepted to the conference of INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an effective probability density distillation (PDD)
algorithm for WaveNet-based parallel waveform generation (PWG) systems.
Recently proposed teacher-student frameworks in the PWG system have
successfully achieved a real-time generation of speech signals. However, the
difficulties optimizing the PDD criteria without auxiliary losses result in
quality degradation of synthesized speech. To generate more natural speech
signals within the teacher-student framework, we propose a novel optimization
criterion based on generative adversarial networks (GANs). In the proposed
method, the inverse autoregressive flow-based student model is incorporated as
a generator in the GAN framework, and jointly optimized by the PDD mechanism
with the proposed adversarial learning method. As this process encourages the
student to model the distribution of realistic speech waveform, the perceptual
quality of the synthesized speech becomes much more natural. Our experimental
results verify that the PWG systems with the proposed method outperform both
those using conventional approaches, and also autoregressive generation systems
with a well-trained teacher WaveNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04485</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04485</id><created>2019-04-09</created><authors><author><keyname>Zhu</keyname><forenames>Jiang</forenames></author></authors><title>A comment on the &quot;A unified Bayesian inference framework for generalized
  linear models&quot;</title><categories>eess.SP</categories><comments>A technical note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent work `A unified Bayesian inference framework for generalized
linear models' \cite{meng1} shows that the GLM can be solved via iterating
between the standard linear module (SLM) (running with standard Bayesian
algorithm) and the minimum mean squared error (MMSE) module. The proposed
framework utilizes expectation propagation and corresponds to the sum-product
version \cite{Rangan1}. While in \cite{Rangan1}, a max-sum GAMP is also
proposed. What is their intrinsic relationship? This comment aims to answer
this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04500</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04500</id><created>2019-04-09</created><authors><author><keyname>Shen</keyname><forenames>Tong</forenames></author><author><keyname>Liu</keyname><forenames>Tingting</forenames></author><author><keyname>Lin</keyname><forenames>Yan</forenames></author><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Shu</keyname><forenames>Feng</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Regional Robust Secure Precise Wireless Transmission Design for
  Multi-user Broadcasting System</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two regional robust schemes for multi-user secure precise
wireless transmission (SPWT), regional signal-to-leakage-and-noise ratio
(R-SLNR) maximization and point SLNR (P-SLNR) maximization, are proposed to
tackle with the estimation errors of the target users' position. Compared to
the traditional robust methods in secure wireless communications which use
statistical methods to optimize the beamforming vector in the desired
positions, regional robust schemes are designed for optimizing the secrecy
performance in the whole error region around the estimated position.
Specifically, we first study the maximal estimation error of the angle and the
distance as the error range which demonstrates the measurement accuracy. Next
we define the region around the estimated positions inside the error range as
the target area. Then the rest area is defined as the potential wiretap region,
and the following work is to design an optimal beamforming vector and
artificial noise projection matrix, which achieve the confidential signal in
the target area having the maximal power while only few signal power is
conserved in the potential wiretap region. Instead of considering the
statistical distributions of the estimated errors into optimization, we
optimize the sum SLNR of the whole target area, which significantly decreases
the complexity. Moreover, the proposed schemes can ensure that the desired
users are located in the optimized region, which are more practical than
conventional methods. Simulation results show that our proposed regional robust
SPWT design is capable of substantially improving the secrecy rate compared to
the conventional non-robust method. The P-SLNR maximization-based method has
the comparable secrecy performance with a lower complexity than that of the
R-SLNR maximization-based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04504</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04504</id><created>2019-04-09</created><authors><author><keyname>Dogan</keyname><forenames>Daghan</forenames></author><author><keyname>Boyraz</keyname><forenames>Pinar</forenames></author></authors><title>Smart Traction Control Systems for Electric Vehicles Using Acoustic
  Road-type Estimation</title><categories>eess.SP cs.SY</categories><comments>Accepted to be published by IEEE Trans. on Intelligent Vehicles, 22
  Jan 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of traction control systems (TCS) for electric vehicles (EV)
has great potential due to easy implementation of torque control with
direct-drive motors. However, the control system usually requires road-tire
friction and slip-ratio values, which must be estimated. While it is not
possible to obtain the first one directly, the estimation of latter value
requires accurate measurements of chassis and wheel velocity. In addition,
existing TCS structures are often designed without considering the robustness
and energy efficiency of torque control. In this work, both problems are
addressed with a smart TCS design having an integrated acoustic road-type
estimation (ARTE) unit. This unit enables the road-type recognition and this
information is used to retrieve the correct look-up table between friction
coefficient and slip-ratio. The estimation of the friction coefficient helps
the system to update the necessary input torque. The ARTE unit utilizes machine
learning, mapping the acoustic feature inputs to road-type as output. In this
study, three existing TCS for EVs are examined with and without the integrated
ARTE unit. The results show significant performance improvement with ARTE,
reducing the slip ratio by 75% while saving energy via reduction of applied
torque and increasing the robustness of the TCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04511</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04511</id><created>2019-04-09</created><authors><author><keyname>Llombart</keyname><forenames>Jorge</forenames></author><author><keyname>Ribas</keyname><forenames>Dayana</forenames></author><author><keyname>Miguel</keyname><forenames>Antonio</forenames></author><author><keyname>Vicente</keyname><forenames>Luis</forenames></author><author><keyname>Ortega</keyname><forenames>Alfonso</forenames></author><author><keyname>Lleida</keyname><forenames>Eduardo</forenames></author></authors><title>Progressive Speech Enhancement with Residual Connections</title><categories>eess.AS</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the Speech Enhancement based on Deep Neural Networks. The
proposed architecture gradually follows the signal transformation during
enhancement by means of a visualization probe at each network block. Alongside
the process, the enhancement performance is visually inspected and evaluated in
terms of regression cost. This progressive scheme is based on Residual
Networks. During the process, we investigate a residual connection with a
constant number of channels, including internal state between blocks, and
adding progressive supervision. The insights provided by the interpretation of
the network enhancement process leads us to design an improved architecture for
the enhancement purpose. Following this strategy, we are able to obtain speech
enhancement results beyond the state-of-the-art, achieving a favorable
trade-off between dereverberation and the amount of spectral distortion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04528</identifier>
 <datestamp>2019-12-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04528</id><created>2019-04-09</created><updated>2019-11-29</updated><authors><author><keyname>G&#xfc;ltekin</keyname><forenames>Yunus Can</forenames></author><author><keyname>van Houtum</keyname><forenames>W. J.</forenames></author><author><keyname>Koppelaar</keyname><forenames>Arie</forenames></author><author><keyname>Willems</keyname><forenames>Frans M. J.</forenames></author></authors><title>Partial Enumerative Sphere Shaping</title><categories>eess.SP</categories><comments>6 pages, 6 figures</comments><doi>10.1109/VTCFall.2019.8891195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dependency between the Gaussianity of the input distribution for the
additive white Gaussian noise (AWGN) channel and the gap-to-capacity is
discussed. We show that a set of particular approximations to the
Maxwell-Boltzmann (MB) distribution virtually closes most of the shaping gap.
We relate these symbol-level distributions to bit-level distributions, and
demonstrate that they correspond to keeping some of the amplitude bit-levels
uniform and independent of the others. Then we propose partial enumerative
sphere shaping (P-ESS) to realize such distributions in the probabilistic
amplitude shaping (PAS) framework. Simulations over the AWGN channel exhibit
that shaping 2 amplitude bits of 16-ASK have almost the same performance as
shaping 3 bits, which is 1.3 dB more power-efficient than uniform signaling at
a rate of 3 bit/symbol. In this way, required storage and computational
complexity of shaping are reduced by factors of 6 and 3, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04548</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04548</id><created>2019-04-09</created><authors><author><keyname>Saeed</keyname><forenames>Sarah O. M.</forenames></author><author><keyname>Mohamed</keyname><forenames>Sanaa Hamid</forenames></author><author><keyname>Alsulami</keyname><forenames>Osama Zwaid</forenames></author><author><keyname>Alresheedi</keyname><forenames>Mohammed T.</forenames></author><author><keyname>Elmirghani</keyname><forenames>Jaafar M. H.</forenames></author></authors><title>Optimized Resource Allocation in Multi-user WDM VLC Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the optimization of wavelength resource allocation
in multi-user WDM Visible Light Communication (VLC) systems. A Mixed Integer
Linear Programming (MILP) model that maximizes the sum of
Signal-to-Interference-plus-Noise-Ratio (SINR) for all users is utilized. The
results show that optimizing the wavelength allocation in multi-user WDM VLC
systems can reduce the impact of the interference and improve the system
throughput in terms of the sum of data rates for up to 7 users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04589</identifier>
 <datestamp>2019-07-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04589</id><created>2019-04-09</created><updated>2019-07-04</updated><authors><author><keyname>Chettri</keyname><forenames>Bhusan</forenames></author><author><keyname>Stoller</keyname><forenames>Daniel</forenames></author><author><keyname>Morfi</keyname><forenames>Veronica</forenames></author><author><keyname>Ram&#xed;rez</keyname><forenames>Marco A. Mart&#xed;nez</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author><author><keyname>Sturm</keyname><forenames>Bob L.</forenames></author></authors><title>Ensemble Models for Spoofing Detection in Automatic Speaker Verification</title><categories>eess.AS cs.SD</categories><comments>Accepted at Interspeech 2019, Graz, Austria</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Detecting spoofing attempts of automatic speaker verification (ASV) systems
is challenging, especially when using only one modeling approach. For
robustness, we use both deep neural networks and traditional machine learning
models and combine them as ensemble models through logistic regression. They
are trained to detect logical access (LA) and physical access (PA) attacks on
the dataset released as part of the ASV Spoofing and Countermeasures Challenge
2019. We propose dataset partitions that ensure different attack types are
present during training and validation to improve system robustness. Our
ensemble model outperforms all our single models and the baselines from the
challenge for both attack types. We investigate why some models on the PA
dataset strongly outperform others and find that spoofed recordings in the
dataset tend to have longer silences at the end than genuine ones. By removing
them, the PA task becomes much more challenging, with the tandem detection cost
function (t-DCF) of our best single model rising from 0.1672 to 0.5018 and
equal error rate (EER) increasing from 5.98% to 19.8% on the development set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04594</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04594</id><created>2019-04-09</created><authors><author><keyname>Sasikumar</keyname><forenames>Harish</forenames></author><author><keyname>Varma</keyname><forenames>Manoj M.</forenames></author></authors><title>Tracking-free Determination of Microparticle Motion from Image Variance</title><categories>physics.app-ph cond-mat.soft eess.IV physics.data-an physics.flu-dyn</categories><comments>14 Pages of main text, 6 Pages of Supplementary Information 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we use the standard deviation of image pixel intensity to
analyse the speed, direction and surface-interaction of microparticles in
fluid. First, we present an analytical model for estimating the total variance
in the image space for directed or diffusive motion of microparticles and show
that this measure is correlated to the density and speed of the particles. The
analytical model was found to have good agreement with numerical simulations
for low particle density. Then, using only the local image variance we obtain
the magnitude and direction of the particle velocity in a rectangular
microfluidic channel, closely matching the theoretical profile. Further, we
also demonstrate the application of this method as a probe for particle-surface
interactions by extracting the differences in distribution and time-evolution
of image variance from mobile microparticles adhering to different surfaces. We
believe that the image variance based method described here presents an
addition to the suite of tracking-free techniques such as Differential Dynamic
Microscopy (DDM) to extract motility parameters from video data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04615</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04615</id><created>2019-04-09</created><authors><author><keyname>Ahmed</keyname><forenames>Abdul Haseeb</forenames></author><author><keyname>Qureshi</keyname><forenames>Ijaz M.</forenames></author><author><keyname>Shah</keyname><forenames>Jawad Ali</forenames></author><author><keyname>Omer</keyname><forenames>Hammad</forenames></author></authors><title>Efficient Reconstruction of Free Breathing Under-Sampled Cardiac Cine
  MRI</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Respiratory motion can cause strong blurring artifacts in the reconstructed
image during MR acquisition. These artifacts become more prominent when use in
the presence of undersampled data. Recently, compressed sensing (CS) is
developed as an MR reconstruction technique, to recover good quality images
from the compressive k-space samples. To maximize the benefits of CS in free
breathing data, it is understandable to use CS with the motion corrected
images. In this paper, we have developed a new CS based motion corrected image
reconstruction technique. In this two-stage technique, we use similarity
measure to sort the motion corrupted data into different respiratory states.
Then, we use a new reconstruction algorithm, which iteratively performs
reconstruction and motion correction. The performance of the proposed method is
qualitatively and quantitively evaluated using simulated data and clinical
data. Results depict that this method performs the better reconstruction of
respiratory motion corrected cardiac cine images as compared to the CS based
reconstruction method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04631</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04631</id><created>2019-04-09</created><authors><author><keyname>Kaneko</keyname><forenames>Takuhiro</forenames></author><author><keyname>Kameoka</keyname><forenames>Hirokazu</forenames></author><author><keyname>Tanaka</keyname><forenames>Kou</forenames></author><author><keyname>Hojo</keyname><forenames>Nobukatsu</forenames></author></authors><title>CycleGAN-VC2: Improved CycleGAN-based Non-parallel Voice Conversion</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Accepted to ICASSP 2019. Project page:
  http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc2/index.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-parallel voice conversion (VC) is a technique for learning the mapping
from source to target speech without relying on parallel data. This is an
important task, but it has been challenging due to the disadvantages of the
training conditions. Recently, CycleGAN-VC has provided a breakthrough and
performed comparably to a parallel VC method without relying on any extra data,
modules, or time alignment procedures. However, there is still a large gap
between the real target and converted speech, and bridging this gap remains a
challenge. To reduce this gap, we propose CycleGAN-VC2, which is an improved
version of CycleGAN-VC incorporating three new techniques: an improved
objective (two-step adversarial losses), improved generator (2-1-2D CNN), and
improved discriminator (PatchGAN). We evaluated our method on a non-parallel VC
task and analyzed the effect of each technique in detail. An objective
evaluation showed that these techniques help bring the converted feature
sequence closer to the target in terms of both global and local structures,
which we assess by using Mel-cepstral distortion and modulation spectra
distance, respectively. A subjective evaluation showed that CycleGAN-VC2
outperforms CycleGAN-VC in terms of naturalness and similarity for every
speaker pair, including intra-gender and inter-gender pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04647</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04647</id><created>2019-04-09</created><authors><author><keyname>Grigorios</keyname><forenames>Kalogiannis</forenames></author><author><keyname>George</keyname><forenames>Chassapis</forenames></author><author><keyname>Magda</keyname><forenames>Tsolaki</forenames></author></authors><title>Removing muscle artifacts from EEG data of people with cognitive
  impairment using high order statistic methods</title><categories>eess.SP</categories><comments>9 pages</comments><journal-ref>Hell J Nucl Med. 2019 Jan-Apr;22 Suppl:165-173</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: Often, people with Subjective Cognitive Impairment (SCI), Mild
Cognitive Impairment (MCI) and dementia are underwent to Electroencephalography
(EEG) in order to evaluate through biological indexes the functional
connectivity between brain regions and activation areas during cognitive
performance. EEG recordings are frequently contaminated by muscle artifacts,
which obscure and complicate their interpretation. These muscle artifacts are
particularly difficult to be removed from the EEG in order the latter to be
used for further clinical evaluation. In this paper, we proposed a new approach
in removing muscle artifacts from EEG data using a method that combines second
and high order statistical information. Subjects and Methods: In the proposed
system the muscle artifacts of the EEG signal are removed by using the
Independent Vector Analysis (IVA). The latter was formulated as a general joint
Blind Source Separation (BSS) method that uses both second-order and higher
order statistical information and thus takes advantage of both Independent
Component Analysis (ICA) and Canonical Correlation Analysis (CCA).
Diagonalization methods for IVA in the proposed system were reworked based on
SCHUR decomposition offering a faster second order blind identification
algorithm that can be used on time demanding applications. Results: The
proposed method is evaluated in both simulated and real EEG data. To
quantitatively examine the performance of the new method, two objective
measures were adopted. The first measure is the Root Mean Square Error (RMSE)
while the second is the Signal-to-noise-ratio (SNR). Conclusion: The proposed
method overcomes with the need of removing muscle artifacts on both realistic
simulated EEG data and brain activity from people with cognitive impairment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04671</identifier>
 <datestamp>2019-04-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04671</id><created>2019-04-07</created><authors><author><keyname>Arikan</keyname><forenames>Selim</forenames></author><author><keyname>Varanasi</keyname><forenames>Kiran</forenames></author><author><keyname>Stricker</keyname><forenames>Didier</forenames></author></authors><title>Surface Defect Classification in Real-Time Using Convolutional Neural
  Networks</title><categories>eess.IV cs.CV cs.LG stat.ML</categories><comments>Supplementary material will follow</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surface inspection systems are an important application domain for computer
vision, as they are used for defect detection and classification in the
manufacturing industry. Existing systems use hand-crafted features which
require extensive domain knowledge to create. Even though Convolutional neural
networks (CNNs) have proven successful in many large-scale challenges,
industrial inspection systems have yet barely realized their potential due to
two significant challenges: real-time processing speed requirements and
specialized narrow domain-specific datasets which are sometimes limited in
size. In this paper, we propose CNN models that are specifically designed to
handle capacity and real-time speed requirements of surface inspection systems.
To train and evaluate our network models, we created a surface image dataset
containing more than 22000 labeled images with many types of surface materials
and achieved 98.0% accuracy in binary defect classification. To solve the class
imbalance problem in our datasets, we introduce neural data augmentation
methods which are also applicable to similar domains that suffer from the same
problem. Our results show that deep learning based methods are feasible to be
used in surface inspection systems and outperform traditional methods in
accuracy and inference time by considerable margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04673</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04673</id><created>2019-04-07</created><updated>2019-06-01</updated><authors><author><keyname>K&#xfc;r&#xfc;m</keyname><forenames>Ulas</forenames></author><author><keyname>Wiecha</keyname><forenames>P. R.</forenames></author><author><keyname>French</keyname><forenames>Rebecca</forenames></author><author><keyname>Muskens</keyname><forenames>Otto L.</forenames></author></authors><title>Deep Learning Enabled Real Time Speckle Recognition and Hyperspectral
  Imaging using a Multimode Fiber Array</title><categories>eess.IV physics.optics</categories><comments>12 pages, 6 figures + Appendix of 5 pages and 5 figures</comments><journal-ref>Optics Express 27(15), 20965-20979 (2019)</journal-ref><doi>10.1364/OE.27.020965</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate the use of deep learning for fast spectral deconstruction of
speckle patterns. The artificial neural network can be effectively trained
using numerically constructed multispectral datasets taken from a measured
spectral transmission matrix. Optimized neural networks trained on these
datasets achieve reliable reconstruction of both discrete and continuous
spectra from a monochromatic camera image. Deep learning is compared to
analytical inversion methods as well as to a compressive sensing algorithm and
shows favourable characteristics both in the oversampling and in the sparse
undersampling (compressive) regimes. The deep learning approach offers
significant advantages in robustness to drift or noise and in reconstruction
speed. In a proof-of-principle demonstrator we achieve real time recovery of
hyperspectral information using a multi-core, multi-mode fiber array as a
random scattering medium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04765</identifier>
 <datestamp>2019-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04765</id><created>2019-04-09</created><updated>2019-10-11</updated><authors><author><keyname>Fang</keyname><forenames>Song</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Generic Variance Bounds on Estimation and Prediction Errors in Time
  Series Analysis: An Entropy Perspective</title><categories>cs.IT cs.LG eess.SP math.IT math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we obtain generic bounds on the variances of estimation and
prediction errors in time series analysis via an information-theoretic
approach. It is seen in general that the error bounds are determined by the
conditional entropy of the data point to be estimated or predicted given the
side information or past observations. Additionally, we discover that in order
to achieve the prediction error bounds asymptotically, the necessary and
sufficient condition is that the &quot;innovation&quot; is asymptotically white Gaussian.
When restricted to Gaussian processes and 1-step prediction, our bounds are
shown to reduce to the Kolmogorov-Szeg\&quot;o formula and Wiener-Masani formula
known from linear prediction theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04794</identifier>
 <datestamp>2019-05-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04794</id><created>2019-04-09</created><updated>2019-05-24</updated><authors><author><keyname>Chaudhuri</keyname><forenames>Ushasi</forenames></author><author><keyname>Banerjee</keyname><forenames>Biplab</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Avik</forenames></author><author><keyname>Datcu</keyname><forenames>Mihai</forenames></author></authors><title>CMIR-NET : A Deep Learning Based Model For Cross-Modal Retrieval In
  Remote Sensing</title><categories>eess.IV cs.CV cs.IR</categories><doi>1904.04794</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of cross-modal information retrieval in the domain of
remote sensing. In particular, we are interested in two application scenarios:
i) cross-modal retrieval between panchromatic (PAN) and multi-spectral imagery,
and ii) multi-label image retrieval between very high resolution (VHR) images
and speech based label annotations. Notice that these multi-modal retrieval
scenarios are more challenging than the traditional uni-modal retrieval
approaches given the inherent differences in distributions between the
modalities. However, with the growing availability of multi-source remote
sensing data and the scarcity of enough semantic annotations, the task of
multi-modal retrieval has recently become extremely important. In this regard,
we propose a novel deep neural network based architecture which is considered
to learn a discriminative shared feature space for all the input modalities,
suitable for semantically coherent information retrieval. Extensive experiments
are carried out on the benchmark large-scale PAN - multi-spectral DSRSID
dataset and the multi-label UC-Merced dataset. Together with the Merced
dataset, we generate a corpus of speech signals corresponding to the labels.
Superior performance with respect to the current state-of-the-art is observed
in all the cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04884</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04884</id><created>2019-04-09</created><authors><author><keyname>Mallery</keyname><forenames>Kevin</forenames></author><author><keyname>Hong</keyname><forenames>Jiarong</forenames></author></authors><title>Regularized Inverse Holographic Volume Reconstruction for 3D Particle
  Tracking</title><categories>eess.IV physics.flu-dyn physics.optics</categories><comments>15 pages, 6 figures</comments><doi>10.1364/OE.27.018069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key limitations of digital inline holography (DIH) for particle tracking
applications are poor longitudinal resolution, particle concentration limits,
and case-specific processing. We utilize an inverse problem method with fused
lasso regularization to perform full volumetric reconstructions of particle
fields. By exploiting data sparsity in the solution and utilizing GPU
processing, we dramatically reduce the computational cost usually associated
with inverse reconstruction approaches. We demonstrate the accuracy of the
proposed method using synthetic and experimental holograms. Finally, we present
two practical applications (high concentration microorganism swimming and
microfiber rotation) to extend the capabilities of DIH beyond what was possible
using prior methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04929</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04929</id><created>2019-04-09</created><authors><author><keyname>Jereminov</keyname><forenames>Marko</forenames></author><author><keyname>Jovicic</keyname><forenames>Aleksandar</forenames></author><author><keyname>Wagner</keyname><forenames>Martin R.</forenames></author><author><keyname>Hug</keyname><forenames>Gabriela</forenames></author><author><keyname>Pileggi</keyname><forenames>Larry</forenames></author></authors><title>Equivalent Circuit Programming for Estimating the State of a Power
  System</title><categories>eess.SP</categories><comments>Accepted for the 2019 Powertech Milan conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Equivalent Circuit Programming (ECP) approach that expresses the
optimality conditions of an optimization problem in terms of an equivalent
circuit model and uses circuit simulation techniques to solve for an optimal
solution, is applied to the state estimation problem for power systems. The
benefits of using an equivalent circuit formulation for incorporating both
Phasor Measurement Units (PMU) and Remote Terminal Units (RTU), as well as for
reducing the nonlinearities of the state estimation problem was previously
demonstrated. In this paper we further exploit the circuit nature of the state
estimation problem to formulate not only the model but also the optimality
conditions as an ECP problem. The efficiency and accuracy of our approach are
demonstrated by estimating the states of large-scale power grids (80k+ buses).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04932</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04932</id><created>2019-04-09</created><authors><author><keyname>Jereminov</keyname><forenames>Marko</forenames></author><author><keyname>Terzakis</keyname><forenames>Athanasios</forenames></author><author><keyname>Wagner</keyname><forenames>Martin</forenames></author><author><keyname>Pandey</keyname><forenames>Amritanshu</forenames></author><author><keyname>Pileggi</keyname><forenames>Larry</forenames></author></authors><title>Robust and Efficient Power Flow Convergence with G-min Stepping Homotopy
  Method</title><categories>eess.SP</categories><comments>Accepted for IEEE Conference on Environment, Electrical Engineering
  and I&amp;CPS Europe, Genoa, Italy, June 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances have shown that the circuit simulation algorithms that allow
for solving highly nonlinear circuits of over one billion variables can be
applicable to power system simulation and optimization problems through the use
of an equivalent circuit formulation. It was demonstrated that large-scale
(80k+ buses) power flow simulations can be robustly solved, independent of the
initial starting point. In this paper, we extend the electronic circuit-based
G-min stepping homotopy method to power flow simulations. Preliminary results
indicate that the proposed algorithm results in significantly better simulation
runtime performance when compared to existing homotopy methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04956</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04956</id><created>2019-04-09</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Cui</keyname><forenames>Xiaodong</forenames></author><author><keyname>Finkler</keyname><forenames>Ulrich</forenames></author><author><keyname>Kingsbury</keyname><forenames>Brian</forenames></author><author><keyname>Saon</keyname><forenames>George</forenames></author><author><keyname>Kung</keyname><forenames>David</forenames></author><author><keyname>Picheny</keyname><forenames>Michael</forenames></author></authors><title>Distributed Deep Learning Strategies For Automatic Speech Recognition</title><categories>cs.SD cs.CL cs.LG eess.AS stat.ML</categories><comments>Published in ICASSP'19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and investigate a variety of distributed deep
learning strategies for automatic speech recognition (ASR) and evaluate them
with a state-of-the-art Long short-term memory (LSTM) acoustic model on the
2000-hour Switchboard (SWB2000), which is one of the most widely used datasets
for ASR performance benchmark. We first investigate what are the proper
hyper-parameters (e.g., learning rate) to enable the training with sufficiently
large batch size without impairing the model accuracy. We then implement
various distributed strategies, including Synchronous (SYNC), Asynchronous
Decentralized Parallel SGD (ADPSGD) and the hybrid of the two HYBRID, to study
their runtime/accuracy trade-off. We show that we can train the LSTM model
using ADPSGD in 14 hours with 16 NVIDIA P100 GPUs to reach a 7.6% WER on the
Hub5- 2000 Switchboard (SWB) test set and a 13.1% WER on the CallHome (CH) test
set. Furthermore, we can train the model using HYBRID in 11.5 hours with 32
NVIDIA V100 GPUs without loss in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.04982</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.04982</id><created>2019-04-09</created><authors><author><keyname>Ahmed</keyname><forenames>Abdul Haseeb</forenames></author><author><keyname>Qureshi</keyname><forenames>Ijaz M.</forenames></author></authors><title>Motion correction in cardiac perfusion data by using robust matrix
  decomposition</title><categories>eess.IV physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion free reconstruction of compressively sampled cardiac perfusion MR
images is a challenging problem. It is due to the aliasing artifacts and the
rapid contrast changes in the reconstructed perfusion images. In addition to
the reconstruction limitations, many registration algorithms under perform in
the presence of the rapid intensity changes. In this paper, we propose a novel
motion correction method that reconstructs the motion free image series from
the undersampled cardiac perfusion MR data. The motion correction method uses
the novel robust principal component analysis based reconstruction along with
the periodic decomposition to separate the respiratory motion component that
can be registered, from the contrast intensity variations. It is tested on
simulated data and the clinically acquired data. The performance of the method
is qualitatively assessed and compared with the existing motion correction
methods. The proposed method is validated by comparing manually acquired
time-intensity curves of the myocardial sectors to automatically generated
curves before and after registration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05009</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05009</id><created>2019-04-10</created><authors><author><keyname>Martin</keyname><forenames>Charles P</forenames></author><author><keyname>Torresen</keyname><forenames>Jim</forenames></author></authors><title>An Interactive Musical Prediction System with Mixture Density Recurrent
  Neural Networks</title><categories>cs.SD cs.HC cs.NE eess.AS</categories><comments>Accepted for presentation at the International Conference on New
  Interfaces for Musical Expression (NIME), June 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is about creating digital musical instruments where a predictive
neural network model is integrated into the interactive system. Rather than
predicting symbolic music (e.g., MIDI notes), we suggest that predicting future
control data from the user and precise temporal information can lead to new and
interesting interactive possibilities. We propose that a mixture density
recurrent neural network (MDRNN) is an appropriate model for this task. The
predictions can be used to fill-in control data when the user stops performing,
or as a kind of filter on the user's own input. We present an interactive MDRNN
prediction server that allows rapid prototyping of new NIMEs featuring
predictive musical interaction by recording datasets, training MDRNN models,
and experimenting with interaction modes. We illustrate our system with several
example NIMEs applying this idea. Our evaluation shows that real-time
predictive interaction is viable even on single-board computers and that small
models are appropriate for small datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05014</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05014</id><created>2019-04-10</created><authors><author><keyname>P</keyname><forenames>Rashmi</forenames></author><author><keyname>A</keyname><forenames>Manoj</forenames></author><author><keyname>Kannu</keyname><forenames>Arun Pachai</forenames></author></authors><title>Cell Discovery in Millimeter Wave Systems: Physical Layer
  Implementations</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell discovery is the procedure in which an user equipment (UE) in a cellular
network finds a suitable base station (BS) and its physical layer cell
identity, in order to establish a link-layer connection. When beamforming with
antenna arrays is done at both transmitter and receiver, cell discovery in mm
wave systems also involves finding the correct angle of arrival (AoA) - angle
of departure (AoD) alignment between the UE and the detected BS. In this paper,
we consider various existing and new schemes for cell discovery, present
analytical studies on their detection probability and compare them in a common
framework. In the first part, we study the conventional beam sweep technique
and its variations, and present their physical layer training phase in detail.
While the traditional beam sweep can not directly find the identity of the
detected BS, we provide modifications in its training phase to enable the cell
identity detection. In the second part of the paper, exploiting the sparseness
of the mm wave channels and using an equivalent compressive sensing measurement
model, we develop new cell discovery schemes with lesser overheads. One such
design involves mutually unbiased bases (MUB) from quantum information theory.
For the MUB based training scheme, we characterize the mutual coherence
parameter of the resulting sensing matrix and establish its connection to the
detection probability. We also present detailed simulation studies using
experimentally driven mm wave channel simulators and show that our MUB based
scheme gives superior performance compared to all the other schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05073</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05073</id><created>2019-04-10</created><authors><author><keyname>Verma</keyname><forenames>Prateek</forenames></author><author><keyname>Chafe</keyname><forenames>Chris</forenames></author><author><keyname>Berger</keyname><forenames>Jonathan</forenames></author></authors><title>Neuralogram: A Deep Neural Network Based Representation for Audio
  Signals</title><categories>cs.SD cs.LG cs.MM eess.AS</categories><comments>Submitted to DAFx 2019, the 22nd International Conference on Digital
  Audio Effects, Birmingham, United Kingdom</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the Neuralogram -- a deep neural network based representation for
understanding audio signals which, as the name suggests, transforms an audio
signal to a dense, compact representation based upon embeddings learned via a
neural architecture. Through a series of probing signals, we show how our
representation can encapsulate pitch, timbre and rhythm-based information, and
other attributes. This representation suggests a method for revealing
meaningful relationships in arbitrarily long audio signals that are not readily
represented by existing algorithms. This has the potential for numerous
applications in audio understanding, music recommendation, meta-data extraction
to name a few.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05078</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05078</id><created>2019-04-10</created><authors><author><keyname>Chen</keyname><forenames>Yi-Chen</forenames></author><author><keyname>Huang</keyname><forenames>Sung-Feng</forenames></author><author><keyname>Lee</keyname><forenames>Hung-yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>From Semi-supervised to Almost-unsupervised Speech Recognition with
  Very-low Resource by Jointly Learning Phonetic Structures from Audio and Text
  Embeddings</title><categories>cs.CL cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Producing a large amount of annotated speech data for training ASR systems
remains difficult for more than 95% of languages all over the world which are
low-resourced. However, we note human babies start to learn the language by the
sounds (or phonetic structures) of a small number of exemplar words, and
&quot;generalize&quot; such knowledge to other words without hearing a large amount of
data. We initiate some preliminary work in this direction. Audio Word2Vec is
used to learn the phonetic structures from spoken words (signal segments),
while another autoencoder is used to learn the phonetic structures from text
words. The relationships among the above two can be learned jointly, or
separately after the above two are well trained. This relationship can be used
in speech recognition with very low resource. In the initial experiments on the
TIMIT dataset, only 2.1 hours of speech data (in which 2500 spoken words were
annotated and the rest unlabeled) gave a word error rate of 44.6%, and this
number can be reduced to 34.2% if 4.1 hr of speech data (in which 20000 spoken
words were annotated) were given. These results are not satisfactory, but a
good starting point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05086</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05086</id><created>2019-04-10</created><authors><author><keyname>Cuesta</keyname><forenames>Helena</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Emilia</forenames></author><author><keyname>Chandna</keyname><forenames>Pritish</forenames></author></authors><title>A Framework for Multi-f0 Modeling in SATB Choir Recordings</title><categories>cs.SD cs.LG cs.MM eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fundamental frequency (f0) modeling is an important but relatively unexplored
aspect of choir singing. Performance evaluation as well as auditory analysis of
singing, whether individually or in a choir, often depend on extracting f0
contours for the singing voice. However, due to the large number of singers,
singing at a similar frequency range, extracting the exact individual pitch
contours from choir recordings is a challenging task. In this paper, we address
this task and develop a methodology for modeling pitch contours of SATB choir
recordings. A typical SATB choir consists of four parts, each covering a
distinct range of pitches and often with multiple singers each. We first
evaluate some state-of-the-art multi-f0 estimation systems for the particular
case of choirs with a single singer per part, and observe that the pitch of
individual singers can be estimated to a relatively high degree of accuracy. We
observe, however, that the scenario of multiple singers for each choir part
(i.e. unison singing) is far more challenging. In this work we propose a
methodology based on combining a multi-f0 estimation methodology based on deep
learning followed by a set of traditional DSP techniques to model f0 and its
dispersion instead of a single f0 trajectory for each choir part. We present
and discuss our observations and test our framework with different singer
configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05166</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05166</id><created>2019-04-10</created><authors><author><keyname>Li</keyname><forenames>Xiaofei</forenames></author><author><keyname>Leglaive</keyname><forenames>Simon</forenames></author><author><keyname>Girin</keyname><forenames>Laurent</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Audio-noise Power Spectral Density Estimation Using Long Short-term
  Memory</title><categories>eess.SP cs.SD eess.AS</categories><comments>Submitted to IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2019.2911879</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method using a long short-term memory (LSTM) network to estimate
the noise power spectral density (PSD) of single-channel audio signals
represented in the short time Fourier transform (STFT) domain. An LSTM network
common to all frequency bands is trained, which processes each frequency band
individually by mapping the noisy STFT magnitude sequence to its corresponding
noise PSD sequence. Unlike deep-learning-based speech enhancement methods that
learn the full-band spectral structure of speech segments, the proposed method
exploits the sub-band STFT magnitude evolution of noise with a long time
dependency, in the spirit of the unsupervised noise estimators described in the
literature. Speaker- and speech-independent experiments with different types of
noise show that the proposed method outperforms the unsupervised estimators,
and generalizes well to noise types that are not present in the training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05167</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05167</id><created>2019-04-09</created><authors><author><keyname>Llombart</keyname><forenames>Jorge</forenames></author><author><keyname>Ribas</keyname><forenames>Dayana</forenames></author><author><keyname>Miguel</keyname><forenames>Antonio</forenames></author><author><keyname>Vicente</keyname><forenames>Luis</forenames></author><author><keyname>Ortega</keyname><forenames>Alfonso</forenames></author><author><keyname>Lleida</keyname><forenames>Eduardo</forenames></author></authors><title>Speech Enhancement with Wide Residual Networks in Reverberant
  Environments</title><categories>eess.AS cs.SD</categories><comments>5 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1901.00660, arXiv:1904.04511</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a speech enhancement method which exploits the high
potential of residual connections in a Wide Residual Network architecture. This
is supported on single dimensional convolutions computed alongside the time
domain, which is a powerful approach to process contextually correlated
representations through the temporal domain, such as speech feature sequences.
We find the residual mechanism extremely useful for the enhancement task since
the signal always has a linear shortcut and the non-linear path enhances it in
several steps by adding or subtracting corrections. The enhancement capability
of the proposal is assessed by objective quality metrics evaluated with
simulated and real samples of reverberated speech signals. Results show that
the proposal outperforms the state-of-the-art method called WPE, which is known
to effectively reduce reverberation and greatly enhance the signal. The
proposed model, trained with artificial synthesized reverberation data, was
able to generalize to real room impulse responses for a variety of conditions
(e.g. different room sizes, $RT_{60}$, near &amp; far field). Furthermore, it
achieves accuracy for real speech with reverberation from two different
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05178</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05178</id><created>2019-04-10</created><authors><author><keyname>Ricco</keyname><forenames>Rodrigo A.</forenames></author><author><keyname>Teixeira</keyname><forenames>Bruno O. S.</forenames></author></authors><title>Least-Squares Parameter Estimation for State-Space Models with State
  Equality Constraints</title><categories>cs.SY eess.SP</categories><comments>Submitted to review</comments><msc-class>93E24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If a dynamic system has active constraints on the state vector and they are
known, then taking them into account during modeling is often advantageous.
Unfortunately, in the constrained discrete-time state-space estimation, the
state equality constraint is defined for a parameter matrix and not on a
parameter vector as commonly found in regression problems. To address this
problem, firstly, we show how to rewrite the state equality constraints as
equality constraints on the state matrices to be estimated. Then, we vectorize
the matricial least squares problem defined for modeling state-space systems
such that any method from the equality-constrained least squares framework may
be employed. Both time-invariant and time-varying cases are considered as well
as the case where the state equality constraint is not exactly known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05204</identifier>
 <datestamp>2019-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05204</id><created>2019-04-10</created><updated>2019-04-26</updated><authors><author><keyname>Song</keyname><forenames>Hongwei</forenames></author><author><keyname>Han</keyname><forenames>Jiqing</forenames></author><author><keyname>Deng</keyname><forenames>Shiwen</forenames></author><author><keyname>Du</keyname><forenames>Zhihao</forenames></author></authors><title>Acoustic Scene Classification by Implicitly Identifying Distinct Sound
  Events</title><categories>cs.SD cs.LG eess.AS</categories><comments>code URL typo, code is available at
  https://github.com/hackerekcah/distinct-events-asc.git</comments><journal-ref>Proc. Interspeech 2019, 3860-3864</journal-ref><doi>10.21437/Interspeech.2019-2231</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new strategy for acoustic scene classification
(ASC) , namely recognizing acoustic scenes through identifying distinct sound
events. This differs from existing strategies, which focus on characterizing
global acoustical distributions of audio or the temporal evolution of
short-term audio features, without analysis down to the level of sound events.
To identify distinct sound events for each scene, we formulate ASC in a
multi-instance learning (MIL) framework, where each audio recording is mapped
into a bag-of-instances representation. Here, instances can be seen as
high-level representations for sound events inside a scene. We also propose a
MIL neural networks model, which implicitly identifies distinct instances
(i.e., sound events). Furthermore, we propose two specially designed modules
that model the multi-temporal scale and multi-modal natures of the sound events
respectively. The experiments were conducted on the official development set of
the DCASE2018 Task1 Subtask B, and our best-performing model improves over the
official baseline by 9.4% (68.3% vs 58.9%) in terms of classification accuracy.
This study indicates that recognizing acoustic scenes by identifying distinct
sound events is effective and paves the way for future studies that combine
this strategy with previous ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05242</identifier>
 <datestamp>2019-06-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05242</id><created>2019-04-10</created><updated>2019-06-11</updated><authors><author><keyname>Liu</keyname><forenames>Xiao</forenames></author><author><keyname>Liu</keyname><forenames>Yuanwei</forenames></author><author><keyname>Chen</keyname><forenames>Yue</forenames></author></authors><title>Reinforcement Learning in Multiple-UAV Networks: Deployment and Movement
  Design</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel framework is proposed for quality of experience (QoE)-driven
deployment and dynamic movement of multiple unmanned aerial vehicles (UAVs).
The problem of joint non-convex three-dimensional (3D) deployment and dynamic
movement of the UAVs is formulated for maximizing the sum mean opinion score
(MOS) of ground users, which is proved to be NP-hard. In the aim of solving
this pertinent problem, a three-step approach is proposed for attaining 3D
deployment and dynamic movement of multiple UAVs. Firstly, genetic algorithm
based K-means (GAK-means) algorithm is utilized for obtaining the cell
partition of the users. Secondly, Q-learning based deployment algorithm is
proposed, in which each UAV acts as an agent, making their own decision for
attaining 3D position by learning from trial and mistake. In contrast to
conventional genetic algorithm based learning algorithms, the proposed
algorithm is capable of training the direction selection strategy offline.
Thirdly, Q-learning based movement algorithm is proposed in the scenario that
the users are roaming. The proposed algorithm is capable of converging to an
optimal state. Numerical results reveal that the proposed algorithms show a
fast convergence rate after a small number of iterations. Additionally, the
proposed Q-learning based deployment algorithm outperforms K-means algorithms
and Iterative-GAKmean (IGK) algorithms with a low complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05243</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05243</id><created>2019-04-10</created><authors><author><keyname>Song</keyname><forenames>Hongwei</forenames></author><author><keyname>Han</keyname><forenames>Jiqing</forenames></author><author><keyname>Deng</keyname><forenames>Shiwen</forenames></author></authors><title>A Compact and Discriminative Feature Based on Auditory Summary
  Statistics for Acoustic Scene Classification</title><categories>cs.SD eess.AS eess.SP</categories><comments>Accepted as a conference paper of Interspeech 2018</comments><journal-ref>in Proceedings of the Annual Conference of the International
  Speech Communication Association, INTERSPEECH, vol. 2018-September, 2018, pp.
  3294-3298</journal-ref><doi>10.21437/Interspeech.2018-1299</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest challenges of acoustic scene classification (ASC) is to
find proper features to better represent and characterize environmental sounds.
Environmental sounds generally involve more sound sources while exhibiting less
structure in temporal spectral representations. However, the background of an
acoustic scene exhibits temporal homogeneity in acoustic properties, suggesting
it could be characterized by distribution statistics rather than temporal
details. In this work, we investigated using auditory summary statistics as the
feature for ASC tasks. The inspiration comes from a recent neuroscience study,
which shows the human auditory system tends to perceive sound textures through
time-averaged statistics. Based on these statistics, we further proposed to use
linear discriminant analysis to eliminate redundancies among these statistics
while keeping the discriminative information, providing an extreme com-pact
representation for acoustic scenes. Experimental results show the outstanding
performance of the proposed feature over the conventional handcrafted features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05249</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05249</id><created>2019-04-10</created><authors><author><keyname>Li</keyname><forenames>Xiaofei</forenames></author><author><keyname>Girin</keyname><forenames>Laurent</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Expectation-Maximization for Speech Source Separation Using Convolutive
  Transfer Function</title><categories>cs.SD eess.AS</categories><journal-ref>CAAI Transactions on Intelligent Technologies, 2019</journal-ref><doi>10.1049/trit.2018.1061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of under-determinded speech source
separation from multichannel microphone singals, i.e. the convolutive mixtures
of multiple sources. The time-domain signals are first transformed to the
short-time Fourier transform (STFT) domain. To represent the room filters in
the STFT domain, instead of the widely-used narrowband assumption, we propose
to use a more accurate model, i.e. the convolutive transfer function (CTF). At
each frequency band, the CTF coefficients of the mixing filters and the STFT
coefficients of the sources are jointly estimated by maximizing the likelihood
of the microphone signals, which is resolved by an Expectation-Maximization
(EM) algorithm. Experiments show that the proposed method provides very
satisfactory performance under highly reverberant environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05259</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05259</id><created>2019-04-10</created><authors><author><keyname>Gosztolya</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Pint&#xe9;r</keyname><forenames>&#xc1;d&#xe1;m</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Gr&#xf3;sz</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Mark&#xf3;</keyname><forenames>Alexandra</forenames></author><author><keyname>Csap&#xf3;</keyname><forenames>Tam&#xe1;s G&#xe1;bor</forenames></author></authors><title>Autoencoder-Based Articulatory-to-Acoustic Mapping for Ultrasound Silent
  Speech Interfaces</title><categories>cs.SD eess.AS</categories><comments>8 pages, 6 figures, Accepted to IJCNN 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When using ultrasound video as input, Deep Neural Network-based Silent Speech
Interfaces usually rely on the whole image to estimate the spectral parameters
required for the speech synthesis step. Although this approach is quite
straightforward, and it permits the synthesis of understandable speech, it has
several disadvantages as well. Besides the inability to capture the relations
between close regions (i.e. pixels) of the image, this pixel-by-pixel
representation of the image is also quite uneconomical. It is easy to see that
a significant part of the image is irrelevant for the spectral parameter
estimation task as the information stored by the neighbouring pixels is
redundant, and the neural network is quite large due to the large number of
input features. To resolve these issues, in this study we train an autoencoder
neural network on the ultrasound image; the estimation of the spectral speech
parameters is done by a second DNN, using the activations of the bottleneck
layer of the autoencoder network as features. In our experiments, the proposed
method proved to be more efficient than the standard approach: the measured
normalized mean squared error scores were lower, while the correlation values
were higher in each case. Based on the result of a listening test, the
synthesized utterances also sounded more natural to native speakers. A further
advantage of our proposed approach is that, due to the (relatively) small size
of the bottleneck layer, we can utilize several consecutive ultrasound images
during estimation without a significant increase in the network size, while
significantly increasing the accuracy of parameter estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05279</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05279</id><created>2019-04-10</created><authors><author><keyname>Hemmati</keyname><forenames>Mohammad</forenames></author><author><keyname>Rashtchi</keyname><forenames>Vahid</forenames></author><author><keyname>Maleki</keyname><forenames>Ahmad</forenames></author><author><keyname>Toofan</keyname><forenames>Siroos</forenames></author></authors><title>A Configurable Memristor-based Finite Impulse Response Filter</title><categories>eess.SP cs.AR</categories><comments>9 pages, 18 figures, 4 tables, and 8 equations, 44 high quality
  references, brief biographies of the authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two main methods to implement FIR filters: software and hardware.
In the software method, an FIR filter can be implemented within the processor
by programming; it uses too much memory and it is extremely time-consuming
while it gives the design more configurability. In most hardware-based
implementations of FIR filters, Analog-to-Digital (A/D) and Digital-to-Analog
(D/A) converters are mandatory and increase the cost. The most important
advantage of hardware implementation of a FIR filter is its higher speed
compared to its software counterpart. In this work, considering the advantages
of software and hardware approaches, a method to implement direct form FIR
filters using analog components and memristors is proposed. Not only the A/D
and D/A converters are omitted, but also using memristors avails
configurability. A new circuit is presented to handle negative coefficients of
the filter and memristance values are calculated using a heuristic method in
order to achieve a better accuracy in setting coefficients. Moreover, an
appropriate sample and delay topology is employed which overcomes the
limitations of the previous research in implementation of high-order filters.
Proper operation and usefulness of the proposed structures are all validated
via simulation in Cadence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05318</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05318</id><created>2019-04-01</created><authors><author><keyname>Sen</keyname><forenames>Arnesh</forenames></author><author><keyname>Sen</keyname><forenames>Kaustav</forenames></author><author><keyname>Das</keyname><forenames>Jayoti</forenames></author></authors><title>Ultrasonic Blind Stick for Completely Blind People to Avoid any Kind of
  Obstacles</title><categories>eess.SP cs.RO</categories><doi>10.1109/ICSENS.2018.8589680</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to live without being controlled by any action, judgment and any
outside factors including any opinions and regulations is defined by the term
Independent. But in reality physical movement for travelling or simply walking
through a crowded street pose great challenge for a visually impaired person.
Also they must learn every detail about the home environment such as placement
of tables; chairs etc. to prevent injury. Because of this disability they have
to sacrifice their independence in daily living by depending on the sighted
people in every busy place like bus, footpaths, railway stations etc. This
paper aims to design an artificial navigating system with adjustable
sensitivity with the help of ultrasonic proximity sensor to assist these blind
persons to walk fearlessly and independently in both indoor and outdoor
environment. This system can detect any type of upcoming obstacles and potholes
using the reflection properties of ultrasound. Attachment of the system to
different body areas makes its utilization more versatile and reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05351</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05351</id><created>2019-04-10</created><authors><author><keyname>He</keyname><forenames>Yunchao</forenames></author><author><keyname>Zhang</keyname><forenames>Haitong</forenames></author><author><keyname>Wang</keyname><forenames>Yujun</forenames></author></authors><title>RawNet: Fast End-to-End Neural Vocoder</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks based vocoders have recently demonstrated the powerful
ability to synthesize high quality speech. These models usually generate
samples by conditioning on some spectrum features, such as Mel-spectrum.
However, these features are extracted by using speech analysis module including
some processing based on the human knowledge. In this work, we proposed RawNet,
a truly end-to-end neural vocoder, which use a coder network to learn the
higher representation of signal, and an autoregressive voder network to
generate speech sample by sample. The coder and voder together act like an
auto-encoder network, and could be jointly trained directly on raw waveform
without any human-designed features. The experiments on the Copy-Synthesis
tasks show that RawNet can achieve the comparative synthesized speech quality
with LPCNet, with a smaller model architecture and faster speech generation at
the inference step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05375</identifier>
 <datestamp>2020-02-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05375</id><created>2019-04-10</created><updated>2020-01-31</updated><authors><author><keyname>Moyer</keyname><forenames>Daniel</forenames></author><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Tax</keyname><forenames>Chantal M. W.</forenames></author><author><keyname>Thompson</keyname><forenames>Paul M.</forenames></author></authors><title>Scanner Invariant Representations for Diffusion MRI Harmonization</title><categories>q-bio.QM cs.LG eess.IV stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: In the present work we describe the correction of diffusion-weighted
MRI for site and scanner biases using a novel method based on invariant
representation.
  Theory and Methods: Pooled imaging data from multiple sources are subject to
variation between the sources. Correcting for these biases has become very
important as imaging studies increase in size and multi-site cases become more
common. We propose learning an intermediate representation invariant to
site/protocol variables, a technique adapted from information theory-based
algorithmic fairness; by leveraging the data processing inequality, such a
representation can then be used to create an image reconstruction that is
uninformative of its original source, yet still faithful to underlying
structures. To implement this, we use a deep learning method based on
variational auto-encoders (VAE) to construct scanner invariant encodings of the
imaging data.
  Results: To evaluate our method, we use training data from the 2018 MICCAI
Computational Diffusion MRI (CDMRI) Challenge Harmonization dataset. Our
proposed method shows improvements on independent test data relative to a
recently published baseline method on each subtask, mapping data from three
different scanning contexts to and from one separate target scanning context.
  Conclusion: As imaging studies continue to grow, the use of pooled multi-site
imaging will similarly increase. Invariant representation presents a strong
candidate for the harmonization of these data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05441</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05441</id><created>2019-04-09</created><updated>2019-04-14</updated><authors><author><keyname>Todisco</keyname><forenames>Massimiliano</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Vestman</keyname><forenames>Ville</forenames></author><author><keyname>Sahidullah</keyname><forenames>Md</forenames></author><author><keyname>Delgado</keyname><forenames>Hector</forenames></author><author><keyname>Nautsch</keyname><forenames>Andreas</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Evans</keyname><forenames>Nicholas</forenames></author><author><keyname>Kinnunen</keyname><forenames>Tomi</forenames></author><author><keyname>Lee</keyname><forenames>Kong Aik</forenames></author></authors><title>ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection</title><categories>eess.AS cs.CR cs.SD</categories><journal-ref>Proc. Interspeech 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ASVspoof, now in its third edition, is a series of community-led challenges
which promote the development of countermeasures to protect automatic speaker
verification (ASV) from the threat of spoofing. Advances in the 2019 edition
include: (i) a consideration of both logical access (LA) and physical access
(PA) scenarios and the three major forms of spoofing attack, namely synthetic,
converted and replayed speech; (ii) spoofing attacks generated with
state-of-the-art neural acoustic and waveform models; (iii) an improved,
controlled simulation of replay attacks; (iv) use of the tandem detection cost
function (t-DCF) that reflects the impact of both spoofing and countermeasures
upon ASV reliability. Even if ASV remains the core focus, in retaining the
equal error rate (EER) as a secondary metric, ASYspoof also embraces the
growing importance of fake audio detection. ASVspoof 2019 attracted the
participation of 63 research teams, with more than half of these reporting
systems that improve upon the performance of two baseline spoofing
countermeasures. This paper describes the 2019 database, protocols and
challenge results. It also outlines major findings which demonstrate the real
progress made in protecting against the threat of spoofing and fake audio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05454</identifier>
 <datestamp>2020-01-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05454</id><created>2019-04-10</created><updated>2020-01-27</updated><authors><author><keyname>Flores</keyname><forenames>Victor H.</forenames></author><author><keyname>Rivera</keyname><forenames>Mariano</forenames></author></authors><title>Robust Two-Step phase estimation using the Simplified Lissajous Ellipse
  Fitting method with Gabor Filter Banks preprocessing</title><categories>eess.IV physics.optics</categories><journal-ref>Optics Communications, Volume 461, 15 April 2020, 125286</journal-ref><doi>10.1016/j.optcom.2020.125286</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Simplified Lissajous Ellipse Fitting (SLEF) method for the
calculation of the random phase step and the phase distribution from two
phase-shifted interferograms. We consider interferograms with spatial and
temporal dependency of background intensities, amplitude modulations and noise.
Given these problems, the use of the Gabor Filters Bank (GFB) allows us to
filter--out the noise, normalize the amplitude and eliminate the background.
The normalized patterns permit to implement the SLEF algorithm, which is based
on reducing the number of estimated coefficients of the ellipse equation, from
five terms to only two. Our method consists of three stages. First, we
preprocess the interferograms with GFB methodology in order to normalize the
fringe patterns. Second, we calculate the phase step by using the proposed SLEF
technique and third, we estimate the phase distribution using a two--steps
formula. For the calculation of the phase step, we present two alternatives:
the use of the Least Squares (LS) method to approximate the values of the
coefficients and, in order to improve the LS estimation, a robust estimation
based on the Leclerc's potential. The SLEF method's performance is evaluated
through synthetic and experimental data to demonstrate its feasibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05508</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05508</id><created>2019-04-10</created><updated>2019-04-17</updated><authors><author><keyname>Celebi</keyname><forenames>Haluk</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>Schulzrinne</keyname><forenames>Henning</forenames></author></authors><title>Capacity and Energy-Efficiency of Delayed Access Scheme for Small Cell
  Networks</title><categories>eess.SP</categories><comments>WTS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data applications may typically tolerate a moderate delay before packet
transmission between user equipment (UE) and cell begins. This delay can be
taken advantage to reduce the communication distance, improve coverage
probability, and increase overall energy-efficiency of the small cell network.
To demonstrate such merits, we suggest a simple access scheme and analyze the
distribution of coverage probability and throughput as a function of delay and
transmit distance. Sufficient number of small base stations (SBSs) handle the
peak traffic load. To improve energy-efficiency of the network, a number of
SBSs are switched off at low traffic periods. Energy-efficiency can be further
improved by turning all of the SBSs on and off, rather than selecting a subset
and leaving them off. By doing so, coverage probability and bit-rate can be
improved by delaying their transmissions and waiting for a closer SBS to become
available. Results show that by turning SBSs on and off continuously and taking
advantage of initial delay to connect a SBS yield an order of magnitude
improvement in energy-efficiency, improves the coverage probability
significantly at low signal to interference and noise (SINR) regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05509</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05509</id><created>2019-04-10</created><authors><author><keyname>Cheng</keyname><forenames>Chongsheng</forenames></author><author><keyname>Shang</keyname><forenames>Zhexiong</forenames></author><author><keyname>Shen</keyname><forenames>Zhigang</forenames></author></authors><title>CNN-Based Deep Architecture for Reinforced Concrete Delamination
  Segmentation Through Thermography</title><categories>eess.IV cs.CV</categories><comments>Accepted for the 2019 ASCE International Conference on Computing in
  Civil Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delamination assessment of the bridge deck plays a vital role for bridge
health monitoring. Thermography as one of the nondestructive technologies for
delamination detection has the advantage of efficient data acquisition. But
there are challenges on the interpretation of data for accurate delamination
shape profiling. Due to the environmental variation and the irregular presence
of delamination size and depth, conventional processing methods based on
temperature contrast fall short in accurate segmentation of delamination.
Inspired by the recent development of deep learning architecture for image
segmentation, the Convolutional Neural Network (CNN) based framework was
investigated for the applicability of delamination segmentation under
variations in temperature contrast and shape diffusion. The models were
developed based on Dense Convolutional Network (DenseNet) and trained on
thermal images collected for mimicked delamination in concrete slabs with
different depths under experimental setup. The results suggested satisfactory
performance of accurate profiling the delamination shapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05516</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05516</id><created>2019-04-10</created><authors><author><keyname>Kumari</keyname><forenames>Preeti</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Adaptive Virtual Waveform Design for Millimeter-Wave Joint
  Communication-Radar</title><categories>eess.SP cs.IT math.IT</categories><doi>10.1109/TSP.2019.2956689</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint communication and radar (JCR) waveforms with fully digital baseband
generation and processing can now be realized at the millimeter-wave (mmWave)
band. Prior work has proposed a mmWave wireless local area network (WLAN)-based
JCR that exploits the WLAN preamble for radars. The performance of target
velocity estimation, however, was limited. In this paper, we propose a virtual
waveform design for an adaptive mmWave JCR. The proposed system transmits a few
non-uniformly placed preambles to construct several receive virtual preambles
for enhancing velocity estimation accuracy, at the cost of only a small
reduction in the communication data rate. We evaluate JCR performance
trade-offs using the Cramer-Rao Bound (CRB) metric for radar estimation and a
novel distortion minimum mean square error (MMSE) metric for data
communication. Additionally, we develop three different MMSE-based optimization
problems for the adaptive JCR waveform design. Simulations show that an optimal
virtual (non-uniform) waveform achieves a significant performance improvement
as compared to a uniform waveform. For a radar CRB constrained optimization,
the optimal radar range of operation and the optimal communication distortion
MMSE (DMMSE) are improved. For a communication DMMSE constrained optimization
with a high DMMSE constraint, the optimal radar CRB is enhanced. For a weighted
MMSE average optimization, the advantage of the virtual waveform over the
uniform waveform is increased with decreased communication weighting.
Comparison of MMSE-based optimization with traditional virtual preamble
count-based optimization indicated that the conventional solution converges to
the MMSE-based one only for a small number of targets and a high
signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05534</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05534</id><created>2019-04-11</created><authors><author><keyname>Shi</keyname><forenames>Yunmei</forenames></author><author><keyname>Mao</keyname><forenames>Xing-Peng</forenames></author><author><keyname>Zhao</keyname><forenames>Chunlei</forenames></author><author><keyname>Liu</keyname><forenames>Yong-Tan</forenames></author></authors><title>A Block Alternating Optimization Method for Direction-of-Arrival
  Estimation with Nested Array</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, direction-of-arrival estimation using nested array is studied
in the framework of sparse signal representation. With the vectorization
operator, a new real-valued nonnegative sparse signal recovery model which has
a wider virtual array aperture is built. To leverage celebrated compressive
sensing algorithms, the continuous parameter space has to be discretized to a
number of fixed grid points, which inevitably incurs modeling error caused by
off-grid gap. To remedy this issue, a block alternating optimization method is
put forth that jointly estimates the sparse signal and refines the locations of
grid points. Specifically, inspired by the majorization minimization, the
proposed method iteratively minimizes a surrogate function majorizing the given
objective function, where only a single block of variables are updated per
iteration while the remaining ones are kept fixed. The proposed method features
affordable computational complexity, and numerical tests corroborate its
superior performance relative to existing alternatives in both overdetermined
and underdetermined scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05576</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05576</id><created>2019-04-11</created><authors><author><keyname>Lavrentyeva</keyname><forenames>Galina</forenames></author><author><keyname>Novoselov</keyname><forenames>Sergey</forenames></author><author><keyname>Tseren</keyname><forenames>Andzhukaev</forenames></author><author><keyname>Volkova</keyname><forenames>Marina</forenames></author><author><keyname>Gorlanov</keyname><forenames>Artem</forenames></author><author><keyname>Kozlov</keyname><forenames>Alexandr</forenames></author></authors><title>STC Antispoofing Systems for the ASVspoof2019 Challenge</title><categories>cs.SD cs.CL cs.CR cs.LG eess.AS stat.ML</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the Speech Technology Center (STC) antispoofing systems
submitted to the ASVspoof 2019 challenge. The ASVspoof2019 is the extended
version of the previous challenges and includes 2 evaluation conditions:
logical access use-case scenario with speech synthesis and voice conversion
attack types and physical access use-case scenario with replay attacks. During
the challenge we developed anti-spoofing solutions for both scenarios. The
proposed systems are implemented using deep learning approach and are based on
different types of acoustic features. We enhanced Light CNN architecture
previously considered by the authors for replay attacks detection and which
performed high spoofing detection quality during the ASVspoof2017 challenge. In
particular here we investigate the efficiency of angular margin based softmax
activation for training robust deep Light CNN classifier to solve the
mentioned-above tasks. Submitted systems achieved EER of 1.86% in logical
access scenario and 0.54% in physical access scenario on the evaluation part of
the Challenge corpora. High performance obtained for the unknown types of
spoofing attacks demonstrates the stability of the offered approach in both
evaluation conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05585</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05585</id><created>2019-04-11</created><authors><author><keyname>Zhang</keyname><forenames>Ling</forenames></author><author><keyname>Gui</keyname><forenames>Lin</forenames></author><author><keyname>Ying</keyname><forenames>Kai</forenames></author><author><keyname>Qin</keyname><forenames>Qibo</forenames></author></authors><title>Clustering Based Hybrid Precoding Design for Multi-User Massive MIMO
  Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid precoding has been recognized as a promising technology to combat the
path loss of millimeter wave signals in massive multiple-input multiple-output
(MIMO) systems. However, due to the joint optimization of the digital and
analog precoding matrices as well as extra constraints for the analog part, the
hybrid precoding design is still a tough issue in current research. In this
paper, we adopt the thought of clustering in unsupervised learning and provide
design schemes for fully-connected hybrid precoding (FHP) and
adaptively-connected hybrid precoding (AHP) in multi-user massive MIMO systems.
For FHP, we propose the hierarchical-agglomerative-clustering-based (HAC-based)
scheme to explore the relevance among RF chains in optimal hybrid procoding
design. The similar RF chains are merged into an individual RF chain when
insufficient RF chains are equipped at the base station (BS). For AHP, we
propose the modified-K-means-based (MKM-based) scheme to explore the relevance
among antennas at the BS. The similar antennas are supported by the same RF
chain to make full use of the flexible connection in AHP. Particularly, in
proposed MKM-based AHP design, the clustering centers are updated by
alternating-optimum-based (AO-based) scheme with a special initialization
method, which is capable to individually provide feasible sub-connected hybrid
precoding (SHP) design. Simulation results highlight the superior spectrum
efficiency of proposed HAC-based FHP scheme, and the high power efficiency of
proposed MKM-based AHP scheme. Moreover, all the proposed schemes are clarified
to effectively handle the inter-user interference and outperform the existing
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05591</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05591</id><created>2019-04-11</created><authors><author><keyname>Zhang</keyname><forenames>Jingjing</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>On Model Coding for Distributed Inference and Transmission in Mobile
  Edge Computing Systems</title><categories>cs.IT eess.SP math.IT</categories><comments>accepted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a mobile edge computing system in which users wish to obtain the
result of a linear inference operation on locally measured input data. Unlike
the offloaded input data, the model weight matrix is distributed across
wireless Edge Nodes (ENs). ENs have non-deterministic computing times, and they
can transmit any shared computed output back to the users cooperatively. This
letter investigates the potential advantages obtained by coding model
information prior to ENs' storage. Through an information-theoretic analysis,
it is concluded that, while generally limiting cooperation opportunities,
coding is instrumental in reducing the overall computation-plus-communication
latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05610</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05610</id><created>2019-04-11</created><updated>2019-05-02</updated><authors><author><keyname>Ihsan</keyname><forenames>Ullah</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Yan</keyname><forenames>Shihao</forenames></author></authors><title>Machine Learning and Location Verification in Vehicular Networks</title><categories>eess.SP</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location information will play a very important role in emerging wireless
networks such as Intelligent Transportation Systems, 5G, and the Internet of
Things. However, wrong location information can result in poor network
outcomes. It is therefore critical to verify all location information before
further utilization in any network operation. In recent years, a number of
information-theoretic Location Verification Systems (LVSs) have been formulated
in attempts to optimally verify the location information supplied by network
users. Such LVSs, however, are somewhat limited since they rely on knowledge of
a number of channel parameters for their operation. To overcome such
limitations, in this work we introduce a Machine Learning based LVS (ML-LVS).
This new form of LVS can adapt itself to changing environments without knowing
the channel parameters. Here, for the first time, we use real-world data to
show how our ML-LVS can outperform information-theoretic LVSs. We demonstrate
this improved performance within the context of vehicular networks using
Received Signal Strength (RSS) measurements at multiple verifying base
stations. We also demonstrate the validity of the ML-LVS even in scenarios
where a sophisticated adversary optimizes her attack location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05635</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05635</id><created>2019-04-11</created><updated>2019-04-14</updated><authors><author><keyname>Kong</keyname><forenames>Qiuqiang</forenames></author><author><keyname>Cao</keyname><forenames>Yin</forenames></author><author><keyname>Iqbal</keyname><forenames>Turab</forenames></author><author><keyname>Xu</keyname><forenames>Yong</forenames></author><author><keyname>Wang</keyname><forenames>Wenwu</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Cross-task learning for audio tagging, sound event detection spatial
  localization: DCASE 2019 baseline systems</title><categories>cs.SD eess.AS</categories><comments>We want to replace but create this submission by mistake. See
  arXiv:1904.03476 instead</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019
challenge focuses on audio tagging, sound event detection and spatial
localisation. DCASE 2019 consists of five tasks: 1) acoustic scene
classification, 2) audio tagging with noisy labels and minimal supervision, 3)
sound event localisation and detection, 4) sound event detection in domestic
environments, and 5) urban sound tagging. In this paper, we propose generic
cross-task baseline systems based on convolutional neural networks (CNNs). The
motivation is to investigate the performance of a variety of models across
several tasks without exploiting the specific characteristics of the tasks. We
looked at CNNs with 5, 9, and 13 layers, and found that the optimal
architecture is task-dependent. For the systems we considered, we found that
the 9-layer CNN with average pooling is a good model for a majority of the
DCASE 2019 tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05644</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05644</id><created>2019-04-11</created><authors><author><keyname>Jiang</keyname><forenames>Yun</forenames></author><author><keyname>Tan</keyname><forenames>Ning</forenames></author><author><keyname>Peng</keyname><forenames>Tingting</forenames></author><author><keyname>Zhang</keyname><forenames>Hai</forenames></author></authors><title>Retinal Vessels Segmentation Based on Dilated Multi-Scale Convolutional
  Neural Network</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate segmentation of retinal vessels is a basic step in Diabetic
retinopathy(DR) detection. Most methods based on deep convolutional neural
network (DCNN) have small receptive fields, and hence they are unable to
capture global context information of larger regions, with difficult to
identify lesions. The final segmented retina vessels contain more noise with
low classification accuracy. Therefore, in this paper, we propose a DCNN
structure named as D-Net. In the proposed D-Net, the dilation convolution is
used in the backbone network to obtain a larger receptive field without losing
spatial resolution, so as to reduce the loss of feature information and to
reduce the difficulty of tiny thin vessels segmentation. The large receptive
field can better distinguished between the lesion area and the blood vessel
area. In the proposed Multi-Scale Information Fusion module (MSIF), parallel
convolution layers with different dilation rates are used, so that the model
can obtain more dense feature information and better capture retinal vessel
information of different sizes. In the decoding module, the skip layer
connection is used to propagate context information to higher resolution
layers, so as to prevent low-level information from passing the entire network
structure. Finally, our method was verified on DRIVE, STARE and CHASE dataset.
The experimental results show that our network structure outperforms some
state-of-art method, such as N4-fields, U-Net, and DRIU in terms of accuracy,
sensitivity, specificity, and AUCROC. Particularly, D-Net outperforms U-Net by
1.04%, 1.23% and 2.79% in DRIVE, STARE, and CHASE three dataset, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05723</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05723</id><created>2019-04-11</created><authors><author><keyname>Cheng</keyname><forenames>Chongsheng</forenames></author><author><keyname>Shang</keyname><forenames>Zhexiong</forenames></author><author><keyname>Shen</keyname><forenames>Zhigang</forenames></author></authors><title>Enhancing Bridge Deck Delamination Detection Based on Aerial
  Thermography Through Grayscale Morphologic Reconstruction: A Case Study</title><categories>eess.IV</categories><comments>Accepted as the presentation for 98th Annual Meeting of the
  Transportation Research Board (TRB)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Environmental-induced temperature variations across the bridge deck were one
of the major factors that degraded the performance of delamination detection
through thermography. The non-uniformly distributed thermal background yields
the assumption of most conventional quantitative methods used in practice such
as global thresholding and k-means clustering. This study proposed a
pre-processing method to estimate the thermal background through iterative
grayscale morphologic reconstruction based on a pre-selected temperature
contrast. After the estimation of the background, the thermal feature of
delamination was kept in the residual image. A UAV-based nondestructive survey
was carried out on an in-service bridge for a case study and two delamination
quantization methods (threshold-based and clustering-based) were applied on
both raw and residual thermal image. Results were compared and evaluated based
on the hammer sounding test on the same bridge. The performance of
detectability was noticeably improved while direct implementation of
post-processing on raw image exhibited over- and under-estimation of
delamination. The selection of pre-defined temperature contrast and stopping
criterion of iteration were discussed. The study concluded the usefulness of
the proposed method for the case study and further evaluation and parameter
tuning are expected to generalize the method and procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05734</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05734</id><created>2019-03-18</created><authors><author><keyname>Abdullah</keyname><forenames>Hadi</forenames></author><author><keyname>Garcia</keyname><forenames>Washington</forenames></author><author><keyname>Peeters</keyname><forenames>Christian</forenames></author><author><keyname>Traynor</keyname><forenames>Patrick</forenames></author><author><keyname>Butler</keyname><forenames>Kevin R. B.</forenames></author><author><keyname>Wilson</keyname><forenames>Joseph</forenames></author></authors><title>Practical Hidden Voice Attacks against Speech and Speaker Recognition
  Systems</title><categories>cs.CR cs.LG cs.SD eess.AS</categories><journal-ref>The Network and Distributed System Security Symposium (NDSS) 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice Processing Systems (VPSes), now widely deployed, have been made
significantly more accurate through the application of recent advances in
machine learning. However, adversarial machine learning has similarly advanced
and has been used to demonstrate that VPSes are vulnerable to the injection of
hidden commands - audio obscured by noise that is correctly recognized by a VPS
but not by human beings. Such attacks, though, are often highly dependent on
white-box knowledge of a specific machine learning model and limited to
specific microphones and speakers, making their use across different acoustic
hardware platforms (and thus their practicality) limited. In this paper, we
break these dependencies and make hidden command attacks more practical through
model-agnostic (blackbox) attacks, which exploit knowledge of the signal
processing algorithms commonly used by VPSes to generate the data fed into
machine learning systems. Specifically, we exploit the fact that multiple
source audio samples have similar feature vectors when transformed by acoustic
feature extraction algorithms (e.g., FFTs). We develop four classes of
perturbations that create unintelligible audio and test them against 12 machine
learning models, including 7 proprietary models (e.g., Google Speech API, Bing
Speech API, IBM Speech API, Azure Speaker API, etc), and demonstrate successful
attacks against all targets. Moreover, we successfully use our maliciously
generated audio samples in multiple hardware configurations, demonstrating
effectiveness across both models and real systems. In so doing, we demonstrate
that domain-specific knowledge of audio signal processing represents a
practical means of generating successful hidden voice command attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05742</identifier>
 <datestamp>2019-08-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05742</id><created>2019-04-10</created><updated>2019-08-22</updated><authors><author><keyname>Chou</keyname><forenames>Ju-chieh</forenames></author><author><keyname>Yeh</keyname><forenames>Cheng-chieh</forenames></author><author><keyname>Lee</keyname><forenames>Hung-yi</forenames></author></authors><title>One-shot Voice Conversion by Separating Speaker and Content
  Representations with Instance Normalization</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, voice conversion (VC) without parallel data has been successfully
adapted to multi-target scenario in which a single model is trained to convert
the input voice to many different speakers. However, such model suffers from
the limitation that it can only convert the voice to the speakers in the
training data, which narrows down the applicable scenario of VC. In this paper,
we proposed a novel one-shot VC approach which is able to perform VC by only an
example utterance from source and target speaker respectively, and the source
and target speaker do not even need to be seen during training. This is
achieved by disentangling speaker and content representations with instance
normalization (IN). Objective and subjective evaluation shows that our model is
able to generate the voice similar to target speaker. In addition to the
performance measurement, we also demonstrate that this model is able to learn
meaningful speaker representations without any supervision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05746</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05746</id><created>2019-04-08</created><authors><author><keyname>Saha</keyname><forenames>Pramit</forenames></author><author><keyname>Abdul-Mageed</keyname><forenames>Muhammad</forenames></author><author><keyname>Fels</keyname><forenames>Sidney</forenames></author></authors><title>SPEAK YOUR MIND! Towards Imagined Speech Recognition With Hierarchical
  Deep Learning</title><categories>cs.LG cs.CL cs.SD eess.AS stat.ML</categories><comments>Under review in INTERSPEECH 2019. arXiv admin note: text overlap with
  arXiv:1904.04358</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech-related Brain Computer Interface (BCI) technologies provide effective
vocal communication strategies for controlling devices through speech commands
interpreted from brain signals. In order to infer imagined speech from active
thoughts, we propose a novel hierarchical deep learning BCI system for
subject-independent classification of 11 speech tokens including phonemes and
words. Our novel approach exploits predicted articulatory information of six
phonological categories (e.g., nasal, bilabial) as an intermediate step for
classifying the phonemes and words, thereby finding discriminative signal
responsible for natural speech synthesis. The proposed network is composed of
hierarchical combination of spatial and temporal CNN cascaded with a deep
autoencoder. Our best models on the KARA database achieve an average accuracy
of 83.42% across the six different binary phonological classification tasks,
and 53.36% for the individual token identification task, significantly
outperforming our baselines. Ultimately, our work suggests the possible
existence of a brain imagery footprint for the underlying articulatory movement
related to different sounds that can be used to aid imagined speech decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05773</identifier>
 <datestamp>2019-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05773</id><created>2019-04-10</created><updated>2019-10-09</updated><authors><author><keyname>Kowsari</keyname><forenames>Kamran</forenames></author><author><keyname>Sali</keyname><forenames>Rasoul</forenames></author><author><keyname>Khan</keyname><forenames>Marium N.</forenames></author><author><keyname>Adorno</keyname><forenames>William</forenames></author><author><keyname>Ali</keyname><forenames>S. Asad</forenames></author><author><keyname>Moore</keyname><forenames>Sean R.</forenames></author><author><keyname>Amadi</keyname><forenames>Beatrice C.</forenames></author><author><keyname>Kelly</keyname><forenames>Paul</forenames></author><author><keyname>Syed</keyname><forenames>Sana</forenames></author><author><keyname>Brown</keyname><forenames>Donald E.</forenames></author></authors><title>Diagnosis of Celiac Disease and Environmental Enteropathy on Biopsy
  Images Using Color Balancing on Convolutional Neural Networks</title><categories>eess.IV cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Celiac Disease (CD) and Environmental Enteropathy (EE) are common causes of
malnutrition and adversely impact normal childhood development. CD is an
autoimmune disorder that is prevalent worldwide and is caused by an increased
sensitivity to gluten. Gluten exposure destructs the small intestinal
epithelial barrier, resulting in nutrient mal-absorption and childhood
under-nutrition. EE also results in barrier dysfunction but is thought to be
caused by an increased vulnerability to infections. EE has been implicated as
the predominant cause of under-nutrition, oral vaccine failure, and impaired
cognitive development in low-and-middle-income countries. Both conditions
require a tissue biopsy for diagnosis, and a major challenge of interpreting
clinical biopsy images to differentiate between these gastrointestinal diseases
is striking histopathologic overlap between them. In the current study, we
propose a convolutional neural network (CNN) to classify duodenal biopsy images
from subjects with CD, EE, and healthy controls. We evaluated the performance
of our proposed model using a large cohort containing 1000 biopsy images. Our
evaluations show that the proposed model achieves an area under ROC of 0.99,
1.00, and 0.97 for CD, EE, and healthy controls, respectively. These results
demonstrate the discriminative power of the proposed model in duodenal biopsies
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05876</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05876</id><created>2019-04-11</created><authors><author><keyname>Schwartz</keyname><forenames>Idan</forenames></author><author><keyname>Schwing</keyname><forenames>Alexander</forenames></author><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author></authors><title>A Simple Baseline for Audio-Visual Scene-Aware Dialog</title><categories>cs.CV cs.AI cs.CL cs.LG cs.SD eess.AS</categories><comments>Accepted to CVPR 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed audio-visual scene-aware dialog task paves the way to a
more data-driven way of learning virtual assistants, smart speakers and car
navigation systems. However, very little is known to date about how to
effectively extract meaningful information from a plethora of sensors that
pound the computational engine of those devices. Therefore, in this paper, we
provide and carefully analyze a simple baseline for audio-visual scene-aware
dialog which is trained end-to-end. Our method differentiates in a data-driven
manner useful signals from distracting ones using an attention mechanism. We
evaluate the proposed approach on the recently introduced and challenging
audio-visual scene-aware dataset, and demonstrate the key features that permit
to outperform the current state-of-the-art by more than 20\% on CIDEr.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05917</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05917</id><created>2019-04-11</created><authors><author><keyname>Liu</keyname><forenames>Fan</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Griffiths</keyname><forenames>Hugh</forenames></author></authors><title>Dual-functional Radar-Communication Waveform Design under
  Constant-modulus and Orthogonality Constraints</title><categories>eess.SP</categories><comments>5 pages, 3 figures, accepted by SSPD 2019, Brighton, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on constant-modulus waveform design for the dual use
of radar target detection and cellular transmission. As the MIMO radar
typically transmits orthogonal waveforms to search potential targets, we aim at
jointly minimizing the downlink multi-user interference and the
non-orthogonality of the transmitted waveform. Given the non-convexity in both
orthogonal and CM constraints, we decompose the formulated optimization problem
as two sub-problems, where we solve one of the sub-problems by singular value
decomposition and the other one by the Riemannian conjugate gradient algorithm.
We then propose an alternating minimization approach to obtain a near-optimal
solution to the original problem by iteratively solve the two sub-problems.
Finally, we assess the effectiveness of the proposed approach via numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05959</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05959</id><created>2019-04-11</created><authors><author><keyname>Ricco</keyname><forenames>Rodrigo A.</forenames></author><author><keyname>Teixeira</keyname><forenames>Bruno O. S.</forenames></author></authors><title>Mapping prior information onto LMI eigenvalue-regions for discrete-time
  subspace identification</title><categories>cs.SY eess.SP</categories><comments>Under review</comments><msc-class>93E12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subspace identification, prior information can be used to constrain the
eigenvalues of the estimated state-space model by defining corresponding LMI
regions. In this paper, first we argue on what kind of practical information
can be extracted from historical data or step-response experiments to possibly
improve the dynamical properties of the corresponding model and, also, on how
to mitigate the effect of the uncertainty on such information. For instance,
prior knowledge regarding the overshoot, the period between damped oscillations
and settling time may be useful to constraint the possible locations of the
eigenvalues of the discrete-time model. Then, we show how to map the prior
information onto LMI regions and, when the obtaining regions are non-convex, to
obtain convex approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.05979</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.05979</id><created>2019-04-11</created><authors><author><keyname>Zhao</keyname><forenames>Hang</forenames></author><author><keyname>Gan</keyname><forenames>Chuang</forenames></author><author><keyname>Ma</keyname><forenames>Wei-Chiu</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>The Sound of Motions</title><categories>cs.CV cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sounds originate from object motions and vibrations of surrounding air.
Inspired by the fact that humans is capable of interpreting sound sources from
how objects move visually, we propose a novel system that explicitly captures
such motion cues for the task of sound localization and separation. Our system
is composed of an end-to-end learnable model called Deep Dense Trajectory
(DDT), and a curriculum learning scheme. It exploits the inherent coherence of
audio-visual signals from a large quantities of unlabeled videos. Quantitative
and qualitative evaluations show that comparing to previous models that rely on
visual appearance cues, our motion based system improves performance in
separating musical instrument sounds. Furthermore, it separates sound
components from duets of the same category of instruments, a challenging
problem that has not been addressed before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06017</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06017</id><created>2019-04-11</created><authors><author><keyname>Fan</keyname><forenames>Rui</forenames></author><author><keyname>Jiao</keyname><forenames>Jianhao</forenames></author><author><keyname>Pan</keyname><forenames>Jie</forenames></author><author><keyname>Huang</keyname><forenames>Huaiyang</forenames></author><author><keyname>Shen</keyname><forenames>Shaojie</forenames></author><author><keyname>Liu</keyname><forenames>Ming</forenames></author></authors><title>Real-Time Dense Stereo Embedded in A UAV for Road Inspection</title><categories>cs.CV cs.RO eess.IV</categories><comments>9 pages, 8 figures, In Proceedings of the IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR) Workshops, June 16-20, 2019, Long
  Beach, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The condition assessment of road surfaces is essential to ensure their
serviceability while still providing maximum road traffic safety. This paper
presents a robust stereo vision system embedded in an unmanned aerial vehicle
(UAV). The perspective view of the target image is first transformed into the
reference view, and this not only improves the disparity accuracy, but also
reduces the algorithm's computational complexity. The cost volumes generated
from stereo matching are then filtered using a bilateral filter. The latter has
been proved to be a feasible solution for the functional minimisation problem
in a fully connected Markov random field model. Finally, the disparity maps are
transformed by minimising an energy function with respect to the roll angle and
disparity projection model. This makes the damaged road areas more
distinguishable from the road surface. The proposed system is implemented on an
NVIDIA Jetson TX2 GPU with CUDA for real-time purposes. It is demonstrated
through experiments that the damaged road areas can be easily distinguished
from the transformed disparity maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06037</identifier>
 <datestamp>2019-06-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06037</id><created>2019-04-12</created><updated>2019-06-25</updated><authors><author><keyname>Jia</keyname><forenames>Ye</forenames></author><author><keyname>Weiss</keyname><forenames>Ron J.</forenames></author><author><keyname>Biadsy</keyname><forenames>Fadi</forenames></author><author><keyname>Macherey</keyname><forenames>Wolfgang</forenames></author><author><keyname>Johnson</keyname><forenames>Melvin</forenames></author><author><keyname>Chen</keyname><forenames>Zhifeng</forenames></author><author><keyname>Wu</keyname><forenames>Yonghui</forenames></author></authors><title>Direct speech-to-speech translation with a sequence-to-sequence model</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Accepted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an attention-based sequence-to-sequence neural network which can
directly translate speech from one language into speech in another language,
without relying on an intermediate text representation. The network is trained
end-to-end, learning to map speech spectrograms into target spectrograms in
another language, corresponding to the translated content (in a different
canonical voice). We further demonstrate the ability to synthesize translated
speech using the voice of the source speaker. We conduct experiments on two
Spanish-to-English speech translation datasets, and find that the proposed
model slightly underperforms a baseline cascade of a direct speech-to-text
translation model and a text-to-speech synthesis model, demonstrating the
feasibility of the approach on this very challenging task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06063</identifier>
 <datestamp>2019-08-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06063</id><created>2019-04-12</created><updated>2019-08-22</updated><authors><author><keyname>Xue</keyname><forenames>Liumeng</forenames></author><author><keyname>Song</keyname><forenames>Wei</forenames></author><author><keyname>Xu</keyname><forenames>Guanghui</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author><author><keyname>Wu</keyname><forenames>Zhizheng</forenames></author></authors><title>Building a mixed-lingual neural TTS system with only monolingual data</title><categories>cs.CL cs.SD eess.AS</categories><comments>To appear in INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When deploying a Chinese neural text-to-speech (TTS) synthesis system, one of
the challenges is to synthesize Chinese utterances with English phrases or
words embedded. This paper looks into the problem in the encoder-decoder
framework when only monolingual data from a target speaker is available.
Specifically, we view the problem from two aspects: speaker consistency within
an utterance and naturalness. We start the investigation with an Average Voice
Model which is built from multi-speaker monolingual data, i.e. Mandarin and
English data. On the basis of that, we look into speaker embedding for speaker
consistency within an utterance and phoneme embedding for naturalness and
intelligibility and study the choice of data for model training. We report the
findings and discuss the challenges to build a mixed-lingual TTS system with
only monolingual data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06075</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06075</id><created>2019-04-12</created><authors><author><keyname>Al-Radhi</keyname><forenames>Mohammed Salah</forenames></author><author><keyname>Csap&#xf3;</keyname><forenames>Tam&#xe1;s G&#xe1;bor</forenames></author><author><keyname>N&#xe9;meth</keyname><forenames>G&#xe9;za</forenames></author></authors><title>RNN-based speech synthesis using a continuous sinusoidal model</title><categories>cs.SD eess.AS</categories><comments>8 pages, 4 figures, Accepted to IJCNN 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently in statistical parametric speech synthesis, we proposed a continuous
sinusoidal model (CSM) using continuous F0 (contF0) in combination with Maximum
Voiced Frequency (MVF), which was successfully giving state-of-the-art vocoders
performance (e.g. similar to STRAIGHT) in synthesized speech. In this paper, we
address the use of sequence-to-sequence modeling with recurrent neural networks
(RNNs). Bidirectional long short-term memory (Bi-LSTM) is investigated and
applied using our CSM to model contF0, MVF, and Mel-Generalized Cepstrum (MGC)
for more natural sounding synthesized speech. For refining the output of the
contF0 estimation, post-processing based on time-warping approach is applied to
reduce the unwanted voiced component of the unvoiced speech sounds, resulting
in an enhanced contF0 track. The overall conclusion is covered by objective
evaluation and subjective listening test, showing that the proposed framework
provides satisfactory results in terms of naturalness and intelligibility, and
is comparable to the high-quality WORLD model based RNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06083</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06083</id><created>2019-04-12</created><authors><author><keyname>Porras</keyname><forenames>Dagoberto</forenames></author><author><keyname>Sep&#xfa;lveda-Sep&#xfa;lveda</keyname><forenames>Alexander</forenames></author><author><keyname>Csap&#xf3;</keyname><forenames>Tam&#xe1;s G&#xe1;bor</forenames></author></authors><title>DNN-based Acoustic-to-Articulatory Inversion using Ultrasound Tongue
  Imaging</title><categories>cs.SD eess.AS q-bio.TO</categories><comments>8 pages, 5 figures, Accepted to IJCNN 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Speech sounds are produced as the coordinated movement of the speaking
organs. There are several available methods to model the relation of
articulatory movements and the resulting speech signal. The reverse problem is
often called as acoustic-to-articulatory inversion (AAI). In this paper we have
implemented several different Deep Neural Networks (DNNs) to estimate the
articulatory information from the acoustic signal. There are several previous
works related to performing this task, but most of them are using
ElectroMagnetic Articulography (EMA) for tracking the articulatory movement.
Compared to EMA, Ultrasound Tongue Imaging (UTI) is a technique of higher
cost-benefit if we take into account equipment cost, portability, safety and
visualized structures. Seeing that, our goal is to train a DNN to obtain UT
images, when using speech as input. We also test two approaches to represent
the articulatory information: 1) the EigenTongue space and 2) the raw
ultrasound image. As an objective quality measure for the reconstructed UT
images, we use MSE, Structural Similarity Index (SSIM) and Complex-Wavelet SSIM
(CW-SSIM). Our experimental results show that CW-SSIM is the most useful error
measure in the UTI context. We tested three different system configurations: a)
simple DNN composed of 2 hidden layers with 64x64 pixels of an UTI file as
target; b) the same simple DNN but with ultrasound images projected to the
EigenTongue space as the target; c) and a more complex DNN composed of 5 hidden
layers with UTI files projected to the EigenTongue space. In a subjective
experiment the subjects found that the neural networks with two hidden layers
were more suitable for this inversion task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06086</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06086</id><created>2019-04-12</created><authors><author><keyname>Park</keyname><forenames>Jong-Hyeon</forenames></author><author><keyname>Oh</keyname><forenames>Myungwoo</forenames></author><author><keyname>Park</keyname><forenames>Hyung-Min</forenames></author></authors><title>Unsupervised Speech Domain Adaptation Based on Disentangled
  Representation Learning for Robust Speech Recognition</title><categories>eess.AS cs.CL cs.SD</categories><comments>Submitted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, the performance of automatic speech recognition (ASR) systems is
significantly degraded due to the mismatch between training and test
environments. Recently, a deep-learning-based image-to-image translation
technique to translate an image from a source domain to a desired domain was
presented, and cycle-consistent adversarial network (CycleGAN) was applied to
learn a mapping for speech-to-speech conversion from a speaker to a target
speaker. However, this method might not be adequate to remove corrupting noise
components for robust ASR because it was designed to convert speech itself. In
this paper, we propose a domain adaptation method based on generative
adversarial nets (GANs) with disentangled representation learning to achieve
robustness in ASR systems. In particular, two separated encoders, context and
domain encoders, are introduced to learn distinct latent variables. The latent
variables allow us to convert the domain of speech according to its context and
domain representation. We improved word accuracies by 6.55~15.70\% for the
CHiME4 challenge corpus by applying a noisy-to-clean environment adaptation for
robust ASR. In addition, similar to the method based on the CycleGAN, this
method can be used for gender adaptation in gender-mismatched recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06093</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06093</id><created>2019-04-12</created><authors><author><keyname>Novoselov</keyname><forenames>Sergey</forenames></author><author><keyname>Gusev</keyname><forenames>Aleksei</forenames></author><author><keyname>Ivanov</keyname><forenames>Artem</forenames></author><author><keyname>Pekhovsky</keyname><forenames>Timur</forenames></author><author><keyname>Shulipa</keyname><forenames>Andrey</forenames></author><author><keyname>Lavrentyeva</keyname><forenames>Galina</forenames></author><author><keyname>Volokhov</keyname><forenames>Vladimir</forenames></author><author><keyname>Kozlov</keyname><forenames>Alexandr</forenames></author></authors><title>STC Speaker Recognition Systems for the VOiCES From a Distance Challenge</title><categories>cs.SD cs.CL cs.LG eess.AS</categories><comments>Submitted to Interspeech 2019, Graz, Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Speech Technology Center (STC) speaker recognition
(SR) systems submitted to the VOiCES From a Distance challenge 2019. The
challenge's SR task is focused on the problem of speaker recognition in single
channel distant/far-field audio under noisy conditions. In this work we
investigate different deep neural networks architectures for speaker embedding
extraction to solve the task. We show that deep networks with residual frame
level connections outperform more shallow architectures. Simple energy based
speech activity detector (SAD) and automatic speech recognition (ASR) based SAD
are investigated in this work. We also address the problem of data preparation
for robust embedding extractors training. The reverberation for the data
augmentation was performed using automatic room impulse response generator. In
our systems we used discriminatively trained cosine similarity metric learning
model as embedding backend. Scores normalization procedure was applied for each
individual subsystem we used. Our final submitted systems were based on the
fusion of different subsystems. The results obtained on the VOiCES development
and evaluation sets demonstrate effectiveness and robustness of the proposed
systems when dealing with distant/far-field audio under noisy conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06157</identifier>
 <datestamp>2019-10-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06157</id><created>2019-04-12</created><updated>2019-10-20</updated><authors><author><keyname>Mimilakis</keyname><forenames>Stylianos Ioannis</forenames></author><author><keyname>Drossos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Cano</keyname><forenames>Estefan&#xed;a</forenames></author><author><keyname>Schuller</keyname><forenames>Gerald</forenames></author></authors><title>Examining the Mapping Functions of Denoising Autoencoders in Singing
  Voice Separation</title><categories>eess.AS cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to investigate what singing voice separation
approaches based on neural networks learn from the data. We examine the mapping
functions of neural networks based on the denoising autoencoder (DAE) model
that are conditioned on the mixture magnitude spectra. To approximate the
mapping functions, we propose an algorithm inspired by the knowledge
distillation, denoted the neural couplings algorithm (NCA). The NCA yields a
matrix that expresses the mapping of the mixture to the target source magnitude
information. Using the NCA, we examine the mapping functions of three
fundamental DAE-based models in music source separation; one with single-layer
encoder and decoder, one with multi-layer encoder and single-layer decoder, and
one using skip-filtering connections (SF) with a single-layer encoding and
decoding. We first train these models with realistic data to estimate the
singing voice magnitude spectra from the corresponding mixture. We then use the
optimized models and test spectral data as input to the NCA. Our experimental
findings show that approaches based on the DAE model learn scalar filtering
operators, exhibiting a predominant diagonal structure in their corresponding
mapping functions, limiting the exploitation of inter-frequency structure of
music data. In contrast, skip-filtering connections are shown to assist the DAE
model in learning filtering operators that exploit richer inter-frequency
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06168</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06168</id><created>2019-04-12</created><authors><author><keyname>Zhao</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Yang</keyname><forenames>Liang</forenames></author><author><keyname>Pan</keyname><forenames>Gaofeng</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Secure mmWave Communications in Cognitive Radio Networks</title><categories>eess.SP</categories><comments>4 pages, 3 figures</comments><doi>10.1109/LWC.2019.2910530</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, the secrecy performance in cognitive radio networks (CRNs)
over fluctuating two-ray (FTR) channels, which is used to model the millimetre
wave channel, is investigated in terms of the secrecy outage probability (SOP).
Specifically, we consider the case where a source (S) transmits confidential
messages to a destination (D), and an eavesdropper wants to wiretap the
information from S to D. In a CRN framework, we assume that the primary user
shares its spectrum with S, where S adopts the underlay strategy to control its
transmit power without impairing the quality of service of the primary user.
After some mathematical manipulations, an exact analytical expression for the
SOP is derived. In order to get physical and technical insights into the effect
of the channel parameters on the SOP, we derive an asymptotic formula for the
SOP in the high signal-to-noise ratio region of the S--D link. We finally show
some selected Monte-Carlo simulation results to validate the correctness of our
derived analytical expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06173</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06173</id><created>2019-04-09</created><authors><author><keyname>Maya</keyname><forenames>Juan Augusto</forenames></author><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author></authors><title>Design and performance analysis of a fully distributed source detection
  algorithm for WSNs</title><categories>eess.SP</categories><comments>Submitted to TSP-IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we consider the detection of a localized source emitting a
signal using a wireless sensor network (WSN). We consider that geographically
distributed sensor nodes obtain energy measurements and compute cooperatively
and in a distributed manner a statistic to decide if the source is present or
absent without the need of a central node or fusion center (FC). We first start
from the continuous-time signal sensed by the nodes and obtain an equivalent
discrete-time hypothesis testing problem. Secondly, we propose a fully
distributed scheme, based on the well-known generalized likelihood ratio (GLR)
test, which is suitable for a WSN, where resources such as energy and
communication bandwidth are typically scarce. In third place, we consider the
asymptotic performance of the proposed GLR test. The derived results provide an
excellent matching with the scenario in which only a finite amount of
measurements are available at each sensor node. We finally show that the
proposed distributed algorithm performs as well as the global GLR test in the
considered scenarios, requiring only a small number of communication exchanges
between nodes and a limited knowledge about the network structure and its
connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06195</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06195</id><created>2019-04-11</created><authors><author><keyname>Kogo</keyname><forenames>Takuma</forenames></author><author><keyname>Tsujikawa</keyname><forenames>Masanori</forenames></author><author><keyname>Kiuchi</keyname><forenames>Yukihiro</forenames></author><author><keyname>Nishino</keyname><forenames>Atsushi</forenames></author><author><keyname>Hashimoto</keyname><forenames>Satoshi</forenames></author></authors><title>Model Predictive Control of Shallow Drowsiness: Improving Productivity
  of Office Workers</title><categories>eess.SP cs.SY</categories><comments>International Conference of the IEEE Engineering in Medicine and
  Biology Society (EMBC) 2019 - accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a methodology of model predictive control for alleviating
shallow drowsiness of office workers and thus improving their productivity. The
methodology is based on dynamically scheduling setting values for air
conditioning and lighting to minimize drowsiness level of office workers on the
basis of a prediction model that represents the relation between future
drowsiness level and combination of indoor temperature and ambient illuminance.
The prediction model can be identified by utilizing state-of-the-art drowsiness
estimation method. The proposed methodology was evaluated in regard to a real
routine task (performed by six subjects over five workdays), and the evaluation
results demonstrate that the proposed methodology improved the processing speed
of the task by 8.3% without degrading comfort of the workers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06215</identifier>
 <datestamp>2019-06-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06215</id><created>2019-04-12</created><updated>2019-06-22</updated><authors><author><keyname>Bitton</keyname><forenames>Adrien</forenames></author><author><keyname>Esling</keyname><forenames>Philippe</forenames></author><author><keyname>Caillon</keyname><forenames>Antoine</forenames></author><author><keyname>Fouilleul</keyname><forenames>Martin</forenames></author></authors><title>Assisted Sound Sample Generation with Musical Conditioning in
  Adversarial Auto-Encoders</title><categories>cs.SD cs.LG eess.AS</categories><comments>this article has been accepted for presentation to the 22nd
  International Conference on Digital Audio Effects (DAFx 2019) ; we provide
  additional content on this companion repository
  https://github.com/acids-ircam/Expressive_WAE_FADER</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have thrived in computer vision, enabling unprecedented
image processes. Yet the results in audio remain less advanced. Our project
targets real-time sound synthesis from a reduced set of high-level parameters,
including semantic controls that can be adapted to different sound libraries
and specific tags. These generative variables should allow expressive
modulations of target musical qualities and continuously mix into new styles.
To this extent we train AEs on an orchestral database of individual note
samples, along with their intrinsic attributes: note class, timbre domain and
extended playing techniques. We condition the decoder for control over the
rendered note attributes and use latent adversarial training for learning
expressive style parameters that can ultimately be mixed. We evaluate both
generative performances and latent representation. Our ablation study
demonstrates the effectiveness of the musical conditioning mechanisms. The
proposed model generates notes as magnitude spectrograms from any probabilistic
latent code samples, with expressive control of orchestral timbres and playing
styles. Its training data subsets can directly be visualized in the 3D latent
representation. Waveform rendering can be done offline with GLA. In order to
allow real-time interactions, we fine-tune the decoder with a pretrained MCNN
and embed the full waveform generation pipeline in a plugin. Moreover the
encoder could be used to process new input samples, after manipulating their
latent attribute representation, the decoder can generate sample variations as
an audio effect would. Our solution remains rather fast to train, it can
directly be applied to other sound domains, including an user's libraries with
custom sound tags that could be mapped to specific generative controls. As a
result, it fosters creativity and intuitive audio style experimentations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06222</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06222</id><created>2019-04-12</created><authors><author><keyname>Zhang</keyname><forenames>Yirun</forenames></author><author><keyname>Wu</keyname><forenames>Qirui</forenames></author><author><keyname>Hou</keyname><forenames>Jiancao</forenames></author><author><keyname>Towhidlou</keyname><forenames>Vahid</forenames></author><author><keyname>Shikh-Bahaei</keyname><forenames>Mohammad</forenames></author></authors><title>A Neural Network Prediction Based Adaptive Mode Selection Scheme in
  Full-Duplex Cognitive Networks</title><categories>eess.SP</categories><comments>7 pages, 6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a neural network (NN) predictor and an adaptive mode selection
scheme for the purpose of both improving secondary user's (SU's) throughput and
reducing collision probability to the primary user (PU) in full-duplex (FD)
cognitive networks. SUs can adaptively switch between FD
transmission-and-reception (TR) and transmission-and-sensing (TS) modes based
on the NN prediction results for each transmission duration. The prediction
performance is then analysed in terms of prediction error probability. We also
compare the performance of our proposed scheme with conventional TR and TS
modes in terms of SUs average throughput and collision probability,
respectively. Simulation results show that our proposed scheme achieves even
better SUs average throughput compared with TR mode. Meanwhile, the collision
probability can be reduced close to the level of TS mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06258</identifier>
 <datestamp>2019-12-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06258</id><created>2019-04-12</created><updated>2019-12-02</updated><authors><author><keyname>Ghoorchian</keyname><forenames>Saeed</forenames></author><author><keyname>Maghsudi</keyname><forenames>Setareh</forenames></author></authors><title>Multi-Armed Bandit for Energy-Efficient and Delay-Sensitive Edge
  Computing in Dynamic Networks with Uncertainty</title><categories>cs.LG eess.SP stat.ML</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the edge computing paradigm, mobile devices offload the computational
tasks to an edge server by routing the required data over the wireless network.
The full potential of edge computing becomes realized only if a smart device
selects the most appropriate server in terms of the latency and energy
consumption, among many available ones. The server selection problem is
challenging due to the randomness of the environment and lack of prior
information about the environment. Therefore, a smart device, which
sequentially chooses a server under uncertainty, aims to improve its decision
based on the historical time and energy consumption. The problem becomes more
complicated in a dynamic environment, where key variables might undergo abrupt
changes. To deal with the aforementioned problem, we first analyze the required
time and energy to data transmission and processing. We then use the analysis
to cast the problem as a budget-limited multi-armed bandit problem, where each
arm is associated with a reward and cost, with time-variant statistical
characteristics. We propose a policy to solve the formulated problem and prove
a regret bound. The numerical results demonstrate the superiority of the
proposed method compared to a number of existing solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06322</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06322</id><created>2019-04-12</created><authors><author><keyname>Mughal</keyname><forenames>M. O.</forenames></author><author><keyname>Toghi</keyname><forenames>Behrad</forenames></author><author><keyname>Hussein</keyname><forenames>Sarfaraz</forenames></author><author><keyname>Fallah</keyname><forenames>Yaser P.</forenames></author></authors><title>Intelligent Wide-band Spectrum Classifier</title><categories>eess.SP</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new technique for narrow-band (NB) signal classification in
sparsely populated wide-band (WB) spectrum using supervised learning approach.
For WB spectrum acquisition, Nyquist rate sampling is required at the
receiver's analog-to-digital converter (ADC), hence we use compressed sensing
(CS) theory to alleviate such high rate sampling requirement at the receiver
ADC. From the estimated WB spectrum, we then extract various spectral features
of each of the NB signal. These features are then used to train and classify
each NB signal into its respective modulation using the random forest
classifier. In the end, we evaluate the performance of the proposed algorithm
under different empirical setups and verify its superior performance in
comparison to a recently proposed signal classification algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06329</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06329</id><created>2019-04-12</created><updated>2019-04-14</updated><authors><author><keyname>Niu</keyname><forenames>Sheng-Yong</forenames></author><author><keyname>Guo</keyname><forenames>Lun-Zhang</forenames></author><author><keyname>Li</keyname><forenames>Yue</forenames></author><author><keyname>Wang</keyname><forenames>Tzung-Dau</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Liu</keyname><forenames>Tzu-Ming</forenames></author></authors><title>Boundary-Preserved Deep Denoising of the Stochastic Resonance Enhanced
  Multiphoton Images</title><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the rapid growth of high-speed and deep-tissue imaging in biomedical
research, it is urgent to find a robust and effective denoising method to
retain morphological features for further texture analysis and segmentation.
Conventional denoising filters and models can easily suppress perturbative
noises in high contrast images. However, for low photon budget multi-photon
images, high detector gain will not only boost signals, but also bring huge
background noises. In such stochastic resonance regime of imaging,
sub-threshold signals may be detectable with the help of noises. Therefore, a
denoising filter that can smartly remove noises without sacrificing the
important cellular features such as cell boundaries is highly desired. In this
paper, we propose a convolutional neural network based autoencoder method,
Fully Convolutional Deep Denoising Autoencoder (DDAE), to improve the quality
of Three-Photon Fluorescence (3PF) and Third Harmonic Generation (THG)
microscopy images. The average of the acquired 200 images of a given location
served as the low-noise answer for DDAE training. Compared with other widely
used denoising methods, our DDAE model shows better signal-to-noise ratio (26.6
and 29.9 for 3PF and THG, respectively), structure similarity (0.86 and 0.87
for 3PF and THG, respectively), and preservation of nuclear or cellular
boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06455</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06455</id><created>2019-04-12</created><authors><author><keyname>Chachlakis</keyname><forenames>Dimitris G.</forenames></author><author><keyname>Prater-Bennette</keyname><forenames>Ashley</forenames></author><author><keyname>Markopoulos</keyname><forenames>Panos P.</forenames></author></authors><title>L1-norm Tucker Tensor Decomposition</title><categories>cs.NA cs.DS eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tucker decomposition is a common method for the analysis of multi-way/tensor
data. Standard Tucker has been shown to be sensitive against heavy corruptions,
due to its L2-norm-based formulation which places squared emphasis to
peripheral entries. In this work, we explore L1-Tucker, an L1-norm based
reformulation of standard Tucker decomposition. After formulating the problem,
we present two algorithms for its solution, namely L1-norm Higher-Order
Singular Value Decomposition (L1-HOSVD) and L1-norm Higher-Order Orthogonal
Iterations (L1-HOOI). The presented algorithms are accompanied by complexity
and convergence analysis. Our numerical studies on tensor reconstruction and
classification corroborate that L1-Tucker, implemented by means of the proposed
methods, attains similar performance to standard Tucker when the processed data
are corruption-free, while it exhibits sturdy resistance against heavily
corrupted entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06457</identifier>
 <datestamp>2020-01-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06457</id><created>2019-04-12</created><updated>2019-08-01</updated><authors><author><keyname>Wang</keyname><forenames>Yilin</forenames></author><author><keyname>Inguva</keyname><forenames>Sasi</forenames></author><author><keyname>Adsumilli</keyname><forenames>Balu</forenames></author></authors><title>YouTube UGC Dataset for Video Compression Research</title><categories>cs.MM eess.IV</categories><journal-ref>2019 IEEE 21st International Workshop on Multimedia Signal
  Processing (MMSP)</journal-ref><doi>10.1109/MMSP.2019.8901772</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-professional video, commonly known as User Generated Content (UGC) has
become very popular in today's video sharing applications. However, traditional
metrics used in compression and quality assessment, like BD-Rate and PSNR, are
designed for pristine originals. Thus, their accuracy drops significantly when
being applied on non-pristine originals (the majority of UGC). Understanding
difficulties for compression and quality assessment in the scenario of UGC is
important, but there are few public UGC datasets available for research. This
paper introduces a large scale UGC dataset (1500 20 sec video clips) sampled
from millions of YouTube videos. The dataset covers popular categories like
Gaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel
sampling method based on features extracted from encoding, challenges for UGC
compression and quality evaluation are also discussed. Shortcomings of
traditional reference-based metrics on UGC are addressed. We demonstrate a
promising way to evaluate UGC quality by no-reference objective quality
metrics, and evaluate the current dataset with three no-reference metrics
(Noise, Banding, and SLEEQ).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06478</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06478</id><created>2019-04-13</created><authors><author><keyname>Yoshioka</keyname><forenames>Takuya</forenames></author><author><keyname>Chen</keyname><forenames>Zhuo</forenames></author><author><keyname>Liu</keyname><forenames>Changliang</forenames></author><author><keyname>Xiao</keyname><forenames>Xiong</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author><author><keyname>Dimitriadis</keyname><forenames>Dimitrios</forenames></author></authors><title>Low-Latency Speaker-Independent Continuous Speech Separation</title><categories>eess.AS cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speaker independent continuous speech separation (SI-CSS) is a task of
converting a continuous audio stream, which may contain overlapping voices of
unknown speakers, into a fixed number of continuous signals each of which
contains no overlapping speech segment. A separated, or cleaned, version of
each utterance is generated from one of SI-CSS's output channels
nondeterministically without being split up and distributed to multiple
channels. A typical application scenario is transcribing multi-party
conversations, such as meetings, recorded with microphone arrays. The output
signals can be simply sent to a speech recognition engine because they do not
include speech overlaps. The previous SI-CSS method uses a neural network
trained with permutation invariant training and a data-driven beamformer and
thus requires much processing latency. This paper proposes a low-latency SI-CSS
method whose performance is comparable to that of the previous method in a
microphone array-based meeting transcription task.This is achieved (1) by using
a new speech separation network architecture combined with a double buffering
scheme and (2) by performing enhancement with a set of fixed beamformers
followed by a neural post-filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06479</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06479</id><created>2019-04-13</created><authors><author><keyname>Yang</keyname><forenames>Haosen</forenames></author><author><keyname>Qiu</keyname><forenames>Robert C.</forenames></author><author><keyname>Chu</keyname><forenames>Lei</forenames></author></authors><title>Optimization based Preprocessing for Power system State Estimation</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the most universal static state estimation method, Weighted Least Square
(WLS) has been widely used in practice in recent decades. By iteratively
solving the normal equation, WLS method attempts to reduce the residual between
measurement and estimation of the measured variables. However, the reduction of
residual is not equal to diminishing the disparity between measurement and real
values, since measurement is different from unknown real values due to
measurement error of instruments, communication noise and random fluctuation,
etc. To solve this problem, this paper proposes a data-driven preprocessing
approach based on matrix construction and random matrices to clean origin
measurement. This Optimization based data-driven preprocessing scheme
significantly reduces the measurement error mainly by cleaning the spectrum of
covariance matrix, contributing to a more accurate state estimation. In this
method, a hermitian matrix is constructed to establish a reversible
relationship between measurement matrix and their covariance matrix, while
random matrix theory combined with optimization is responsible for cleaning the
measurement noise. Our approach, with great robustness and generality, is
particularly suitable for large interconnected power grid. Our method is tested
in cases with systems of different sizes, magnitudes of measured noise and
matrix size ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06508</identifier>
 <datestamp>2019-07-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06508</id><created>2019-04-13</created><updated>2019-07-02</updated><authors><author><keyname>Tu</keyname><forenames>Tao</forenames></author><author><keyname>Chen</keyname><forenames>Yuan-Jui</forenames></author><author><keyname>Yeh</keyname><forenames>Cheng-chieh</forenames></author><author><keyname>Lee</keyname><forenames>Hung-yi</forenames></author></authors><title>End-to-end Text-to-speech for Low-resource Languages by Cross-Lingual
  Transfer Learning</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Accepted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  End-to-end text-to-speech (TTS) has shown great success on large quantities
of paired text plus speech data. However, laborious data collection remains
difficult for at least 95% of the languages over the world, which hinders the
development of TTS in different languages. In this paper, we aim to build TTS
systems for such low-resource (target) languages where only very limited paired
data are available. We show such TTS can be effectively constructed by
transferring knowledge from a high-resource (source) language. Since the model
trained on source language cannot be directly applied to target language due to
input space mismatch, we propose a method to learn a mapping between source and
target linguistic symbols. Benefiting from this learned mapping, pronunciation
information can be preserved throughout the transferring procedure. Preliminary
experiments show that we only need around 15 minutes of paired data to obtain a
relatively good TTS system. Furthermore, analytic studies demonstrated that the
automatically discovered mapping correlate well with the phonetic expertise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06511</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06511</id><created>2019-04-13</created><authors><author><keyname>Hisham</keyname><forenames>Anver</forenames></author><author><keyname>Yuan</keyname><forenames>Di</forenames></author><author><keyname>Str&#xf6;m</keyname><forenames>Erik G.</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author></authors><title>Joint Scheduling and Power Control for V2V Broadcast Communication with
  Adjacent Channel Interference</title><categories>eess.SP math.OC</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper investigates how to mitigate the impact of adjacent channel
interference (ACI) in vehicular broadcast communication, using scheduling and
power control. Our objective is to maximize the number of connected vehicles.
First, we formulate the joint scheduling and power control problem as a mixed
Boolean linear programming (MBLP) problem. From this problem formulation, we
derive scheduling alone problem as Boolean linear programming (BLP) problem,
and power control alone problem as an MBLP problem. Due to the hardness in
solving joint scheduling and power control for multiple timeslots, we propose a
column generation method to reduce the computational complexity. We also
observe that the problem is highly numerically sensitive due to the high
dynamic range of channel parameters and adjacent channel interference ratio
(ACIR) values. Therefore, we propose a novel sensitivity reduction technique,
which can compute the optimal solution. Finally, we compare the results for
optimal scheduling, near-optimal joint scheduling and power control schemes,
and conclude that the effective scheduling and power control schemes indeed
significantly improve the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06529</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06529</id><created>2019-04-13</created><authors><author><keyname>Wang</keyname><forenames>Gao</forenames></author><author><keyname>Zheng</keyname><forenames>Huaibin</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author><author><keyname>Chen</keyname><forenames>Hui</forenames></author><author><keyname>Liu</keyname><forenames>Jianbin</forenames></author><author><keyname>He</keyname><forenames>Yuchen</forenames></author><author><keyname>Yuan</keyname><forenames>Yuan</forenames></author><author><keyname>Li</keyname><forenames>Fuli</forenames></author><author><keyname>Xu</keyname><forenames>Zhuo</forenames></author></authors><title>Naked-Eye Ghost Imaging via Photoelectric-Feedback</title><categories>eess.IV physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on optical correlations, ghost imaging is usually reconstructed by
computer algorithm from the acquired data. We here proposed an alternatively
high contrast naked-eye ghost imaging scheme which avoids computer algorithm
processing. Instead, the proposed scheme uses a photoelectric feedback loop to
realize the multiplication process of traditional ghost imaging. Meanwhile, it
exploits the vision persistence effect to implement integral process and to
generate negative images observed by naked eyes. To realize high contrast
naked-eye ghost imaging, a special pattern-scanning architecture on a low-speed
light-modulation mask is designed, which enables high-resolution imaging with
lower-order Hadamard vectors and boosts the imaging speed as well. Moreover,
two kinds of feedback circuits, the digital circuit and the analog circuit, are
presented respectively, which can achieve high-speed feedback operation on the
light intensity. With this approach, we demonstrate high-contrast real-time
imaging for moving objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06530</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06530</id><created>2019-04-13</created><authors><author><keyname>Wang</keyname><forenames>Gao</forenames></author><author><keyname>Zheng</keyname><forenames>Huaibin</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author><author><keyname>Chen</keyname><forenames>Hui</forenames></author><author><keyname>Liu</keyname><forenames>Jianbin</forenames></author><author><keyname>He</keyname><forenames>Yuchen</forenames></author><author><keyname>Yuan</keyname><forenames>Yuan</forenames></author><author><keyname>Li</keyname><forenames>Fuli</forenames></author><author><keyname>Xu</keyname><forenames>Zhuo</forenames></author></authors><title>All-optical naked-eye ghost imaging</title><categories>eess.IV physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ghost imaging is usually based on optoelectronic process and eletronic
computing. We here propose a new ghost imaging scheme, which avoids any
optoelectronic or electronic process. Instead, the proposed scheme exploits
all-optical correlation via the light-light interaction and the vision
persistence effect to generate images observed by naked eyes. To realize high
contrast naked-eye ghost imaging, a special pattern-scanning architecture on a
low-speed light-modulation disk is designed, which also enables high-resolution
imaging with lower-order Hadamard vectors and boosts the imaging speed. With
this approach, we realize high-contrast real-time ghost imaging for moving
colored objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06576</identifier>
 <datestamp>2019-05-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06576</id><created>2019-04-13</created><updated>2019-05-15</updated><authors><author><keyname>Ahadipour</keyname><forenames>Alireza</forenames></author><author><keyname>Mohammadi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Keshavarz-Haddad</keyname><forenames>Alireza</forenames></author></authors><title>Statistical-Based Privacy-Preserving Scheme with Malicious Consumers
  Identification for Smart Grid</title><categories>cs.CR eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As smart grids are getting popular and being widely implemented, preserving
the privacy of consumers is becoming more substantial. Power generation and
pricing in smart grids depends on the continuously gathered information from
the consumers. However, having access to the data relevant to the electricity
consumption of each individual consumer is in conflict with its privacy. One
common approach for preserving privacy is to aggregate data of different
consumers and to use their smart-meters for calculating the bills. But in this
approach, malicious consumers who send erroneous data to take advantage or
disrupt smart grid cannot be identified. In this paper, we propose a new
statistical-based scheme for data gathering and billing in which the privacy of
consumers is preserved, and at the same time, if any consumer with erroneous
data can be detected. Our simulation results verify these matters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06579</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06579</id><created>2019-04-13</created><authors><author><keyname>Fouladi</keyname><forenames>Ehsan</forenames></author><author><keyname>Mojallali</keyname><forenames>Hamed</forenames></author></authors><title>Design of optimized backstepping controller for the synchronization of
  chaotic Colpitts oscillator using shark smell algorithm</title><categories>cs.SY eess.SP math.OC</categories><doi>10.1007/s12043-017-1504-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an adaptive backstepping controller has been tuned to
synchronize two chaotic Colpitts oscillators in a master slave configuration.
The parameters of the controller are determined using shark smell optimization
(SSO) algorithm. Numerical results are presented and compared with those of
particle swarm optimization (PSO) algorithm. Simulation results show better
performance in terms of accuracy and convergence for the proposed optimized
method compared to PSO optimized controller or any non-optimized backstepping
controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06588</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06588</id><created>2019-04-13</created><authors><author><keyname>Farzaneh</keyname><forenames>Majid</forenames></author><author><keyname>Mahdian</keyname><forenames>Rahil</forenames></author><author><keyname>Asgari</keyname><forenames>Mohammad</forenames></author></authors><title>Audio Compression Using Graph-based Transform</title><categories>eess.AS cs.SD eess.SP</categories><comments>2018 9th International Symposium on Telecommunications (IST)</comments><doi>10.1109/ISTEL.2018.8661027</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Graph-based Transform is one of the recent transform coding methods which has
been used successfully in the state-of-art data decorrelation applications. In
this paper, we propose a Graph-based Transform (GT) for audio compression.
Hence, we introduce a proper graph structure for audio. Then the audio frames
are projected onto an orthogonal matrix consisting of eigenvectors of the
introduced graph matrix, leading to the sparse coefficients. The results show
that the proposed method outperforms the conventional transform methods like
Discrete Cosine Transform (DCT) and Walsh-Hadamard Transform (WHT) in
decorrelation of the audio signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06590</identifier>
 <datestamp>2019-09-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06590</id><created>2019-04-13</created><updated>2019-09-25</updated><authors><author><keyname>Nachmani</keyname><forenames>Eliya</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author></authors><title>Unsupervised Singing Voice Conversion</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Accepted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deep learning method for singing voice conversion. The proposed
network is not conditioned on the text or on the notes, and it directly
converts the audio of one singer to the voice of another. Training is performed
without any form of supervision: no lyrics or any kind of phonetic features, no
notes, and no matching samples between singers. The proposed network employs a
single CNN encoder for all singers, a single WaveNet decoder, and a classifier
that enforces the latent representation to be singer-agnostic. Each singer is
represented by one embedding vector, which the decoder is conditioned on. In
order to deal with relatively small datasets, we propose a new data
augmentation scheme, as well as new training losses and protocols that are
based on backtranslation. Our evaluation presents evidence that the conversion
produces natural signing voices that are highly recognizable as the target
singer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06591</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06591</id><created>2019-04-13</created><authors><author><keyname>Malik</keyname><forenames>Khalid Mahmood</forenames></author><author><keyname>Malik</keyname><forenames>Hafiz</forenames></author><author><keyname>Baumann</keyname><forenames>Roland</forenames></author></authors><title>Towards Vulnerability Analysis of Voice-Driven Interfaces and
  Countermeasures for Replay</title><categories>cs.CR cs.SD eess.AS</categories><comments>6 pages, IEEE 2nd International Conference on Multimedia Information
  Processing and Retrieval (IEEE MIPR 2019), March 28-30, 2019, San Jose, CA,
  USA</comments><msc-class>92C55</msc-class><acm-class>I.2.1; I.5.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fake audio detection is expected to become an important research area in the
field of smart speakers such as Google Home, Amazon Echo and chatbots developed
for these platforms. This paper presents replay attack vulnerability of
voice-driven interfaces and proposes a countermeasure to detect replay attack
on these platforms. This paper presents a novel framework to model replay
attack distortion, and then use a non-learning-based method for replay attack
detection on smart speakers. The reply attack distortion is modeled as a
higher-order nonlinearity in the replay attack audio. Higher-order spectral
analysis (HOSA) is used to capture characteristics distortions in the replay
audio. Effectiveness of the proposed countermeasure scheme is evaluated on
original speech as well as corresponding replayed recordings. The replay attack
recordings are successfully injected into the Google Home device via Amazon
Alexa using the drop-in conferencing feature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06599</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06599</id><created>2019-04-13</created><authors><author><keyname>Shen</keyname><forenames>Jinlu</forenames></author><author><keyname>Sun</keyname><forenames>Xueliang</forenames></author><author><keyname>Sivakumar</keyname><forenames>Krishnamoorthy</forenames></author><author><keyname>Belzer</keyname><forenames>Benjamin J.</forenames></author><author><keyname>Chan</keyname><forenames>Kheong Sann</forenames></author><author><keyname>James</keyname><forenames>Ashish</forenames></author></authors><title>TDMR Detection System with Local Area Influence Probabilistic a Priori
  Detector</title><categories>eess.SP cs.IT math.IT</categories><comments>Accepted to the 2019 IEEE International Conference on Communications
  (ICC-2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a three-track detection system for two dimensional magnetic
recording (TDMR) in which a local area influence probabilistic (LAIP) detector
works with a trellis-based Bahl-Cocke-Jelinek-Raviv (BCJR) detector to remove
intersymbol interference (ISI) and intertrack interference (ITI) among coded
data bits as well as media noise due to magnetic grain-bit interactions. Two
minimum mean-squared error (MMSE) linear equalizers with different response
targets are employed before the LAIP and BCJR detectors. The LAIP detector
considers local grain-bit interactions and passes coded bit log-likelihood
ratios (LLRs) to the channel decoder, whose output LLRs serve as a priori
information to the BCJR detector, which is followed by a second channel
decoding pass. Simulation results under 1-shot decoding on a
grain-flipping-probability (GFP) media model show that the proposed LAIP/BCJR
detection system achieves density gains of 6.8% for center-track detection and
1.2% for three-track detection compared to a standard BCJR/1D-PDNP. The
proposed system's BCJR detector bit error rates (BERs) are lower than those of
a recently proposed two-track BCJR/2D-PDNP system by factors of (0.55, 0.08)
for tracks 1 and 2 respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06648</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06648</id><created>2019-04-14</created><authors><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Lu</keyname><forenames>Jing</forenames></author></authors><title>A robust DOA estimation method for a linear microphone array under
  reverberant and noisy environments</title><categories>eess.AS cs.SD</categories><comments>7 pages, 4 figures, 3 tables, 33 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust method for linear array is proposed to address the difficulty of
direction-of-arrival (DOA) estimation in reverberant and noisy environments. A
direct-path dominance test based on the onset detection is utilized to extract
time-frequency bins containing the direct propagation of the speech. The
influence of the transient noise, which severely contaminates the onset test,
is mitigated by a proper transient noise determination scheme. Then for voice
features, a two-stage procedure is designed based on the extracted bins and an
effective dereverberation method, with robust but possibly biased estimation
from middle frequency bins followed by further refinement in higher frequency
bins. The proposed method effectively alleviates the estimation bias caused by
the linear arrangement of microphones, and has stable performance under noisy
and reverberant environments. Experimental evaluation using a 4-element
microphone array demonstrates the efficacy of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06656</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06656</id><created>2019-04-14</created><authors><author><keyname>Zhang</keyname><forenames>Na</forenames></author><author><keyname>Guan</keyname><forenames>Xuefeng</forenames></author><author><keyname>Cao</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Xinglei</forenames></author><author><keyname>Wu</keyname><forenames>Huayi</forenames></author></authors><title>A Hybrid Traffic Speed Forecasting Approach Integrating Wavelet
  Transform and Motif-based Graph Convolutional Recurrent Neural Network</title><categories>cs.CV cs.LG eess.SP</categories><comments>7 pages, IJCAI19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic forecasting is crucial for urban traffic management and guidance.
However, existing methods rarely exploit the time-frequency properties of
traffic speed observations, and often neglect the propagation of traffic flows
from upstream to downstream road segments. In this paper, we propose a hybrid
approach that learns the spatio-temporal dependency in traffic flows and
predicts short-term traffic speeds on a road network. Specifically, we employ
wavelet transform to decompose raw traffic data into several components with
different frequency sub-bands. A Motif-based Graph Convolutional Recurrent
Neural Network (Motif-GCRNN) and Auto-Regressive Moving Average (ARMA) are used
to train and predict low-frequency components and high-frequency components,
respectively. In the Motif-GCRNN framework, we integrate Graph Convolutional
Networks (GCNs) with local sub-graph structures - Motifs - to capture the
spatial correlations among road segments, and apply Long Short-Term Memory
(LSTM) to extract the short-term and periodic patterns in traffic speeds.
Experiments on a traffic dataset collected in Chengdu, China, demonstrate that
the proposed hybrid method outperforms six state-of-art prediction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06659</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06659</id><created>2019-04-14</created><authors><author><keyname>Zhou</keyname><forenames>Da-Peng</forenames></author><author><keyname>Peng</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Liang</forenames></author><author><keyname>Bao</keyname><forenames>Xiaoyi</forenames></author></authors><title>Computational distributed fiber-optic sensing</title><categories>eess.SP physics.optics</categories><comments>10 pages, 5 figures</comments><doi>10.1364/OE.27.017069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ghost imaging allows image reconstruction by correlation measurements between
a light beam that interacts with the object without spatial resolution and a
spatially resolved light beam that never interacts with the object. The two
light beams are copies of each other. Its computational version removes the
requirement of a spatially resolved detector when the light intensity pattern
is pre-known. Here, we exploit the temporal analogue of computational ghost
imaging, and demonstrate a computational distributed fiber-optic sensing
technique. Temporal images containing spatially distributed scattering
information used for sensing purposes are retrieved through correlating the
&quot;integrated&quot; backscattered light and the pre-known binary patterns. The
sampling rate required for our technique is inversely proportional to the total
time duration of a binary sequence, so that it can be significantly reduced
compared to that of the traditional methods. Our experiments demonstrate a 3
orders of magnitude reduction in the sampling rate, offering great
simplification and cost reduction in the distributed fiber-optic sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06679</identifier>
 <datestamp>2019-09-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06679</id><created>2019-04-14</created><updated>2019-09-20</updated><authors><author><keyname>Chen</keyname><forenames>Bin</forenames></author><author><keyname>Okonkwo</keyname><forenames>Chigo</forenames></author><author><keyname>Hafermann</keyname><forenames>Hartmut</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author></authors><title>Eight-dimensional Polarization-ring-switching Modulation Formats</title><categories>eess.SP</categories><journal-ref>IEEE Photonics Technology Letters 2019</journal-ref><doi>10.1109/LPT.2019.2943400</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two 8-dimensional (8D) modulation formats (8D-2048PRS-T1 and
8D-2048PRS-T2) with a spectral efficiency of 5.5 bit/4D-sym, where the 8
dimensions are obtained from two time slots and two polarizations. Both formats
provide a higher tolerance to nonlinearity by selecting symbols with
nonidentical states of polarization (SOPs) in two time slots. The performance
of these novel 8D modulation formats is assessed in terms of the effective
signal-to-noise ratio (SNR) and normalized generalized mutual information.
8D-2048PRS-T1 is more suitable for high SNRs, while 8D-2048PRS-T2 is shown to
be more tolerant to nonlinearities. A sensitivity improvement of at least 0.25
dB is demonstrated by maximizing normalized generalized mutual information
(NGMI). For a long-haul nonlinear optical fiber transmission system, the
benefit of mitigating the nonlinearity is demonstrated and a reach increase of
6.7% (560 km) over time-domain hybrid four-dimensional two-amplitude
eight-phase shift keying (TDH-4D-2A8PSK) is observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06704</identifier>
 <datestamp>2020-01-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06704</id><created>2019-04-14</created><updated>2020-01-29</updated><authors><author><keyname>Basar</keyname><forenames>Ertugrul</forenames></author></authors><title>Reconfigurable Intelligent Surface-Based Index Modulation: A New Beyond
  MIMO Paradigm for 6G</title><categories>cs.IT eess.SP math.IT</categories><comments>To appear in IEEE Transactions on Communications</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transmission through reconfigurable intelligent surfaces (RISs), which
control the reflection/scattering characteristics of incident waves in a
deliberate manner to enhance the signal quality at the receiver, appears as a
promising candidate for future wireless communication systems. In this paper,
we bring the concept of RIS-assisted communications to the realm of index
modulation (IM) by proposing RIS-space shift keying (RIS-SSK) and RIS-spatial
modulation (RIS-SM) schemes. These two schemes are realized through not only
intelligent reflection of the incoming signals to improve the reception but
also utilization of the IM principle for the indices of multiple receive
antennas in a clever way to improve the spectral efficiency. Maximum
energy-based suboptimal (greedy) and exhaustive search-based optimal (maximum
likelihood) detectors of the proposed RIS-SSK/SM schemes are formulated and a
unified framework is presented for the derivation of their theoretical average
bit error probability. Extensive computer simulation results are provided to
assess the potential of RIS-assisted IM schemes as well as to verify our
theoretical derivations. Our findings also reveal that RIS-based IM, which
enables high data rates with remarkably low error rates, can become a potential
candidate for future wireless communication systems in the context of beyond
multiple-input multiple-output (MIMO) solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06705</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06705</id><created>2019-04-14</created><authors><author><keyname>Tayeh</keyname><forenames>Gaby Bou</forenames></author><author><keyname>Makhoul</keyname><forenames>Abdallah</forenames></author><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Demerjian</keyname><forenames>Jacques</forenames></author></authors><title>A Spatial-Temporal Correlation Approach for Data Reduction in
  Cluster-Based Sensor Networks</title><categories>cs.NI eess.SP</categories><journal-ref>IEEE ACCESS, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a resource-constrained Wireless Sensor Networks (WSNs), the optimization
of the sampling and the transmission rates of each individual node is a crucial
issue. A high volume of redundant data transmitted through the network will
result in collisions, data loss, and energy dissipation. This paper proposes a
novel data reduction scheme, that exploits the spatial-temporal correlation
among sensor data in order to determine the optimal sampling strategy for the
deployed sensor nodes. This strategy reduces the overall sampling/transmission
rates while preserving the quality of the data. Moreover, a back-end
reconstruction algorithm is deployed on the workstation (Sink). This algorithm
can reproduce the data that have not been sampled by finding the spatial and
temporal correlation among the reported data set, and filling the 'non-sampled'
parts with predictions. We have used real sensor data of a network that was
deployed at the Grand-St-Bernard pass located between Switzerland and Italy. We
tested our approach using the previously mentioned data-set and compared it to
a recent adaptive sampling based data reduction approach. The obtained results
show that our proposed method consumes up to 60% less energy and can handle
non-stationary data more effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06711</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06711</id><created>2019-04-14</created><authors><author><keyname>Groisser</keyname><forenames>Benjamin</forenames></author></authors><title>Geometry of the EOS(R) Radiographic Scanner</title><categories>eess.IV</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The EOS(R) scanner is a radiographic system that captures PA and lateral
images in standing posture. The system is widely used in diagnosis and
assessment of scoliosis, as it provides a low-dose alternative to traditional
X-ray and can capture full-body images. Furthermore, spacial calibration
between the two imaging views is implemented in hardware, facilitating 3D
reconstruction of imaging targets. In this paper, a brief description of the
system is followed by an explanation of the geometric relationship between 3D
space and radiographic image space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06735</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06735</id><created>2019-04-14</created><authors><author><keyname>Bekerman</keyname><forenames>Amit</forenames></author><author><keyname>Froim</keyname><forenames>Sahar</forenames></author><author><keyname>Hadad</keyname><forenames>Barak</forenames></author><author><keyname>Bahabad</keyname><forenames>Alon</forenames></author></authors><title>Beam Profiler Network (BPNet) -- A Deep Learning Approach to Mode
  Demultiplexing of Laguerre-Gaussian Optical Beams</title><categories>eess.IV physics.optics</categories><doi>10.1364/OL.44.003629</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transverse field profile of light is being recognized as a resource for
classical and quantum communications for which reliable methods of sorting or
demultiplexing spatial optical modes are required. Here, we demonstrate,
experimentally, state-of-the-art mode demultiplexing of Laguerre-Gaussian beams
according to both their orbital angular momentum and radial topological numbers
using a flow of two concatenated deep neural networks. The first network serves
as a transfer function from experimentally-generated to ideal
numerically-generated data, while using a unique &quot;Histogram Weighted Loss&quot;
function that solves the problem of images with limited significant
information. The second network acts as a spatial-modes classifier. Our method
uses only the intensity profile of modes or their superposition, making the
phase information redundant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06788</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06788</id><created>2019-04-14</created><authors><author><keyname>Sofuoglu</keyname><forenames>Seyyid Emre</forenames></author><author><keyname>Aviyente</keyname><forenames>Selin</forenames></author></authors><title>Multi-Branch Tensor Network Structure for Tensor-Train Discriminant
  Analysis</title><categories>eess.SP cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order data with high dimensionality arise in a diverse set of
application areas such as computer vision, video analytics and medical imaging.
Tensors provide a natural tool for representing these types of data. Although
there has been a lot of work in the area of tensor decomposition and low-rank
tensor approximation, extensions to supervised learning, feature extraction and
classification are still limited. Moreover, most of the existing supervised
tensor learning approaches are based on the orthogonal Tucker model. However,
this model has some limitations for large tensors including high memory and
computational costs. In this paper, we introduce a supervised learning approach
for tensor classification based on the tensor-train model. In particular, we
introduce a multi-branch tensor network structure for efficient implementation
of tensor-train discriminant analysis (TTDA). The proposed approach takes
advantage of the flexibility of the tensor train structure to implement various
computationally efficient versions of TTDA. This approach is then evaluated on
image and video classification tasks with respect to computation time, storage
cost and classification accuracy and is compared to both vector and tensor
based discriminant analysis methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06837</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06837</id><created>2019-04-15</created><authors><author><keyname>Meir</keyname><forenames>Elad</forenames></author><author><keyname>Routtenberg</keyname><forenames>Tirza</forenames></author></authors><title>Cramer-Rao Bound for Estimation After Model Selection and its
  Application to Sparse Vector Estimation</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many practical parameter estimation problems, such as coefficient
estimation of polynomial regression and direction-of-arrival (DOA) estimation,
model selection is performed prior to estimation. In these cases, it is assumed
that the true measurement model belongs to a set of candidate models. The
data-based model selection step affects the subsequent estimation, which may
result in a biased estimation. In particular, the oracle Cramer-Rao bound
(CRB), which assumes knowledge of the model, is inappropriate for
post-model-selection performance analysis and system design outside the
asymptotic region. In this paper, we analyze the estimation performance of
post-model-selection estimators, by using the mean-squared-selected-error
(MSSE) criterion. We assume coherent estimators that force unselected
parameters to zero, and introduce the concept of selective unbiasedness in the
sense of Lehmann unbiasedness. We derive a non-Bayesian Cramer-Rao-type bound
on the MSSE and on the mean-squared-error (MSE) of any coherent and selective
unbiased estimators. As an important special case, we illustrate the
computation and applicability of the proposed selective CRB for sparse vector
estimation, in which the selection of a model is equivalent to the recovery of
the support. Finally, we demonstrate in numerical simulations that the proposed
selective CRB is a valid lower bound on the performance of the
post-model-selection maximum likelihood estimator for general linear model with
different model selection criteria, and for sparse vector estimation with
one-step thresholding. It is shown that for these cases the selective CRB
outperforms the existing bounds: oracle CRB, averaged CRB, and the SMS-CRB from
[1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06839</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06839</id><created>2019-04-15</created><authors><author><keyname>Ataie</keyname><forenames>Ali</forenames></author><author><keyname>Kanaanian</keyname><forenames>Borna</forenames></author><author><keyname>Khalaj</keyname><forenames>Babak H.</forenames></author></authors><title>Minimizing Uplink Delay in Delay-Sensitive 5G CRAN platforms</title><categories>eess.SP</categories><comments>15 pages,4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of minimizing the uplink delays of
users in a 5G cellular network. Such cellular network is based on a Cloud Radio
Access Network (CRAN) architecture with limited fronthaul capacity, where our
goal is to minimize delays of all users through an optimal resource allocation.
Earlier works minimize average delay of each user assuming same transmit power
for all users. Combining Pareto optimization and Markov Decision Process (MDP),
we show that every desired balance in the trade-off among infinite-horizon
average-reward delays, is achievable by minimizing a properly weighted sum
delays. In addition, we solve the problem in two realistic scenarios;
considering both power control and different (random) service times for the
users. In the latter scenario, we are able to define and minimize the more
preferred criterion of total delay vs. average delay for each user. We will
show that the resulting problem is equivalent to a discounted-reward
infinite-horizon MDP. Simulations show significant improvement in terms of
wider stability region for arrival rates in power-controlled scenario and
considerably reduced sum of users total delays in the case of random service
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06851</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06851</id><created>2019-04-15</created><authors><author><keyname>Honda</keyname><forenames>Shiori</forenames></author><author><keyname>Ishikawa</keyname><forenames>Yuri</forenames></author><author><keyname>Konno</keyname><forenames>Rei</forenames></author><author><keyname>Imai</keyname><forenames>Eiko</forenames></author><author><keyname>Nomiyama</keyname><forenames>Natsumi</forenames></author><author><keyname>Sakurada</keyname><forenames>Kazuki</forenames></author><author><keyname>Koumura</keyname><forenames>Takuya</forenames></author><author><keyname>Kondo</keyname><forenames>Hirohito</forenames></author><author><keyname>Furukawa</keyname><forenames>Shigeto</forenames></author><author><keyname>Fujii</keyname><forenames>Shinya</forenames></author><author><keyname>Nakatani</keyname><forenames>Masashi</forenames></author></authors><title>Proximal binaural sound can induce subjective frisson</title><categories>cs.SD cs.MM eess.AS</categories><comments>23 pages, 5 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sound frisson is a subjective experience wherein people tend to perceive the
feeling of chills in addition to a physiological response, such as goosebumps.
Multiple examples of frisson inducing sounds have been reported in the large
online community, but the mechanism of sound frisson is still elusive. Typical
frisson inducing sounds contain a looming effect, in which a sound seems to be
approaching close to one's peripersonal space. Previous studies on sound in
peripersonal space have reported objective measurements of sound-inducing
effects, but few studies have investigated the subjective experience of
frisson-inducing sound. Here, we investigate whether sound stimulus moving
around the human head can also produce subjective ratings of frisson. Our
results show that the participants experienced sound-induced frisson when
auditory stimuli were rotated around the head, regardless of the sound sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.06868</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.06868</id><created>2019-04-15</created><updated>2019-06-25</updated><authors><author><keyname>Nakamura</keyname><forenames>Kazuhiro</forenames></author><author><keyname>Hashimoto</keyname><forenames>Kei</forenames></author><author><keyname>Oura</keyname><forenames>Keiichiro</forenames></author><author><keyname>Nankaku</keyname><forenames>Yoshihiko</forenames></author><author><keyname>Tokuda</keyname><forenames>Keiichi</forenames></author></authors><title>Singing voice synthesis based on convolutional neural networks</title><categories>eess.AS cs.LG cs.SD</categories><comments>Singing voice samples (Japanese, English, Chinese):
  https://www.techno-speech.com/news-20181214a-en</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper describes a singing voice synthesis based on convolutional
neural networks (CNNs). Singing voice synthesis systems based on deep neural
networks (DNNs) are currently being proposed and are improving the naturalness
of synthesized singing voices. In these systems, the relationship between
musical score feature sequences and acoustic feature sequences extracted from
singing voices is modeled by DNNs. Then, an acoustic feature sequence of an
arbitrary musical score is output in units of frames by the trained DNNs, and a
natural trajectory of a singing voice is obtained by using a parameter
generation algorithm. As singing voices contain rich expression, a powerful
technique to model them accurately is required. In the proposed technique,
long-term dependencies of singing voices are modeled by CNNs. An acoustic
feature sequence is generated in units of segments that consist of long-term
frames, and a natural trajectory is obtained without the parameter generation
algorithm. Experimental results in a subjective listening test show that the
proposed architecture can synthesize natural sounding singing voices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07078</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07078</id><created>2019-04-15</created><authors><author><keyname>Kamper</keyname><forenames>Herman</forenames></author><author><keyname>Anastassiou</keyname><forenames>Aristotelis</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author></authors><title>Semantic query-by-example speech search using visual grounding</title><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of recent studies have started to investigate how speech systems can
be trained on untranscribed speech by leveraging accompanying images at
training time. Examples of tasks include keyword prediction and within- and
across-mode retrieval. Here we consider how such models can be used for
query-by-example (QbE) search, the task of retrieving utterances relevant to a
given spoken query. We are particularly interested in semantic QbE, where the
task is not only to retrieve utterances containing exact instances of the
query, but also utterances whose meaning is relevant to the query. We follow a
segmental QbE approach where variable-duration speech segments (queries, search
utterances) are mapped to fixed-dimensional embedding vectors. We show that a
QbE system using an embedding function trained on visually grounded speech data
outperforms a purely acoustic QbE system in terms of both exact and semantic
retrieval performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07116</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07116</id><created>2019-04-01</created><authors><author><keyname>Routray</keyname><forenames>Sudhir K.</forenames></author><author><keyname>Anand</keyname><forenames>Sharath</forenames></author></authors><title>Narrowband IoT for Healthcare</title><categories>eess.SP</categories><comments>5 pages, Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) is going to have its presence in all the
essential sectors of human lives. It has the ability to provide both mainstream
as well as the value added services in almost all the sectors. Healthcare is an
important service sector for overall development. It has far reaching
implications in the quality of living. In the modern world where the quality of
living has been degraded significantly IoT can certainly play a constructive
role in providing better services. In healthcare, there are several occasions
such as patient health monitoring, remote observation and emergency proceedings
outside the hospital where sensors can play essential roles. The coordinated
sensor networks can provide even better services. IoT has the ability to
provide all these coordinated services. Narrowband IoT (NBIoT) is an economical
and simpler version of IoT which can handle these tasks effectively. Due to the
widespread requirement of healthcare, NBIoT is a preferred solution as it needs
fewer amounts of resources. In this article, we provide the main issues and
difficulties of NBIoT in healthcare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07152</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07152</id><created>2019-04-10</created><authors><author><keyname>Mishra</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Essop</keyname><forenames>Mohamed Hoosain</forenames></author></authors><title>Low-cost spectrogram based counterfeit medicine detection</title><categories>eess.SP</categories><comments>Under revision to be published in IEEE Consumer Electronics Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contaminated substances such as counterfeit medication and food contami-nated
with pesticide residue is a pandemic of utmost urgency. Spectroscopy and
chromatography methods are often used but are expensive and complex and as such
a need exists for a device that can be easily operated in developing
commu-nities. We present a hacked visible spectrometer based contaminated
substance detector using machine learning. The Support Vector Machine (SVM),
Logistic Regression, linear Regression and Convolutional Neural Network (CNN)
models have been implemented and are trained on the acquired spectrum data. Our
results show that a lowcost method of identifying contaminated substances is
achievable with very high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07154</identifier>
 <datestamp>2019-10-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07154</id><created>2019-04-15</created><updated>2019-10-17</updated><authors><author><keyname>Kim</keyname><forenames>Jaehun</forenames></author><author><keyname>Urbano</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Liem</keyname><forenames>Cynthia C. S.</forenames></author><author><keyname>Hanjalic</keyname><forenames>Alan</forenames></author></authors><title>Are Nearby Neighbors Relatives?: Testing Deep Music Embeddings</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>this work was accepted for publication in the &quot;Frontiers in Applied
  Mathematics and Statistics (Deep Learning: Status, Applications and
  Algorithms)&quot;</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Deep neural networks have frequently been used to directly learn
representations useful for a given task from raw input data. In terms of
overall performance metrics, machine learning solutions employing deep
representations frequently have been reported to greatly outperform those using
hand-crafted feature representations. At the same time, they may pick up on
aspects that are predominant in the data, yet not actually meaningful or
interpretable. In this paper, we therefore propose a systematic way to test the
trustworthiness of deep music representations, considering musical semantics.
The underlying assumption is that in case a deep representation is to be
trusted, distance consistency between known related points should be maintained
both in the input audio space and corresponding latent deep space. We generate
known related points through semantically meaningful transformations, both
considering imperceptible and graver transformations. Then, we examine within-
and between-space distance consistencies, both considering audio space and
latent embedded space, the latter either being a result of a conventional
feature extractor or a deep encoder. We illustrate how our method, as a
complement to task-specific performance, provides interpretable insight into
what a network may have captured from training data signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07163</identifier>
 <datestamp>2019-07-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07163</id><created>2019-04-15</created><updated>2019-07-17</updated><authors><author><keyname>Mirakhorli</keyname><forenames>Jalal</forenames></author><author><keyname>Amindavar</keyname><forenames>Hamidreza</forenames></author><author><keyname>Mirakhorli</keyname><forenames>Mojgan</forenames></author></authors><title>Graph-Based Method for Anomaly Prediction in Brain Network</title><categories>eess.IV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resting-state functional MRI (rs-fMRI) in functional neuroimaging techniques
have improved in brain disorders, dysfunction studies via mapping the topology
of the brain connections, i.e. connectopic mapping. Since, there are the slight
differences between healthy and unhealthy brain regions and functions,
investigation into the complex topology of functional and structural brain
networks in human is a complicated task with the growth of evaluation criteria.
Irregular graph deep learning applications have widely spread to understanding
human cognitive functions that are linked to gene expression and related
distributed spatial patterns, because the neuronal networks of the brain can
hold dynamically a variety of brain solutions with different activity patterns
and functional connectivity, these applications might also be involved with
both node-centric and graph-centric tasks. In this paper, we performed a novel
approach of individual generative model and high order graph analysis for the
region of interest recognition areas of the brain which do not have a normal
connection during applying certain tasks. Here, we proposed a high order
framework of Graph Auto-Encoder (GAE) with a hypersphere distributer for
functional data analysis in brain imaging studies that is underlying
non-Euclidean structure in the learning of strong non-rigid graphs among large
scale data. In addition, we distinguished the possible modes of correlations in
abnormal brain connections. Our finding will show the degree of correlation
between the affected regions and their simultaneous occurrence over time that
can be used to diagnose brain diseases or revealing the ability of the nervous
system to modify in brain topology at all angles, brain plasticity, according
to input stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07294</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07294</id><created>2019-04-15</created><authors><author><keyname>Abdulbaqi</keyname><forenames>Jalal</forenames></author><author><keyname>Gu</keyname><forenames>Yue</forenames></author><author><keyname>Marsic</keyname><forenames>Ivan</forenames></author></authors><title>RHR-Net: A Residual Hourglass Recurrent Neural Network for Speech
  Enhancement</title><categories>eess.AS cs.LG cs.SD eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most current speech enhancement models use spectrogram features that require
an expensive transformation and result in phase information loss. Previous work
has overcome these issues by using convolutional networks to learn long-range
temporal correlations across high-resolution waveforms. These models, however,
are limited by memory-intensive dilated convolution and aliasing artifacts from
upsampling. We introduce an end-to-end fully-recurrent hourglass-shaped neural
network architecture with residual connections for waveform-based
single-channel speech enhancement. Our model can efficiently capture long-range
temporal dependencies by reducing the features resolution without information
loss. Experimental results show that our model outperforms state-of-the-art
approaches in six evaluation metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07329</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07329</id><created>2019-04-15</created><authors><author><keyname>Alhujaili</keyname><forenames>Khaled</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Rangaswamy</keyname><forenames>Muralidhar</forenames></author></authors><title>Transmit MIMO Radar Beampattern Design Via Optimization on the Complex
  Circle Manifol</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2914884</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability of Multiple-Input Multiple-Output (MIMO) radar systems to adapt
waveforms across antennas allows flexibility in the transmit beampattern
design. In cognitive radar, a popular cost function is to minimize the
deviation against an idealized beampattern (which is arrived at with knowledge
of the environment). The optimization of the transmit beampattern becomes
particularly challenging in the presence of practical constraints on the
transmit waveform. One of the hardest of such constraints is the non-convex
constant modulus constraint, which has been the subject of much recent work. In
a departure from most existing approaches, we develop a solution that involves
direct optimization over the non-convex complex circle manifold. That is, we
derive a new projection, descent, and retraction (PDR) update strategy that
allows for monotonic cost function improvement while maintaining feasibility
over the complex circle manifold (constant modulus set). For quadratic cost
functions (as is the case with beampattern deviation), we provide analytical
guarantees of monotonic cost function improvement along with proof of
convergence to a local minima. We evaluate the proposed PDR algorithm against
other candidate MIMO beampattern design methods and show that PDR can
outperform competing wideband beampattern design methods while being
computationally less expensive. Finally, orthogonality across antennas is
incorporated in the PDR framework by adding a penalty term to the beampattern
cost function. Enabled by orthogonal waveforms, robustness to target direction
mismatch is also demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07368</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07368</id><created>2019-04-15</created><authors><author><keyname>Shabanighazikelayeh</keyname><forenames>Maryam</forenames></author><author><keyname>Koyuncu</keyname><forenames>Erdem</forenames></author></authors><title>Outage-Optimized Deployment of UAVs</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multiple unmanned aerial vehicles (UAVs) serving a density of
ground terminals (GTs) as mobile base stations. The objective is to minimize
the outage probability of GT-to-UAV transmissions. In this context, the optimal
placement of UAVs under different UAV altitude constraints and GT densities is
studied. First, using a random deployment argument, a general upper bound on
the optimal outage probability is found for any density of GTs and any number
of UAVs. Lower bounds on the performance of optimal deployments are also
determined. The upper and lower bounds are combined to show that the optimal
outage probability decays exponentially with the number of UAVs for GT
densities with finite support. Next, the structure of optimal deployments are
studied when the common altitude constraint is large. In this case, for a wide
class of GT densities, it is shown that all UAVs should be placed to the same
location in an optimal deployment. A design implication is that one can use a
single multi-antenna UAV as opposed to multiple single-antenna UAVs without
loss of optimality. Numerical optimization of UAV deployments are carried out
using particle swarm optimization. Simulation results are also presented to
confirm the analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07376</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07376</id><created>2019-04-15</created><authors><author><keyname>Islam</keyname><forenames>Md Tauhidul</forenames></author><author><keyname>Righetti</keyname><forenames>Raffaella</forenames></author></authors><title>A spline interpolation based data reconstruction technique for
  estimation of strain time constant in ultrasound poroelastography</title><categories>eess.SP</categories><comments>11 pages, 3 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultrasound poroelastography is a cost-effective non-invasive imaging
technique, which is able to reconstruct several mechanical parameters of cancer
and normal tissue such as Young's modulus, Poisson's ratio, interstitial
permeability and vascular permeability. To estimate the permeabilities,
estimation of the strain time constant (TC) is required, which is a challenging
task because of non-linearity of the exponential strain curve and noise present
in the experimental data. Moreover, noise in many strain frames becomes very
high because of motion artifacts from the sonographer, animal/patient and/or
the environment. Therefore, using these frames in computation of strain TC can
lead to inaccurate estimates of the mechanical parameters. In this letter, we
introduce a cubic spline based interpolation method, which uses only the good
frames (frame of high SNR) to reconstruct the information of the bad frames
(frames of low SNR) and estimate the strain TC. We prove with finite element
simulation that the proposed reconstruction method can improve the estimation
accuracy of the strain TC by 46% in comparison to the estimates from noisy
data, and 37% in comparison to the estimates from Kalman filtered data at an
SNR of 30dB. Based on the high accuracy of the proposed method in estimating
strain TC from poroelastography data, the proposed method can be preferred
technique by the clinicians and researchers interested in non-invasive imaging
of tissue mechanical parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07377</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07377</id><created>2019-04-15</created><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author></authors><title>Non-Stochastic Hypothesis Testing with Application to Privacy Against
  Hypothesis-Testing Adversary</title><categories>cs.IT cs.SY eess.SP math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider privacy against hypothesis testing adversaries
within a non-stochastic framework. We develop a theory of non-stochastic
hypothesis testing by borrowing the notion of uncertain variables from
non-stochastic information theory. We define tests as binary-valued mappings on
uncertain variables and prove a fundamental bound on the best performance of
tests in non-stochastic hypothesis testing. We use this bound to develop a
measure of privacy. We then construct reporting policies with prescribed
privacy and utility guarantees. The utility of a reporting policy is measured
by the distance between the reported and original values. We illustrate the
effects of using such privacy-preserving reporting polices on a
publicly-available practical dataset of preferences and demographics of young
individuals, aged between 15-30, with Slovakian nationality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07386</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07386</id><created>2019-04-15</created><authors><author><keyname>Lee</keyname><forenames>Kong Aik</forenames></author><author><keyname>Hautamaki</keyname><forenames>Ville</forenames></author><author><keyname>Kinnunen</keyname><forenames>Tomi</forenames></author><author><keyname>Yamamoto</keyname><forenames>Hitoshi</forenames></author><author><keyname>Okabe</keyname><forenames>Koji</forenames></author><author><keyname>Vestman</keyname><forenames>Ville</forenames></author><author><keyname>Huang</keyname><forenames>Jing</forenames></author><author><keyname>Ding</keyname><forenames>Guohong</forenames></author><author><keyname>Sun</keyname><forenames>Hanwu</forenames></author><author><keyname>Larcher</keyname><forenames>Anthony</forenames></author><author><keyname>Das</keyname><forenames>Rohan Kumar</forenames></author><author><keyname>Li</keyname><forenames>Haizhou</forenames></author><author><keyname>Rouvier</keyname><forenames>Mickael</forenames></author><author><keyname>Bousquet</keyname><forenames>Pierre-Michel</forenames></author><author><keyname>Rao</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Qing</forenames></author><author><keyname>Zhang</keyname><forenames>Chunlei</forenames></author><author><keyname>Bahmaninezhad</keyname><forenames>Fahimeh</forenames></author><author><keyname>Delgado</keyname><forenames>Hector</forenames></author><author><keyname>Patino</keyname><forenames>Jose</forenames></author><author><keyname>Wang</keyname><forenames>Qiongqiong</forenames></author><author><keyname>Guo</keyname><forenames>Ling</forenames></author><author><keyname>Koshinaka</keyname><forenames>Takafumi</forenames></author><author><keyname>Zhang</keyname><forenames>Jiacen</forenames></author><author><keyname>Shinoda</keyname><forenames>Koichi</forenames></author><author><keyname>Trong</keyname><forenames>Trung Ngo</forenames></author><author><keyname>Sahidullah</keyname><forenames>Md</forenames></author><author><keyname>Lu</keyname><forenames>Fan</forenames></author><author><keyname>Tang</keyname><forenames>Yun</forenames></author><author><keyname>Tu</keyname><forenames>Ming</forenames></author><author><keyname>Teh</keyname><forenames>Kah Kuan</forenames></author><author><keyname>Tran</keyname><forenames>Huy Dat</forenames></author><author><keyname>George</keyname><forenames>Kuruvachan K.</forenames></author><author><keyname>Kukanov</keyname><forenames>Ivan</forenames></author><author><keyname>Desnous</keyname><forenames>Florent</forenames></author><author><keyname>Yang</keyname><forenames>Jichen</forenames></author><author><keyname>Yilmaz</keyname><forenames>Emre</forenames></author><author><keyname>Xu</keyname><forenames>Longting</forenames></author><author><keyname>Bonastre</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Xu</keyname><forenames>Chenglin</forenames></author><author><keyname>Lim</keyname><forenames>Zhi Hao</forenames></author><author><keyname>Chng</keyname><forenames>Eng Siong</forenames></author><author><keyname>Ranjan</keyname><forenames>Shivesh</forenames></author><author><keyname>Hansen</keyname><forenames>John H. L.</forenames></author><author><keyname>Todisco</keyname><forenames>Massimiliano</forenames></author><author><keyname>Evans</keyname><forenames>Nicholas</forenames></author></authors><title>I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared
  Experiences</title><categories>eess.AS cs.CL cs.SD</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The I4U consortium was established to facilitate a joint entry to NIST
speaker recognition evaluations (SRE). The latest edition of such joint
submission was in SRE 2018, in which the I4U submission was among the
best-performing systems. SRE'18 also marks the 10-year anniversary of I4U
consortium into NIST SRE series of evaluation. The primary objective of the
current paper is to summarize the results and lessons learned based on the
twelve sub-systems and their fusion submitted to SRE'18. It is also our
intention to present a shared view on the advancements, progresses, and major
paradigm shifts that we have witnessed as an SRE participant in the past decade
from SRE'08 to SRE'18. In this regard, we have seen, among others, a paradigm
shift from supervector representation to deep speaker embedding, and a switch
of research challenge from channel compensation to domain adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07441</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07441</id><created>2019-04-15</created><authors><author><keyname>Seraj</keyname><forenames>Esmaeil</forenames></author><author><keyname>Yazdi</keyname><forenames>Mehran</forenames></author><author><keyname>Shahparian</keyname><forenames>Nastaran</forenames></author></authors><title>fMRI Based Cerebral Instantaneous Parameters for Automatic Alzheimer's,
  Mild Cognitive Impairment and Healthy Subject Classification</title><categories>eess.SP cs.CE eess.IV q-bio.NC q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic identification and categorization of Alzheimer's patients and the
ability to distinguish between different levels of this disease can be very
helpful to the research community in this field, since other non-automatic
approaches are very time-consuming and are highly dependent on experts'
experience. Herein, we propose the utility of cerebral instantaneous phase and
envelope information in order to discriminate between Alzheimer's patients, MCI
subjects and healthy normal individuals from functional magnetic resonance
imaging (fMRI) data. To this end, after performing the region-of-interest (ROI)
analysis on fMRI data, different features covering power, entropy and coherency
aspects of data are derived from instantaneous phase and envelope sequences of
ROI signals. Various sets of features are calculated and fed to a sequential
forward floating feature selection (SFFFS) to choose the most discriminative
and informative sets of features. A Student's t-test has been used to select
the most relevant features from chosen sets. Finally, a K-NN classifier is used
to distinguish between classes in a three-class categorization problem. The
reported performance in overall accuracy using fMRI data of 111 combined
subjects, is 80.1% with 80.0% Sensitivity to both Alzheimer's and Normal
categories distinction and is comparable to the state-of-the-art approaches
recently proposed in this regard. The significance of obtained results was
statistically confirmed by evaluating through standard classification
performance indicators. The obtained results illustrate that introduced
analytic phase and envelope feature indexes derived from the ROI signals are
significantly discriminative in distinguishing between Alzheimer's patients and
Normal healthy subject.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07453</identifier>
 <datestamp>2020-01-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07453</id><created>2019-04-16</created><updated>2020-01-23</updated><authors><author><keyname>Kumar</keyname><forenames>Mari Ganesh</forenames></author><author><keyname>Kumar</keyname><forenames>Suvidha Rupesh</forenames></author><author><keyname>M</keyname><forenames>Saranya</forenames></author><author><keyname>Bharathi</keyname><forenames>B.</forenames></author><author><keyname>Murthy</keyname><forenames>Hema A.</forenames></author></authors><title>Spoof detection using time-delay shallow neural network and feature
  switching</title><categories>eess.AS cs.CR cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting spoofed utterances is a fundamental problem in voice-based
biometrics. Spoofing can be performed either by logical accesses like speech
synthesis, voice conversion or by physical accesses such as replaying the
pre-recorded utterance. Inspired by the state-of-the-art \emph{x}-vector based
speaker verification approach, this paper proposes a time-delay shallow neural
network (TD-SNN) for spoof detection for both logical and physical access. The
novelty of the proposed TD-SNN system vis-a-vis conventional DNN systems is
that it can handle variable length utterances during testing. Performance of
the proposed TD-SNN systems and the baseline Gaussian mixture models (GMMs) is
analyzed on the ASV-spoof-2019 dataset. The performance of the systems is
measured in terms of the minimum normalized tandem detection cost function
(min-t-DCF). When studied with individual features, the TD-SNN system
consistently outperforms the GMM system for physical access. For logical
access, GMM surpasses TD-SNN systems for certain individual features. When
combined with the decision-level feature switching (DLFS) paradigm, the best
TD-SNN system outperforms the best baseline GMM system on evaluation data with
a relative improvement of 48.03\% and 49.47\% for both logical and physical
access, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07478</identifier>
 <datestamp>2019-07-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07478</id><created>2019-04-16</created><authors><author><keyname>Simpson</keyname><forenames>Becks</forenames></author><author><keyname>Dutil</keyname><forenames>Francis</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Cohen</keyname><forenames>Joseph Paul</forenames></author></authors><title>GradMask: Reduce Overfitting by Regularizing Saliency</title><categories>cs.CV cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With too few samples or too many model parameters, overfitting can inhibit
the ability to generalise predictions to new data. Within medical imaging, this
can occur when features are incorrectly assigned importance such as distinct
hospital specific artifacts, leading to poor performance on a new dataset from
a different institution without those features, which is undesirable. Most
regularization methods do not explicitly penalize the incorrect association of
these features to the target class and hence fail to address this issue. We
propose a regularization method, GradMask, which penalizes saliency maps
inferred from the classifier gradients when they are not consistent with the
lesion segmentation. This prevents non-tumor related features to contribute to
the classification of unhealthy samples. We demonstrate that this method can
improve test accuracy between 1-3% compared to the baseline without GradMask,
showing that it has an impact on reducing overfitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07506</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07506</id><created>2019-04-16</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Kim</keyname><forenames>Taejoon</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Perrins</keyname><forenames>Erik</forenames></author></authors><title>Leveraging the Restricted Isometry Property: Improved Low-Rank Subspace
  Decomposition for Hybrid Millimeter-Wave Systems</title><categories>eess.SP</categories><journal-ref>IEEE Transactions on Communications ( Volume: 66 , Issue: 11 ,
  Nov. 2018 )</journal-ref><doi>10.1109/TCOMM.2018.2854779</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication at millimeter wave frequencies will be one of the essential new
technologies in 5G. Acquiring an accurate channel estimate is the key to
facilitate advanced millimeter wave hybrid multiple-input multiple-output
(MIMO) precoding techniques. Millimeter wave MIMO channel estimation, however,
suffers from a considerably increased channel use overhead. This happens due to
the limited number of radio frequency (RF) chains that prevent the digital
baseband from directly accessing the signal at each antenna. To address this
issue, recent research has focused on adaptive closed-loop and two-way channel
estimation techniques. In this paper, unlike the prior approaches, we study a
non-adaptive, hence rather simple, open-loop millimeter wave MIMO channel
estimation technique. We present a simple random design of channel subspace
sampling signals and show that they obey the restricted isometry property (RIP)
with high probability. We then formulate the channel estimation as a low-rank
subspace decomposition problem and, based on the RIP, show that the proposed
framework reveals resilience to a low signal-to-noise ratio. It is revealed
that the required number of channel uses ensuring a bounded estimation error is
linearly proportional to the degrees of freedom of the channel, whereas it
converges to a constant value if the number of RF chains can grow
proportionally to the channel dimension while keeping the channel rank fixed.
In particular, we show that the tighter the RIP characterization the lower the
channel estimation error is. We also devise an iterative technique that
effectively finds a suboptimal but stationary solution to the formulated
problem. The proposed technique is shown to have improved channel estimation
accuracy with a low channel use overhead as compared to that of previous
closed-loop and two-way adaptation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07515</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07515</id><created>2019-04-16</created><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Kim</keyname><forenames>Taejoon</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author></authors><title>Sparse Subspace Decomposition for Millimeter Wave MIMO Channel
  Estimation</title><categories>eess.SP</categories><journal-ref>2016 IEEE Global Communications Conference (GLOBECOM)</journal-ref><doi>10.1109/GLOCOM.2016.7842278</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave multiple-input multiple-output (MIMO) communication systems
must operate over sparse wireless links and will require large antenna arrays
to provide high throughput. To achieve sufficient array gains, these systems
must learn and adapt to the channel state conditions. However, conventional
MIMO channel estimation can not be directly extended to millimeter wave due to
the constraints on cost-effective millimeter wave operation imposed on the
number of available RF chains. Sparse subspace scanning techniques that search
for the best subspace sample from the sounded subspace samples have been
investigated for channel estimation.However, the performance of these
techniques starts to deteriorate as the array size grows, especially for the
hybrid precoding architecture. The millimeter wave channel estimation challenge
still remains and should be properly addressed before the system can be
deployed and used to its full potential. In this work, we propose a sparse
subspace decomposition (SSD) technique for sparse millimeter wave MIMO channel
estimation. We formulate the channel estimation as an optimization problem that
minimizes the subspace distance from the received subspace samples. Alternating
optimization techniques are devised to tractably handle the non-convex problem.
Numerical simulations demonstrate that the proposed method outperforms other
existing techniques with remarkably low overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07556</identifier>
 <datestamp>2019-07-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07556</id><created>2019-04-16</created><updated>2019-06-28</updated><authors><author><keyname>Eloff</keyname><forenames>Ryan</forenames></author><author><keyname>Nortje</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>van Niekerk</keyname><forenames>Benjamin</forenames></author><author><keyname>Govender</keyname><forenames>Avashna</forenames></author><author><keyname>Nortje</keyname><forenames>Leanne</forenames></author><author><keyname>Pretorius</keyname><forenames>Arnu</forenames></author><author><keyname>van Biljon</keyname><forenames>Elan</forenames></author><author><keyname>van der Westhuizen</keyname><forenames>Ewald</forenames></author><author><keyname>van Staden</keyname><forenames>Lisa</forenames></author><author><keyname>Kamper</keyname><forenames>Herman</forenames></author></authors><title>Unsupervised acoustic unit discovery for speech synthesis using discrete
  latent-variable neural networks</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For our submission to the ZeroSpeech 2019 challenge, we apply discrete
latent-variable neural networks to unlabelled speech and use the discovered
units for speech synthesis. Unsupervised discrete subword modelling could be
useful for studies of phonetic category learning in infants or in low-resource
speech technology requiring symbolic input. We use an autoencoder (AE)
architecture with intermediate discretisation. We decouple acoustic unit
discovery from speaker modelling by conditioning the AE's decoder on the
training speaker identity. At test time, unit discovery is performed on speech
from an unseen speaker, followed by unit decoding conditioned on a known target
speaker to obtain reconstructed filterbanks. This output is fed to a neural
vocoder to synthesise speech in the target speaker's voice. For discretisation,
categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs)
and straight-through estimation are compared at different compression levels on
two languages. Our final model uses convolutional encoding, VQ-VAE
discretisation, deconvolutional decoding and an FFTNet vocoder. We show that
decoupled speaker conditioning intrinsically improves discrete acoustic
representations, yielding competitive synthesis quality compared to the
challenge baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07572</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07572</id><created>2019-04-16</created><authors><author><keyname>Davis</keyname><forenames>Solomon</forenames></author><author><keyname>Bucher</keyname><forenames>Izhak</forenames></author></authors><title>A new approach to single-tone frequency estimation via linear least
  squares curve fitting</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presented is a new algorithm for estimating the frequency of a single-tone
noisy signal using linear least squares (LLS). Frequency estimation is a
nonlinear problem, and typically, methods such as Nonlinear Least Squares (NLS)
(batch) or a digital phase locked loop (DPLL) (online) are employed for such an
estimate. However, with the linearization approach presented here, one can
harness the efficiency of LLS to obtain very good estimates, while experiencing
little penalty for linearizing. In this paper, the mathematical basis of this
algorithm is described, and the bias and variance are analyzed analytically and
numerically. With the batch version of this algorithm, it will be demonstrated
that the estimator is just as good as NLS. But because LLS is non recursive,
the estimate it produces much more efficiently than from NLS. When the proposed
algorithm is implemented online, it will be demonstrated that performance is
comparable to a digital phase locked loop, with some stability and tracking
range advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07577</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07577</id><created>2019-04-16</created><authors><author><keyname>Eslami</keyname><forenames>Taban</forenames></author><author><keyname>Mirjalili</keyname><forenames>Vahid</forenames></author><author><keyname>Fong</keyname><forenames>Alvis</forenames></author><author><keyname>Laird</keyname><forenames>Angela</forenames></author><author><keyname>Saeed</keyname><forenames>Fahad</forenames></author></authors><title>ASD-DiagNet: A hybrid learning approach for detection of Autism Spectrum
  Disorder using fMRI data</title><categories>cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mental disorders such as Autism Spectrum Disorders (ASD) are heterogeneous
disorders that are notoriously difficult to diagnose, especially in children.
The current psychiatric diagnostic process is based purely on the behavioural
observation of symptomology (DSM-5/ICD-10) and may be prone to over-prescribing
of drugs due to misdiagnosis. In order to move the field towards more
quantitative fashion, we need advanced and scalable machine learning
infrastructure that will allow us to identify reliable biomarkers of mental
health disorders. In this paper, we propose a framework called ASD-DiagNet for
classifying subjects with ASD from healthy subjects by using only fMRI data. We
designed and implemented a joint learning procedure using an autoencoder and a
single layer perceptron which results in improved quality of extracted features
and optimized parameters for the model. Further, we designed and implemented a
data augmentation strategy, based on linear interpolation on available feature
vectors, that allows us to produce synthetic datasets needed for training of
machine learning models. The proposed approach is evaluated on a public dataset
provided by Autism Brain Imaging Data Exchange including 1035 subjects coming
from 17 different brain imaging centers. Our machine learning model outperforms
other state of the art methods from 13 imaging centers with increase in
classification accuracy up to 20% with maximum accuracy of 80%. The machine
learning technique presented in this paper, in addition to yielding better
quality, gives enormous advantages in terms of execution time (40 minutes vs. 6
hours on other methods). The implemented code is available as GPL license on
GitHub portal of our lab (https://github.com/pcdslab/ASD-DiagNet).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07612</identifier>
 <datestamp>2019-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07612</id><created>2019-04-16</created><updated>2019-11-12</updated><authors><author><keyname>Michelashvili</keyname><forenames>Michael</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author></authors><title>Audio Denoising with Deep Network Priors</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a method for audio denoising that combines processing done in both
the time domain and the time-frequency domain. Given a noisy audio clip, the
method trains a deep neural network to fit this signal. Since the fitting is
only partly successful and is able to better capture the underlying clean
signal than the noise, the output of the network helps to disentangle the clean
audio from the rest of the signal. The method is completely unsupervised and
only trains on the specific audio clip that is being denoised. Our experiments
demonstrate favorable performance in comparison to the literature methods, and
our code and audio samples are available at https: //github.com/mosheman5/DNP.
Index Terms: Audio denoising; Unsupervised learning
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07654</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07654</id><created>2019-04-16</created><authors><author><keyname>Papacharalampopoulos</keyname><forenames>Alexios</forenames></author></authors><title>Investigating Data-driven systems as digital twins: Numerical behavior
  of Ho-Kalman method for order estimation</title><categories>cs.SY eess.SP</categories><comments>8 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  System identification has been a major advancement in the evolution of
engineering. As it is by default the first step towards a significant set of
adaptive control techniques, it is imperative for engineers to apply it in
order to practice control. Given that system identification could be useful in
creating a digital twin, this work focuses on the initial stage of the
procedure by discussing simplistic system order identification. Through
specific numerical examples, this study constitutes an investigation on the
most \natural&quot; method for estimating the order from responses in a convenient
and seamless way in time-domain. The method itself, originally proposed by Ho
and Kalman and utilizing linear algebra, is an intuitive tool retrieving
information out of the data themselves. Finally, with the help of the
limitations of the methods, the potential future outlook is discussed, under
the prism of forming a digital twin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07704</identifier>
 <datestamp>2019-09-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07704</id><created>2019-04-14</created><updated>2019-06-30</updated><authors><author><keyname>Segal</keyname><forenames>Yael</forenames></author><author><keyname>Fuchs</keyname><forenames>Tzeviya Sylvia</forenames></author><author><keyname>Keshet</keyname><forenames>Joseph</forenames></author></authors><title>SpeechYOLO: Detection and Localization of Speech Objects</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><journal-ref>Interspeech 2019, pp. 4210-4214</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to apply object detection methods from the vision
domain on the speech recognition domain, by treating audio fragments as
objects. More specifically, we present SpeechYOLO, which is inspired by the
YOLO algorithm for object detection in images. The goal of SpeechYOLO is to
localize boundaries of utterances within the input signal, and to correctly
classify them. Our system is composed of a convolutional neural network, with a
simple least-mean-squares loss function. We evaluated the system on several
keyword spotting tasks, that include corpora of read speech and spontaneous
speech. Our system compares favorably with other algorithms trained for both
localization and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07750</identifier>
 <datestamp>2019-08-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07750</id><created>2019-04-16</created><updated>2019-08-20</updated><authors><author><keyname>Gao</keyname><forenames>Ruohan</forenames></author><author><keyname>Grauman</keyname><forenames>Kristen</forenames></author></authors><title>Co-Separating Sounds of Visual Objects</title><categories>cs.CV cs.MM cs.SD eess.AS</categories><comments>ICCV 2019, Project page:
  http://vision.cs.utexas.edu/projects/coseparation/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning how objects sound from video is challenging, since they often
heavily overlap in a single audio channel. Current methods for visually-guided
audio source separation sidestep the issue by training with artificially mixed
video clips, but this puts unwieldy restrictions on training data collection
and may even prevent learning the properties of &quot;true&quot; mixed sounds. We
introduce a co-separation training paradigm that permits learning object-level
sounds from unlabeled multi-source videos. Our novel training objective
requires that the deep neural network's separated audio for similar-looking
objects be consistently identifiable, while simultaneously reproducing accurate
video-level audio tracks for each source training pair. Our approach
disentangles sounds in realistic test videos, even in cases where an object was
not observed individually during training. We obtain state-of-the-art results
on visually-guided audio source separation and audio denoising for the MUSIC,
AudioSet, and AV-Bench datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07773</identifier>
 <datestamp>2019-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07773</id><created>2019-04-16</created><updated>2019-10-16</updated><authors><author><keyname>Wen</keyname><forenames>Junhao</forenames></author><author><keyname>Thibeau-Sutre</keyname><forenames>Elina</forenames></author><author><keyname>Diaz-Melo</keyname><forenames>Mauricio</forenames></author><author><keyname>Samper-Gonzalez</keyname><forenames>Jorge</forenames></author><author><keyname>Routier</keyname><forenames>Alexandre</forenames></author><author><keyname>Bottani</keyname><forenames>Simona</forenames></author><author><keyname>Dormont</keyname><forenames>Didier</forenames></author><author><keyname>Durrleman</keyname><forenames>Stanley</forenames></author><author><keyname>Burgos</keyname><forenames>Ninon</forenames></author><author><keyname>Colliot</keyname><forenames>Olivier</forenames></author></authors><title>Convolutional Neural Networks for Classification of Alzheimer's Disease:
  Overview and Reproducible Evaluation</title><categories>cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over 30 papers have proposed to use convolutional neural network (CNN) for AD
classification from anatomical MRI. However, the classification performance is
difficult to compare across studies due to variations in components such as
participant selection, image preprocessing or validation procedure. Moreover,
these studies are hardly reproducible because their frameworks are not publicly
accessible and because implementation details are lacking. Lastly, some of
these papers may report a biased performance due to inadequate or unclear
validation or model selection procedures. In the present work, we aim to
address these limitations through three main contributions. First, we performed
a systematic literature review and found that more than half of the surveyed
papers may have suffered from data leakage. Our second contribution is the
extension of our open-source framework for classification of AD using CNN and
T1-weighted MRI. Finally, we used this framework to rigorously compare
different CNN architectures. The data was split into training/validation/test
sets at the very beginning and only the training/validation sets were used for
model selection. To avoid any overfitting, the test sets were left untouched
until the end of the peer-review process. Overall, the different 3D approaches
(3D-subject, 3D-ROI, 3D-patch) achieved similar performances while that of the
2D slice approach was lower. Of note, the different CNN approaches did not
perform better than a SVM with voxel-based features. The different approaches
generalized well to similar populations but not to datasets with different
inclusion criteria or demographical characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07781</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07781</id><created>2019-04-16</created><authors><author><keyname>Rahmati</keyname><forenames>Ali</forenames></author><author><keyname>Hosseinalipour</keyname><forenames>Seyyedali</forenames></author><author><keyname>Yapici</keyname><forenames>Yavuz</forenames></author><author><keyname>He</keyname><forenames>Xiaofan</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>Dai</keyname><forenames>Huaiyu</forenames></author><author><keyname>Bhuyan</keyname><forenames>Arupjyoti</forenames></author></authors><title>Interference Avoidance in UAV-Assisted Networks: Joint 3D Trajectory
  Design and Power Allocation</title><categories>cs.NI eess.SP</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of the unmanned aerial vehicle (UAV) has been foreseen as a promising
technology for the next generation communication networks. Since there are no
regulations for UAVs deployment yet, most likely they form a network in
coexistence with an already existed network. In this work, we consider a
transmission mechanism that aims to improve the data rate between a terrestrial
base station (BS) and user equipment (UE) through deploying multiple UAVs
relaying the desired data flow. Considering the coexistence of this network
with other established communication networks, we take into account the effect
of interference, which is incurred by the existing nodes. Our primary goal is
to optimize the three-dimensional (3D) trajectories and power allocation for
the relaying UAVs to maximize the data flow while keeping the interference to
existing nodes below a predefined threshold. An alternating-maximization
strategy is proposed to solve the joint 3D trajectory design and power
allocation for the relaying UAVs. To this end, we handle the information
exchange within the network by resorting to spectral graph theory and
subsequently address the power allocation through convex optimization
techniques. Simulation results show that our approach can considerably improve
the information flow while the interference threshold constraint is met.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07813</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07813</id><created>2019-04-12</created><authors><author><keyname>Yadav</keyname><forenames>Milan</forenames></author><author><keyname>Khanna</keyname><forenames>Kanak</forenames></author></authors><title>Energy Saving Strategy Based on Profiling</title><categories>eess.SP cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraints imposed by power consumption and the related costs are one of the
key roadblocks to the design and development of next generation exascale
systems. To mitigate these issues, strategies that reduce the power consumption
of the processor are the need of the hour. Techniques such as Dynamic Voltage
and Frequency Scaling (DVFS) exist which reduce the power consumption of a
processor at runtime but they should be used in such a manner so that their
overhead does not hamper application performance. In this paper, we propose an
energy saving strategy which operates on timeslice basis to apply DVFS under a
user defined performance constraint. Results show energy savings up to 7% when
NAS benchmarks are tested on a laptop platform
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07838</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07838</id><created>2019-04-16</created><authors><author><keyname>Dub&#xe9;</keyname><forenames>Danny</forenames></author></authors><title>Efficient Encoding of Data into Two-Dimensional Constrained Bit Patterns</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-dimensional constrained coding is a problem that is much more difficult
than its one-dimensional counterpart. Indeed, in two dimensions, obtaining the
answers to very natural questions becomes uncomputable. In particular, it is
undecidable to determine if it is possible to fill the infinite plane with
symbols in such a way that no forbidden pattern appears. Also, even when we
know that such an infinite plane exists, it is uncomputable to determine the
maximal rate at which payload data can be embedded into the selection of a
valid infinite plane. Recently, Nakamura et al. presented a technique that
efficiently performs the construction of a matrix of symbols that embeds
payload data. Their technique is efficient in the sense that the construction
takes time that is proportional to the area of the constructed matrix. Their
technique is based on the offline elaboration of a collection of tiles, which
is then used for the matrix construction. The collection-elaboration step is
time consuming and it might even never terminate nor succeed. In this work, we
extend their technique by generalizing their notion of tile. Our technique has
the potential to achieve much higher data-embedding rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07845</identifier>
 <datestamp>2019-04-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07845</id><created>2019-04-16</created><authors><author><keyname>Yang</keyname><forenames>Gene-Ping</forenames></author><author><keyname>Tuan</keyname><forenames>Chao-I</forenames></author><author><keyname>Lee</keyname><forenames>Hung-Yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>Improved Speech Separation with Time-and-Frequency Cross-domain Joint
  Embedding and Clustering</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Submitted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech separation has been very successful with deep learning techniques.
Substantial effort has been reported based on approaches over spectrogram,
which is well known as the standard time-and-frequency cross-domain
representation for speech signals. It is highly correlated to the phonetic
structure of speech, or &quot;how the speech sounds&quot; when perceived by human, but
primarily frequency domain features carrying temporal behaviour. Very
impressive work achieving speech separation over time domain was reported
recently, probably because waveforms in time domain may describe the different
realizations of speech in a more precise way than spectrogram. In this paper,
we propose a framework properly integrating the above two directions, hoping to
achieve both purposes. We construct a time-and-frequency feature map by
concatenating the 1-dim convolution encoded feature map (for time domain) and
the spectrogram (for frequency domain), which was then processed by an
embedding network and clustering approaches very similar to those used in time
and frequency domain prior works. In this way, the information in the time and
frequency domains, as well as the interactions between them, can be jointly
considered during embedding and clustering. Very encouraging results
(state-of-the-art to our knowledge) were obtained with WSJ0-2mix dataset in
preliminary experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07933</identifier>
 <datestamp>2020-02-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07933</id><created>2019-04-16</created><updated>2020-02-11</updated><authors><author><keyname>P&#xe9;rez</keyname><forenames>Andr&#xe9;s F.</forenames></author><author><keyname>Sanguineti</keyname><forenames>Valentina</forenames></author><author><keyname>Morerio</keyname><forenames>Pietro</forenames></author><author><keyname>Murino</keyname><forenames>Vittorio</forenames></author></authors><title>Audio-Visual Model Distillation Using Acoustic Images</title><categories>cs.CV cs.SD eess.AS</categories><comments>Accepted at WACV 2020; supplementary material at page 11; code
  available at https://github.com/afperezm/acoustic-images-distillation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate how to learn rich and robust feature
representations for audio classification from visual data and acoustic images,
a novel audio data modality. Former models learn audio representations from raw
signals or spectral data acquired by a single microphone, with remarkable
results in classification and retrieval. However, such representations are not
so robust towards variable environmental sound conditions. We tackle this
drawback by exploiting a new multimodal labeled action recognition dataset
acquired by a hybrid audio-visual sensor that provides RGB video, raw audio
signals, and spatialized acoustic data, also known as acoustic images, where
the visual and acoustic images are aligned in space and synchronized in time.
Using this richer information, we train audio deep learning models in a
teacher-student fashion. In particular, we distill knowledge into audio
networks from both visual and acoustic image teachers. Our experiments suggest
that the learned representations are more powerful and have better
generalization capabilities than the features learned from models trained using
just single-microphone audio data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07944</identifier>
 <datestamp>2019-07-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07944</id><created>2019-04-16</created><updated>2019-07-25</updated><authors><author><keyname>Neekhara</keyname><forenames>Paarth</forenames></author><author><keyname>Donahue</keyname><forenames>Chris</forenames></author><author><keyname>Puckette</keyname><forenames>Miller</forenames></author><author><keyname>Dubnov</keyname><forenames>Shlomo</forenames></author><author><keyname>McAuley</keyname><forenames>Julian</forenames></author></authors><title>Expediting TTS Synthesis with Adversarial Vocoding</title><categories>cs.SD cs.LG eess.AS</categories><comments>Published as a conference paper at INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent approaches in text-to-speech (TTS) synthesis employ neural network
strategies to vocode perceptually-informed spectrogram representations directly
into listenable waveforms. Such vocoding procedures create a computational
bottleneck in modern TTS pipelines. We propose an alternative approach which
utilizes generative adversarial networks (GANs) to learn mappings from
perceptually-informed spectrograms to simple magnitude spectrograms which can
be heuristically vocoded. Through a user study, we show that our approach
significantly outperforms na\&quot;ive vocoding strategies while being hundreds of
times faster than neural network vocoders used in state-of-the-art TTS systems.
We also show that our method can be used to achieve state-of-the-art results in
unsupervised synthesis of individual words of speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07958</identifier>
 <datestamp>2019-04-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07958</id><created>2019-04-16</created><authors><author><keyname>Nie</keyname><forenames>Shuai</forenames></author><author><keyname>Jornet</keyname><forenames>Josep M.</forenames></author><author><keyname>Akyildiz</keyname><forenames>Ian F.</forenames></author></authors><title>Intelligent Environments based on Ultra-Massive MIMO Platforms for
  Wireless Communication in Millimeter Wave and Terahertz Bands</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Millimeter-wave (30-300 GHz) and Terahertz-band communications (0.3-10 THz)
are envisioned as key wireless technologies to satisfy the demand for
Terabit-per-second (Tbps) links in the 5G and beyond eras. The very large
available bandwidth in this ultra-broadband frequency range comes at the cost
of a very high propagation loss, which combined with the low power of mm-wave
and THz-band transceivers limits the communication distance and data-rates. In
this paper, the concept of intelligent communication environments enabled by
Ultra-Massive MIMO platforms is proposed to increase the communication distance
and data-rates at mm-wave and THz-band frequencies. An end-to-end physical
model is developed by taking into account the capabilities of novel intelligent
plasmonic antenna arrays which can operate in transmission, reception,
reflection and waveguiding, as well as the peculiarities of the mm-wave and
THz-band multi-path channel. Based on the developed model, extensive
quantitative results for different scenarios are provided to illustrate the
performance improvements in terms of both achievable distance and data-rate in
Ultra-Massive MIMO environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07971</identifier>
 <datestamp>2019-04-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07971</id><created>2019-03-29</created><authors><author><keyname>Haigh</keyname><forenames>Paul Anthony</forenames></author><author><keyname>Minotto</keyname><forenames>Alessandro</forenames></author><author><keyname>Burton</keyname><forenames>Andrew</forenames></author><author><keyname>Ghassemlooy</keyname><forenames>Zabih</forenames></author><author><keyname>Murto</keyname><forenames>Petri</forenames></author><author><keyname>Genene</keyname><forenames>Zewdneh</forenames></author><author><keyname>Mammo</keyname><forenames>Wendimagegn</forenames></author><author><keyname>Andersson</keyname><forenames>Mats R.</forenames></author><author><keyname>Wang</keyname><forenames>Ergang</forenames></author><author><keyname>Cacialli</keyname><forenames>Franco</forenames></author><author><keyname>Darwazeh</keyname><forenames>Izzat</forenames></author></authors><title>Experimental Demonstration of Staggered CAP Modulation for Low Bandwidth
  Red-Emitting Polymer-LED based Visible Light Communications</title><categories>cs.NI eess.SP physics.app-ph</categories><comments>6 pages, 9 figures, IEEE ICC 2019 conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we experimentally demonstrate, for the first time, staggered
carrier-less amplitude and phase (sCAP) modulation for visible light
communication systems based on polymer light-emitting diodes emitting at ~639
nm. The key advantage offered by sCAP in comparison to conventional multiband
CAP is its full use of the available spectrum. In this work, we compare sCAP,
which utilises four orthogonal filters to generate the signal, with a
conventional 4-band multi-CAP system and on-off keying (OOK). We transmit each
modulation format with equal energy and present a record un-coded transmission
speed of ~6 Mb/s. This represents gains of 25% and 65% over the achievable rate
using 4-CAP and OOK, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.07979</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.07979</id><created>2019-04-16</created><authors><author><keyname>Dev</keyname><forenames>Soumyabrata</forenames></author><author><keyname>Nautiyal</keyname><forenames>Atul</forenames></author><author><keyname>Lee</keyname><forenames>Yee Hui</forenames></author><author><keyname>Winkler</keyname><forenames>Stefan</forenames></author></authors><title>CloudSegNet: A Deep Network for Nychthemeron Cloud Image Segmentation</title><categories>physics.ao-ph cs.CV eess.IV</categories><comments>Published in IEEE Geoscience and Remote Sensing Letters, 2019</comments><doi>10.1109/LGRS.2019.2912140</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze clouds in the earth's atmosphere using ground-based sky cameras.
An accurate segmentation of clouds in the captured sky/cloud image is
difficult, owing to the fuzzy boundaries of clouds. Several techniques have
been proposed that use color as the discriminatory feature for cloud detection.
In the existing literature, however, analysis of daytime and nighttime images
is considered separately, mainly because of differences in image
characteristics and applications. In this paper, we propose a light-weight
deep-learning architecture called CloudSegNet. It is the first that integrates
daytime and nighttime (also known as nychthemeron) image segmentation in a
single framework, and achieves state-of-the-art results on public databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08031</identifier>
 <datestamp>2019-04-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08031</id><created>2019-04-16</created><authors><author><keyname>Xue</keyname><forenames>Jiabin</forenames></author><author><keyname>Han</keyname><forenames>Jiqing</forenames></author><author><keyname>Zheng</keyname><forenames>Tieran</forenames></author><author><keyname>Guo</keyname><forenames>Jiaxing</forenames></author><author><keyname>Wu</keyname><forenames>Boyong</forenames></author></authors><title>Hard Sample Mining for the Improved Retraining of Automatic Speech
  Recognition</title><categories>cs.SD cs.LG eess.AS</categories><comments>Submitted to Interspeech 2019;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is an effective way that improves the performance of the existing
Automatic Speech Recognition (ASR) systems by retraining with more and more new
training data in the target domain. Recently, Deep Neural Network (DNN) has
become a successful model in the ASR field. In the training process of the DNN
based methods, a back propagation of error between the transcription and the
corresponding annotated text is used to update and optimize the parameters.
Thus, the parameters are more influenced by the training samples with a big
propagation error than the samples with a small one. In this paper, we define
the samples with significant error as the hard samples and try to improve the
performance of the ASR system by adding many of them. Unfortunately, the hard
samples are sparse in the training data of the target domain, and manually
label them is expensive. Therefore, we propose a hard samples mining method
based on an enhanced deep multiple instance learning, which can find the hard
samples from unlabeled training data by using a small subset of the dataset
with manual labeling in the target domain. We applied our method to an End2End
ASR task and obtained the best performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08039</identifier>
 <datestamp>2019-04-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08039</id><created>2019-04-16</created><authors><author><keyname>Xue</keyname><forenames>Jiabin</forenames></author><author><keyname>Han</keyname><forenames>Jiqing</forenames></author><author><keyname>Zheng</keyname><forenames>Tieran</forenames></author><author><keyname>Gao</keyname><forenames>Xiang</forenames></author><author><keyname>Guo</keyname><forenames>Jiaxing</forenames></author></authors><title>A Multi-Task Learning Framework for Overcoming the Catastrophic
  Forgetting in Automatic Speech Recognition</title><categories>cs.SD cs.LG eess.AS</categories><comments>Submitted to Interspeech 2019;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, data-driven based Automatic Speech Recognition (ASR) systems have
achieved state-of-the-art results. And transfer learning is often used when
those existing systems are adapted to the target domain, e.g., fine-tuning,
retraining. However, in the processes, the system parameters may well deviate
too much from the previously learned parameters. Thus, it is difficult for the
system training process to learn knowledge from target domains meanwhile not
forgetting knowledge from the previous learning process, which is called as
catastrophic forgetting (CF). In this paper, we attempt to solve the CF problem
with the lifelong learning and propose a novel multi-task learning (MTL)
training framework for ASR. It considers reserving original knowledge and
learning new knowledge as two independent tasks, respectively. On the one hand,
we constrain the new parameters not to deviate too far from the original
parameters and punish the new system when forgetting original knowledge. On the
other hand, we force the new system to solve new knowledge quickly. Then, a MTL
mechanism is employed to get the balance between the two tasks. We applied our
method to an End2End ASR task and obtained the best performance in both target
and original datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08072</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08072</id><created>2019-04-16</created><updated>2019-06-04</updated><authors><author><keyname>Tiwari</keyname><forenames>Nilesh K</forenames></author><author><keyname>Jha</keyname><forenames>A K</forenames></author><author><keyname>Singh</keyname><forenames>S P</forenames></author><author><keyname>Akhtar</keyname><forenames>M Jaleel</forenames></author></authors><title>Planar Sensor for RF Characterization of magnetic samples</title><categories>eess.SP</categories><comments>1 figure, 1 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A magnetic measurement of the bar shaped test specimen placed inside the
planar sensor is presented. A magnetic material characterization approach using
planar cavity is proposed in this work. The proposed planar sensor relaxes the
main limitations of conventional approach by using the proper feeding section.
The proposed sensor is numerically verified using the full wave EM simulator
for the magnetic property estimation. It is found that the developed sensor is
able to characterize the test specimen with improved accuracy than that of
conventional approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08104</identifier>
 <datestamp>2019-07-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08104</id><created>2019-04-17</created><updated>2019-07-16</updated><authors><author><keyname>Jung</keyname><forenames>Jee-weon</forenames></author><author><keyname>Heo</keyname><forenames>Hee-Soo</forenames></author><author><keyname>Kim</keyname><forenames>Ju-ho</forenames></author><author><keyname>Shim</keyname><forenames>Hye-jin</forenames></author><author><keyname>Yu</keyname><forenames>Ha-Jin</forenames></author></authors><title>RawNet: Advanced end-to-end deep neural network using raw waveforms for
  text-independent speaker verification</title><categories>eess.AS cs.LG cs.SD</categories><comments>Accepted for oral presentation at Interspeech 2019, code available at
  http://github.com/Jungjee/RawNet</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, direct modeling of raw waveforms using deep neural networks has
been widely studied for a number of tasks in audio domains. In speaker
verification, however, utilization of raw waveforms is in its preliminary
phase, requiring further investigation. In this study, we explore end-to-end
deep neural networks that input raw waveforms to improve various aspects:
front-end speaker embedding extraction including model architecture,
pre-training scheme, additional objective functions, and back-end
classification. Adjustment of model architecture using a pre-training scheme
can extract speaker embeddings, giving a significant improvement in
performance. Additional objective functions simplify the process of extracting
speaker embeddings by merging conventional two-phase processes: extracting
utterance-level features such as i-vectors or x-vectors and the feature
enhancement phase, e.g., linear discriminant analysis. Effective back-end
classification models that suit the proposed speaker embedding are also
explored. We propose an end-to-end system that comprises two deep neural
networks, one front-end for utterance-level speaker embedding extraction and
the other for back-end classification. Experiments conducted on the VoxCeleb1
dataset demonstrate that the proposed model achieves state-of-the-art
performance among systems without data augmentation. The proposed system is
also comparable to the state-of-the-art x-vector system that adopts data
augmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08138</identifier>
 <datestamp>2019-12-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08138</id><created>2019-04-17</created><updated>2019-12-11</updated><authors><author><keyname>Chen</keyname><forenames>Feiyang</forenames></author><author><keyname>Luo</keyname><forenames>Ziqian</forenames></author><author><keyname>Xu</keyname><forenames>Yanyan</forenames></author><author><keyname>Ke</keyname><forenames>Dengfeng</forenames></author></authors><title>Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment
  Analysis</title><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted by AAAI2020 Workshop: AffCon2020</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sentiment analysis, mostly based on text, has been rapidly developing in the
last decade and has attracted widespread attention in both academia and
industry. However, the information in the real world usually comes from
multiple modalities, such as audio and text. Therefore, in this paper, based on
audio and text, we consider the task of multimodal sentiment analysis and
propose a novel fusion strategy including both multi-feature fusion and
multi-modality fusion to improve the accuracy of audio-text sentiment analysis.
We call it the DFF-ATMF (Deep Feature Fusion - Audio and Text Modality Fusion)
model, which consists of two parallel branches, the audio modality based branch
and the text modality based branch. Its core mechanisms are the fusion of
multiple feature vectors and multiple modality attention. Experiments on the
CMU-MOSI dataset and the recently released CMU-MOSEI dataset, both collected
from YouTube for sentiment analysis, show the very competitive results of our
DFF-ATMF model. Furthermore, by virtue of attention weight distribution
heatmaps, we also demonstrate the deep features learned by using DFF-ATMF are
complementary to each other and robust. Surprisingly, DFF-ATMF also achieves
new state-of-the-art results on the IEMOCAP dataset, indicating that the
proposed fusion strategy also has a good generalization ability for multimodal
emotion recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08248</identifier>
 <datestamp>2019-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08248</id><created>2019-04-16</created><updated>2019-11-27</updated><authors><author><keyname>Pasa</keyname><forenames>Luca</forenames></author><author><keyname>Morrone</keyname><forenames>Giovanni</forenames></author><author><keyname>Badino</keyname><forenames>Leonardo</forenames></author></authors><title>An Analysis of Speech Enhancement and Recognition Losses in Limited
  Resources Multi-talker Single Channel Audio-Visual ASR</title><categories>eess.AS cs.CL cs.SD stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyzed how audio-visual speech enhancement can help to
perform the ASR task in a cocktail party scenario. Therefore we considered two
simple end-to-end LSTM-based models that perform single-channel audio-visual
speech enhancement and phone recognition respectively. Then, we studied how the
two models interact, and how to train them jointly affects the final result. We
analyzed different training strategies that reveal some interesting and
unexpected behaviors. The experiments show that during optimization of the ASR
task the speech enhancement capability of the model significantly decreases and
vice-versa. Nevertheless the joint optimization of the two tasks shows a
remarkable drop of the Phone Error Rate (PER) compared to the audio-visual
baseline models trained only to perform phone recognition. We analyzed the
behaviors of the proposed models by using two limited-size datasets, and in
particular we used the mixed-speech versions of GRID and TCD-TIMIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08352</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08352</id><created>2019-04-17</created><updated>2019-04-26</updated><authors><author><keyname>Lo</keyname><forenames>Chen-Chou</forenames></author><author><keyname>Fu</keyname><forenames>Szu-Wei</forenames></author><author><keyname>Huang</keyname><forenames>Wen-Chin</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Hsin-Min</forenames></author></authors><title>MOSNet: Deep Learning based Objective Assessment for Voice Conversion</title><categories>cs.SD cs.LG eess.AS</categories><comments>Submitted to Interspeech2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing objective evaluation metrics for voice conversion (VC) are not
always correlated with human perception. Therefore, training VC models with
such criteria may not effectively improve naturalness and similarity of
converted speech. In this paper, we propose deep learning-based assessment
models to predict human ratings of converted speech. We adopt the convolutional
and recurrent neural network models to build a mean opinion score (MOS)
predictor, termed as MOSNet. The proposed models are tested on large-scale
listening test results of the Voice Conversion Challenge (VCC) 2018.
Experimental results show that the predicted scores of the proposed MOSNet are
highly correlated with human MOS ratings at the system level while being fairly
correlated with human MOS ratings at the utterance level. Meanwhile, we have
modified MOSNet to predict the similarity scores, and the preliminary results
show that the predicted scores are also fairly correlated with human ratings.
These results confirm that the proposed models could be used as a computational
evaluator to measure the MOS of VC systems to reduce the need for expensive
human rating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08369</identifier>
 <datestamp>2019-12-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08369</id><created>2019-04-17</created><updated>2019-12-09</updated><authors><author><keyname>Mack</keyname><forenames>Wolfgang</forenames></author><author><keyname>Habets</keyname><forenames>Emanu&#xeb;l A. P.</forenames></author></authors><title>Deep Filtering: Signal Extraction and Reconstruction Using Complex
  Time-Frequency Filters</title><categories>cs.SD eess.AS</categories><doi>10.1109/LSP.2019.2955818</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal extraction from a single-channel mixture with additional undesired
signals is most commonly performed using time-frequency (TF) masks. Typically,
the mask is estimated with a deep neural network (DNN), and element-wise
applied to the complex mixture short-time Fourier transform (STFT)
representation to perform the extraction. Ideal mask magnitudes are zero for
solely undesired signals in a TF bin and undefined for total destructive
interference. Usually, masks have an upper bound to provide well-defined DNN
outputs at the cost of limited extraction capabilities. We propose to estimate
with a DNN a complex TF filter for each mixture TF bin which maps an STFT area
in the respective mixture to the desired TF bin to address destructive
interference in mixture TF bins. The DNN is optimized by minimizing the error
between the extracted and the ground-truth desired signal allowing to learn the
TF filters without having to specify ground-truth TF filters. We compare our
approach with complex and real-valued TF masks by separating speech from a
variety of different sound and noise classes from the Google AudioSet corpus.
We also process the mixture STFT with notch-filters and zero whole time-frames,
to simulate packet-loss during transmission, to demonstrate the reconstruction
capabilities of our approach. The proposed method outperformed the baselines,
especially when notch-filters and time-frame zeroing were applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08452</identifier>
 <datestamp>2020-02-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08452</id><created>2019-04-17</created><updated>2019-07-09</updated><authors><author><keyname>Tang</keyname><forenames>Zhenyu</forenames></author><author><keyname>Kanu</keyname><forenames>John D.</forenames></author><author><keyname>Hogan</keyname><forenames>Kevin</forenames></author><author><keyname>Manocha</keyname><forenames>Dinesh</forenames></author></authors><title>Regression and Classification for Direction-of-Arrival Estimation with
  Convolutional Recurrent Neural Networks</title><categories>cs.SD cs.LG eess.AS</categories><journal-ref>Proc. Interspeech 2019, 654-658</journal-ref><doi>10.21437/Interspeech.2019-1111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel learning-based approach to estimate the
direction-of-arrival (DOA) of a sound source using a convolutional recurrent
neural network (CRNN) trained via regression on synthetic data and Cartesian
labels. We also describe an improved method to generate synthetic data to train
the neural network using state-of-the-art sound propagation algorithms that
model specular as well as diffuse reflections of sound. We compare our model
against three other CRNNs trained using different formulations of the same
problem: classification on categorical labels, and regression on spherical
coordinate labels. In practice, our model achieves up to 43% decrease in
angular error over prior methods. The use of diffuse reflection results in 34%
and 41% reduction in angular prediction errors on LOCATA and SOFA datasets,
respectively, over prior methods based on image-source methods. Our method
results in an additional 3% error reduction over prior schemes that use
classification based networks, and we use 36% fewer network parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08490</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08490</id><created>2019-04-17</created><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Li</keyname><forenames>Huiying</forenames></author><author><keyname>Nagels</keyname><forenames>Steven</forenames></author><author><keyname>Li</keyname><forenames>Zhijing</forenames></author><author><keyname>Lopes</keyname><forenames>Pedro</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author><author><keyname>Zheng</keyname><forenames>Haitao</forenames></author></authors><title>Understanding the Effectiveness of Ultrasonic Microphone Jammer</title><categories>cs.CR cs.HC eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works have explained the principle of using ultrasonic transmissions
to jam nearby microphones. These signals are inaudible to nearby users, but
leverage &quot;hardware nonlinearity&quot; to induce a jamming signal inside microphones
that disrupts voice recordings. This has great implications on audio privacy
protection. In this work, we gain a deeper understanding on the effectiveness
of ultrasonic jammer under practical scenarios, with the goal of disabling both
visible and hidden microphones in the surrounding area. We first experiment
with existing jammer designs (both commercial products and that proposed by
recent papers), and find that they all offer limited angular coverage, and can
only target microphones in a particular direction. We overcome this limitation
by building a circular transducer array as a wearable bracelet. It emits
ultrasonic signals simultaneously from many directions, targeting surrounding
microphones without needing to point at any. More importantly, as the bracelet
moves with the wearer, its motion increases jamming coverage and diminishes
blind spots (the fundamental problem facing any transducer array). We evaluate
the jammer bracelet under practical scenarios, confirming that it can
effectively disrupt visible and hidden microphones in the surrounding areas,
preventing recognition of recorded speech. We also identify limitations and
areas for improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08500</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08500</id><created>2019-04-01</created><authors><author><keyname>Wang</keyname><forenames>Jingfan</forenames></author><author><keyname>Tchapmi</keyname><forenames>Lyne P.</forenames></author><author><keyname>Ravikumara</keyname><forenames>Arvind P.</forenames></author><author><keyname>McGuire</keyname><forenames>Mike</forenames></author><author><keyname>Bell</keyname><forenames>Clay S.</forenames></author><author><keyname>Zimmerle</keyname><forenames>Daniel</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author><author><keyname>Brandt</keyname><forenames>Adam R.</forenames></author></authors><title>Machine Vision for Natural Gas Methane Emissions Detection Using an
  Infrared Camera</title><categories>cs.CV cs.LG eess.IV</categories><comments>This paper was submitted to Applied Energy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is crucial to reduce natural gas methane emissions, which can potentially
offset the climate benefits of replacing coal with gas. Optical gas imaging
(OGI) is a widely-used method to detect methane leaks, but is labor-intensive
and cannot provide leak detection results without operators' judgment. In this
paper, we develop a computer vision approach to OGI-based leak detection using
convolutional neural networks (CNN) trained on methane leak images to enable
automatic detection. First, we collect ~1 M frames of labeled video of methane
leaks from different leaking equipment for building CNN model, covering a wide
range of leak sizes (5.3-2051.6 gCH4/h) and imaging distances (4.6-15.6 m).
Second, we examine different background subtraction methods to extract the
methane plume in the foreground. Third, we then test three CNN model variants,
collectively called GasNet, to detect plumes in videos taken at other pieces of
leaking equipment. We assess the ability of GasNet to perform leak detection by
comparing it to a baseline method that uses optical-flow based change detection
algorithm. We explore the sensitivity of results to the CNN structure, with a
moderate-complexity variant performing best across distances. We find that the
detection accuracy can reach as high as 99%, the overall detection accuracy can
exceed 95% for a case across all leak sizes and imaging distances. Binary
detection accuracy exceeds 97% for large leaks (~710 gCH4/h) imaged closely
(~5-7 m). At closer imaging distances (~5-10 m), CNN-based models have greater
than 94% accuracy across all leak sizes. At farthest distances (~13-16 m),
performance degrades rapidly, but it can achieve above 95% accuracy to detect
large leaks (&gt;950 gCH4/h). The GasNet-based computer vision approach could be
deployed in OGI surveys to allow automatic vigilance of methane leak detection
with high detection accuracy in the real world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08511</identifier>
 <datestamp>2019-12-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08511</id><created>2019-04-17</created><authors><author><keyname>Lukens</keyname><forenames>Joseph M.</forenames></author><author><keyname>Lu</keyname><forenames>Hsuan-Hao</forenames></author><author><keyname>Qi</keyname><forenames>Bing</forenames></author><author><keyname>Lougovski</keyname><forenames>Pavel</forenames></author><author><keyname>Weiner</keyname><forenames>Andrew M.</forenames></author><author><keyname>Williams</keyname><forenames>Brian P.</forenames></author></authors><title>All-optical frequency processor for networking applications</title><categories>eess.SP physics.app-ph physics.optics</categories><comments>11 pages, 5 figures</comments><doi>10.1109/JLT.2019.2953363</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an electro-optic approach for transparent optical networking, in
which frequency channels are actively transformed into any desired mapping in a
wavelength-multiplexed environment. Based on electro-optic phase modulators and
Fourier-transform pulse shapers, our all-optical frequency processor (AFP) is
examined numerically for the specific operations of frequency channel hopping
and broadcasting, and found capable of implementing these transformations with
favorable component requirements. Extending our analysis via a
mutual-information--based metric for system optimization, we show how to
optimize transformation performance under limited resources in a classical
context, contrasting the results with those found using metrics motivated by
quantum information, such as fidelity and success probability. Given its
compatibility with on-chip implementation, as well as elimination of
optical-to-electrical conversion in frequency channel switching, the AFP looks
to offer valuable potential in silicon photonic network design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08519</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08519</id><created>2019-04-17</created><authors><author><keyname>Molev-Shteiman</keyname><forenames>Arkady</forenames></author><author><keyname>Qi</keyname><forenames>Xiao-Feng</forenames></author><author><keyname>Mailaender</keyname><forenames>Laurence</forenames></author><author><keyname>Prasad</keyname><forenames>Narayan</forenames></author><author><keyname>Hochwald</keyname><forenames>Bertrand</forenames></author></authors><title>New equivalent model of quantizer with noisy input and its application
  for ADC resolution determination in an uplink MIMO receiver</title><categories>cs.NI eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a quantizer input signal is the sum of the desired signal and input
white noise, the quantization error is a function of total input signal. Our
new equivalent model splits the quantization error into two components: a
non-linear distortion (NLD) that is a function of only the desired part of
input signal (without noise), and an equivalent out-put white noise. This
separation is important because these two terms affect MIMO system performance
differently. This paper introduces our model, and applies it to determine the
minimal Analog-to-Digital Converter (ADC) resolution necessary to operate a
conventional MIMO receiver with negligible performance degradation. We also
provide numerical simulations to confirm the theory. Broad ramifications of our
model are further demonstrated in two companion papers presenting
low-complexity suppression of the NLD arising from insufficient ADC resolution,
and a digital dithering that significantly reduces the MIMO transmitter
Digital-to-Analog Converters (DAC) resolution requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08534</identifier>
 <datestamp>2019-07-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08534</id><created>2019-04-17</created><updated>2019-07-25</updated><authors><author><keyname>Bertrand</keyname><forenames>Hadrien</forenames></author><author><keyname>Hashir</keyname><forenames>Mohammad</forenames></author><author><keyname>Cohen</keyname><forenames>Joseph Paul</forenames></author></authors><title>Do Lateral Views Help Automated Chest X-ray Predictions?</title><categories>cs.CV cs.LG eess.IV</categories><comments>3 pages and 1 figure. Under review as extended abstract at MIDL 2019
  [arXiv:1907.08612]</comments><report-no>MIDL/2019/ExtendedAbstract/ryeLXFe494</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most convolutional neural networks in chest radiology use only the frontal
posteroanterior (PA) view to make a prediction. However the lateral view is
known to help the diagnosis of certain diseases and conditions. The recently
released PadChest dataset contains paired PA and lateral views, allowing us to
study for which diseases and conditions the performance of a neural network
improves when provided a lateral x-ray view as opposed to a frontal
posteroanterior (PA) view. Using a simple DenseNet model, we find that using
the lateral view increases the AUC of 8 of the 56 labels in our data and
achieves the same performance as the PA view for 21 of the labels. We find that
using the PA and lateral views jointly doesn't trivially lead to an increase in
performance but suggest further investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08573</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08573</id><created>2019-04-17</created><authors><author><keyname>He</keyname><forenames>Jiaxi</forenames></author><author><keyname>Xing</keyname><forenames>Frank Z.</forenames></author><author><keyname>Yang</keyname><forenames>Ran</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author></authors><title>Fast Single Image Dehazing via Multilevel Wavelet Transform based
  Optimization</title><categories>cs.CV eess.IV</categories><comments>23 pages, 13 figures</comments><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of images captured in outdoor environments can be affected by
poor weather conditions such as fog, dust, and atmospheric scattering of other
particles. This problem can bring extra challenges to high-level computer
vision tasks like image segmentation and object detection. However, previous
studies on image dehazing suffer from a huge computational workload and
corruption of the original image, such as over-saturation and halos. In this
paper, we present a novel image dehazing approach based on the optical model
for haze images and regularized optimization. Specifically, we convert the
non-convex, bilinear problem concerning the unknown haze-free image and light
transmission distribution to a convex, linear optimization problem by
estimating the atmosphere light constant. Our method is further accelerated by
introducing a multilevel Haar wavelet transform. The optimization, instead, is
applied to the low frequency sub-band decomposition of the original image. This
dimension reduction significantly improves the processing speed of our method
and exhibits the potential for real-time applications. Experimental results
show that our approach outperforms state-of-the-art dehazing algorithms in
terms of both image reconstruction quality and computational efficiency. For
implementation details, source code can be publicly accessed via
http://github.com/JiaxiHe/Image-and-Video-Dehazing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08582</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08582</id><created>2019-04-17</created><authors><author><keyname>Fan</keyname><forenames>Rui</forenames></author><author><keyname>Bocus</keyname><forenames>Mohammud Junaid</forenames></author><author><keyname>Zhu</keyname><forenames>Yilong</forenames></author><author><keyname>Jiao</keyname><forenames>Jianhao</forenames></author><author><keyname>Wang</keyname><forenames>Li</forenames></author><author><keyname>Ma</keyname><forenames>Fulong</forenames></author><author><keyname>Cheng</keyname><forenames>Shanshan</forenames></author><author><keyname>Liu</keyname><forenames>Ming</forenames></author></authors><title>Road Crack Detection Using Deep Convolutional Neural Network and
  Adaptive Thresholding</title><categories>cs.CV cs.LG eess.IV</categories><comments>6 pages, 8 figures, 2019 IEEE Intelligent Vehicles Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crack is one of the most common road distresses which may pose road safety
hazards. Generally, crack detection is performed by either certified inspectors
or structural engineers. This task is, however, time-consuming, subjective and
labor-intensive. In this paper, we propose a novel road crack detection
algorithm based on deep learning and adaptive image segmentation. Firstly, a
deep convolutional neural network is trained to determine whether an image
contains cracks or not. The images containing cracks are then smoothed using
bilateral filtering, which greatly minimizes the number of noisy pixels.
Finally, we utilize an adaptive thresholding method to extract the cracks from
road surface. The experimental results illustrate that our network can classify
images with an accuracy of 99.92%, and the cracks can be successfully extracted
from the images using our proposed thresholding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08601</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08601</id><created>2019-04-18</created><authors><author><keyname>Chang</keyname><forenames>Julie</forenames></author><author><keyname>Wetzstein</keyname><forenames>Gordon</forenames></author></authors><title>Deep Optics for Monocular Depth Estimation and 3D Object Detection</title><categories>cs.CV eess.IV</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depth estimation and 3D object detection are critical for scene understanding
but remain challenging to perform with a single image due to the loss of 3D
information during image capture. Recent models using deep neural networks have
improved monocular depth estimation performance, but there is still difficulty
in predicting absolute depth and generalizing outside a standard dataset. Here
we introduce the paradigm of deep optics, i.e. end-to-end design of optics and
image processing, to the monocular depth estimation problem, using coded
defocus blur as an additional depth cue to be decoded by a neural network. We
evaluate several optical coding strategies along with an end-to-end
optimization scheme for depth estimation on three datasets, including NYU Depth
v2 and KITTI. We find an optimized freeform lens design yields the best
results, but chromatic aberration from a singlet lens offers significantly
improved performance as well. We build a physical prototype and validate that
chromatic aberrations improve depth estimation on real-world results. In
addition, we train object detection networks on the KITTI dataset and show that
the lens optimized for depth estimation also results in improved 3D object
detection performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08614</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08614</id><created>2019-04-18</created><authors><author><keyname>Nosrati</keyname><forenames>Hamed</forenames></author><author><keyname>Aboutanios</keyname><forenames>Elias</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author></authors><title>Multi-stage Antenna Selection for Adaptive Beamforming in MIMO Arrays</title><categories>eess.SP</categories><comments>Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing the number of transmit and receive elements in
multiple-input-multiple-output (MIMO) antenna arrays imposes a substantial
increase in hardware and computational costs. We mitigate this problem by
employing a reconfigurable MIMO array where large transmit and receive arrays
are multiplexed in a smaller set of k baseband signals. We consider four stages
for the MIMO array configuration and propose four different selection
strategies to offer dimensionality reduction in post-processing and achieve
hardware cost reduction in digital signal processing (DSP) and radio-frequency
(RF) stages. We define the problem as a determinant maximization and develop a
unified formulation to decouple the joint problem and select antennas/elements
in various stages in one integrated problem. We then analyze the performance of
the proposed selection approaches and prove that, in terms of the output SINR,
a joint transmit-receive selection method performs best followed by
matched-filter, hybrid and factored selection methods. The theoretical results
are validated numerically, demonstrating that all methods allow an excellent
trade-off between performance and cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08697</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08697</id><created>2019-04-18</created><authors><author><keyname>Kazaz</keyname><forenames>Tarik</forenames></author><author><keyname>Rajan</keyname><forenames>Raj Thilak</forenames></author><author><keyname>Janssen</keyname><forenames>Gerard J. M.</forenames></author><author><keyname>van der Veen</keyname><forenames>Alle-Jan</forenames></author></authors><title>Multiresolution time-of-arrival estimation from multiband radio channel
  measurements</title><categories>eess.SP cs.NI</categories><comments>5 pages, 5 figures, conference</comments><doi>10.1109/ICASSP.2019.8683601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving high resolution time-of-arrival (TOA) estimation in multipath
propagation scenarios from bandlimited observations of communication signals is
challenging because the multipath channel impulse response (CIR) is not
bandlimited. Modeling the CIR as a sparse sequence of Diracs, TOA estimation
becomes a problem of parametric spectral inference from observed bandlimited
signals. To increase resolution without arriving at unrealistic sampling rates,
we consider multiband sampling approach, and propose a practical multibranch
receiver for the acquisition. The resulting data model exhibits multiple shift
invariance structures, and we propose a corresponding multiresolution TOA
estimation algorithm based on the ESPRIT algorithm. The performance of the
algorithm is compared against the derived Cram\'er Rao Lower Bound, using
simulations with standardized ultra-wideband (UWB) channel models. We show that
the proposed approach provides high-resolution estimates while reducing
spectral occupancy and sampling costs compared to traditional UWB approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08704</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08704</id><created>2019-04-18</created><authors><author><keyname>Ruby</keyname><forenames>Rukhsana</forenames></author><author><keyname>Zhong</keyname><forenames>Shuxin</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Wu</keyname><forenames>Kaishun</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Enhanced Energy-Efficient Downlink Resource Allocation in Green
  Non-Orthogonal Multiple Access Systems</title><categories>cs.NI eess.SP</categories><comments>29 pages (Accepted)</comments><journal-ref>Elsevier Computer Communications 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite numerous advantages, non-orthogonal multiple access (NOMA) technique
can bring additional interference for the neighboring ultra-dense networks if
the power consumption of the system is not properly optimized. While targeting
on the green communication concept, in this paper, we propose an
energy-efficient downlink resource allocation scheme for a NOMA-equipped
cellular network. The objective of this work is to allocate subchannels and
power of the base station among the users so that the overall energy efficiency
is maximized. Since this problem is NP-hard, we attempt to find an elegant
solution with reasonable complexity that provides good performance for some
realistic applications. To this end, we decompose the problem into a subchannel
allocation subproblem followed by a power loading subproblem that allocates
power to each user's data stream on each of its allocated subchannels. We first
employ a many-to-many matching model under the assumption of uniform power
loading in order to obtain the solution of the first subproblem with reasonable
performance. Once the the subchannel-user mapping information is known from the
first solution, we propose a geometric programming (GP)-based power loading
scheme upon approximating the energy efficiency of the system by a ratio of two
posynomials. The techniques adopted for these subproblems better exploit the
available multi-user diversity compared to the techniques used in an earlier
work. Having observed the computational overhead of the GP-based power loading
scheme, we also propose a suboptimal computationally-efficient algorithm for
the power loading subproblem with a polynomial time complexity that provides
reasonably good performance. Extensive simulation has been conducted to verify
that our proposed solution schemes always outperform the existing work while
consuming much less power at the base station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08764</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08764</id><created>2019-04-16</created><authors><author><keyname>Sahlsten</keyname><forenames>Jaakko</forenames></author><author><keyname>Jaskari</keyname><forenames>Joel</forenames></author><author><keyname>Kivinen</keyname><forenames>Jyri</forenames></author><author><keyname>Turunen</keyname><forenames>Lauri</forenames></author><author><keyname>Jaanio</keyname><forenames>Esa</forenames></author><author><keyname>Hietala</keyname><forenames>Kustaa</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author></authors><title>Deep Learning Fundus Image Analysis for Diabetic Retinopathy and Macular
  Edema Grading</title><categories>eess.IV cs.CV cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diabetes is a globally prevalent disease that can cause visible microvascular
complications such as diabetic retinopathy and macular edema in the human eye
retina, the images of which are today used for manual disease screening. This
labor-intensive task could greatly benefit from automatic detection using deep
learning technique. Here we present a deep learning system that identifies
referable diabetic retinopathy comparably or better than presented in the
previous studies, although we use only a small fraction of images (&lt;1/4) in
training but are aided with higher image resolutions. We also provide novel
results for five different screening and clinical grading systems for diabetic
retinopathy and macular edema classification, including results for accurately
classifying images according to clinical five-grade diabetic retinopathy and
four-grade diabetic macular edema scales. These results suggest, that a deep
learning system could increase the cost-effectiveness of screening while
attaining higher than recommended performance, and that the system could be
applied in clinical examinations requiring finer grading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08775</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08775</id><created>2019-04-17</created><authors><author><keyname>Anand</keyname><forenames>Prashant</forenames></author><author><keyname>Singh</keyname><forenames>Ajeet Kumar</forenames></author><author><keyname>Srivastava</keyname><forenames>Siddharth</forenames></author><author><keyname>Lall</keyname><forenames>Brejesh</forenames></author></authors><title>Few Shot Speaker Recognition using Deep Neural Networks</title><categories>eess.AS cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent advances in deep learning are mostly driven by availability of
large amount of training data. However, availability of such data is not always
possible for specific tasks such as speaker recognition where collection of
large amount of data is not possible in practical scenarios. Therefore, in this
paper, we propose to identify speakers by learning from only a few training
examples. To achieve this, we use a deep neural network with prototypical loss
where the input to the network is a spectrogram. For output, we project the
class feature vectors into a common embedding space, followed by
classification. Further, we show the effectiveness of capsule net in a few shot
learning setting. To this end, we utilize an auto-encoder to learn generalized
feature embeddings from class-specific embeddings obtained from capsule
network. We provide exhaustive experiments on publicly available datasets and
competitive baselines, demonstrating the superiority and generalization ability
of the proposed few shot learning pipelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08779</identifier>
 <datestamp>2019-12-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08779</id><created>2019-04-18</created><updated>2019-12-03</updated><authors><author><keyname>Park</keyname><forenames>Daniel S.</forenames></author><author><keyname>Chan</keyname><forenames>William</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Chiu</keyname><forenames>Chung-Cheng</forenames></author><author><keyname>Zoph</keyname><forenames>Barret</forenames></author><author><keyname>Cubuk</keyname><forenames>Ekin D.</forenames></author><author><keyname>Le</keyname><forenames>Quoc V.</forenames></author></authors><title>SpecAugment: A Simple Data Augmentation Method for Automatic Speech
  Recognition</title><categories>eess.AS cs.CL cs.LG cs.SD stat.ML</categories><comments>5 pages, 3 figures, 6 tables; v3: references added</comments><journal-ref>Proc. Interspeech 2019, 2613-2617</journal-ref><doi>10.21437/Interspeech.2019-2680</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present SpecAugment, a simple data augmentation method for speech
recognition. SpecAugment is applied directly to the feature inputs of a neural
network (i.e., filter bank coefficients). The augmentation policy consists of
warping the features, masking blocks of frequency channels, and masking blocks
of time steps. We apply SpecAugment on Listen, Attend and Spell networks for
end-to-end speech recognition tasks. We achieve state-of-the-art performance on
the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.
On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language
model, and 5.8% WER with shallow fusion with a language model. This compares to
the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we
achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set
without the use of a language model, and 6.8%/14.1% with shallow fusion, which
compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08796</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08796</id><created>2019-04-05</created><authors><author><keyname>Reid</keyname><forenames>Julia E.</forenames></author><author><keyname>Eaton</keyname><forenames>Eric</forenames></author></authors><title>Artificial Intelligence for Pediatric Ophthalmology</title><categories>physics.med-ph cs.CV cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PURPOSE OF REVIEW: Despite the impressive results of recent artificial
intelligence (AI) applications to general ophthalmology, comparatively less
progress has been made toward solving problems in pediatric ophthalmology using
similar techniques. This article discusses the unique needs of pediatric
ophthalmology patients and how AI techniques can address these challenges,
surveys recent applications of AI to pediatric ophthalmology, and discusses
future directions in the field.
  RECENT FINDINGS: The most significant advances involve the automated
detection of retinopathy of prematurity (ROP), yielding results that rival
experts. Machine learning (ML) has also been successfully applied to the
classification of pediatric cataracts, prediction of post-operative
complications following cataract surgery, detection of strabismus and
refractive error, prediction of future high myopia, and diagnosis of reading
disability via eye tracking. In addition, ML techniques have been used for the
study of visual development, vessel segmentation in pediatric fundus images,
and ophthalmic image synthesis.
  SUMMARY: AI applications could significantly benefit clinical care for
pediatric ophthalmology patients by optimizing disease detection and grading,
broadening access to care, furthering scientific discovery, and improving
clinical efficiency. These methods need to match or surpass physician
performance in clinical trials before deployment with patients. Due to
widespread use of closed-access data sets and software implementations, it is
difficult to directly compare the performance of these approaches, and
reproducibility is poor. Open-access data sets and software implementations
could alleviate these issues, and encourage further AI applications to
pediatric ophthalmology.
  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,
deep learning
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08842</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08842</id><created>2019-04-18</created><authors><author><keyname>Yang</keyname><forenames>Ruihan</forenames></author><author><keyname>Chen</keyname><forenames>Tianyao</forenames></author><author><keyname>Zhang</keyname><forenames>Yiyi</forenames></author><author><keyname>Xia</keyname><forenames>Gus</forenames></author></authors><title>Inspecting and Interacting with Meaningful Music Representations using
  VAE</title><categories>cs.SD cs.HC cs.IR cs.LG eess.AS</categories><comments>Accepted for poster at the International Conference on New Interfaces
  for Musical Expression (NIME), June 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational Autoencoders(VAEs) have already achieved great results on image
generation and recently made promising progress on music generation. However,
the generation process is still quite difficult to control in the sense that
the learned latent representations lack meaningful music semantics. It would be
much more useful if people can modify certain music features, such as rhythm
and pitch contour, via latent representations to test different composition
ideas. In this paper, we propose a new method to inspect the pitch and rhythm
interpretations of the latent representations and we name it disentanglement by
augmentation. Based on the interpretable representations, an intuitive
graphical user interface is designed for users to better direct the music
creation process by manipulating the pitch contours and rhythmic complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08846</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08846</id><created>2019-04-12</created><authors><author><keyname>Wang</keyname><forenames>Jiasong</forenames></author><author><keyname>Yin</keyname><forenames>Changchuan</forenames></author></authors><title>A novel algorithm to get the Fourier power spectra of a real sequence</title><categories>eess.SP</categories><msc-class>42A38, 42B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a real sequence of length of m = nl, we may deduce its congruence
derivative sequence with length of l. The discrete Fourier transform of
original sequence can be calculated by the discrete Fourier transform of the
congruence derivative sequence. Based on the relation of discrete Fourier
transforms between the two sequences, the features of Fourier power spectra of
the integer and fractional periods for a real sequence have been investigated.
It has proved mathematically that after calculating the Fourier power spectrum
at an integer period, the Fourier power spectra of the fractional periods
associated this integer period can be easily represented by the computational
result of the Fourier power spectrum at the integer period for the sequence. A
computational experience using a protein sequence shows that some of the
computed results are a kind of Fourier power spectra corresponding to new
frequencies which can't be obtained from the traditional discrete Fourier
transform. Therefore, the algorithm would be a new realization method for
discrete Fourier transform of the real sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08848</identifier>
 <datestamp>2019-04-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08848</id><created>2019-04-16</created><authors><author><keyname>Ghiaus</keyname><forenames>Christian</forenames><affiliation>CETHIL</affiliation></author><author><keyname>Alzetto</keyname><forenames>Florent</forenames><affiliation>LPS</affiliation></author></authors><title>Design of experiments for Quick U-building method for building energy
  performance measurement</title><categories>eess.SP cs.SY</categories><proxy>ccsd</proxy><journal-ref>Journal of Building Performance Simulation, Taylor &amp; Francis,
  2019, 12 (4), pp.465-479</journal-ref><doi>10.1080/19401493.2018.1561753</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quick U-building (QUB) is a method for short time measurement of energy
performance of buildings, typically one night. It uses the indoor air
temperature response to power delivered to the indoor air by electric heaters.
This paper introduces a method for estimating the expected measurement error as
a function of the amplitude and the time duration of the input signal based on
the decomposition of the time response of a state-space model into a sum of
exponentials by using the eigenvalues of the state matrix. It is shown that the
buildings have a group of dominant time constants, which gives an exponential
response, and many very short and very large time constants, which have a small
influence on the response. The analysis of the eigenvalues demonstrates that
the QUB experiment may be done in a rather short time as compared with the
largest time constant of the building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08849</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08849</id><created>2019-04-17</created><authors><author><keyname>Spangenberg</keyname><forenames>Dirk-Mathys</forenames></author><author><keyname>Rohwer</keyname><forenames>Erich</forenames></author><author><keyname>Br&#xfc;gmann</keyname><forenames>Michael</forenames></author><author><keyname>Feurer</keyname><forenames>Thomas</forenames></author></authors><title>Extending time-domain ptychography to generalized phase-only transfer
  functions</title><categories>eess.SP eess.IV physics.optics</categories><comments>5 pages, 4 figures, submitted to Optica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the time-domain ptychographic iterative engine to generalized
spectral phase-only transfer functions. The modified algorithm, i$^2$PIE, is
described and its robustness is demonstrated by different numeric simulations.
The concept is experimentally verified by reconstruction of a complex
supercontinuum pulse from an all normal dispersion fiber.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08858</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08858</id><created>2019-04-18</created><authors><author><keyname>Valagiannopoulos</keyname><forenames>Constantinos</forenames></author><author><keyname>Tsiftsis</keyname><forenames>Theodoros A.</forenames></author><author><keyname>Kovanis</keyname><forenames>Vassilios</forenames></author></authors><title>Metasurface-Enabled Interference Suppression at Visible-Light
  Communications</title><categories>eess.SP physics.optics</categories><comments>8 pages, 6 figures</comments><doi>10.1088/2040-8986/ab4c08</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Light can be used for wireless information transmission apart from
illumination; that is the key idea behind visible-light communication (VLC),
one of the disruptive technologies of our days. It combines remarkably high
data rates due to ultrashort wavelengths with huge reliability and security due
to small distances; nevertheless, it substantially suffers from interference of
neighboring light sources in multiple-link configurations. In this manuscript,
we investigate a pair of light emitting diodes (LEDs) interfering each other
and propose a simple nanoslit metasurface that radically increases the
directivity of the transmitting beams. As a result, enhancement of the
signal-to-interference ratio by several orders of magnitude is reported. The
considered generic setup retains its beneficial features in the presence of
realistic design defects and, accordingly, may inspire standardization efforts
towards the adoption of VLC in next-generation heterogeneous communication
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08863</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08863</id><created>2019-04-18</created><authors><author><keyname>Fan</keyname><forenames>Rui</forenames></author><author><keyname>Yin</keyname><forenames>Tianzhixi</forenames></author></authors><title>Convolutional Neural Network and Transfer Learning for High Impedance
  Fault Detection</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter presents a novel high impedance fault (HIF) detection approach
using a convolutional neural network (CNN). Compared to traditional artificial
neural networks, a CNN offers translation invariance and it can accurately
detect HIFs in spite of variance and noise in the input data. A transfer
learning method is used to address the common challenge of a system with little
training data. Extensive studies have demonstrated the accuracy and
effectiveness of using a CNNbased approach for HIF detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.08971</identifier>
 <datestamp>2019-06-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.08971</id><created>2019-04-18</created><authors><author><keyname>Chhetri</keyname><forenames>Amit</forenames></author><author><keyname>Mansour</keyname><forenames>Mohamed</forenames></author><author><keyname>Kim</keyname><forenames>Wontak</forenames></author><author><keyname>Pan</keyname><forenames>Guangdong</forenames></author></authors><title>On Acoustic Modeling for Broadband Beamforming</title><categories>cs.SD cs.MM eess.AS</categories><comments>5 pages, conference</comments><msc-class>94A12, 94A40, 94A15</msc-class><acm-class>H.1.2; H.5.1</acm-class><journal-ref>European Signal Processing Conference (EUSIPCO 2019)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we describe limitations of the free-field propagation model for
designing broadband beamformers for microphone arrays on a rigid surface.
Towards this goal, we describe a general framework for quantifying the
microphone array performance in a general wave-field by directly solving the
acoustic wave equation. The model utilizes Finite-Element-Method (FEM) for
evaluating the response of the microphone array surface to background 3D planar
and spherical waves. The effectiveness of the framework is established by
designing and evaluating a representative broadband beamformer under realistic
acoustic conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09013</identifier>
 <datestamp>2019-04-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09013</id><created>2019-04-18</created><authors><author><keyname>Rouditchenko</keyname><forenames>Andrew</forenames></author><author><keyname>Zhao</keyname><forenames>Hang</forenames></author><author><keyname>Gan</keyname><forenames>Chuang</forenames></author><author><keyname>McDermott</keyname><forenames>Josh</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Self-Supervised Audio-Visual Co-Segmentation</title><categories>cs.CV cs.SD eess.AS eess.IV</categories><comments>Accepted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmenting objects in images and separating sound sources in audio are
challenging tasks, in part because traditional approaches require large amounts
of labeled data. In this paper we develop a neural network model for visual
object segmentation and sound source separation that learns from natural videos
through self-supervision. The model is an extension of recently proposed work
that maps image pixels to sounds. Here, we introduce a learning approach to
disentangle concepts in the neural networks, and assign semantic categories to
network feature channels to enable independent image segmentation and sound
source separation after audio-visual training on videos. Our evaluations show
that the disentangled model outperforms several baselines in semantic
segmentation and sound source separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09030</identifier>
 <datestamp>2019-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09030</id><created>2019-04-14</created><updated>2019-09-23</updated><authors><author><keyname>Bitar</keyname><forenames>Ahmad W.</forenames></author><author><keyname>Ovarlez</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Cheong</keyname><forenames>Loong-Fah</forenames></author><author><keyname>Chehab</keyname><forenames>Ali</forenames></author></authors><title>Automatic Target Detection for Sparse Hyperspectral Images</title><categories>eess.IV cs.CV</categories><comments>Accepted for publication in the book &quot;Hyperspectral Image Analysis -
  Advances in Signal Processing and Machine Learning&quot;. arXiv admin note: text
  overlap with arXiv:1711.08970, arXiv:1808.06490</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a novel target detector for hyperspectral imagery is developed.
The detector is independent on the unknown covariance matrix, behaves well in
large dimensions, distributional free, invariant to atmospheric effects, and
does not require a background dictionary to be constructed. Based on a
modification of the Robust Principal Component Analysis (RPCA), a given
hyperspectral image (HSI) is regarded as being made up of the sum of low-rank
background HSI and a sparse target HSI that contains the targets based on a
pre-learned target dictionary specified by the user. The sparse component is
directly used for the detection, that is, the targets are simply detected at
the non-zero entries of the sparse target HSI. Hence, a novel target detector
is developed, which is simply a sparse HSI generated automatically from the
original HSI, but containing only the targets with the background is
suppressed. The detector is evaluated on real experiments, and the results of
which demonstrate its effectiveness for hyperspectral target detection
especially when the targets are well matched to the surroundings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09038</identifier>
 <datestamp>2019-04-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09038</id><created>2019-04-18</created><authors><author><keyname>Ghorbani</keyname><forenames>Shahram</forenames></author><author><keyname>Hansen</keyname><forenames>John H. L.</forenames></author></authors><title>Leveraging native language information for improved accented speech
  recognition</title><categories>eess.AS</categories><comments>Accepted at Interspeech 2018</comments><doi>10.21437/Interspeech.2018-1378</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recognition of accented speech is a long-standing challenge for automatic
speech recognition (ASR) systems, given the increasing worldwide population of
bi-lingual speakers with English as their second language. If we consider
foreign-accented speech as an interpolation of the native language (L1) and
English (L2), using a model that can simultaneously address both languages
would perform better at the acoustic level for accented speech. In this study,
we explore how an end-to-end recurrent neural network (RNN) trained system with
English and native languages (Spanish and Indian languages) could leverage data
of native languages to improve performance for accented English speech. To this
end, we examine pre-training with native languages, as well as multi-task
learning (MTL) in which the main task is trained with native English and the
secondary task is trained with Spanish or Indian Languages. We show that the
proposed MTL model performs better than the pre-training approach and
outperforms a baseline model trained simply with English data. We suggest a new
setting for MTL in which the secondary task is trained with both English and
the native language, using the same output set. This proposed scenario yields
better performance with +11.95% and +17.55% character error rate gains over
baseline for Hispanic and Indian accents, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09049</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09049</id><created>2019-04-18</created><updated>2019-04-28</updated><authors><author><keyname>Subramanian</keyname><forenames>Aswin Shanmugam</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofei</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Taniguchi</keyname><forenames>Toru</forenames></author><author><keyname>Tran</keyname><forenames>Dung</forenames></author><author><keyname>Fujita</keyname><forenames>Yuya</forenames></author></authors><title>An Investigation of End-to-End Multichannel Speech Recognition for
  Reverberant and Mismatch Conditions</title><categories>eess.AS cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence-to-sequence (S2S) modeling is becoming a popular paradigm for
automatic speech recognition (ASR) because of its ability to jointly optimize
all the conventional ASR components in an end-to-end (E2E) fashion. This report
investigates the ability of E2E ASR from standard close-talk to far-field
applications by encompassing entire multichannel speech enhancement and ASR
components within the S2S model. There have been previous studies on jointly
optimizing neural beamforming alongside E2E ASR for denoising. It is clear from
both recent challenge outcomes and successful products that far-field systems
would be incomplete without solving both denoising and dereverberation
simultaneously. This report uses a recently developed architecture for
far-field ASR by composing neural extensions of dereverberation and beamforming
modules with the S2S ASR module as a single differentiable neural network and
also clearly defining the role of each subnetwork. The original implementation
of this architecture was successfully applied to the noisy speech recognition
task (CHiME-4), while we applied this implementation to noisy reverberant tasks
(DIRHA and REVERB). Our investigation shows that the method achieves better
performance than conventional pipeline methods on the DIRHA English dataset and
comparable performance on the REVERB dataset. It also has additional advantages
of being neither iterative nor requiring parallel noisy and clean speech data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09062</identifier>
 <datestamp>2019-04-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09062</id><created>2019-04-18</created><authors><author><keyname>Chen</keyname><forenames>Honglin</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author><author><keyname>Song</keyname><forenames>Alexander</forenames></author><author><keyname>Haberland</keyname><forenames>Matt</forenames></author><author><keyname>Akar</keyname><forenames>Osman</forenames></author><author><keyname>Dhillon</keyname><forenames>Adam</forenames></author><author><keyname>Zhou</keyname><forenames>Tiankuang</forenames></author><author><keyname>Bertozzi</keyname><forenames>Andrea L.</forenames></author><author><keyname>Brantingham</keyname><forenames>P. Jeffrey</forenames></author></authors><title>Semi-Supervised First-Person Activity Recognition in Body-Worn Video</title><categories>eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Body-worn cameras are now commonly used for logging daily life, sports, and
law enforcement activities, creating a large volume of archived footage. This
paper studies the problem of classifying frames of footage according to the
activity of the camera-wearer with an emphasis on application to real-world
police body-worn video. Real-world datasets pose a different set of challenges
from existing egocentric vision datasets: the amount of footage of different
activities is unbalanced, the data contains personally identifiable
information, and in practice it is difficult to provide substantial training
footage for a supervised approach. We address these challenges by extracting
features based exclusively on motion information then segmenting the video
footage using a semi-supervised classification algorithm. On publicly available
datasets, our method achieves results comparable to, if not better than,
supervised and/or deep learning methods using a fraction of the training data.
It also shows promising results on real-world police body-worn video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09115</identifier>
 <datestamp>2019-04-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09115</id><created>2019-04-19</created><authors><author><keyname>Hu</keyname><forenames>Di</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Wang</keyname><forenames>Qi</forenames></author></authors><title>Listen to the Image</title><categories>cs.CV cs.HC cs.MM cs.SD eess.AS</categories><comments>Accepted by CVPR2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual-to-auditory sensory substitution devices can assist the blind in
sensing the visual environment by translating the visual information into a
sound pattern. To improve the translation quality, the task performances of the
blind are usually employed to evaluate different encoding schemes. In contrast
to the toilsome human-based assessment, we argue that machine model can be also
developed for evaluation, and more efficient. To this end, we firstly propose
two distinct cross-modal perception model w.r.t. the late-blind and
congenitally-blind cases, which aim to generate concrete visual contents based
on the translated sound. To validate the functionality of proposed models, two
novel optimization strategies w.r.t. the primary encoding scheme are presented.
Further, we conduct sets of human-based experiments to evaluate and compare
them with the conducted machine-based assessments in the cross-modal generation
task. Their highly consistent results w.r.t. different encoding schemes
indicate that using machine model to accelerate optimization evaluation and
reduce experimental cost is feasible to some extent, which could dramatically
promote the upgrading of encoding scheme then help the blind to improve their
visual perception ability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09252</identifier>
 <datestamp>2019-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09252</id><created>2019-04-19</created><updated>2019-11-04</updated><authors><author><keyname>Song</keyname><forenames>Jinxiang</forenames></author><author><keyname>Peng</keyname><forenames>Bile</forenames></author><author><keyname>H&#xe4;ger</keyname><forenames>Christian</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author><author><keyname>Sahai</keyname><forenames>Anant</forenames></author></authors><title>Learning Physical-Layer Communication with Quantized Feedback</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-driven optimization of transmitters and receivers can reveal new
modulation and detection schemes and enable physical-layer communication over
unknown channels. Previous work has shown that practical implementations of
this approach require a feedback signal from the receiver to the transmitter.
In this paper, we study the impact of quantized feedback in data-driven
learning of physical-layer communication. A novel quantization method is
proposed, which exploits the specific properties of the feedback signal and is
suitable for non-stationary signal distributions. The method is evaluated for
linear and nonlinear channels. Simulation results show that feedback
quantization does not appreciably affect the learning process and can lead to
excellent performance, even with $1$-bit quantization. In addition, it is shown
that learning is surprisingly robust to noisy feedback where random bit flips
are applied to the quantization bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09257</identifier>
 <datestamp>2020-01-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09257</id><created>2019-04-19</created><updated>2020-01-17</updated><authors><author><keyname>Al-Aboosi</keyname><forenames>Yasin Yousif</forenames></author></authors><title>Image Denosing In Underwater Acoustic Noise Using Discrete Wavelet
  Transform With Different Noise Level Estimation</title><categories>eess.IV eess.SP</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In many applications, Image de-noising and improvement represent essential
processes in presence of colored noise such that in underwater. Power spectral
density of the noise is changeable within a definite frequenc range, and
autocorrelation noise function is does not like delta function. So, noise in
underwater is characterized as colored noise. In this paper, a novel image
de-noising method is proposed using multi-level noise power estimation in
discrete wavelet transform with different basis functions. Peak signal to noise
ratio (PSNR) and mean squared error represented performance measures that the
results of this studay depend on it. The results of various bases of wavelet
such as: Debauchies (db), biorthogonal (bior.) and symlet (sym.), show that
denoising process that uses in this method produces extra prominent images and
improved values of PSNR than other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09312</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09312</id><created>2019-04-19</created><authors><author><keyname>Molev-Shteiman</keyname><forenames>Arkady</forenames></author><author><keyname>Qi</keyname><forenames>Xiao-Feng</forenames></author><author><keyname>Mailaender</keyname><forenames>Laurence</forenames></author></authors><title>Low Resolution Digital-to-Analog Converter with Digital Dithering for
  MIMO Transmitter</title><categories>eess.SP cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on an equivalent model for quantizers with noisy inputs recently
presented in [35], we propose a method of digital dithering at the transmitter
that may significantly reduce the resolution requirements of MIMO downlink
Digital to Analog Convertors (DAC). We use this equivalent model to analyze the
effect of the dither Probability Density Function (PFD), and show that the
uniform PDF produces an optimal (linear) result. Relative to other methods of
DAC quantization error reduction our approach has the benefits of low
computational complexity, compatibility with all existing standards, and
blindness (no need for channel state information).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09316</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09316</id><created>2019-04-19</created><authors><author><keyname>Molev-Shteiman</keyname><forenames>Arkady</forenames></author><author><keyname>Qi</keyname><forenames>Xiao-Feng</forenames></author><author><keyname>Mailaender</keyname><forenames>Laurence</forenames></author></authors><title>A Low Complexity Near-Maximum Likelihood MIMO Receiver with Low
  Resolution Analog-to-Digital Converters</title><categories>eess.SP cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a new equivalent model of quantizer with noisy input recently
presented in [23], we propose a new low complexity receiver that takes into
account the nonlinear distortion (NLD) generated by Analog to Digital converter
(ADC) with insufficient resolution. The strength of new model is that it
presents the NLD as a function of only the desired part of input signal
(without noise). Therefore it can easily be used in a variety of NLD mitigation
techniques. Here, as an illustration of this, we use a pseudo-ML approach to
detect the original QAM modulation based on the equivalent transfer function
and exhaustive search. Simulation results for a single user QAM under flat
fading show performance equivalent to a true ML receiver, but with much lower
computational complexity. The excellent performance of our receiver is an
independent validation of the model [23].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09346</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09346</id><created>2019-04-19</created><authors><author><keyname>Balevi</keyname><forenames>Eren</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Deep Learning-Based Channel Estimation for High-Dimensional Signals</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel deep learning-based channel estimation technique for
high-dimensional communication signals that does not require any training. Our
method is broadly applicable to channel estimation for multicarrier signals
with any number of antennas, and has low enough complexity to be used in a
mobile station. The proposed deep channel estimator can outperform LS
estimation with nearly the same complexity, and approach MMSE estimation
performance to within 1 dB without knowing the second order statistics. The
only complexity increase with respect to LS estimator lies in fitting the
parameters of a deep neural network (DNN) periodically on the order of the
channel coherence time. We empirically show that the main benefit of this
method accrues from the ability of this specially designed DNN to exploit
correlations in the time-frequency grid. The proposed estimator can also reduce
the number of pilot tones needed in an OFDM time-frequency grid, e.g. in an LTE
scenario by 98% (68%) when the channel coherence time interval is 73ms (4.5ms).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09360</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09360</id><created>2019-04-19</created><authors><author><keyname>Newman</keyname><forenames>Jennifer</forenames></author><author><keyname>Lin</keyname><forenames>Li</forenames></author><author><keyname>Chen</keyname><forenames>Wenhao</forenames></author><author><keyname>Reinders</keyname><forenames>Stephanie</forenames></author><author><keyname>Wang</keyname><forenames>Yangxiao</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author><author><keyname>Guan</keyname><forenames>Yong</forenames></author></authors><title>StegoAppDB: a Steganography Apps Forensics Image Database</title><categories>eess.IV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new reference dataset simulating digital evidence
for image steganography. Steganography detection is a digital image forensic
topic that is relatively unknown in practical forensics, although stego app use
in the wild is on the rise. This paper introduces the first database consisting
of mobile phone photographs and stego images produced from mobile stego apps,
including a rich set of side information, offering simulated digital evidence.
StegoAppDB, a steganography apps forensics image database, contains over
810,000 innocent and stego images using a minimum of 10 different phone models
from 24 distinct devices, with detailed provenanced data comprising a wide
range of ISO and exposure settings, EXIF data, message information, embedding
rates, etc. We develop a camera app, Cameraw, specifically for data
acquisition, with multiple images per scene, saving simultaneously in both DNG
and high-quality JPEG formats. Stego images are created from these original
images using selected mobile stego apps through a careful process of reverse
engineering. StegoAppDB contains cover-stego image pairs including for apps
that resize the stego dimensions. We retainthe original devices and continue to
enlarge the database, and encourage the image forensics community to use
StegoAppDB. While designed for steganography, we discuss uses of this publicly
available database to other digital image forensic topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09376</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09376</id><created>2019-04-19</created><authors><author><keyname>Wu</keyname><forenames>Ting</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>Knox</keyname><forenames>Michael E.</forenames></author><author><keyname>Shahrjerdi</keyname><forenames>Davood</forenames></author></authors><title>A Wideband Sliding Correlator-Based Channel Sounder with Synchronization
  in 65 nm CMOS</title><categories>eess.SP</categories><comments>5 pages, 7 figures, IEEE International Symposium on Circuits and
  Systems (ISCAS) 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A programmable ultra-wideband sliding correlator-based channel sounder with
high temporal and spatial resolution is designed in standard 65 nm CMOS. The
baseband chip can be configured either as a baseband transmitter to generate a
pseudorandom spread spectrum signal with flexible sequence lengths, or as a
baseband receiver with sliding correlator having an absolute timing reference
to obtain power delay profiles of the multipath components of the wireless
channel. The sequence achieved a chip rate of one Giga-bit-per-second,
resulting in a multipath delay resolution of 1 ns. The baseband chip occupies
an area of 0.66 mm x 1 mm with a power dissipation of 6 mA at 1.1 V in 65 nm
CMOS. The sliding correlator-based channel sounder in this work is a critical
block for future low-cost, miniaturized channel sounding systems used in
accurate and efficient channel propagation measurements at millimeter-wave
frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09377</identifier>
 <datestamp>2019-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09377</id><created>2019-04-19</created><updated>2019-09-26</updated><authors><author><keyname>Muniraju</keyname><forenames>Gowtham</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Analysis and Design of Robust Max Consensus for Wireless Sensor Networks</title><categories>eess.SP cs.SY math.OC</categories><comments>Accepted by IEEE Transactions on Signal and Information Processing
  over Networks</comments><journal-ref>IEEE Transactions on Signal and Information Processing over
  Networks, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel distributed algorithm for estimating the maximum of the node initial
state values in a network, in the presence of additive communication noise is
proposed. Conventionally, the maximum is estimated locally at each node by
updating the node state value with the largest received measurements in every
iteration. However, due to the additive channel noise, the estimate of the
maximum at each node drifts at each iteration and this results in nodes
diverging from the true max value. Max-plus algebra is used as a tool to study
this ergodic process. The subadditive ergodic theorem is invoked to establish a
constant growth rate for the state values due to noise, which is studied by
analyzing the max-plus Lyapunov exponent of the product of noise matrices in a
max-plus semiring. The growth rate of the state values is upper bounded by a
constant which depends on the spectral radius of the network and the noise
variance. Upper and lower bounds are derived for both fixed and random graphs.
Finally, a two-run algorithm robust to additive noise in the network is
proposed and its variance is analyzed using concentration inequalities.
Simulation results supporting the theory are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09390</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09390</id><created>2019-04-19</created><updated>2019-04-23</updated><authors><author><keyname>Kim</keyname><forenames>Tae Hyung</forenames></author><author><keyname>Garg</keyname><forenames>Pratyush</forenames></author><author><keyname>Haldar</keyname><forenames>Justin P.</forenames></author></authors><title>LORAKI: Autocalibrated Recurrent Neural Networks for Autoregressive MRI
  Reconstruction in k-Space</title><categories>eess.IV cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and evaluate a new MRI reconstruction method named LORAKI that
trains an autocalibrated scan-specific recurrent neural network (RNN) to
recover missing k-space data. Methods like GRAPPA, SPIRiT, and AC-LORAKS assume
that k-space data has shift-invariant autoregressive structure, and that the
scan-specific autoregression relationships needed to recover missing samples
can be learned from fully-sampled autocalibration (ACS) data. Recently, the
structure of the linear GRAPPA method has been translated into a nonlinear deep
learning method named RAKI. RAKI uses ACS data to train an artificial neural
network to interpolate missing k-space samples, and often outperforms GRAPPA.
In this work, we apply a similar principle to translate the linear AC-LORAKS
method (simultaneously incorporating support, phase, and parallel imaging
constraints) into a nonlinear deep learning method named LORAKI. Since
AC-LORAKS is iterative and convolutional, LORAKI takes the form of a
convolutional RNN. This new architecture admits a wide range of sampling
patterns, and even calibrationless patterns are possible if synthetic ACS data
is generated. The performance of LORAKI was evaluated with retrospectively
undersampled brain datasets, with comparisons against other related
reconstruction methods. Results suggest that LORAKI can provide improved
reconstruction compared to other scan-specific autocalibrated reconstruction
methods like GRAPPA, RAKI, and AC-LORAKS. LORAKI offers a new deep-learning
approach to MRI reconstruction based on RNNs in k-space, and enables improved
image quality and enhanced sampling flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09425</identifier>
 <datestamp>2019-06-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09425</id><created>2019-04-20</created><updated>2019-06-26</updated><authors><author><keyname>Chen</keyname><forenames>Shichuan</forenames></author><author><keyname>Zheng</keyname><forenames>Shilian</forenames></author><author><keyname>Yang</keyname><forenames>Lifeng</forenames></author><author><keyname>Yang</keyname><forenames>Xiaoniu</forenames></author></authors><title>Deep Learning for Large-Scale Real-World ACARS and ADS-B Radio Signal
  Classification</title><categories>cs.LG eess.SP</categories><comments>Accepted by IEEE Access</comments><doi>10.1109/ACCESS.2019.2925569</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio signal classification has a very wide range of applications in the
field of wireless communications and electromagnetic spectrum management. In
recent years, deep learning has been used to solve the problem of radio signal
classification and has achieved good results. However, the radio signal data
currently used is very limited in scale. In order to verify the performance of
the deep learning-based radio signal classification on real-world radio signal
data, in this paper we conduct experiments on large-scale real-world ACARS and
ADS-B signal data with sample sizes of 900,000 and 13,000,000, respectively,
and with categories of 3,143 and 5,157 respectively. We use the same
Inception-Residual neural network model structure for ACARS signal
classification and ADS-B signal classification to verify the ability of a
single basic deep neural network model structure to process different types of
radio signals, i.e., communication bursts in ACARS and pulse bursts in ADS-B.
We build an experimental system for radio signal deep learning experiments.
Experimental results show that the signal classification accuracy of ACARS and
ADS-B is 98.1% and 96.3%, respectively. When the signal-to-noise ratio (with
injected additive white Gaussian noise) is greater than 9 dB, the
classification accuracy is greater than 92%. These experimental results
validate the ability of deep learning to classify large-scale real-world radio
signals. The results of the transfer learning experiment show that the model
trained on large-scale ADS-B datasets is more conducive to the learning and
training of new tasks than the model trained on small-scale datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09525</identifier>
 <datestamp>2019-08-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09525</id><created>2019-04-20</created><updated>2019-08-08</updated><authors><author><keyname>Su</keyname><forenames>Pei-Chun</forenames></author><author><keyname>Miller</keyname><forenames>Stephen</forenames></author><author><keyname>Idriss</keyname><forenames>Salim</forenames></author><author><keyname>Barker</keyname><forenames>Piers</forenames></author><author><keyname>Wu</keyname><forenames>Hau-Tieng</forenames></author></authors><title>Recovery of the fetal electrocardiogram for morphological analysis from
  two trans-abdominal channels via optimal shrinkage</title><categories>eess.SP stat.AP</categories><comments>25 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel algorithm to recover fetal electrocardiogram (ECG) for
both the fetal heart rate analysis and morphological analysis of its waveform
from two or three trans-abdominal maternal ECG channels. We design an algorithm
based on the optimal-shrinkage and the nonlocal Euclidean median under the
wave-shape manifold model. For the fetal heart rate analysis, the algorithm is
evaluated on publicly available database, 2013 PhyioNet/Computing in Cardiology
Challenge, set A. For the morphological analysis, we propose to simulate
semi-real databases by mixing the MIT-BIH Normal Sinus Rhythm Database and
MITDB Arrhythmia Database. For the fetal R peak detection, the proposed
algorithm outperforms all algorithms under comparison. For the morphological
analysis, the algorithm provides an encouraging result in recovery of the fetal
ECG waveform, including PR, QT and ST intervals, even when the fetus has
arrhythmia. To the best of our knowledge, this is the first work focusing on
recovering the fetal ECG for morphological analysis from two or three channels
with an algorithm potentially applicable for continuous fetal
electrocardiographic monitoring, which creates the potential for long term
monitoring purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09533</identifier>
 <datestamp>2019-07-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09533</id><created>2019-04-20</created><updated>2019-04-27</updated><authors><author><keyname>Mishra</keyname><forenames>Saumitra</forenames></author><author><keyname>Stoller</keyname><forenames>Daniel</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author><author><keyname>Sturm</keyname><forenames>Bob L.</forenames></author><author><keyname>Dixon</keyname><forenames>Simon</forenames></author></authors><title>GAN-based Generation and Automatic Selection of Explanations for Neural
  Networks</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>8 pages plus references and appendix. Accepted at the ICLR 2019
  Workshop &quot;Safe Machine Learning: Specification, Robustness and Assurance&quot;.
  Camera-ready version. v2: Corrected page header</comments><journal-ref>SafeML Workshop at the International Conference on Learning
  Representations (ICLR) 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One way to interpret trained deep neural networks (DNNs) is by inspecting
characteristics that neurons in the model respond to, such as by iteratively
optimising the model input (e.g., an image) to maximally activate specific
neurons. However, this requires a careful selection of hyper-parameters to
generate interpretable examples for each neuron of interest, and current
methods rely on a manual, qualitative evaluation of each setting, which is
prohibitively slow. We introduce a new metric that uses Fr\'echet Inception
Distance (FID) to encourage similarity between model activations for real and
generated data. This provides an efficient way to evaluate a set of generated
examples for each setting of hyper-parameters. We also propose a novel
GAN-based method for generating explanations that enables an efficient search
through the input space and imposes a strong prior favouring realistic outputs.
We apply our approach to a classification model trained to predict whether a
music audio recording contains singing voice. Our results suggest that this
proposed metric successfully selects hyper-parameters leading to interpretable
examples, avoiding the need for manual evaluation. Moreover, we see that
examples synthesised to maximise or minimise the predicted probability of
singing voice presence exhibit vocal or non-vocal characteristics,
respectively, suggesting that our approach is able to generate suitable
explanations for understanding concepts learned by a neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09539</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09539</id><created>2019-04-21</created><authors><author><keyname>Rahmati</keyname><forenames>Mehdi</forenames></author><author><keyname>Petroccia</keyname><forenames>Roberto</forenames></author><author><keyname>Pompili</keyname><forenames>Dario</forenames></author></authors><title>In-network Collaboration for CDMA-based Reliable Underwater Acoustic
  Communications</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving high throughput and reliability in underwater acoustic networks for
transmitting distributed and large volume of data is a challenging task due to
the bandwidth-limited and unpredictable nature of the acoustic channel. In a
multi-node network, such as in the Internet of Underwater Things (IoUT),
communication link efficiency varies dynamically: if the channel is not in good
condition, e.g., when in deep fade, channel coding techniques may fail to
deliver the information even with multiple retransmissions. Hence, an efficient
and agile collaborative strategy is required to allocate appropriate resources
to the communication links based on their status. The proposed solution adjusts
the physical and link-layer parameters collaboratively for a Code Division
Multiple Access (CDMA)-based underwater network. An adaptive Hybrid Automatic
Repeat Request (HARQ) solution is employed to guarantee reliable communications
against errors in poor links. Results were validated using data collected from
the LOON testbed-hosted at the NATO STO Centre for Maritime Research and
Experimentation (CMRE) in La Spezia, Italy-and from the REP18-Atlantic sea
trial conducted in Sept'18 in Portuguese water.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09554</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09554</id><created>2019-04-21</created><authors><author><keyname>Zhang</keyname><forenames>Yawen</forenames></author><author><keyname>Wang</keyname><forenames>Runsheng</forenames></author><author><keyname>Zhang</keyname><forenames>Xinyue</forenames></author><author><keyname>Zhang</keyname><forenames>Zherui</forenames></author><author><keyname>Song</keyname><forenames>Jiahao</forenames></author><author><keyname>Zhang</keyname><forenames>Zuodong</forenames></author><author><keyname>Wang</keyname><forenames>Yuan</forenames></author><author><keyname>Huang</keyname><forenames>Ru</forenames></author></authors><title>A Parallel Bitstream Generator for Stochastic Computing</title><categories>cs.ET cs.AR eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic computing (SC) presents high error tolerance and low hardware
cost, and has great potential in applications such as neural networks and image
processing. However, the bitstream generator, which converts a binary number to
bitstreams, occupies a large area and energy consumption, thus weakening the
superiority of SC. In this paper, we propose a novel technique for generating
bitstreams in parallel, which needs only one clock for conversion and
significantly reduces the hardware cost. Synthesis results demonstrate that the
proposed parallel bitstream generator improves 2.5x area and 712x energy
consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09559</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09559</id><created>2019-04-21</created><authors><author><keyname>Yin</keyname><forenames>Feng</forenames><affiliation>Tom</affiliation></author><author><keyname>Pan</keyname><forenames>Lishuo</forenames><affiliation>Tom</affiliation></author><author><keyname>He</keyname><forenames>Xinwei</forenames><affiliation>Tom</affiliation></author><author><keyname>Chen</keyname><forenames>Tianshi</forenames><affiliation>Tom</affiliation></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames><affiliation>Tom</affiliation></author><author><keyname>Zhi-Quan</keyname><affiliation>Tom</affiliation></author><author><keyname>Luo</keyname></author></authors><title>Linear Multiple Low-Rank Kernel Based Stationary Gaussian Processes
  Regression for Time Series</title><categories>cs.LG eess.SP stat.ML</categories><comments>15 pages, 5 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GP) for machine learning have been studied systematically
over the past two decades and they are by now widely used in a number of
diverse applications. However, GP kernel design and the associated
hyper-parameter optimization are still hard and to a large extend open
problems. In this paper, we consider the task of GP regression for time series
modeling and analysis. The underlying stationary kernel can be approximated
arbitrarily close by a new proposed grid spectral mixture (GSM) kernel, which
turns out to be a linear combination of low-rank sub-kernels. In the case where
a large number of the sub-kernels are used, either the Nystr\&quot;{o}m or the
random Fourier feature approximations can be adopted to deal efficiently with
the computational demands. The unknown GP hyper-parameters consist of the
non-negative weights of all sub-kernels as well as the noise variance; their
estimation is performed via the maximum-likelihood (ML) estimation framework.
Two efficient numerical optimization methods for solving the unknown
hyper-parameters are derived, including a sequential majorization-minimization
(MM) method and a non-linearly constrained alternating direction of multiplier
method (ADMM). The MM matches perfectly with the proven low-rank property of
the proposed GSM sub-kernels and turns out to be a part of efficiency, stable,
and efficient solver, while the ADMM has the potential to generate better local
minimum in terms of the test MSE. Experimental results, based on various
classic time series data sets, corroborate that the proposed GSM kernel-based
GP regression model outperforms several salient competitors of similar kind in
terms of prediction mean-squared-error and numerical stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09613</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09613</id><created>2019-04-21</created><authors><author><keyname>Yang</keyname><forenames>Duotong</forenames></author><author><keyname>Chou</keyname><forenames>Hung-Ming</forenames></author><author><keyname>Thomas</keyname><forenames>Kyle</forenames></author><author><keyname>Kynev</keyname><forenames>Sergey</forenames></author><author><keyname>Rye</keyname><forenames>Rebecca</forenames></author></authors><title>STATCOM Performance Evaluation Using Operation Data from Digital Fault
  Recorder</title><categories>eess.SP</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static Synchronous Compensators (STATCOMs) are being employed by Dominion
Energy to control voltage and enhance system stability. Due to the complexity
of the control systems, operational modes, and nonlinearities, it is essential
to evaluate STATCOMs' behavior to ensure their correct and proper response to
dynamic events such as line faults, generator trip, or load rejection
throughout the grid. This procedure brings benefit to device management by
identifying potential equipment problems and improve dynamic model for
simulation study. The proposed framework utilizes operation data collected in
Digital Fault Recorder (DFR) to evaluate STATCOM's response to dynamic system
events. One of the challenges on getting accurate model response comes from
STATCOMs with automatic gain adjustment feature. This feature actively measures
the external system's Thevanin impedance and accordingly changes the STATCOM
gain. Therefore, when trying to recreate the field measurements in the
simulation environment, it is necessary to match the short circuit level (SCL)
of the external system equivalent. This paper presents how these issues have
been solved in order to evaluate STATCOM's performance. Finally, the
effectiveness of proposed performance evaluation method is validated based on
two actual events caused by faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09651</identifier>
 <datestamp>2020-01-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09651</id><created>2019-04-21</created><updated>2019-12-30</updated><authors><author><keyname>Gupta</keyname><forenames>Ujjwal</forenames></author><author><keyname>Bansal</keyname><forenames>Hritik</forenames></author><author><keyname>Joshi</keyname><forenames>Deepak</forenames></author></authors><title>An improved sex specific and age dependent classification model for
  Parkinson's diagnosis using handwriting measurement</title><categories>cs.LG eess.SP q-bio.NC stat.ML</categories><comments>Journal of Computer Methods and Programs in Biomedicine(Accepted on
  27 December 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate diagnosis is crucial for preventing the progression of Parkinson's,
as well as improving the quality of life with individuals with Parkinson's
disease. In this paper, we develop a sex-specific and age-dependent
classification method to diagnose the Parkinson's disease using the online
handwriting recorded from individuals with
Parkinson's(n=37;m/f-19/18;age-69.3+-10.9years) and healthy
controls(n=38;m/f-20/18;age-62.4+-11.3 years).The sex specific and age
dependent classifier was observed significantly outperforming the generalized
classifier. An improved accuracy of 83.75%(SD+1.63) with female specific
classifier, and 79.55%(SD=1.58) with old age dependent classifier was observed
in comparison to 75.76%(SD=1.17) accuracy with the generalized classifier.
Finally, combining the age and sex information proved to be encouraging in
classification. We performed a rigorous analysis to observe the dominance of
sex specific and age dependent features for Parkinson's detection and ranked
them using the support vector machine(SVM) ranking method. Distinct set of
features were observed to be dominating for higher classification accuracy in
different category of classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09673</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09673</id><created>2019-04-21</created><authors><author><keyname>Huang</keyname><forenames>Hongji</forenames></author><author><keyname>Guo</keyname><forenames>Song</forenames></author><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Yang</keyname><forenames>Zhen</forenames></author><author><keyname>Zhang</keyname><forenames>Jianhua</forenames></author><author><keyname>Sari</keyname><forenames>Hikmet</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Deep Learning for Physical-Layer 5G Wireless Techniques: Opportunities,
  Challenges and Solutions</title><categories>eess.SP cs.AI</categories><comments>Submitted a possible publication to IEEE Wireless Communications
  Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new demands for high-reliability and ultra-high capacity wireless
communication have led to extensive research into 5G communications. However,
the current communication systems, which were designed on the basis of
conventional communication theories, signficantly restrict further performance
improvements and lead to severe limitations. Recently, the emerging deep
learning techniques have been recognized as a promising tool for handling the
complicated communication systems, and their potential for optimizing wireless
communications has been demonstrated. In this article, we first review the
development of deep learning solutions for 5G communication, and then propose
efficient schemes for deep learning-based 5G scenarios. Specifically, the key
ideas for several important deep learningbased communication methods are
presented along with the research opportunities and challenges. In particular,
novel communication frameworks of non-orthogonal multiple access (NOMA),
massive multiple-input multiple-output (MIMO), and millimeter wave (mmWave) are
investigated, and their superior performances are demonstrated. We vision that
the appealing deep learning-based wireless physical layer frameworks will bring
a new direction in communication theories and that this work will move us
forward along this road.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09715</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09715</id><created>2019-04-21</created><authors><author><keyname>Simoni</keyname><forenames>Renato</forenames></author><author><keyname>Mateos-N&#xfa;&#xf1;ez</keyname><forenames>David</forenames></author><author><keyname>Gonz&#xe1;lez-Huici</keyname><forenames>Mar&#xed;a A.</forenames></author><author><keyname>Correas-Serrano</keyname><forenames>Aitor</forenames></author></authors><title>Height estimation for automotive MIMO radar with group-sparse
  reconstruction</title><categories>eess.SP</categories><comments>12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method is developed for sequential azimuth and height estimation of small
objects at far distances in front of a moving vehicle using coherent or
mutually incoherent MIMO arrays. The model considers phases and amplitudes for
near-field multipath signals produced by specular non-diffusive
ground-reflections where the reflection phase shift and power attenuation due
to the interaction with the ground is assumed unknown. Group-sparsity allows
combining measurements along the trajectory of the vehicle provided that the
road is flat as well as measurements from multiple incoherent sensors at
different locations in the vehicle. It is shown in simulations that the
proposed approach significantly increases estimation accuracy and decreases
false alarms, both crucial for the detection of small objects at far distances.
This model is suitable for non-uniform sparse arrays and can be used for height
estimation using efficient methods such as block orthogonal matching pursuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09757</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09757</id><created>2019-04-22</created><authors><author><keyname>Liu</keyname><forenames>Haojie</forenames></author><author><keyname>Chen</keyname><forenames>Tong</forenames></author><author><keyname>Guo</keyname><forenames>Peiyao</forenames></author><author><keyname>Shen</keyname><forenames>Qiu</forenames></author><author><keyname>Cao</keyname><forenames>Xun</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Ma</keyname><forenames>Zhan</forenames></author></authors><title>Non-local Attention Optimized Deep Image Compression</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel Non-Local Attention Optimized Deep Image
Compression (NLAIC) framework, which is built on top of the popular variational
auto-encoder (VAE) structure. Our NLAIC framework embeds non-local operations
in the encoders and decoders for both image and latent feature probability
information (known as hyperprior) to capture both local and global
correlations, and apply attention mechanism to generate masks that are used to
weigh the features for the image and hyperprior, which implicitly adapt bit
allocation for different features based on their importance. Furthermore, both
hyperpriors and spatial-channel neighbors of the latent features are used to
improve entropy coding. The proposed model outperforms the existing methods on
Kodak dataset, including learned (e.g., Balle2019, Balle2018) and conventional
(e.g., BPG, JPEG2000, JPEG) image compression methods, for both PSNR and
MS-SSIM distortion metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09765</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09765</id><created>2019-04-22</created><authors><author><keyname>Rengaswamy</keyname><forenames>Pradeep</forenames></author><author><keyname>M</keyname><forenames>Gurunath Reddy</forenames></author><author><keyname>Rao</keyname><forenames>Krothapalli Sreenivasa</forenames></author></authors><title>hf0: A hybrid pitch extraction method for multimodal voice</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Pitch Extraction, F0 extraction, harmonic signals, speech, monophonic
  songs, Convolutional Neural Network, 5 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Pitch or fundamental frequency (f0) extraction is a fundamental problem
studied extensively for its potential applications in speech and clinical
applications. In literature, explicit mode specific (modal speech or singing
voice or emotional/ expressive speech or noisy speech) signal processing and
deep learning f0 extraction methods that exploit the quasi periodic nature of
the signal in time, harmonic property in spectral or combined form to extract
the pitch is developed. Hence, there is no single unified method which can
reliably extract the pitch from various modes of the acoustic signal. In this
work, we propose a hybrid f0 extraction method which seamlessly extracts the
pitch across modes of speech production with very high accuracy required for
many applications. The proposed hybrid model exploits the advantages of deep
learning and signal processing methods to minimize the pitch detection error
and adopts to various modes of acoustic signal. Specifically, we propose an
ordinal regression convolutional neural networks to map the periodicity rich
input representation to obtain the nominal pitch classes which drastically
reduces the number of classes required for pitch detection unlike other deep
learning approaches. Further, the accurate f0 is estimated from the nominal
pitch class labels by filtering and autocorrelation. We show that the proposed
method generalizes to the unseen modes of voice production and various noises
for large scale datasets. Also, the proposed hybrid model significantly reduces
the learning parameters required to train the deep model compared to other
methods. Furthermore,the evaluation measures showed that the proposed method is
significantly better than the state-of-the-art signal processing and deep
learning approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09807</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09807</id><created>2019-04-22</created><authors><author><keyname>H&#xe4;ger</keyname><forenames>Christian</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author><author><keyname>B&#xfc;tler</keyname><forenames>Rick M.</forenames></author><author><keyname>Liga</keyname><forenames>Gabriele</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author></authors><title>Revisiting Multi-Step Nonlinearity Compensation with Machine Learning</title><categories>eess.SP cs.AI cs.IT math.IT stat.ML</categories><comments>4 pages, 3 figures, This is a preprint of a paper submitted to the
  2019 European Conference on Optical Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the efficient compensation of fiber nonlinearity, one of the guiding
principles appears to be: fewer steps are better and more efficient. We
challenge this assumption and show that carefully designed multi-step
approaches can lead to better performance-complexity trade-offs than their
few-step counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09848</identifier>
 <datestamp>2019-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09848</id><created>2019-04-12</created><updated>2019-10-26</updated><authors><author><keyname>Khodadadian</keyname><forenames>Amirreza</forenames></author><author><keyname>Stadlbauer</keyname><forenames>Benjamin</forenames></author><author><keyname>Heitzinger</keyname><forenames>Clemens</forenames></author></authors><title>Bayesian inversion for nanowire field-effect sensors</title><categories>math.NA cs.NA eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanowire field-effect sensors have recently been developed for label-free
detection of biomolecules. In this work, we introduce a computational technique
based on Bayesian estimation to determine the physical parameters of the sensor
and, more importantly, the properties of the analyte molecules. To that end, we
first propose a PDE based model to simulate the device charge transport and
electrochemical behavior. Then, the adaptive Metropolis algorithm with delayed
rejection (DRAM) is applied to estimate the posterior distribution of unknown
parameters, namely molecule charge density, molecule density, doping
concentration, and electron and hole mobilities. We determine the device and
molecules properties simultaneously, and we also calculate the molecule density
as the only parameter after having determined the device parameters. This
approach makes it possible not only to determine unknown parameters, but it
also shows how well each parameter can be determined by yielding the
probability density function (pdf).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09870</identifier>
 <datestamp>2019-12-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09870</id><created>2019-04-22</created><authors><author><keyname>Kumari</keyname><forenames>Vineeta</forenames></author><author><keyname>Ahmed</keyname><forenames>Aijaz</forenames></author><author><keyname>Kanumuri</keyname><forenames>Tirupathiraju</forenames></author><author><keyname>Shakher</keyname><forenames>Chandra</forenames></author><author><keyname>Sheoran</keyname><forenames>Gyanendra</forenames></author></authors><title>Early Detection of Cancerous Tissues in Human Breast utilizing Near
  field Microwave Holography</title><categories>physics.med-ph eess.IV</categories><comments>15 pages, 12 figures</comments><journal-ref>International Journal of Imaging Systems and Technology, 2019</journal-ref><doi>10.1002/ima.22384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work demonstrates an application of near field indirect microwave
holography for the detection of malignant tissues in the human breast in an
effective way. The holograms are recorded by two directive antennas aligned
along each other's boresight while performing a raster scan over a 2D plane
utilizing XY-linear motorized translation stage and a uniform reference wave.
The whole information i.e. amplitude and phase of an object has been provided
by indirect holography at microwave frequencies. The extracted phase values are
used to determine the dielectric permittivity values which are further utilized
for the identification and validating the positions of malignant tissues in the
breast phantom. The experimental evaluations performed on the in-house designed
and developed tissue mimicking 3D printed breast phantoms. The experimental
results demonstrate the ability of microwave holography using directive
antennas in locating and identifying the tumors up to the minimum size of 4mm
and a maximum depth of 25mm in fabricated phantom. The preliminary results
present the potential of the Near Field Indirect Holographic Imaging (NFIHI) in
order to develop an efficient and economical tool for breast cancer detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09977</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09977</id><created>2019-04-20</created><authors><author><keyname>Karsch</keyname><forenames>Kevin</forenames></author><author><keyname>Grinstead</keyname><forenames>Brian</forenames></author><author><keyname>He</keyname><forenames>Qing</forenames></author><author><keyname>Duan</keyname><forenames>Ye</forenames></author></authors><title>Web Based Brain Volume Calculation for Magnetic Resonance Images</title><categories>eess.IV cs.CV</categories><doi>10.1109/IEMBS.2008.4649380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain volume calculations are crucial in modern medical research, especially
in the study of neurodevelopmental disorders. In this paper, we present an
algorithm for calculating two classifications of brain volume, total brain
volume (TBV) and intracranial volume (ICV). Our algorithm takes MRI data as
input, performs several preprocessing and intermediate steps, and then returns
each of the two calculated volumes. To simplify this process and make our
algorithm publicly accessible to anyone, we have created a web-based interface
that allows users to upload their own MRI data and calculate the TBV and ICV
for the given data. This interface provides a simple and efficient method for
calculating these two classifications of brain volume, and it also removes the
need for the user to download or install any applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09978</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09978</id><created>2019-04-20</created><authors><author><keyname>Karsch</keyname><forenames>Kevin</forenames></author><author><keyname>He</keyname><forenames>Qing</forenames></author><author><keyname>Duan</keyname><forenames>Ye</forenames></author></authors><title>A Fast, Semi-Automatic Brain Structure Segmentation Algorithm for
  Magnetic Resonance Imaging</title><categories>eess.IV cs.CV</categories><doi>10.1109/BIBM.2009.40</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical image segmentation has become an essential technique in clinical and
research-oriented applications. Because manual segmentation methods are
tedious, and fully automatic segmentation lacks the flexibility of human
intervention or correction, semi-automatic methods have become the preferred
type of medical image segmentation. We present a hybrid, semi-automatic
segmentation method in 3D that integrates both region-based and boundary-based
procedures. Our method differs from previous hybrid methods in that we perform
region-based and boundary-based approaches separately, which allows for more
efficient segmentation. A region-based technique is used to generate an initial
seed contour that roughly represents the boundary of a target brain structure,
alleviating the local minima problem in the subsequent model deformation phase.
The contour is deformed under a unique force equation independent of image
edges. Experiments on MRI data show that this method can achieve high accuracy
and efficiency primarily due to the unique seed initialization technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.09979</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.09979</id><created>2019-04-21</created><authors><author><keyname>Moussilli</keyname><forenames>Mariam</forenames></author><author><keyname>Falou</keyname><forenames>Abdul Rahman El</forenames></author><author><keyname>Shubair</keyname><forenames>Raed</forenames></author></authors><title>Plasmonics Theory for Biosensor Design: Mathematical Formulations and
  Practical Applications</title><categories>physics.optics eess.SP physics.app-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last two decades have witnessed an exponential growth and tremendous
developments in wireless technologies and systems, and their associated
applications. In the recent years following 2006, there has been a great surge
in interest in the newly emerging plasmonics nanotechnology because this new
device technology provides tremendous synergy between electronic and photonic
devices. Electronics devices are down-scalable up to the nanoscale size but
have limited processor speed due to thermal and signal delay issues associated
with electronic devices. On the other hand, photonic devices have extremely
high speed and high data carrying capacity but are limited in size to the
diffraction law such that the size of a photonic device should be equal to
about half of its operational wavelength. The size mismatch between electronic
devices and photonic devices inhibits the advantageous interfacing between
these two device technologies and here plasmonics nanotechnology plays the
important role of interfacing these two technologies. Plasmonics technology
provides high speed interconnections with high data carrying capacity between
nano-scale electronic devices opening a new field of research which is on-chip
high speed nano-networks [28]. It is this great advantage of plasmonics
technology that made it a very interesting technology for implementation for
the design of a miniature real-time biosensor. In our plasmonic biosensor
design, we utilized a subset of plasmonics technology which is surface plasmon
wave generation in order to continuously monitor the concentration of a desired
analyte.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10030</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10030</id><created>2019-04-22</created><authors><author><keyname>Karimi</keyname><forenames>Davood</forenames></author><author><keyname>Salcudean</keyname><forenames>Septimiu E.</forenames></author></authors><title>Reducing the Hausdorff Distance in Medical Image Segmentation with
  Convolutional Neural Networks</title><categories>eess.IV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hausdorff Distance (HD) is widely used in evaluating medical image
segmentation methods. However, existing segmentation methods do not attempt to
reduce HD directly. In this paper, we present novel loss functions for training
convolutional neural network (CNN)-based segmentation methods with the goal of
reducing HD directly. We propose three methods to estimate HD from the
segmentation probability map produced by a CNN. One method makes use of the
distance transform of the segmentation boundary. Another method is based on
applying morphological erosion on the difference between the true and estimated
segmentation maps. The third method works by applying circular/spherical
convolution kernels of different radii on the segmentation probability maps.
Based on these three methods for estimating HD, we suggest three loss functions
that can be used for training to reduce HD. We use these loss functions to
train CNNs for segmentation of the prostate, liver, and pancreas in ultrasound,
magnetic resonance, and computed tomography images and compare the results with
commonly-used loss functions. Our results show that the proposed loss functions
can lead to approximately 18-45 % reduction in HD without degrading other
segmentation performance criteria such as the Dice similarity coefficient. The
proposed loss functions can be used for training medical image segmentation
methods in order to reduce the large segmentation errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10045</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10045</id><created>2019-03-27</created><authors><author><keyname>Zhang</keyname><forenames>Shiliang</forenames></author><author><keyname>Lei</keyname><forenames>Ming</forenames></author><author><keyname>Yan</keyname><forenames>Zhijie</forenames></author></authors><title>Automatic Spelling Correction with Transformer for CTC-based End-to-End
  Speech Recognition</title><categories>eess.AS cs.NE cs.SD</categories><comments>6pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connectionist Temporal Classification (CTC) based end-to-end speech
recognition system usually need to incorporate an external language model by
using WFST-based decoding in order to achieve promising results. This is more
essential to Mandarin speech recognition since it owns a special phenomenon,
namely homophone, which causes a lot of substitution errors. The linguistic
information introduced by language model will help to distinguish these
substitution errors. In this work, we propose a transformer based spelling
correction model to automatically correct errors especially the substitution
errors made by CTC-based Mandarin speech recognition system. Specifically, we
investigate using the recognition results generated by CTC-based systems as
input and the ground-truth transcriptions as output to train a transformer with
encoder-decoder architecture, which is much similar to machine translation.
Results in a 20,000 hours Mandarin speech recognition task show that the
proposed spelling correction model can achieve a CER of 3.41%, which results in
22.9% and 53.2% relative improvement compared to the baseline CTC-based systems
decoded with and without language model respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10102</identifier>
 <datestamp>2020-01-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10102</id><created>2019-04-22</created><updated>2020-01-23</updated><authors><author><keyname>Bondorf</keyname><forenames>Steffen</forenames></author><author><keyname>Chen</keyname><forenames>Binbin</forenames></author><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Yu</keyname><forenames>Haifeng</forenames></author><author><keyname>Zhao</keyname><forenames>Yuda</forenames></author></authors><title>Sublinear-Time Non-Adaptive Group Testing with $O(k \log n)$ Tests via
  Bit-Mixing Coding</title><categories>cs.IT eess.SP math.IT math.PR</categories><comments>(v2) Expanded related work section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The group testing problem consists of determining a small set of defective
items from a larger set of items based on tests on groups of items, and is
relevant in applications such as medical testing, communication protocols,
pattern matching, and many more. While rigorous group testing algorithms have
long been known with runtime at least linear in the number of items, a recent
line of works has sought to reduce the runtime to ${\rm poly}(k \log n)$, where
$n$ is the number of items and $k$ is the number of defectives. In this paper,
we present such an algorithm for non-adaptive probabilistic group testing
termed {\em bit mixing coding} (BMC), which builds on techniques that encode
item indices in the test matrix, while incorporating novel ideas based on
erasure-correction coding. We show that BMC achieves asymptotically vanishing
error probability with $O(k \log n)$ tests and $O(k^2 \cdot \log k \cdot \log
n)$ runtime, in the limit as $n \to \infty$ (with $k$ having an arbitrary
dependence on $n$). This closes a recently-proposed open problem of
simultaneously achieving ${\rm poly}(k \log n)$ decoding time using $O(k \log
n)$ tests without any assumptions on $k$. In addition, we show that the same
scaling laws can be attained in a commonly-considered noisy setting, in which
each test outcome is flipped with constant probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10134</identifier>
 <datestamp>2019-07-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10134</id><created>2019-04-22</created><updated>2019-07-17</updated><authors><author><keyname>Jung</keyname><forenames>Jee-weon</forenames></author><author><keyname>Shim</keyname><forenames>Hye-jin</forenames></author><author><keyname>Heo</keyname><forenames>Hee-Soo</forenames></author><author><keyname>Yu</keyname><forenames>Ha-Jin</forenames></author></authors><title>Replay attack detection with complementary high-resolution information
  using end-to-end DNN for the ASVspoof 2019 Challenge</title><categories>eess.AS cs.CR cs.SD</categories><comments>Accepted for oral presentation at Interspeech 2019, code available at
  https://github.com/Jungjee/ASVspoof2019_PA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we concentrate on replacing the process of extracting
hand-crafted acoustic feature with end-to-end DNN using complementary
high-resolution spectrograms. As a result of advance in audio devices, typical
characteristics of a replayed speech based on conventional knowledge alter or
diminish in unknown replay configurations. Thus, it has become increasingly
difficult to detect spoofed speech with a conventional knowledge-based
approach. To detect unrevealed characteristics that reside in a replayed
speech, we directly input spectrograms into an end-to-end DNN without
knowledge-based intervention. Explorations dealt in this study that
differentiates from existing spectrogram-based systems are twofold:
complementary information and high-resolution. Spectrograms with different
information are explored, and it is shown that additional information such as
the phase information can be complementary. High-resolution spectrograms are
employed with the assumption that the difference between a bona-fide and a
replayed speech exists in the details. Additionally, to verify whether other
features are complementary to spectrograms, we also examine raw waveform and an
i-vector based system. Experiments conducted on the ASVspoof 2019 physical
access challenge show promising results, where t-DCF and equal error rates are
0.0570 and 2.45 % for the evaluation set, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10135</identifier>
 <datestamp>2019-07-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10135</id><created>2019-04-22</created><updated>2019-07-17</updated><authors><author><keyname>Heo</keyname><forenames>Hee-Soo</forenames></author><author><keyname>Jung</keyname><forenames>Jee-weon</forenames></author><author><keyname>Shim</keyname><forenames>Hye-jin</forenames></author><author><keyname>Yu</keyname><forenames>Ha-Jin</forenames></author></authors><title>Acoustic scene classification using teacher-student learning with
  soft-labels</title><categories>eess.AS cs.SD</categories><comments>Accepted for presentation at Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustic scene classification identifies an input segment into one of the
pre-defined classes using spectral information. The spectral information of
acoustic scenes may not be mutually exclusive due to common acoustic properties
across different classes, such as babble noises included in both airports and
shopping malls. However, conventional training procedure based on one-hot
labels does not consider the similarities between different acoustic scenes. We
exploit teacher-student learning with the purpose to derive soft-labels that
consider common acoustic properties among different acoustic scenes. In
teacher-student learning, the teacher network produces soft-labels, based on
which the student network is trained. We investigate various methods to extract
soft-labels that better represent similarities across different scenes. Such
attempts include extracting soft-labels from multiple audio segments that are
defined as an identical acoustic scene. Experimental results demonstrate the
potential of our approach, showing a classification accuracy of 77.36 % on the
DCASE 2018 task 1 validation set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10136</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10136</id><created>2019-04-22</created><updated>2019-04-30</updated><authors><author><keyname>Taha</keyname><forenames>Abdelrahman</forenames></author><author><keyname>Alrabeiah</keyname><forenames>Muhammad</forenames></author><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author></authors><title>Enabling Large Intelligent Surfaces with Compressive Sensing and Deep
  Learning</title><categories>cs.IT eess.SP math.IT</categories><comments>Submitted to IEEE Access. The code will be available soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Employing large intelligent surfaces (LISs) is a promising solution for
improving the coverage and rate of future wireless systems. These surfaces
comprise a massive number of nearly-passive elements that interact with the
incident signals, for example by reflecting them, in a smart way that improves
the wireless system performance. Prior work focused on the design of the LIS
reflection matrices assuming full knowledge of the channels. Estimating these
channels at the LIS, however, is a key challenging problem, and is associated
with large training overhead given the massive number of LIS elements. This
paper proposes efficient solutions for these problems by leveraging tools from
compressive sensing and deep learning. First, a novel LIS architecture based on
sparse channel sensors is proposed. In this architecture, all the LIS elements
are passive except for a few elements that are active (connected to the
baseband of the LIS controller). We then develop two solutions that design the
LIS reflection matrices with negligible training overhead. In the first
approach, we leverage compressive sensing tools to construct the channels at
all the LIS elements from the channels seen only at the active elements. These
full channels can then be used to design the LIS reflection matrices with no
training overhead. In the second approach, we develop a deep learning based
solution where the LIS learns how to optimally interact with the incident
signal given the channels at the active elements, which represent the current
state of the environment and transmitter/receiver locations. We show that the
achievable rates of the proposed compressive sensing and deep learning
solutions approach the upper bound, that assumes perfect channel knowledge,
with negligible training overhead and with less than 1% of the elements being
active.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10235</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10235</id><created>2019-04-23</created><authors><author><keyname>Cogranne</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Slysz</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Moreau</keyname><forenames>Laurence</forenames></author><author><keyname>Borouchaki</keyname><forenames>Houman</forenames></author></authors><title>A new Edge Detector Based on Parametric Surface Model: Regression
  Surface Descriptor</title><categories>eess.IV cs.CV</categories><comments>21 pages, 13 figures and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new methodology for edge detection in digital
images. The first originality of the proposed method is to consider image
content as a parametric surface. Then, an original parametric local model of
this surface representing image content is proposed. The few parameters
involved in the proposed model are shown to be very sensitive to
discontinuities in surface which correspond to edges in image content. This
naturally leads to the design of an efficient edge detector. Moreover, a
thorough analysis of the proposed model also allows us to explain how these
parameters can be used to obtain edge descriptors such as orientations and
curvatures.
  In practice, the proposed methodology offers two main advantages. First, it
has high customization possibilities in order to be adjusted to a wide range of
different problems, from coarse to fine scale edge detection. Second, it is
very robust to blurring process and additive noise. Numerical results are
presented to emphasis these properties and to confirm efficiency of the
proposed method through a comparative study with other edge detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10237</identifier>
 <datestamp>2020-01-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10237</id><created>2019-04-23</created><updated>2020-01-01</updated><authors><author><keyname>Nakamura</keyname><forenames>Eita</forenames></author><author><keyname>Saito</keyname><forenames>Yasuyuki</forenames></author><author><keyname>Yoshii</keyname><forenames>Kazuyoshi</forenames></author></authors><title>Statistical Learning and Estimation of Piano Fingering</title><categories>cs.LG cs.SD eess.AS</categories><comments>30 pages, 8 figures, tex style changed, minor modifications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic estimation of piano fingering is important for understanding the
computational process of music performance and applicable to performance
assistance and education systems. While a natural way to formulate the quality
of fingerings is to construct models of the constraints/costs of performance,
it is generally difficult to find appropriate parameter values for these
models. Here we study an alternative data-driven approach based on statistical
modeling in which the appropriateness of a given fingering is described by
probabilities. Specifically, we construct two types of hidden Markov models
(HMMs) and their higher-order extensions. We also study deep neural network
(DNN)-based methods for comparison. Using a newly released dataset of fingering
annotations, we conduct systematic evaluations of these models as well as a
representative constraint-based method. We find that the methods based on
high-order HMMs outperform the other methods in terms of estimation accuracies.
We also quantitatively study individual difference of fingering and propose
evaluation measures that can be used with multiple ground truth data. We
conclude that the HMM-based methods are currently state of the art and generate
acceptable fingerings in most parts and that they have certain limitations such
as ignorance of phrase boundaries and interdependence of the two hands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10242</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10242</id><created>2019-04-23</created><authors><author><keyname>Zhang</keyname><forenames>Xinyue</forenames></author><author><keyname>Wang</keyname><forenames>Yuan</forenames></author><author><keyname>Zhang</keyname><forenames>Yawen</forenames></author><author><keyname>Song</keyname><forenames>Jiahao</forenames></author><author><keyname>Zhang</keyname><forenames>Zuodong</forenames></author><author><keyname>Cheng</keyname><forenames>Kaili</forenames></author><author><keyname>Wang</keyname><forenames>Runsheng</forenames></author><author><keyname>Huang</keyname><forenames>Ru</forenames></author></authors><title>Memory System Designed for Multiply-Accumulate (MAC) Engine Based on
  Stochastic Computing</title><categories>eess.SP</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural network (CNN) achieves excellent performance on
fascinating tasks such as image recognition and natural language processing at
the cost of high power consumption. Stochastic computing (SC) is an attractive
paradigm implemented in low power applications which performs arithmetic
operations with simple logic and low hardware cost. However, conventional
memory structure designed and optimized for binary computing leads to extra
data conversion costs, which significantly decreases the energy efficiency.
Therefore, a new memory system designed for SC-based multiply-accumulate (MAC)
engine applied in CNN which is compatible with conventional memory system is
proposed in this paper. As a result, the overall energy consumption of our new
computing structure is 0.91pJ, which is reduced by 82.1% compared with the
conventional structure, and the energy efficiency achieves 164.8 TOPS/W.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10255</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10255</id><created>2019-04-23</created><authors><author><keyname>Humayun</keyname><forenames>Ahmed Imtiaz</forenames></author><author><keyname>Sushmit</keyname><forenames>Asif Shahriyar</forenames></author><author><keyname>Hasan</keyname><forenames>Taufiq</forenames></author><author><keyname>Bhuiyan</keyname><forenames>Mohammed Imamul Hassan</forenames></author></authors><title>End-to-end Sleep Staging with Raw Single Channel EEG using Deep Residual
  ConvNets</title><categories>cs.LG cs.CV eess.SP stat.ML</categories><comments>5 pages, 3 Figures, Appendix, IEEE BHI 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Humans approximately spend a third of their life sleeping, which makes
monitoring sleep an integral part of well-being. In this paper, a 34-layer deep
residual ConvNet architecture for end-to-end sleep staging is proposed. The
network takes raw single channel electroencephalogram (Fpz-Cz) signal as input
and yields hypnogram annotations for each 30s segments as output. Experiments
are carried out for two different scoring standards (5 and 6 stage
classification) on the expanded PhysioNet Sleep-EDF dataset, which contains
multi-source data from hospital and household polysomnography setups. The
performance of the proposed network is compared with that of the
state-of-the-art algorithms in patient independent validation tasks. The
experimental results demonstrate the superiority of the proposed network
compared to the best existing method, providing a relative improvement in
epoch-wise average accuracy of 6.8% and 6.3% on the household data and
multi-source data, respectively. Codes are made publicly available on Github.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10341</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10341</id><created>2019-04-22</created><authors><author><keyname>Li</keyname><forenames>Zheng-Ping</forenames></author><author><keyname>Huang</keyname><forenames>Xin</forenames></author><author><keyname>Cao</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Yu-Huai</forenames></author><author><keyname>Jin</keyname><forenames>Weijie</forenames></author><author><keyname>Yu</keyname><forenames>Chao</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Qiang</forenames></author><author><keyname>Peng</keyname><forenames>Cheng-Zhi</forenames></author><author><keyname>Xu</keyname><forenames>Feihu</forenames></author><author><keyname>Pan</keyname><forenames>Jian-Wei</forenames></author></authors><title>Single-photon computational 3D imaging at 45 km</title><categories>eess.IV physics.optics</categories><comments>22 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long-range active imaging has a variety of applications in remote sensing and
target recognition. Single-photon LiDAR (light detection and ranging) offers
single-photon sensitivity and picosecond timing resolution, which is desirable
for high-precision three-dimensional (3D) imaging over long distances. Despite
important progress, further extending the imaging range presents enormous
challenges because only weak echo photons return and are mixed with strong
noise. Herein, we tackled these challenges by constructing a high-efficiency,
low-noise confocal single-photon LiDAR system, and developing a
long-range-tailored computational algorithm that provides high photon
efficiency and super-resolution in the transverse domain. Using this technique,
we experimentally demonstrated active single-photon 3D-imaging at a distance of
up to 45 km in an urban environment, with a low return-signal level of $\sim$1
photon per pixel. Our system is feasible for imaging at a few hundreds of
kilometers by refining the setup, and thus represents a significant milestone
towards rapid, low-power, and high-resolution LiDAR over extra-long ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10380</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10380</id><created>2019-04-23</created><authors><author><keyname>Huang</keyname><forenames>Feng</forenames></author><author><keyname>Balazs</keyname><forenames>Peter</forenames></author></authors><title>Harmonic-aligned Frame Mask Based on Non-stationary Gabor Transform with
  Application to Content-dependent Speaker Comparison</title><categories>cs.SD eess.AS math.SP</categories><comments>Interspeech2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose harmonic-aligned frame mask for speech signals using
non-stationary Gabor transform (NSGT). A frame mask operates on the transfer
coefficients of a signal and consequently converts the signal into a
counterpart signal. It depicts the difference between the two signals. In
preceding studies, frame masks based on regular Gabor transform were applied to
single-note instrumental sound analysis. This study extends the frame mask
approach to speech signals. For voiced speech, the fundamental frequency is
usually changing consecutively over time. We employ NSGT with pitch-dependent
and therefore time-varying frequency resolution to attain harmonic alignment in
the transform domain and hence yield harmonic-aligned frame masks for speech
signals. We propose to apply the harmonic-aligned frame mask to
content-dependent speaker comparison. Frame masks, computed from voiced signals
of a same vowel but from different speakers, were utilized as similarity
measures to compare and distinguish the speaker identities (SID). Results
obtained with deep neural networks demonstrate that the proposed frame mask is
valid in representing speaker characteristics and shows a potential for SID
applications in limited data scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10408</identifier>
 <datestamp>2019-07-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10408</id><created>2019-04-23</created><updated>2019-07-01</updated><authors><author><keyname>Bear</keyname><forenames>Helen L.</forenames></author><author><keyname>Nolasco</keyname><forenames>Ines</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author></authors><title>Towards joint sound scene and polyphonic sound event recognition</title><categories>eess.AS cs.SD</categories><comments>Accepted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustic Scene Classification (ASC) and Sound Event Detection (SED) are two
separate tasks in the field of computational sound scene analysis. In this
work, we present a new dataset with both sound scene and sound event labels and
use this to demonstrate a novel method for jointly classifying sound scenes and
recognizing sound events. We show that by taking a joint approach, learning is
more efficient and whilst improvements are still needed for sound event
detection, SED results are robust in a dataset where the sample distribution is
skewed towards sound scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10444</identifier>
 <datestamp>2019-12-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10444</id><created>2019-04-23</created><updated>2019-12-25</updated><authors><author><keyname>Bastopcu</keyname><forenames>Melih</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Age of Information for Updates with Distortion</title><categories>cs.IT cs.NI eess.SP math.IT</categories><comments>This arxiv upload is withdrawn because a longer journal version with
  the title &quot;Age of Information for Updates with Distortion: Constant and
  Age-Dependent Distortion Constraints&quot; is uploaded as a separate article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an information update system where an information receiver
requests updates from an information provider in order to minimize its age of
information. The updates are generated at the transmitter as a result of
completing a set of tasks such as collecting data and performing computations.
We refer to this as the update generation process. We model the $quality$
(i.e., $distortion$) of an update as an increasing (resp. decreasing) function
of the processing time spent while generating the update at the transmitter.
While processing longer at the transmitter results in a better quality (lower
distortion) update, it causes the update to age. We determine the age-optimal
policies for the update request times at the receiver and update processing
times at the transmitter subject to a minimum required quality (maximum allowed
distortion) constraint on the updates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10450</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10450</id><created>2019-04-23</created><authors><author><keyname>Guo</keyname><forenames>Lijiang</forenames></author></authors><title>Latent Variable Algorithms for Multimodal Learning and Sensor Fusion</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimodal learning has been lacking principled ways of combining information
from different modalities and learning a low-dimensional manifold of meaningful
representations. We study multimodal learning and sensor fusion from a latent
variable perspective. We first present a regularized recurrent attention filter
for sensor fusion. This algorithm can dynamically combine information from
different types of sensors in a sequential decision making task. Each sensor is
bonded with a modular neural network to maximize utility of its own
information. A gating modular neural network dynamically generates a set of
mixing weights for outputs from sensor networks by balancing utility of all
sensors' information. We design a co-learning mechanism to encourage
co-adaption and independent learning of each sensor at the same time, and
propose a regularization based co-learning method. In the second part, we focus
on recovering the manifold of latent representation. We propose a co-learning
approach using probabilistic graphical model which imposes a structural prior
on the generative model: multimodal variational RNN (MVRNN) model, and derive a
variational lower bound for its objective functions. In the third part, we
extend the siamese structure to sensor fusion for robust acoustic event
detection. We perform experiments to investigate the latent representations
that are extracted; works will be done in the following months. Our experiments
show that the recurrent attention filter can dynamically combine different
sensor inputs according to the information carried in the inputs. We consider
MVRNN can identify latent representations that are useful for many downstream
tasks such as speech synthesis, activity recognition, and control and planning.
Both algorithms are general frameworks which can be applied to other tasks
where different types of sensors are jointly used for decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10481</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10481</id><created>2019-04-23</created><authors><author><keyname>Zhu</keyname><forenames>Qiang</forenames></author><author><keyname>Tian</keyname><forenames>Xin</forenames></author><author><keyname>Wong</keyname><forenames>Chau-Wai</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author></authors><title>ECG Reconstruction via PPG: A Pilot Study</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the relation between electrocardiogram (ECG) and
photoplethysmogram (PPG) signals is studied, and the waveform of ECG is
inferred via the PPG signals. In order to address this inverse problem, a
transform is proposed to map the discrete cosine transform (DCT) coefficients
of each PPG cycle to those of the corresponding ECG cycle. The resulting DCT
coefficients of the ECG cycle are inversely transformed to obtain the
reconstructed ECG waveform. The proposed method is evaluated on a benchmark
dataset of subjects with a variety of combinations of age and weight.
Experimental results show that the proposed method can achieve a high accuracy
at 0.98 in averaged correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10500</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10500</id><created>2019-04-23</created><authors><author><keyname>Okur</keyname><forenames>Eda</forenames></author><author><keyname>Kumar</keyname><forenames>Shachi H</forenames></author><author><keyname>Sahay</keyname><forenames>Saurav</forenames></author><author><keyname>Esme</keyname><forenames>Asli Arslan</forenames></author><author><keyname>Nachman</keyname><forenames>Lama</forenames></author></authors><title>Natural Language Interactions in Autonomous Vehicles: Intent Detection
  and Slot Filling from Passenger Utterances</title><categories>cs.CL cs.HC cs.LG cs.SD eess.AS</categories><comments>Accepted and presented as a full paper at 20th International
  Conference on Computational Linguistics and Intelligent Text Processing
  (CICLing 2019), April 7-13, 2019, La Rochelle, France</comments><journal-ref>Springer LNCS Proceedings for CICLing 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding passenger intents and extracting relevant slots are important
building blocks towards developing contextual dialogue systems for natural
interactions in autonomous vehicles (AV). In this work, we explored AMIE
(Automated-vehicle Multi-modal In-cabin Experience), the in-cabin agent
responsible for handling certain passenger-vehicle interactions. When the
passengers give instructions to AMIE, the agent should parse such commands
properly and trigger the appropriate functionality of the AV system. In our
current explorations, we focused on AMIE scenarios describing usages around
setting or changing the destination and route, updating driving behavior or
speed, finishing the trip and other use-cases to support various natural
commands. We collected a multi-modal in-cabin dataset with multi-turn dialogues
between the passengers and AMIE using a Wizard-of-Oz scheme via a realistic
scavenger hunt game activity. After exploring various recent Recurrent Neural
Networks (RNN) based techniques, we introduced our own hierarchical joint
models to recognize passenger intents along with relevant slots associated with
the action to be performed in AV scenarios. Our experimental results
outperformed certain competitive baselines and achieved overall F1 scores of
0.91 for utterance-level intent detection and 0.96 for slot filling tasks. In
addition, we conducted initial speech-to-text explorations by comparing
intent/slot models trained and tested on human transcriptions versus noisy
Automatic Speech Recognition (ASR) outputs. Finally, we compared the results
with single passenger rides versus the rides with multiple passengers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10506</identifier>
 <datestamp>2019-05-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10506</id><created>2019-04-24</created><updated>2019-05-08</updated><authors><author><keyname>Zhu</keyname><forenames>Hao</forenames></author><author><keyname>Zuo</keyname><forenames>Xinxin</forenames></author><author><keyname>Wang</keyname><forenames>Sen</forenames></author><author><keyname>Cao</keyname><forenames>Xun</forenames></author><author><keyname>Yang</keyname><forenames>Ruigang</forenames></author></authors><title>Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh
  Deformation</title><categories>cs.CV eess.IV</categories><comments>CVPR 2019 Oral</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel framework to recover detailed human body shapes
from a single image. It is a challenging task due to factors such as variations
in human shapes, body poses, and viewpoints. Prior methods typically attempt to
recover the human body shape using a parametric based template that lacks the
surface details. As such the resulting body shape appears to be without
clothing. In this paper, we propose a novel learning-based framework that
combines the robustness of parametric model with the flexibility of free-form
3D deformation. We use the deep neural networks to refine the 3D shape in a
Hierarchical Mesh Deformation (HMD) framework, utilizing the constraints from
body joints, silhouettes, and per-pixel shading information. We are able to
restore detailed human body shapes beyond skinned models. Experiments
demonstrate that our method has outperformed previous state-of-the-art
approaches, achieving better accuracy in terms of both 2D IoU number and 3D
metric distance. The code is available in https://github.com/zhuhao-nju/hmd.git
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10535</identifier>
 <datestamp>2019-11-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10535</id><created>2019-04-23</created><authors><author><keyname>Xiao</keyname><forenames>Yiming</forenames></author><author><keyname>Rivaz</keyname><forenames>Hassan</forenames></author><author><keyname>Chabanas</keyname><forenames>Matthieu</forenames></author><author><keyname>Fortin</keyname><forenames>Maryse</forenames></author><author><keyname>Machado</keyname><forenames>Ines</forenames></author><author><keyname>Ou</keyname><forenames>Yangming</forenames></author><author><keyname>Heinrich</keyname><forenames>Mattias P.</forenames></author><author><keyname>Schnabel</keyname><forenames>Julia A.</forenames></author><author><keyname>Zhong</keyname><forenames>Xia</forenames></author><author><keyname>Maier</keyname><forenames>Andreas</forenames></author><author><keyname>Wein</keyname><forenames>Wolfgang</forenames></author><author><keyname>Shams</keyname><forenames>Roozbeh</forenames></author><author><keyname>Kadoury</keyname><forenames>Samuel</forenames></author><author><keyname>Drobny</keyname><forenames>David</forenames></author><author><keyname>Modat</keyname><forenames>Marc</forenames></author><author><keyname>Reinertsen</keyname><forenames>Ingerid</forenames></author></authors><title>Evaluation of MRI to ultrasound registration methods for brain shift
  correction: The CuRIOUS2018 Challenge</title><categories>eess.IV</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><journal-ref>IEEE transactions on medical imaging,2019</journal-ref><doi>10.1109/TMI.2019.2935060</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In brain tumor surgery, the quality and safety of the procedure can be
impacted by intra-operative tissue deformation, called brain shift. Brain shift
can move the surgical targets and other vital structures such as blood vessels,
thus invalidating the pre-surgical plan. Intra-operative ultrasound (iUS) is a
convenient and cost-effective imaging tool to track brain shift and tumor
resection. Accurate image registration techniques that update pre-surgical MRI
based on iUS are crucial but challenging. The MICCAI Challenge 2018 for
Correction of Brain shift with Intra-Operative UltraSound (CuRIOUS2018)
provided a public platform to benchmark MRI-iUS registration algorithms on
newly released clinical datasets. In this work, we present the data, setup,
evaluation, and results of CuRIOUS 2018, which received 6 fully automated
algorithms from leading academic and industrial research groups. All algorithms
were first trained with the public RESECT database, and then ranked based on
test dataset of 10 additional cases with identical data curation and annotation
protocols as the RESECT database. The article compares the results of all
participating teams and discusses the insights gained from the challenge, as
well as future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10584</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10584</id><created>2019-04-23</created><authors><author><keyname>Parthasarathi</keyname><forenames>Sree Hari Krishnan</forenames></author><author><keyname>Sivakrishnan</keyname><forenames>Nitin</forenames></author><author><keyname>Ladkat</keyname><forenames>Pranav</forenames></author><author><keyname>Strom</keyname><forenames>Nikko</forenames></author></authors><title>Realizing Petabyte Scale Acoustic Modeling</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>2156-3357 \copyright 2019 IEEE. Personal use is permitted, but
  republication/redistribution requires IEEE permission. See
  http://www.ieee.org/publications standards/publications/rights/index.html for
  more information</comments><doi>10.1109/JETCAS.2019.2912353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale machine learning (ML) systems such as the Alexa automatic speech
recognition (ASR) system continue to improve with increasing amounts of
manually transcribed training data. Instead of scaling manual transcription to
impractical levels, we utilize semi-supervised learning (SSL) to learn acoustic
models (AM) from the vast firehose of untranscribed audio data. Learning an AM
from 1 Million hours of audio presents unique ML and system design challenges.
We present the design and evaluation of a highly scalable and resource
efficient SSL system for AM. Employing the student/teacher learning paradigm,
we focus on the student learning subsystem: a scalable and robust data pipeline
that generates features and targets from raw audio, and an efficient model
pipeline, including the distributed trainer, that builds a student model. Our
evaluations show that, even without extensive hyper-parameter tuning, we obtain
relative accuracy improvements in the 10 to 20$\%$ range, with higher gains in
noisier conditions. The end-to-end processing time of this SSL system was 12
days, and several components in this system can trivially scale linearly with
more compute resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10674</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10674</id><created>2019-04-24</created><authors><author><keyname>Audebert</keyname><forenames>Nicolas</forenames><affiliation>OBELIX</affiliation></author><author><keyname>Saux</keyname><forenames>Bertrand</forenames><affiliation>OBELIX</affiliation></author><author><keyname>Lef&#xe8;vre</keyname><forenames>S&#xe9;bastien</forenames><affiliation>OBELIX</affiliation></author></authors><title>Deep Learning for Classification of Hyperspectral Data: A Comparative
  Review</title><categories>cs.LG cs.CV cs.NE eess.IV</categories><proxy>ccsd</proxy><doi>10.1109/MGRS.2019.2912563</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, deep learning techniques revolutionized the way remote
sensing data are processed. Classification of hyperspectral data is no
exception to the rule, but has intrinsic specificities which make application
of deep learning less straightforward than with other optical data. This
article presents a state of the art of previous machine learning approaches,
reviews the various deep learning approaches currently proposed for
hyperspectral classification, and identifies the problems and difficulties
which arise to implement deep neural networks for this task. In particular, the
issues of spatial and spectral resolution, data volume, and transfer of models
from multimedia images to hyperspectral data are addressed. Additionally, a
comparative study of various families of network architectures is provided and
a software toolbox is publicly released to allow experimenting with these
methods. 1 This article is intended for both data scientists with interest in
hyperspectral data and remote sensing experts eager to apply deep learning
techniques to their own dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10678</identifier>
 <datestamp>2019-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10678</id><created>2019-04-24</created><updated>2019-11-06</updated><authors><author><keyname>Drossos</keyname><forenames>Konstantinos</forenames></author><author><keyname>Magron</keyname><forenames>Paul</forenames></author><author><keyname>Virtanen</keyname><forenames>Tuomas</forenames></author></authors><title>Unsupervised Adversarial Domain Adaptation Based On The Wasserstein
  Distance For Acoustic Scene Classification</title><categories>cs.SD cs.LG eess.AS</categories><comments>Updated indices at Eq 6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A challenging problem in deep learning-based machine listening field is the
degradation of the performance when using data from unseen conditions. In this
paper we focus on the acoustic scene classification (ASC) task and propose an
adversarial deep learning method to allow adapting an acoustic scene
classification system to deal with a new acoustic channel resulting from data
captured with a different recording device. We build upon the theoretical model
of H{\Delta}H-distance and previous adversarial discriminative deep learning
method for ASC unsupervised domain adaptation, and we present an adversarial
training based method using the Wasserstein distance. We improve the
state-of-the-art mean accuracy on the data from the unseen conditions from 32%
to 45%, using the TUT Acoustic Scenes dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10727</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10727</id><created>2019-04-24</created><authors><author><keyname>Nguyen</keyname><forenames>Trung-Hien</forenames></author><author><keyname>Determe</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Van Eeckhaute</keyname><forenames>Mathieu</forenames></author><author><keyname>Louveaux</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>De Doncker</keyname><forenames>Philippe</forenames></author><author><keyname>Horlin</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Frequency-Domain Time-Reversal Precoding in Wideband MISO OFDM
  Communication Systems</title><categories>eess.SP</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time reversal (TR) recently emerged as an interesting communication
technology capable of providing a good spatio-temporal signal focusing effect.
New generations of large-bandwidth devices with reduced cost leverage the use
of TR wideband communication systems. While TR is usually implemented in the
time domain, the same benefit can be obtained in an orthogonal frequency
division multiplexing (OFDM) system by precoding the information in the
frequency domain. Besides using multiple antennas, the focusing effect of TR
also comes from the use of a high rate back-off factor (BOF), which is the
signal up-sampling (or down-sampling) rate in the original time-domain TR
precoding. However, a frequency-domain TR precoding in the literature has only
considered BOF of one, which does not fully exploit the focusing property of
TR. In this paper, we discuss how to properly implement different BOFs using
frequency-domain TR precoding in the OFDM system. Moreover, we demonstrate that
increasing the BOF and/or the number of transmit antennas significantly
improves the focusing gain at the intended position. In contrast, the
unintended positions receive less useful power. Furthermore, closed-form
approximations of the mean-square-errors (MSEs) of equalized received signals
at either intended or unintended positions are derived, expressing the focusing
gain as a function of the BOF and the number of antennas. Numerical simulations
with multi-path channels are carried out to validate the MSE expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10736</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10736</id><created>2019-04-24</created><authors><author><keyname>Blackwell</keyname><forenames>Robert</forenames></author><author><keyname>Harvey</keyname><forenames>Richard</forenames></author><author><keyname>Queste</keyname><forenames>Bastien</forenames></author><author><keyname>Fielding</keyname><forenames>Sophie</forenames></author></authors><title>Aliased seabed detection in fisheries acoustic data</title><categories>eess.SP</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aliased seabed echoes, also known as &quot;false bottoms&quot; or &quot;shadow bottoms&quot;, are
a form of echogram corruption caused by seabed reverberation from preceding
pings coinciding with echoes from the current ping. These aliases are usually
either avoided by adjusting the survey parameters, or identified and removed by
hand - a subjective and laborious process. This paper describes a simple
algorithm that uses volume backscatter and split-beam angle to detect and
remove aliased seabed using single frequency, split-beam echo sounder data
without the need for bathymetry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10739</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10739</id><created>2019-04-24</created><authors><author><keyname>Liu</keyname><forenames>Pengfei</forenames></author><author><keyname>Liu</keyname><forenames>Yimin</forenames></author><author><keyname>Huang</keyname><forenames>Tianyao</forenames></author><author><keyname>Lu</keyname><forenames>Yuxiang</forenames></author><author><keyname>Wang</keyname><forenames>Xiqin</forenames></author></authors><title>Cognitive Radar Using Reinforcement Learning in Automotive Applications</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of cognitive radar (CR) enables radar systems to achieve
intelligent adaption to a changeable environment with feedback facility from
receiver to transmitter. However, the implementation of CR in a fast-changing
environment usually requires a well-known environmental model. In our work, we
stress the learning ability of CR in an unknown environment using a combination
of CR and reinforcement learning (RL), called RL-CR. Less or no model of the
environment is required. We also apply the general RL-CR to a specific problem
of automotive radar spectrum allocation to mitigate mutual interference. Using
RL-CR, each vehicle can autonomously choose a frequency subband according to
its own observation of the environment. Since radar's single observation is
quite limited compared to the overall information of the environment, a long
short-term memory (LSTM) network is utilized so that radar can decide the next
transmitted subband by aggregating its observations over time. Compared with
centralized spectrum allocation approaches, our approach has the advantage of
reducing communication between vehicles and the control center. It also
outperforms some other distributive frequency subband selecting policies in
reducing interference under certain circumstances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10760</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10760</id><created>2019-04-23</created><authors><author><keyname>Guo</keyname><forenames>Michelle</forenames></author><author><keyname>Haque</keyname><forenames>Albert</forenames></author><author><keyname>Verma</keyname><forenames>Prateek</forenames></author></authors><title>End-to-End Spoken Language Translation</title><categories>cs.CL cs.SD eess.AS</categories><comments>Technical Report. Stanford University, 2017. arXiv admin note: text
  overlap with arXiv:1804.00047</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the task of spoken language understanding. We
present a method for translating spoken sentences from one language into spoken
sentences in another language. Given spectrogram-spectrogram pairs, our model
can be trained completely from scratch to translate unseen sentences. Our
method consists of a pyramidal-bidirectional recurrent network combined with a
convolutional network to output sentence-level spectrograms in the target
language. Empirically, our model achieves competitive performance with
state-of-the-art methods on multiple languages and can generalize to unseen
speakers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10763</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10763</id><created>2019-04-23</created><updated>2019-04-25</updated><authors><author><keyname>Lazzarini</keyname><forenames>Victor</forenames></author><author><keyname>Timoney</keyname><forenames>Joseph</forenames></author></authors><title>The Analogue Computer as a Voltage-Controlled Synthesiser</title><categories>eess.AS cs.SD</categories><msc-class>68U99</msc-class><acm-class>J.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper re-appraises the role of analogue computers within electronic and
computer music and provides some pointers to future areas of research. It
begins by introducing the idea of analogue computing and placing in the context
of sound and music applications. This is followed by a brief examination of the
classic constituents of an analogue computer, contrasting these with the
typical modular voltage-controlled synthesiser. Two examples are presented,
leading to a discussion on some parallels between these two technologies. This
is followed by an examination of the current state-of-the-art in analogue
computation and its prospects for applications in computer and electronic
music.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10788</identifier>
 <datestamp>2019-12-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10788</id><created>2019-04-23</created><updated>2019-05-09</updated><authors><author><keyname>Yoon</keyname><forenames>Seunghyun</forenames></author><author><keyname>Byun</keyname><forenames>Seokhyun</forenames></author><author><keyname>Dey</keyname><forenames>Subhadeep</forenames></author><author><keyname>Jung</keyname><forenames>Kyomin</forenames></author></authors><title>Speech Emotion Recognition Using Multi-hop Attention Mechanism</title><categories>eess.AS cs.AI cs.CL cs.LG cs.SD</categories><comments>5 pages, Accepted as a conference paper at ICASSP 2019 (oral
  presentation)</comments><doi>10.1109/ICASSP.2019.8683483</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are interested in exploiting textual and acoustic data of
an utterance for the speech emotion classification task. The baseline approach
models the information from audio and text independently using two deep neural
networks (DNNs). The outputs from both the DNNs are then fused for
classification. As opposed to using knowledge from both the modalities
separately, we propose a framework to exploit acoustic information in tandem
with lexical data. The proposed framework uses two bi-directional long
short-term memory (BLSTM) for obtaining hidden representations of the
utterance. Furthermore, we propose an attention mechanism, referred to as the
multi-hop, which is trained to automatically infer the correlation between the
modalities. The multi-hop attention first computes the relevant segments of the
textual data corresponding to the audio signal. The relevant textual data is
then applied to attend parts of the audio signal. To evaluate the performance
of the proposed system, experiments are performed in the IEMOCAP dataset.
Experimental results show that the proposed technique outperforms the
state-of-the-art system by 6.5% relative improvement in terms of weighted
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10844</identifier>
 <datestamp>2019-07-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10844</id><created>2019-04-24</created><updated>2019-06-28</updated><authors><author><keyname>Tato</keyname><forenames>Anxo</forenames></author><author><keyname>Mosquera</keyname><forenames>Carlos</forenames></author><author><keyname>Henarejos</keyname><forenames>Pol</forenames></author><author><keyname>P&#xe9;rez-Neira</keyname><forenames>Ana</forenames></author></authors><title>Neural Network Aided Computation of Mutual Information for Adaptation of
  Spatial Modulation</title><categories>eess.SP</categories><comments>Enhanced with some new results in the form of some 3D plot of the MI
  and new table entry, apart than from a slight change in the nomenclature</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Index Modulations, in the form of Spatial Modulation or Polarized Modulation,
are gaining traction for both satellite and terrestrial next generation
communication systems. Adaptive Index Modulation based links are needed to
fully exploit the transmission capacity of time-variant channels. The
adaptation of code and/or modulation requires a real-time evaluation of the
channel achievable rates. Some existing results in the literature present a
computational complexity which scales quadratically with the number of transmit
antennas and the constellation order. Moreover, the accuracy of these
approximations is low and it can lead to wrong Modulation and Coding Scheme
selection. In this work we apply a Multilayer Feedforward Neural Network to
compute the achievable rate of a generic Index Modulation link. The case of two
antennas/polarizations is analyzed throughly showing the neural network not
only a one-hundred fold decrement of the Mean Square Error in the estimation of
the capacity compared with existing analytical approximations, but it also
reduces fifty times the computational complexity. Moreover, the extension to an
arbitrary number of antennas is explained and supported with simulations. More
generally, neural networks can be considered as promising candidates for the
practical estimation of complex metrics in communication related settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10881</identifier>
 <datestamp>2019-09-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10881</id><created>2019-04-24</created><authors><author><keyname>Lai</keyname><forenames>Chuan-Chi</forenames></author><author><keyname>Chen</keyname><forenames>Chun-Ting</forenames></author><author><keyname>Wang</keyname><forenames>Li-Chun</forenames></author></authors><title>On-Demand Density-Aware UAV Base Station 3D Placement for Arbitrarily
  Distributed Users with Guaranteed Data Rates</title><categories>cs.NI cs.GT eess.SP</categories><comments>Accepted by IEEE Wireless Communications Letters</comments><doi>10.1109/LWC.2019.2899599</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we study the on-demand UAV-BS placement problem for
arbitrarily distributed users. This UAV-BS placement problem is modeled as a
knapsack-like problem, which is NP-complete. We propose a density-aware
placement algorithm to maximize the number of covered users subject to the
constraint of the minimum required data rates per user. Simulations are
conducted to evaluate the performance of the proposed algorithm in a real
environment with different user densities. Our numerical results indicate that
for various user densities our proposed solution can service more users with
guaranteed data rates compared to the existing method, while reducing the
transmit power by 29%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10882</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10882</id><created>2019-04-24</created><authors><author><keyname>Wu</keyname><forenames>Xiongwei</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Li</keyname><forenames>Xiuhua</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author><author><keyname>Ching</keyname><forenames>P. C.</forenames></author></authors><title>Joint Long-Term Cache Allocation and Short-Term Content Delivery in
  Green Cloud Small Cell Networks</title><categories>cs.IT eess.SP math.IT</categories><comments>ICC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed an exponential growth of mobile data traffic,
which may lead to a serious traffic burn on the wireless networks and
considerable power consumption. Network densification and edge caching are
effective approaches to addressing these challenges. In this study, we
investigate joint long-term cache allocation and short-term content delivery in
cloud small cell networks (C-SCNs), where multiple smallcell BSs (SBSs) are
connected to the central processor via fronthaul and can store popular contents
so as to reduce the duplicated transmissions in networks. Accordingly, a
long-term power minimization problem is formulated by jointly optimizing
multicast beamforming, BS clustering, and cache allocation under quality of
service (QoS) and storage constraints. The resultant mixed timescale design
problem is an anticausal problem because the optimal cache allocation depends
on the future file requests. To handle it, a two-stage optimization scheme is
proposed by utilizing historical knowledge of users' requests and channel state
information. Specifically, the online content delivery design is tackled with a
penalty-based approach, and the periodic cache updating is optimized with a
distributed alternating method. Simulation results indicate that the proposed
scheme significantly outperforms conventional schemes and performs extremely
close to a genie-aided lower bound in the low caching region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10947</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10947</id><created>2019-04-24</created><updated>2019-08-30</updated><authors><author><keyname>Pasad</keyname><forenames>Ankita</forenames></author><author><keyname>Shi</keyname><forenames>Bowen</forenames></author><author><keyname>Kamper</keyname><forenames>Herman</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author></authors><title>On the Contributions of Visual and Textual Supervision in Low-Resource
  Semantic Speech Retrieval</title><categories>cs.CL cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that speech paired with images can be used to learn
semantically meaningful speech representations even without any textual
supervision. In real-world low-resource settings, however, we often have access
to some transcribed speech. We study whether and how visual grounding is useful
in the presence of varying amounts of textual supervision. In particular, we
consider the task of semantic speech retrieval in a low-resource setting. We
use a previously studied data set and task, where models are trained on images
with spoken captions and evaluated on human judgments of semantic relevance. We
propose a multitask learning approach to leverage both visual and textual
modalities, with visual supervision in the form of keyword probabilities from
an external tagger. We find that visual grounding is helpful even in the
presence of textual supervision, and we analyze this effect over a range of
sizes of transcribed data sets. With ~5 hours of transcribed speech, we obtain
23% higher average precision when also using visual supervision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10960</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10960</id><created>2019-04-24</created><authors><author><keyname>Tachibana</keyname><forenames>Yasuhiko</forenames><affiliation>Applied MRI Research, Department of Molecular imaging and Theranostics, National Institute of Radiological Sciences, QST</affiliation><affiliation>Department of Radiology, Juntendo University School of Medicine</affiliation></author><author><keyname>Hagiwara</keyname><forenames>Akifumi</forenames><affiliation>Department of Radiology, Juntendo University School of Medicine</affiliation><affiliation>Department of Radiology, Graduate School of Medicine, The University of Tokyo</affiliation></author><author><keyname>Hori</keyname><forenames>Masaaki</forenames><affiliation>Department of Radiology, Juntendo University School of Medicine</affiliation></author><author><keyname>Kershaw</keyname><forenames>Jeff</forenames><affiliation>Applied MRI Research, Department of Molecular imaging and Theranostics, National Institute of Radiological Sciences, QST</affiliation></author><author><keyname>Nakazawa</keyname><forenames>Misaki</forenames><affiliation>Department of Radiology, Juntendo University School of Medicine</affiliation></author><author><keyname>Omatsu</keyname><forenames>Tokuhiko</forenames><affiliation>Applied MRI Research, Department of Molecular imaging and Theranostics, National Institute of Radiological Sciences, QST</affiliation></author><author><keyname>Kishimoto</keyname><forenames>Riwa</forenames><affiliation>Applied MRI Research, Department of Molecular imaging and Theranostics, National Institute of Radiological Sciences, QST</affiliation></author><author><keyname>Yokoyama</keyname><forenames>Kazumasa</forenames><affiliation>Department of Neurology, Juntendo University School of Medicine</affiliation></author><author><keyname>Hattori</keyname><forenames>Nobutaka</forenames><affiliation>Department of Neurology, Juntendo University School of Medicine</affiliation></author><author><keyname>Aoki</keyname><forenames>Shigeki</forenames><affiliation>Department of Radiology, Juntendo University School of Medicine</affiliation></author><author><keyname>Higashi</keyname><forenames>Tatsuya</forenames><affiliation>Department of Molecular imaging and Theranostics, National Institute of Radiological Sciences, QST</affiliation></author><author><keyname>Obata</keyname><forenames>Takayuki</forenames><affiliation>Applied MRI Research, Department of Molecular imaging and Theranostics, National Institute of Radiological Sciences, QST</affiliation><affiliation>Department of Molecular imaging and Theranostics, National Institute of Radiological Sciences, QST</affiliation></author></authors><title>The utility of a convolutional neural network for generating a myelin
  volume index map from rapid simultaneous relaxometry imaging</title><categories>eess.IV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background and Purpose: A current algorithm to obtain a synthetic myelin
volume fraction map (SyMVF) from rapid simultaneous relaxometry imaging (RSRI)
has a potential problem, that it does not incorporate information from
surrounding pixels. The purpose of this study was to develop a method that
utilizes a convolutional neural network (CNN) to overcome this problem.
Methods: RSRI and magnetization transfer images from 20 healthy volunteers were
included. A CNN was trained to reconstruct RSRI-related metric maps into a
myelin volume-related index (generated myelin volume index: GenMVI) map using
the myelin volume index map calculated from magnetization transfer images
(MTMVI) as reference. The SyMVF and GenMVI maps were statistically compared by
testing how well they correlated with the MTMVI map. The correlations were
evaluated based on: (i) averaged values obtained from 164 atlas-based ROIs, and
(ii) pixel-based comparison for ROIs defined in four different tissue types
(cortical and subcortical gray matter, white matter, and whole brain). Results:
For atlas-based ROIs, the overall correlation with the MTMVI map was higher for
the GenMVI map than for the SyMVF map. In the pixel-based comparison,
correlation with the MTMVI map was stronger for the GenMVI map than for the
SyMVF map, and the difference in the distribution for the volunteers was
significant (Wilcoxon sign-rank test, P&lt;.001) in all tissue types. Conclusion:
The proposed method is useful, as it can incorporate more specific information
about local tissue properties than the existing method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10987</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10987</id><created>2019-04-24</created><authors><author><keyname>Mathias</keyname><forenames>Luis Carlos</forenames></author><author><keyname>Filho</keyname><forenames>Jose Carlos Marinello</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>Pre-distortion and Pre-equalization for Non-Linearities and Low-Pass
  Effect Mitigation in OFDM-VLC Systems</title><categories>eess.SP</categories><comments>25 pages, 14 figures, 2 tables</comments><doi>10.1364/AO.58.005328</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The orthogonal frequency division multiplexing (OFDM) transmission has shown
promise in applications of visible light communication (VLC). However, the
variation of the nonlinearity of the optical power emitted by the high power
light emitting diode (HPLED) as a function of current and temperature implies
in drastic OFDM-VLC performance degradation. The first part of this work,
experimentally confirms and models this degradation due to temperature in a
high power white HPLED. The higher attenuation at high frequencies, which is
inherent to the HPLED and which is accentuated by the effect of the intrinsic
capacitance of the photodiode, is another factor of degradation due to the
reduction of the signal-to-noise ratio (SNR) at the receiver for such
frequencies. For the mitigation of these effects, we propose a pre-distortion
and digital pre-equalization scheme using a luminous feedback signal in the
transmitter module. The system is modeled so that the operating points are
mathematically deduced and evaluated by simulations and by an experimental
setup. By allowing the linearization of the transmitted light signal and the
maintenance of an average SNR in all OFDM subcarriers, the performance
improvement is confirmed in comparison with other schemes, such as with
non-predistortion, pre-distortion with fixed parameters, and simple
post-equalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.10990</identifier>
 <datestamp>2019-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.10990</id><created>2019-04-24</created><updated>2019-11-25</updated><authors><author><keyname>Esmaeilpour</keyname><forenames>Mohammad</forenames></author><author><keyname>Cardinal</keyname><forenames>Patrick</forenames></author><author><keyname>Koerich</keyname><forenames>Alessandro Lameiras</forenames></author></authors><title>A Robust Approach for Securing Audio Classification Against Adversarial
  Attacks</title><categories>cs.LG cs.CR cs.SD eess.AS stat.ML</categories><comments>Paper Accepted for Publication in IEEE Transactions on Information
  Forensics and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial audio attacks can be considered as a small perturbation
unperceptive to human ears that is intentionally added to the audio signal and
causes a machine learning model to make mistakes. This poses a security concern
about the safety of machine learning models since the adversarial attacks can
fool such models toward the wrong predictions. In this paper we first review
some strong adversarial attacks that may affect both audio signals and their 2D
representations and evaluate the resiliency of the most common machine learning
model, namely deep learning models and support vector machines (SVM) trained on
2D audio representations such as short time Fourier transform (STFT), discrete
wavelet transform (DWT) and cross recurrent plot (CRP) against several
state-of-the-art adversarial attacks. Next, we propose a novel approach based
on pre-processed DWT representation of audio signals and SVM to secure audio
systems against adversarial attacks. The proposed architecture has several
preprocessing modules for generating and enhancing spectrograms including
dimension reduction and smoothing. We extract features from small patches of
the spectrograms using speeded up robust feature (SURF) algorithm which are
further used to generate a codebook using the K-Means++ algorithm. Finally,
codewords are used to train a SVM on the codebook of the SURF-generated
vectors. All these steps yield to a novel approach for audio classification
that provides a good trade-off between accuracy and resilience. Experimental
results on three environmental sound datasets show the competitive performance
of proposed approach compared to the deep neural networks both in terms of
accuracy and robustness against strong adversarial attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11022</identifier>
 <datestamp>2019-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11022</id><created>2019-04-20</created><updated>2019-11-26</updated><authors><author><keyname>Belmekki</keyname><forenames>Baha Eddine Youcef</forenames></author><author><keyname>Hamza</keyname><forenames>Abdelkrim</forenames></author><author><keyname>Escrig</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>Outage Analysis of Cooperative NOMA in Millimeter Wave Vehicular Network
  at Intersections</title><categories>eess.SP cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper, we study the impact and the improvement of using cooperative
non-orthogonal multiple access scheme (NOMA) on a millimeter wave (mmWave)
vehicular network at intersection roads. The intersections consists of two
perpendicular roads. The transmission occurs between a source, and two
destinations nodes with a help of a relay. We assume that the interference
comes from as set of vehicles that are distributed as a one dimensional
homogeneous Poisson point process (PPP). We derive closed form outage
probability expressions for cooperative NOMA, and compare them with cooperative
orthogonal multiple access (OMA). We show that, NOMA offers a significant
improvement, especially for high data rates. However, there a condition imposed
to the data rate, otherwise, the performance of NOMA will decreases
dramatically. We show that as the nodes approach the intersection, the outage
probability increases. Counter-intuitively, We show that, the non line of sigh
(NLOS) scenario has a better performance than the line of sigh (LOS) scenario.
The analysis is conducted using tools from stochastic geometry and is verified
with Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11031</identifier>
 <datestamp>2020-01-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11031</id><created>2019-04-24</created><authors><author><keyname>Behboodi</keyname><forenames>Bahareh</forenames></author><author><keyname>Rivaz</keyname><forenames>Hassan</forenames></author></authors><title>Ultrasound segmentation using U-Net: learning from simulated data and
  testing on real data</title><categories>eess.IV cs.CV</categories><comments>Accepted in EMBC 2019</comments><doi>10.1109/EMBC.2019.8857218</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmentation of ultrasound images is an essential task in both diagnosis and
image-guided interventions given the ease-of-use and low cost of this imaging
modality. As manual segmentation is tedious and time consuming, a growing body
of research has focused on the development of automatic segmentation
algorithms. Deep learning algorithms have shown remarkable achievements in this
regard; however, they need large training datasets. Unfortunately, preparing
large labeled datasets in ultrasound images is prohibitively difficult.
Therefore, in this study, we propose the use of simulated ultrasound (US)
images for training the U-Net deep learning segmentation architecture and test
on tissue-mimicking phantom data collected by an ultrasound machine. We
demonstrate that the trained architecture on the simulated data is
transferrable to real data, and therefore, simulated data can be considered as
an alternative training dataset when real datasets are not available. The
second contribution of this paper is that we train our U- Net network on
envelope and B-mode images of the simulated dataset, and test the trained
network on real envelope and B- mode images of phantom, respectively. We show
that test results are superior for the envelope data compared to B-mode image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11074</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11074</id><created>2019-04-24</created><authors><author><keyname>Arronte-Alvarez</keyname><forenames>Aitor</forenames></author><author><keyname>Gomez-Martin</keyname><forenames>Francisco</forenames></author></authors><title>An Attentional Neural Network Architecture for Folk Song Classification</title><categories>cs.SD cs.CL eess.AS</categories><comments>Accepted for ICMC 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we present an attentional neural network for folk song
classification. We introduce the concept of musical motif embedding, and show
how using melodic local context we are able to model monophonic folk song
motifs using the skipgram version of the word2vec algorithm. We use the motif
embeddings to represent folk songs from Germany, China, and Sweden, and
classify them using an attentional neural network that is able to discern
relevant motifs in a song. The results show how the network obtains state of
the art accuracy in a completely unsupervised manner, and how motif embeddings
produce high quality motif representations from folk songs. We conjecture on
the advantages of this type of representation in large symbolic music corpora,
and how it can be helpful in the musicological analysis of folk song
collections from different cultures and geographical areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11075</identifier>
 <datestamp>2019-05-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11075</id><created>2019-04-24</created><updated>2019-05-23</updated><authors><author><keyname>Cui</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Jun</forenames></author><author><keyname>Zhou</keyname><forenames>Zhou</forenames></author></authors><title>State-domain Change Point Detection for Nonlinear Time Series Regression</title><categories>stat.ME eess.SP math.ST stat.TH</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change point detection in time series has attracted substantial interest, but
most of the existing results have been focused on detecting change points in
the time domain. This paper considers the situation where nonlinear time series
have potential change points in the state domain. We apply a density-weighted
anti-symmetric kernel function to the state domain and therefore propose a
nonparametric procedure to test the existence of change points. When the
existence of change points is affirmative, we further introduce an algorithm to
estimate their number together with locations and show the convergence result
on the estimation procedure. A real dataset of the U.S. Treasury yield rates is
used to illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11125</identifier>
 <datestamp>2019-11-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11125</id><created>2019-04-24</created><updated>2019-11-21</updated><authors><author><keyname>Pandey</keyname><forenames>Amritanshu</forenames></author><author><keyname>Agarwal</keyname><forenames>Aayushya</forenames></author><author><keyname>Jereminov</keyname><forenames>Marko</forenames></author><author><keyname>Wagner</keyname><forenames>Martin R.</forenames></author><author><keyname>Bromberg</keyname><forenames>David M.</forenames></author><author><keyname>Pileggi</keyname><forenames>Larry</forenames></author></authors><title>Robust Sequential Steady-State Analysis of Cascading Outages</title><categories>eess.SP</categories><comments>Presented in IEEE PES Innovative Smart Grid Technologies Europe
  Conference, Bucharest, Romania, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulating potential cascading failures can be useful for avoiding or
mitigating such events. Currently, existing steady-state analysis tools are
ill-suited for simulating cascading outages as they do not model frequency
dependencies, they require good initial conditions to converge, and they are
unable to distinguish between a collapsed grid state from a hard-to-solve test
case. In this paper, we extend a circuit-theoretic approach for simulating the
steady-state of a power grid to incorporate frequency deviations and implicit
models for underfrequency and undervoltage load shedding. Using these models,
we introduce a framework capable of robustly solving cascading outages of
large-scale systems that can also locate infeasible regions. We demonstrate the
efficacy of our approach by simulating entire cascading outages on more than
8000 nodes sample testcase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11126</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11126</id><created>2019-04-24</created><authors><author><keyname>Alom</keyname><forenames>Md Zahangir</forenames></author><author><keyname>Aspiras</keyname><forenames>Theus</forenames></author><author><keyname>Taha</keyname><forenames>Tarek M.</forenames></author><author><keyname>Asari</keyname><forenames>Vijayan K.</forenames></author></authors><title>Skin Cancer Segmentation and Classification with NABLA-N and Inception
  Recurrent Residual Convolutional Networks</title><categories>cs.CV eess.IV</categories><comments>7 pages, 7 figures, 2 Tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the last few years, Deep Learning (DL) has been showing superior
performance in different modalities of biomedical image analysis. Several DL
architectures have been proposed for classification, segmentation, and
detection tasks in medical imaging and computational pathology. In this paper,
we propose a new DL architecture, the NABLA-N network, with better feature
fusion techniques in decoding units for dermoscopic image segmentation tasks.
The NABLA-N network has several advances for segmentation tasks. First, this
model ensures better feature representation for semantic segmentation with a
combination of low to high-level feature maps. Second, this network shows
better quantitative and qualitative results with the same or fewer network
parameters compared to other methods. In addition, the Inception Recurrent
Residual Convolutional Neural Network (IRRCNN) model is used for skin cancer
classification. The proposed NABLA-N network and IRRCNN models are evaluated
for skin cancer segmentation and classification on the benchmark datasets from
the International Skin Imaging Collaboration 2018 (ISIC-2018). The experimental
results show superior performance on segmentation tasks compared to the
Recurrent Residual U-Net (R2U-Net). The classification model shows around 87%
testing accuracy for dermoscopic skin cancer classification on ISIC2018
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11130</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11130</id><created>2019-04-24</created><authors><author><keyname>He</keyname><forenames>Liang</forenames></author><author><keyname>Chen</keyname><forenames>Xianhong</forenames></author><author><keyname>Xu</keyname><forenames>Can</forenames></author><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Liu</keyname><forenames>Jia</forenames></author><author><keyname>Johnson</keyname><forenames>Michael T</forenames></author></authors><title>Latent Class Model with Application to Speaker Diarization</title><categories>eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply a latent class model (LCM) to the task of speaker
diarization. LCM is similar to Patrick Kenny's variational Bayes (VB) method in
that it uses soft information and avoids premature hard decisions in its
iterations. In contrast to the VB method, which is based on a generative model,
LCM provides a framework allowing both generative and discriminative models.
The discriminative property is realized through the use of i-vector (Ivec),
probabilistic linear discriminative analysis (PLDA), and a support vector
machine (SVM) in this work. Systems denoted as LCM-Ivec-PLDA, LCM-Ivec-SVM, and
LCM-Ivec-Hybrid are introduced. In addition, three further improvements are
applied to enhance its performance. 1) Adding neighbor windows to extract more
speaker information for each short segment. 2) Using a hidden Markov model to
avoid frequent speaker change points. 3) Using an agglomerative hierarchical
cluster to do initialization and present hard and soft priors, in order to
overcome the problem of initial sensitivity. Experiments on the National
Institute of Standards and Technology Rich Transcription 2009 speaker
diarization database, under the condition of a single distant microphone, show
that the diarization error rate (DER) of the proposed methods has substantial
relative improvements compared with mainstream systems. Compared to the VB
method, the relative improvements of LCM-Ivec-PLDA, LCM-Ivec-SVM, and
LCM-Ivec-Hybrid systems are 23.5%, 27.1%, and 43.0%, respectively. Experiments
on our collected database, CALLHOME97, CALLHOME00 and SRE08 short2-summed trial
conditions also show that the proposed LCM-Ivec-Hybrid system has the best
overall performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11148</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11148</id><created>2019-04-24</created><authors><author><keyname>Liu</keyname><forenames>Yuzhou</forenames></author><author><keyname>Wang</keyname><forenames>DeLiang</forenames></author></authors><title>Divide and Conquer: A Deep CASA Approach to Talker-independent Monaural
  Speaker Separation</title><categories>cs.SD cs.LG eess.AS</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address talker-independent monaural speaker separation from the
perspectives of deep learning and computational auditory scene analysis (CASA).
Specifically, we decompose the multi-speaker separation task into the stages of
simultaneous grouping and sequential grouping. Simultaneous grouping is first
performed in each time frame by separating the spectra of different speakers
with a permutation-invariantly trained neural network. In the second stage, the
frame-level separated spectra are sequentially grouped to different speakers by
a clustering network. The proposed deep CASA approach optimizes frame-level
separation and speaker tracking in turn, and produces excellent results for
both objectives. Experimental results on the benchmark WSJ0-2mix database show
that the new approach achieves the state-of-the-art results with a modest model
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11163</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11163</id><created>2019-04-25</created><authors><author><keyname>Thakur</keyname><forenames>Ravi Kumar</forenames></author><author><keyname>Mukherjee</keyname><forenames>Snehasis</forenames></author></authors><title>A Conditional Adversarial Network for Scene Flow Estimation</title><categories>cs.CV cs.LG cs.RO eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of Scene flow estimation in depth videos has been attracting
attention of researchers of robot vision, due to its potential application in
various areas of robotics. The conventional scene flow methods are difficult to
use in reallife applications due to their long computational overhead. We
propose a conditional adversarial network SceneFlowGAN for scene flow
estimation. The proposed SceneFlowGAN uses loss function at two ends: both
generator and descriptor ends. The proposed network is the first attempt to
estimate scene flow using generative adversarial networks, and is able to
estimate both the optical flow and disparity from the input stereo images
simultaneously. The proposed method is experimented on a large RGB-D benchmark
sceneflow dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11176</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11176</id><created>2019-04-25</created><updated>2019-08-31</updated><authors><author><keyname>Kim</keyname><forenames>Soo Ye</forenames></author><author><keyname>Oh</keyname><forenames>Jihyong</forenames></author><author><keyname>Kim</keyname><forenames>Munchurl</forenames></author></authors><title>Deep SR-ITM: Joint Learning of Super-Resolution and Inverse Tone-Mapping
  for 4K UHD HDR Applications</title><categories>eess.IV cs.CV</categories><comments>Accepted at ICCV 2019 (Oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent modern displays are now able to render high dynamic range (HDR), high
resolution (HR) videos of up to 8K UHD (Ultra High Definition). Consequently,
UHD HDR broadcasting and streaming have emerged as high quality premium
services. However, due to the lack of original UHD HDR video content,
appropriate conversion technologies are urgently needed to transform the legacy
low resolution (LR) standard dynamic range (SDR) videos into UHD HDR versions.
In this paper, we propose a joint super-resolution (SR) and inverse
tone-mapping (ITM) framework, called Deep SR-ITM, which learns the direct
mapping from LR SDR video to their HR HDR version. Joint SR and ITM is an
intricate task, where high frequency details must be restored for SR, jointly
with the local contrast, for ITM. Our network is able to restore fine details
by decomposing the input image and focusing on the separate base (low
frequency) and detail (high frequency) layers. Moreover, the proposed
modulation blocks apply location-variant operations to enhance local contrast.
The Deep SR-ITM shows good subjective quality with increased contrast and
details, outperforming the previous joint SR-ITM method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11184</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11184</id><created>2019-04-25</created><authors><author><keyname>Aziz</keyname><forenames>Farhan M.</forenames></author><author><keyname>Li</keyname><forenames>Lichun</forenames></author><author><keyname>Shamma</keyname><forenames>Jeff S.</forenames></author><author><keyname>Stuber</keyname><forenames>Gordon L.</forenames></author></authors><title>Smart Jammer and LTE Network Strategies in An Infinite-Horizon Zero-Sum
  Repeated Game with Asymmetric and Incomplete Information</title><categories>cs.GT cs.SY eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE/LTE-Advanced networks are known to be vulnerable to denial-of-service and
loss-of-service attacks from smart jammers. In this article, the interaction
between a smart jammer and LTE network is modeled as an infinite-horizon,
zero-sum, asymmetric repeated game. The smart jammer and eNode B are modeled as
the informed and the uninformed player, respectively. The main purpose of this
article is to construct efficient suboptimal strategies for both players that
can be used to solve the above-mentioned infinite-horizon repeated game with
asymmetric and incomplete information. It has been shown in game-theoretic
literature that security strategies provide optimal solution in zero-sum games.
It is also shown that both players' security strategies in an infinite-horizon
asymmetric game depend only on the history of the informed player's actions.
However, fixed-sized sufficient statistics are needed for both players to solve
the above-mentioned game efficiently. The smart jammer uses its evolving belief
state as the fixed-sized sufficient statistics for the repeated game. Whereas,
the LTE network (uninformed player) uses worst-case regret of its security
strategy and its anti-discounted update as the fixed-sized sufficient
statistics. Although fixed-sized sufficient statistics are employed by both
players, optimal security strategy computation in {\lambda}-discounted
asymmetric games is still hard to perform because of non-convexity. Hence, the
problem is convexified in this article by devising `approximated' security
strategies for both players that are based on approximated optimal game value.
However, `approximated' strategies require full monitoring. Therefore, a
simplistic yet effective `expected' strategy is also constructed for the LTE
network that does not require full monitoring. The simulation results show that
the smart jammer plays non-revealing and misleading strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11226</identifier>
 <datestamp>2019-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11226</id><created>2019-04-25</created><authors><author><keyname>Arnould</keyname><forenames>Aymeric</forenames></author><author><keyname>Ghazisaeidi</keyname><forenames>Amirhossein</forenames></author></authors><title>Equalization Enhanced Phase Noise in Coherent Receivers: DSP-Aware
  Analysis and Shaped Constellations</title><categories>eess.SP</categories><doi>10.1109/JLT.2019.2931841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the analysis of equalization-enhanced phase noise (EEPN) arising
in coherent receivers from the interaction between the chromatic dispersion
compensation by an electronic equalizer and the phase noise of the local
oscillator. Through numerical simulations we highlight EEPN characteristics and
investigate its impact on the behavior of the carrier phase recovery algorithm.
We show that the blind phase search, which is usually used in practice to
recover the carrier phase, partially mitigates the EEPN. We detail a numerical
approach to predict the system performance including the phase recovery
algorithm, and show that taking into account EEPN characteristics relaxes the
constraint on the system laser phase noise given by previous pessimistic
analytical models. We present experimental validations of our claims, and
address future advanced transoceanic systems using 98 GBd probabilistically
shaped QAM formats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11258</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11258</id><created>2019-04-25</created><authors><author><keyname>Das</keyname><forenames>Sumanta Kumar</forenames></author><author><keyname>Singh</keyname><forenames>Randhir</forenames></author></authors><title>Performance of Kriging Based Soft Classification on WiFS/IRS- 1D image
  using Ground Hyperspectral Signatures</title><categories>eess.IV stat.AP</categories><comments>5 pages,3 figures 3 tables</comments><journal-ref>IEEE GEPSCIENCE AND REMOTE SENSING LETTERS, 2009, VOL 6, issue 3</journal-ref><doi>10.1109/LGRS.2008.2005851</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hard and soft classification techniques are the conventional ways of image
classification on satellite data. These classifiers have number of drawbacks.
Firstly, these approaches are inappropriate for mixed pixels. Secondly, these
approaches do not consider spatial variability. Kriging based soft classifier
(KBSC) is a non-parametric geostatistical method. It exploits the spatial
variability of the classes within the image. This letter compares the
performance of KBSC with other conventional hard/soft classification
techniques. The satellite data used in this study is the Wide Field Sensor
(WiFS) from the Indian Remote Sensing Satellite -1D (IRS-1D). The ground
hyperspectral signatures acquired from the agricultural fields by a hand held
spectroradiometer are used to detect subpixel targets from the satellite
images. Two measures of closeness have been used for accuracy assessment of the
KBSC to that of the conventional classifications. The results prove that the
KBSC is statistically more accurate than the other conventional techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11293</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11293</id><created>2019-04-25</created><authors><author><keyname>Mirjalili</keyname><forenames>Fereshteh</forenames></author><author><keyname>Luo</keyname><forenames>Ming Ronnier</forenames></author><author><keyname>Cui</keyname><forenames>Guihua</forenames></author><author><keyname>Morovic</keyname><forenames>Jan</forenames></author></authors><title>A color-difference formula for evaluating color pairs with no separation
  -- $\Delta E_{NS}$</title><categories>eess.IV</categories><journal-ref>Journal of the Optical Society of America A Vol. 36, Issue 5, pp.
  789-799 (2019)</journal-ref><doi>10.1364/JOSAA.36.000789</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All color-difference formulas are developed to evaluate color differences for
pairs of stimuli with hair-line separation. In printing applications, however,
color differences are frequently judged between a pair of samples with
no-separation because they are printed adjacent on the same piece of paper. A
new formula, dENS has been developed for pairs of stimuli with no-separation
(NS). An experiment was conducted to investigate the effect of different
color-difference magnitudes using sample pairs with NS. 1,012 printed pairs
with NS were prepared around 11 CIE recommended color centers. The pairs,
representing four color-difference magnitudes of 1, 2, 4 and 8 CIELAB units
were visually evaluated by a panel of 19 observers using the gray-scale method.
Comparison of the present data based on pairs with NS, and previously generated
data using pairs with hair-line separation, showed a clear separation effect. A
new color-difference equation for the NS viewing condition (dENS) is proposed
by modifying the CIEDE2000 formula. The separation effect can be well described
by the new formula. For a sample pair with NS, when the CIEDE2000 color
difference is less than 9.1, a larger color difference leads to a larger
lightness difference, and thus the total color difference increases. When the
CIEDE2000 color difference is greater than 9.1, the effect is opposite, i.e.
the lightness difference decreases, and thus the total color difference also
decreases. The new formula is recommended for future research to evaluate its
performance in appropriate applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11296</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11296</id><created>2019-04-25</created><authors><author><keyname>Itani</keyname><forenames>Sarah</forenames></author><author><keyname>Thanou</keyname><forenames>Dorina</forenames></author></authors><title>Combining Anatomical and Functional Networks for Neuropathology
  Identification: A Case Study on Autism Spectrum Disorder</title><categories>eess.IV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the prevalence of Autism Spectrum Disorder (ASD) is increasing,
research towards the definition of a common etiology is still ongoing. In this
regard, modern machine learning and network science pave the way for a better
understanding of the pathology and the development of diagnosis aid systems. At
the same time, the culture of data sharing heads favorably in that direction,
with the availability of large datasets such as the Autism Brain Imaging Data
Exchange (ABIDE) one. The present work addresses the classification of
neurotypical and ASD subjects by combining knowledge about both the anatomy and
the functional activity of the brain. In particular, we model the brain
structure as a graph, and the time-varying resting-state functional MRI
(rs-fMRI) signals as values that live on the nodes of that graph. We then
borrow tools from the emerging field of Graph Signal Processing (GSP) to build
features related to the frequency content of these signals. In order to make
these features highly discriminative, we apply an extension of the
Fukunaga-Koontz transform. Finally, we use these new markers to train a
decision tree, an interpretable classification scheme, which results in a final
diagnosis aid model. Interestingly, the resulting decision tree outperforms
state-of-the-art methods on the ABIDE dataset. Moreover, the analysis of the
predictive markers reveals the influence of the frontal and temporal lobes in
the diagnosis of the disorder, which is in line with previous findings in the
literature of neuroscience. Our results indicate that exploiting jointly
structural and functional information of the brain can reveal important
information about the complexity of the neuropathology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11301</identifier>
 <datestamp>2019-08-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11301</id><created>2019-04-25</created><updated>2019-08-19</updated><authors><author><keyname>I&#x15f;&#x131;l</keyname><forenames>&#xc7;a&#x11f;atay</forenames></author><author><keyname>Oktem</keyname><forenames>Figen S.</forenames></author><author><keyname>Ko&#xe7;</keyname><forenames>Aykut</forenames></author></authors><title>Deep Iterative Reconstruction for Phase Retrieval</title><categories>eess.IV cs.LG</categories><comments>14 pages, 8 figures, published in Applied Optics (Vol. 58, Issue 20,
  pp. 5422-5431 (2019))</comments><journal-ref>\c{C}a\u{g}atay I\c{s}{\i}l, Figen S. Oktem, and Aykut Ko\c{c},
  &quot;Deep iterative reconstruction for phase retrieval,&quot; Appl. Opt. 58, 5422-5431
  (2019)</journal-ref><doi>10.1364/AO.58.005422</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical phase retrieval problem is the recovery of a constrained image from
the magnitude of its Fourier transform. Although there are several well-known
phase retrieval algorithms including the hybrid input-output (HIO) method, the
reconstruction performance is generally sensitive to initialization and
measurement noise. Recently, deep neural networks (DNNs) have been shown to
provide state-of-the-art performance in solving several inverse problems such
as denoising, deconvolution, and superresolution. In this work, we develop a
phase retrieval algorithm that utilizes two DNNs together with the model-based
HIO method. First, a DNN is trained to remove the HIO artifacts and is used
iteratively with the HIO method to improve the reconstructions. After this
iterative phase, a second DNN is trained to remove the remaining artifacts.
Numerical results demonstrate the effectiveness of ourapproach, which has
little additional computational cost compared to the HIO method. Our approach
not only achieves state-of-the-art reconstruction performance but also is more
robust to different initialization and noise levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11303</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11303</id><created>2019-04-25</created><authors><author><keyname>Amichi</keyname><forenames>Licia</forenames></author><author><keyname>Kaneko</keyname><forenames>Megumi</forenames></author><author><keyname>Fukuda</keyname><forenames>Ellen Hidemi</forenames></author><author><keyname>Rachkidy</keyname><forenames>Nancy El</forenames></author><author><keyname>Guitton</keyname><forenames>Alexandre</forenames></author></authors><title>Joint Allocation Strategies of Power and Spreading Factors with
  Imperfect Orthogonality in LoRa Networks</title><categories>cs.NI eess.SP</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The LoRa physical layer is one of the most promising Low Power Wide-Area
Network (LPWAN) technologies for future Internet of Things (IoT) applications.
It provides a flexible adaptation of coverage and data rate by allocating
different Spreading Factors (SFs) and transmit powers to end-devices. We focus
on improving throughput fairness while reducing energy consumption. Whereas
most existing methods assume perfect SF orthogonality and ignore the harmful
effects of inter-SF interferences, we formulate a joint SF and power allocation
problem to maximize the minimum uplink throughput of end-devices, subject to
co-SF and inter-SF interferences, and power constraints. This results into a
mixed-integer non-linear optimization, which, for tractability, is split into
two sub-problems: firstly, the SF assignment for fixed transmit powers, and
secondly, the power allocation given the previously obtained assignment
solution. For the first sub-problem, we propose a low-complexity many-to-one
matching algorithm between SFs and end-devices. For the second one, given its
intractability, we transform it using two types of constraints approximation: a
linearized and a quadratic version. Our performance evaluation demonstrates
that the proposed joint SF allocation and power optimization enables to
drastically enhance various performance objectives such as throughput, fairness
and power consumption, and that it outperforms baseline schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11319</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11319</id><created>2019-04-25</created><updated>2019-07-23</updated><authors><author><keyname>Dalca</keyname><forenames>Adrian V.</forenames></author><author><keyname>Yu</keyname><forenames>Evan</forenames></author><author><keyname>Golland</keyname><forenames>Polina</forenames></author><author><keyname>Fischl</keyname><forenames>Bruce</forenames></author><author><keyname>Sabuncu</keyname><forenames>Mert R.</forenames></author><author><keyname>Iglesias</keyname><forenames>Juan Eugenio</forenames></author></authors><title>Unsupervised Deep Learning for Bayesian Brain MRI Segmentation</title><categories>cs.CV eess.IV</categories><comments>MICCAI 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic atlas priors have been commonly used to derive adaptive and
robust brain MRI segmentation algorithms. Widely-used neuroimage analysis
pipelines rely heavily on these techniques, which are often computationally
expensive. In contrast, there has been a recent surge of approaches that
leverage deep learning to implement segmentation tools that are computationally
efficient at test time. However, most of these strategies rely on learning from
manually annotated images. These supervised deep learning methods are therefore
sensitive to the intensity profiles in the training dataset. To develop a deep
learning-based segmentation model for a new image dataset (e.g., of different
contrast), one usually needs to create a new labeled training dataset, which
can be prohibitively expensive, or rely on suboptimal ad hoc adaptation or
augmentation approaches. In this paper, we propose an alternative strategy that
combines a conventional probabilistic atlas-based segmentation with deep
learning, enabling one to train a segmentation model for new MRI scans without
the need for any manually segmented images. Our experiments include thousands
of brain MRI scans and demonstrate that the proposed method achieves good
accuracy for a brain MRI segmentation task for different MRI contrasts,
requiring only approximately 15 seconds at test time on a GPU. The code is
freely available at http://voxelmorph.mit.edu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11322</identifier>
 <datestamp>2019-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11322</id><created>2019-04-25</created><updated>2019-11-24</updated><authors><author><keyname>Fang</keyname><forenames>Zhihao</forenames></author><author><keyname>Zhang</keyname><forenames>Wanyi</forenames></author><author><keyname>Ma</keyname><forenames>He</forenames></author></authors><title>Breast Cancer Classification with Ultrasound Images Based on SLIC</title><categories>cs.CV eess.IV</categories><comments>This is a pre-print of a contribution published in Frontier Computing
  - Theory, Technologies and Applications (FC 2019) published by Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultrasound image diagnosis of breast tumors has been widely used in recent
years. However, there are some problems of it, for instance, poor quality,
intense noise and uneven echo distribution, which has created a huge obstacle
to diagnosis. To overcome these problems, we propose a novel method, a breast
cancer classification with ultrasound images based on SLIC (BCCUI). We first
utilize the Region of Interest (ROI) extraction based on Simple Linear
Iterative Clustering (SLIC) algorithm and region growing algorithm to extract
the ROI at the super-pixel level. Next, the features of ROI are extracted.
Furthermore, the Support Vector Machine (SVM) classifier is applied. The
calculation states that the accuracy of this segment algorithm is up to 88.00%
and the sensitivity of the algorithm is up to 92.05%, which proves that the
classifier presents in this paper has certain research meaning and applied
worthiness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11419</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11419</id><created>2019-04-25</created><authors><author><keyname>Fu</keyname><forenames>Rao</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Zeng</keyname><forenames>Shutian</forenames></author><author><keyname>Zhuang</keyname><forenames>Yiping</forenames></author><author><keyname>Sudjianto</keyname><forenames>Agus</forenames></author></authors><title>Time Series Simulation by Conditional Generative Adversarial Net</title><categories>stat.ML cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative Adversarial Net (GAN) has been proven to be a powerful machine
learning tool in image data analysis and generation. In this paper, we propose
to use Conditional Generative Adversarial Net (CGAN) to learn and simulate time
series data. The conditions can be both categorical and continuous variables
containing different kinds of auxiliary information. Our simulation studies
show that CGAN is able to learn different kinds of normal and heavy tail
distributions, as well as dependent structures of different time series and it
can further generate conditional predictive distributions consistent with the
training data distributions. We also provide an in-depth discussion on the
rationale of GAN and the neural network as hierarchical splines to draw a clear
connection with the existing statistical method for distribution generation. In
practice, CGAN has a wide range of applications in the market risk and
counterparty risk analysis: it can be applied to learn the historical data and
generate scenarios for the calculation of Value-at-Risk (VaR) and Expected
Shortfall (ES) and predict the movement of the market risk factors. We present
a real data analysis including a backtesting to demonstrate CGAN is able to
outperform the Historic Simulation, a popular method in market risk analysis
for the calculation of VaR. CGAN can also be applied in the economic time
series modeling and forecasting, and an example of hypothetical shock analysis
for economic models and the generation of potential CCAR scenarios by CGAN is
given at the end of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11469</identifier>
 <datestamp>2019-07-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11469</id><created>2019-04-25</created><updated>2019-07-07</updated><authors><author><keyname>Dunbar</keyname><forenames>Ewan</forenames></author><author><keyname>Algayres</keyname><forenames>Robin</forenames></author><author><keyname>Karadayi</keyname><forenames>Julien</forenames></author><author><keyname>Bernard</keyname><forenames>Mathieu</forenames></author><author><keyname>Benjumea</keyname><forenames>Juan</forenames></author><author><keyname>Cao</keyname><forenames>Xuan-Nga</forenames></author><author><keyname>Miskic</keyname><forenames>Lucie</forenames></author><author><keyname>Dugrain</keyname><forenames>Charlotte</forenames></author><author><keyname>Ondel</keyname><forenames>Lucas</forenames></author><author><keyname>Black</keyname><forenames>Alan W.</forenames></author><author><keyname>Besacier</keyname><forenames>Laurent</forenames></author><author><keyname>Sakti</keyname><forenames>Sakriani</forenames></author><author><keyname>Dupoux</keyname><forenames>Emmanuel</forenames></author></authors><title>The Zero Resource Speech Challenge 2019: TTS without T</title><categories>cs.CL cs.SD eess.AS</categories><comments>Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Zero Resource Speech Challenge 2019, which proposes to build a
speech synthesizer without any text or phonetic labels: hence, TTS without T
(text-to-speech without text). We provide raw audio for a target voice in an
unknown language (the Voice dataset), but no alignment, text or labels.
Participants must discover subword units in an unsupervised way (using the Unit
Discovery dataset) and align them to the voice recordings in a way that works
best for the purpose of synthesizing novel utterances from novel speakers,
similar to the target speaker's voice. We describe the metrics used for
evaluation, a baseline system consisting of unsupervised subword unit discovery
plus a standard TTS system, and a topline TTS using gold phoneme
transcriptions. We present an overview of the 19 submitted systems from 10
teams and discuss the main results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11481</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11481</id><created>2019-04-25</created><authors><author><keyname>Buyukates</keyname><forenames>Baturalp</forenames></author><author><keyname>Soysal</keyname><forenames>Alkan</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Age of Information in Multicast Networks with Multiple Update Streams</title><categories>cs.IT cs.NI eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the age of information in a multicast network where there is a
single source node that sends time-sensitive updates to $n$ receiver nodes.
Each status update is one of two kinds: type I or type II. To study the age of
information experienced by the receiver nodes for both types of updates, we
consider two cases: update streams are generated by the source node at-will and
update streams arrive exogenously to the source node. We show that using an
earliest $k_1$ and $k_2$ transmission scheme for type I and type II updates,
respectively, the age of information of both update streams at the receiver
nodes can be made a constant independent of $n$. In particular, the source node
transmits each type I update packet to the earliest $k_1$ and each type II
update packet to the earliest $k_2$ of $n$ receiver nodes. We determine the
optimum $k_1$ and $k_2$ stopping thresholds for arbitrary shifted exponential
link delays to individually and jointly minimize the average age of both update
streams and characterize the pareto optimal curve for the two ages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11520</identifier>
 <datestamp>2019-08-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11520</id><created>2019-04-25</created><updated>2019-08-14</updated><authors><author><keyname>Roheda</keyname><forenames>Siddharth</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author><author><keyname>Wu</keyname><forenames>Tianfu</forenames></author></authors><title>Event Driven Fusion</title><categories>eess.SP</categories><comments>Preprint submitted to Journal of Information Fusion. arXiv admin
  note: text overlap with arXiv:1809.09166</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a technique which exploits the occurrence of certain
events as observed by different sensors, to detect and classify objects. This
technique explores the extent of dependence between features being observed by
the sensors, and generates more informed probability distributions over the
events. Provided some additional information about the features of the object,
this fusion technique can outperform other existing decision level fusion
approaches that may not take into account the relationship between different
features. Furthermore, this paper addresses the issue of coping with damaged
sensors when using the model, by learning a hidden space between sensor
modalities which can be exploited to safeguard detection performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11546</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11546</id><created>2019-04-25</created><authors><author><keyname>Bublin</keyname><forenames>Mugdim</forenames></author></authors><title>Machine Learning For Distributed Acoustic Sensors, Classic versus Image
  and Deep Neural Networks Approach</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Acoustic Sensing (DAS) using fiber optic cables is a promising
new technology for pipeline monitoring and protection. In this work, we applied
and compared two approaches for event detection using DAS: Classic machine
learning approach and the approach based on image processing and deep learning.
Although with both approaches acceptable performance can be achieved, the
preliminary results show that image based deep learning is more promising
approach, offering six times lower event detection delay and twelve times lower
execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11580</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11580</id><created>2019-04-24</created><authors><author><keyname>Kahl</keyname><forenames>Matthias</forenames></author><author><keyname>Kriechbaumer</keyname><forenames>Thomas</forenames></author><author><keyname>Jorde</keyname><forenames>Daniel</forenames></author><author><keyname>Haq</keyname><forenames>Anwar Ul</forenames></author><author><keyname>Jacobsen</keyname><forenames>Hans-Arno</forenames></author></authors><title>Appliance Event Detection -- A Multivariate, Supervised Classification
  Approach</title><categories>cs.OH cs.SY eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-intrusive load monitoring (NILM) is a modern and still expanding
technique, helping to understand fundamental energy consumption patterns and
appliance characteristics. Appliance event detection is an elementary step in
the NILM pipeline. Unfortunately, several types of appliances (e.g., switching
mode power supply (SMPS) or multi-state) are known to challenge
state-of-the-art event detection systems due to their noisy consumption
profiles. Classical rule-based event detection system become infeasible and
complex for these appliances. By stepping away from distinct event definitions,
we can learn from a consumer-configured event model to differentiate between
relevant and irrelevant event transients.
  We introduce a boosting oriented adaptive training, that uses false positives
from the initial training area to reduce the number of false positives on the
test area substantially. The results show a false positive decrease by more
than a factor of eight on a dataset that has a strong focus on SMPS-driven
appliances. To obtain a stable event detection system, we applied several
experiments on different parameters to measure its performance. These
experiments include the evaluation of six event features from the spectral and
time domain, different types of feature space normalization to eliminate
undesired feature weighting, the conventional and adaptive training, and two
common classifiers with its optimal parameter settings. The evaluations are
performed on two publicly available energy datasets with high sampling rates:
BLUED and BLOND-50.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11617</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11617</id><created>2019-04-25</created><authors><author><keyname>Li</keyname><forenames>Ming</forenames></author><author><keyname>Ye</keyname><forenames>Chunyang</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author></authors><title>High-Resolution Network for Photorealistic Style Transfer</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photorealistic style transfer aims to transfer the style of one image to
another, but preserves the original structure and detail outline of the content
image, which makes the content image still look like a real shot after the
style transfer. Although some realistic image styling methods have been
proposed, these methods are vulnerable to lose the details of the content image
and produce some irregular distortion structures. In this paper, we use a
high-resolution network as the image generation network. Compared to other
methods, which reduce the resolution and then restore the high resolution, our
generation network maintains high resolution throughout the process. By
connecting high-resolution subnets to low-resolution subnets in parallel and
repeatedly multi-scale fusion, high-resolution subnets can continuously receive
information from low-resolution subnets. This allows our network to discard
less information contained in the image, so the generated images may have a
more elaborate structure and less distortion, which is crucial to the visual
quality. We conducted extensive experiments and compared the results with
existing methods. The experimental results show that our model is effective and
produces better results than existing methods for photorealistic image
stylization. Our source code with PyTorch framework will be publicly available
at https://github.com/limingcv/Photorealistic-Style-Transfer
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11620</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11620</id><created>2019-04-25</created><authors><author><keyname>Yun</keyname><forenames>Kyongsik</forenames></author><author><keyname>Yu</keyname><forenames>Kevin</forenames></author><author><keyname>Osborne</keyname><forenames>Joseph</forenames></author><author><keyname>Eldin</keyname><forenames>Sarah</forenames></author><author><keyname>Nguyen</keyname><forenames>Luan</forenames></author><author><keyname>Huyen</keyname><forenames>Alexander</forenames></author><author><keyname>Lu</keyname><forenames>Thomas</forenames></author></authors><title>Improved visible to IR image transformation using synthetic data
  augmentation with cycle-consistent adversarial networks</title><categories>eess.IV cs.CV cs.LG</categories><comments>8 pages, 6 figures, SPIE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infrared (IR) images are essential to improve the visibility of dark or
camouflaged objects. Object recognition and segmentation based on a neural
network using IR images provide more accuracy and insight than color visible
images. But the bottleneck is the amount of relevant IR images for training. It
is difficult to collect real-world IR images for special purposes, including
space exploration, military and fire-fighting applications. To solve this
problem, we created color visible and IR images using a Unity-based 3D game
editor. These synthetically generated color visible and IR images were used to
train cycle consistent adversarial networks (CycleGAN) to convert visible
images to IR images. CycleGAN has the advantage that it does not require
precisely matching visible and IR pairs for transformation training. In this
study, we discovered that additional synthetic data can help improve CycleGAN
performance. Neural network training using real data (N = 20) performed more
accurate transformations than training using real (N = 10) and synthetic (N =
10) data combinations. The result indicates that the synthetic data cannot
exceed the quality of the real data. Neural network training using real (N =
10) and synthetic (N = 100) data combinations showed almost the same
performance as training using real data (N = 20). At least 10 times more
synthetic data than real data is required to achieve the same performance. In
summary, CycleGAN is used with synthetic data to improve the IR image
conversion performance of visible images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11641</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11641</id><created>2019-04-25</created><authors><author><keyname>Senoussaoui</keyname><forenames>Mohammed</forenames></author><author><keyname>Cardinal</keyname><forenames>Patrick</forenames></author><author><keyname>Dehak</keyname><forenames>Najim</forenames></author><author><keyname>Koerich</keyname><forenames>Alessandro Lameiras</forenames></author></authors><title>Speaker Sincerity Detection based on Covariance Feature Vectors and
  Ensemble Methods</title><categories>cs.SD cs.CL eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic measuring of speaker sincerity degree is a novel research problem
in computational paralinguistics. This paper proposes covariance-based feature
vectors to model speech and ensembles of support vector regressors to estimate
the degree of sincerity of a speaker. The elements of each covariance vector
are pairwise statistics between the short-term feature components. These
features are used alone as well as in combination with the ComParE acoustic
feature set. The experimental results on the development set of the Sincerity
Speech Corpus using a cross-validation procedure have shown an 8.1% relative
improvement in the Spearman's correlation coefficient over the baseline system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11655</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11655</id><created>2019-04-25</created><updated>2019-09-16</updated><authors><author><keyname>Ji</keyname><forenames>Feng</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author></authors><title>A Hilbert Space Theory of Generalized Graph Signal Processing</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2952055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph signal processing (GSP) has become an important tool in many areas such
as image processing, networking learning and analysis of social network data.
In this paper, we propose a broader framework that not only encompasses
traditional GSP as a special case, but also includes a hybrid framework of
graph and classical signal processing over a continuous domain. Our framework
relies extensively on concepts and tools from functional analysis to generalize
traditional GSP to graph signals in a separable Hilbert space with infinite
dimensions. We develop a concept analogous to Fourier transform for generalized
GSP and the theory of filtering and sampling such signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11678</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11678</id><created>2019-04-26</created><authors><author><keyname>das</keyname><forenames>Sumanta kumar</forenames></author><author><keyname>Singh</keyname><forenames>R. S.</forenames></author></authors><title>Performance modeling of electro-optical devices for military target
  acquisition</title><categories>stat.AP eess.IV</categories><comments>9 pages,4 tables, 5 figures</comments><journal-ref>Defence science journal, vol 57, no. 3, may 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate predictions of electro-optical imager performance are important for
defence decision-making. The predictions serve as a guide for system
development and are used in war game, simulations that directly influence
engagement tactics. In the present study, mathematical models have been
developed which involves detection of different military targets using their
opto-electronics properties in different environmental conditions. The method
first calculates the signal-to-noise ratio received by the observing sensors
reflected from the target by quantifying the light energy in terms of photons,
which is used for evaluating the detection probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11688</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11688</id><created>2019-04-26</created><authors><author><keyname>Upadhyay</keyname><forenames>Ashish</forenames></author><author><keyname>Kotyan</keyname><forenames>Shashank</forenames></author><author><keyname>Tripathi</keyname><forenames>Shrivishal</forenames></author><author><keyname>Yadav</keyname><forenames>Sandeep</forenames></author></authors><title>Analysis and Comparison of Different Fuzzy Inference Systems used in
  Decision Making for Secondary Users in Cognitive Radio Network</title><categories>eess.SP</categories><comments>Pre-print of the published article at Wireless Personal
  Communications, Volume 104, Issue 3, pp 1175-1208</comments><journal-ref>Wireless Pers Commun (2019) 104: 1175</journal-ref><doi>10.1007/s11277-018-6075-9.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum scarcity is one of the major challenges that the modern
communication engineers are going through because of inefficient utilization of
allocated frequency spectrum. The spectrum scarcity is a problem because there
is not enough wavelengths/frequency to match the number of channels which are
required to broadcast in a given bandwidth. Therefore, the utilization of
available allocated spectrum when licensed users are not in use offers an
opportunity as well as challenge, also, to increase the efficiency of spectrum
utilization. Cognitive Radio offers a promising solution by reutilisation of
unused allocated frequency spectrum. It helps to fulfil the demand of frequency
requirement for modern communication system to accommodate more data
transmission. In this optimum utilization of reuse of frequency spectrum
required optimising algorithms in all parts of Cognitive Cycle. This paper
focuses on designing a system based on fuzzy logic with a set of input and
output parameters to obtain an optimised solution. A comparative analysis is
also carried out among various types of membership functions of input and
output on Mamdani Fuzzy Inference System and Sugeno Fuzzy Inference System. The
proposed approach is applicable to design a better system model for a given set
of rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11702</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11702</id><created>2019-04-26</created><authors><author><keyname>Jarrasse</keyname><forenames>Nathanael</forenames><affiliation>AGATHE, ISIR, CNRS</affiliation></author><author><keyname>Nicol</keyname><forenames>Caroline</forenames><affiliation>ISM</affiliation></author><author><keyname>Richer</keyname><forenames>Florian</forenames><affiliation>ISIR</affiliation></author><author><keyname>Touillet</keyname><forenames>Amelie</forenames><affiliation>IRR</affiliation></author><author><keyname>Martinet</keyname><forenames>No&#xeb;l</forenames><affiliation>IRR</affiliation></author><author><keyname>Paysant</keyname><forenames>Jean</forenames><affiliation>IRR</affiliation></author><author><keyname>de Graaf</keyname><forenames>Jozina</forenames><affiliation>ISM</affiliation></author></authors><title>Voluntary phantom hand and finger movements in transhumeral amputees
  could be used to naturally control polydigital prostheses</title><categories>q-bio.NC eess.SP physics.med-ph q-bio.TO</categories><proxy>ccsd</proxy><journal-ref>2017 International Conference on Rehabilitation Robotics (ICORR),
  Jul 2017, London, France. pp.1239-1245</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An arm amputation is extremely invalidating since many of our daily tasks
require bi-manual and precise control of hand movements. Perfect hand
prostheses should therefore offer a natural, intuitive and cognitively simple
control over their numerous biomimetic active degrees of freedom. While
efficient polydigital prostheses are commercially available, their control
remains complex to master and offers limited possibilities, especially for high
amputation levels. In this pilot study, we demonstrate the possibility for
upper-arm amputees to intuitively control a polydigital hand prosthesis by
using surface myoelectric activities of residual limb muscles (sEMG) associated
with phantom limb movements, even if these residual arm muscles on which the
phantom activity is measured were not naturally associated with hand movements
before amputation. Using pattern recognition methods, three arm amputees were
able, without training, to initiate 5-8 movements of a robotic hand (including
individual finger movements) by simply mobilizing their phantom limb while the
robotic hand was mimicking the action in real time. This innovative control
approach could offer to numerous upper-limb amputees an access to recent
biomimetic prostheses with multiple controllable joints, without requiring
surgery or complex training; and might deeply change the way the phantom limb
is apprehended by both patients and clinicians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11754</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11754</id><created>2019-04-26</created><authors><author><keyname>Stankiewicz</keyname><forenames>Olgierd</forenames></author></authors><title>Video coding technique with parametric modeling of noise</title><categories>eess.IV cs.MM</categories><comments>16 pages, 10 figures, submitted to Opto-Electronics Review - Journal
  - Elsevier</comments><doi>10.1016/j.opelre.2019.05.006</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents a video encoding method in which noise is encoded using a
novel parametric model representing spectral envelope and spatial distribution
of energy. The proposed method has been experimentally assessed using video
test sequences in a practical setup consisting of a simple, real-time noise
reduction technique and High Efficiency Video Codec (HEVC). The attained
results show that the use of the proposed parametric modeling of noise can
improve the subjective quality of reconstructed video by approximately 1.8 Mean
Opinion Scope (MOS) points (in 11-point scale) related to the classical video
coding. Moreover, the present work confirms results attained in the previous
works that the usage of even sole noise reduction prior to the encoding
provides quality increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11828</identifier>
 <datestamp>2019-12-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11828</id><created>2019-04-26</created><updated>2019-10-17</updated><authors><author><keyname>Kahl</keyname><forenames>Dominik</forenames></author><author><keyname>Wendland</keyname><forenames>Philipp</forenames></author><author><keyname>Neidhardt</keyname><forenames>Matthias</forenames></author><author><keyname>Weber</keyname><forenames>Andreas</forenames></author><author><keyname>Kschischo</keyname><forenames>Maik</forenames></author></authors><title>Structural Invertibility and Optimal Sensor Node Placement for Error and
  Input Reconstruction in Dynamic Systems</title><categories>math.DS eess.SP</categories><msc-class>93C15</msc-class><journal-ref>Phys. Rev. X 9, 041046 (2019)</journal-ref><doi>10.1103/PhysRevX.9.041046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite recent progress in our understanding of complex dynamic networks, it
remains challenging to devisesufficiently accurate models to observe, control
or predict the state of real systems in biology, economics or other fields. A
largely overlooked fact is that these systems are typically open and receive
unknown inputs from their environment. A further fundamental obstacle are
structural model errors caused by insufficient or inaccurate knowledge about
the quantitative interactions in the real system.
  Here, we show that unknown inputs to open systems and model errors can be
treated under the common framework of invertibility, which is a requirement for
reconstructing these disturbances from output measurements. By exploiting the
fact that invertibility can be decided from the influence graph of the system,
we analyse the relationship between structural network properties and
invertibility under different realistic scenarios. We show that sparsely
connected scale free networks are the most difficult to invert. We introduce a
new sensor node placement algorithm to select a minimum set of measurement
positions in the network required for invertibility. This algorithm facilitates
optimal experimental design for the reconstruction of inputs or model errors
from output measurements. Our results have both fundamental and practical
implications for nonlinear systems analysis, modelling and design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11832</identifier>
 <datestamp>2019-07-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11832</id><created>2019-04-25</created><updated>2019-06-05</updated><authors><author><keyname>Song</keyname><forenames>Pengming</forenames></author><author><keyname>Jiang</keyname><forenames>Shaowei</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author><author><keyname>Bian</keyname><forenames>Zichao</forenames></author><author><keyname>Guo</keyname><forenames>Chengfei</forenames></author><author><keyname>Hoshino</keyname><forenames>Kazunori</forenames></author><author><keyname>Zheng</keyname><forenames>Guoan</forenames></author></authors><title>Super-resolution microscopy via ptychographic structured modulation of a
  diffuser</title><categories>eess.IV physics.optics</categories><journal-ref>Optics Letters, 44(15), 3645-3648, (2019)</journal-ref><doi>10.1364/OL.44.003645</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report a new coherent imaging technique, termed ptychographic structured
modulation (PSM), for quantitative super-resolution microscopy. In this
technique, we place a thin diffuser (i.e., a scattering lens) in between the
sample and the objective lens to modulate the complex light waves from the
object. The otherwise inaccessible high-resolution object information can thus
be encoded into the captured images. We then employ a ptychographic phase
retrieval process to jointly recover the exit wavefront of the complex object
and the unknown diffuser profile. Unlike the illumination-based
super-resolution approach, the recovered image of our approach depends upon how
the complex wavefront exits the sample - not enters it. Therefore, the sample
thickness becomes irrelevant during reconstruction. After recovery, we can
propagate the super-resolution complex wavefront to any position along the
optical axis. We validate our approach using a resolution target, a
quantitative phase target, a two-layer sample, and a thick PDMS sample. We
demonstrate a 4.5-fold resolution gain over the diffraction limit. We also show
that a 4-fold resolution gain can be achieved with as few as ~30 images. The
reported approach may provide a quantitative super-resolution strategy for
coherent light, X-ray, and electron imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11874</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11874</id><created>2019-04-12</created><authors><author><keyname>Kokalj-Filipovic</keyname><forenames>Silvija</forenames></author><author><keyname>Miller</keyname><forenames>Rob</forenames></author><author><keyname>Morman</keyname><forenames>Joshua</forenames></author></authors><title>AutoEncoders for Training Compact Deep Learning RF Classifiers for
  Wireless Protocols</title><categories>cs.NI cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that compact fully connected (FC) deep learning networks trained to
classify wireless protocols using a hierarchy of multiple denoising
autoencoders (AEs) outperform reference FC networks trained in a typical way,
i.e., with a stochastic gradient based optimization of a given FC architecture.
Not only is the complexity of such FC network, measured in number of trainable
parameters and scalar multiplications, much lower than the reference FC and
residual models, its accuracy also outperforms both models for nearly all
tested SNR values (0 dB to 50dB). Such AE-trained networks are suited for
in-situ protocol inference performed by simple mobile devices based on noisy
signal measurements. Training is based on the data transmitted by real devices,
and collected in a controlled environment, and systematically augmented by a
policy-based data synthesis process by adding to the signal any subset of
impairments commonly seen in a wireless receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11882</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11882</id><created>2019-04-14</created><authors><author><keyname>Sheth</keyname><forenames>Dwij Sukeshkumar</forenames></author><author><keyname>Singh</keyname><forenames>Shantanu</forenames></author><author><keyname>Mathur</keyname><forenames>Prakhar S</forenames></author><author><keyname>D</keyname><forenames>Vydeki</forenames></author></authors><title>Smart Laptop Bag with Machine Learning for Activity Recognition</title><categories>cs.OH cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In todays world of smart living, the smart laptop bag, presented in this
paper, provides a better solution to keep track of our precious possessions and
monitoring them in real time. As the world moves towards a much tech-savvy
direction, the novel laptop bag discussed here facilitates the user to perform
location tracking, ambiance monitoring, user-state monitoring etc. in one
device. The innovative design uses cloud computing and machine learning
algorithms to monitor the health of the user and many parameters of the bag.
The emergency alert system in this bag could be trained to send appropriate
notifications to emergency contacts of the user, in case of abnormal health
conditions or theft of the bag. The experimental smart laptop bag uses deep
neural network, which was trained and tested over the various parameters from
the bag and produces above 95% accurate results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11914</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11914</id><created>2019-04-26</created><updated>2019-07-23</updated><authors><author><keyname>Adiban</keyname><forenames>Mohammad</forenames></author><author><keyname>BabaAli</keyname><forenames>Bagher</forenames></author><author><keyname>Shehnepoor</keyname><forenames>Saeedreza</forenames></author></authors><title>I-vector Based Features Embedding for Heart Sound Classification</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cardiovascular Disease (CVD) is considered as one of the principal causes of
death in the world. Over recent years, this field of study has attracted
researchers' attention to investigate heart sounds' patterns for disease
diagnostics. In this study, an approach is proposed for normal/abnormal heart
sound classification on the Physionet challenge 2016 dataset. For the first
time, a fixed length feature vector; called i-vector; is extracted from each
heart sound using Mel Frequency Cepstral Coefficient (MFCC) features.
Afterwards, Principal Component Analysis (PCA) transform and Variational
Autoencoder (VAE) are applied on the i-vector to achieve dimension reduction.
Eventually, the reduced size vector is fed to Gaussian Mixture Models (GMMs)
and Support Vector Machine (SVM) for classification purpose. Experimental
results demonstrate the proposed method could achieve a performance improvement
of 16\% based on Modified Accuracy (MAcc) compared with the baseline system on
the Physionet2016 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11929</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11929</id><created>2019-04-26</created><authors><author><keyname>Venet</keyname><forenames>Ludovic</forenames></author><author><keyname>Pati</keyname><forenames>Sarthak</forenames></author><author><keyname>Yushkevich</keyname><forenames>Paul</forenames></author><author><keyname>Bakas</keyname><forenames>Spyridon</forenames></author></authors><title>Accurate and Robust Alignment of Variable-stained Histologic Images
  Using a General-purpose Greedy Diffeomorphic Registration Tool</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variously stained histology slices are routinely used by pathologists to
assess extracted tissue samples from various anatomical sites and determine the
presence or extent of a disease. Evaluation of sequential slides is expected to
enable a better understanding of the spatial arrangement and growth patterns of
cells and vessels. In this paper we present a practical two-step approach based
on diffeomorphic registration to align digitized sequential histopathology
stained slides to each other, starting with an initial affine step followed by
the estimation of a detailed deformation field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11948</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11948</id><created>2019-04-25</created><authors><author><keyname>Rykaczewski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Nikadon</keyname><forenames>Jan</forenames></author><author><keyname>Duch</keyname><forenames>W&#x142;odzis&#x142;aw</forenames></author><author><keyname>Piotrowski</keyname><forenames>Tomasz</forenames></author></authors><title>supFunSim: : spatial filtering toolbox for EEG</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition and interpretation of brain activity patterns from EEG or MEG
signals is one of the most important tasks in cognitive neuroscience, requiring
sophisticated methods of signal processing. The supFunSim library is a new
Matlab toolbox which generates accurate EEG forward models and implements a
collection of spatial filters for EEG source reconstruction, including linearly
constrained minimum-variance (LCMV), eigenspace LCMV, nulling (NL), and
minimum-variance pseudo-unbiased reduced-rank (MV-PURE) filters in various
versions. It also enables source-level directed connectivity analysis using
partial directed coherence (PDC) and directed transfer function (DTF) measures.
The supFunSim library is based on the well-known FieldTrip toolbox for EEG and
MEG analysis and is written using object-oriented programming paradigm. The
resulting modularity of the toolbox enables its simple extensibility. This
paper gives a complete overview of the toolbox from both developer and end-user
perspectives, including description of the installation process and some use
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11949</identifier>
 <datestamp>2019-06-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11949</id><created>2019-04-24</created><updated>2019-06-06</updated><authors><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author><author><keyname>Letizia</keyname><forenames>Nunzio A.</forenames></author><author><keyname>Righini</keyname><forenames>Davide</forenames></author><author><keyname>Marcuzzi</keyname><forenames>Francesco</forenames></author></authors><title>Machine Learning Tips and Tricks for Power Line Communications</title><categories>eess.SP cs.LG stat.ML</categories><comments>Accepted for publication in IEEE Access. 19 pages, 15 figures, 142
  references. Added Sec. II-C</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A great deal of attention has been recently given to Machine Learning (ML)
techniques in many different application fields. This paper provides a vision
of what ML can do in Power Line Communications (PLC). We firstly and briefly
describe classical formulations of ML, and distinguish deterministic from
statistical learning models with relevance to communications. We then discuss
ML applications in PLC for each layer, namely, for characterization and
modeling, for the development of physical layer algorithms, for media access
control and networking. Finally, other applications of PLC that can benefit
from the usage of ML, as grid diagnostics, are analyzed. Illustrative numerical
examples are reported to serve the purpose of validating the ideas and motivate
future research endeavors in this stimulating signal/data processing field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11950</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11950</id><created>2019-04-25</created><authors><author><keyname>Tan</keyname><forenames>Chuanqi</forenames></author><author><keyname>Sun</keyname><forenames>Fuchun</forenames></author><author><keyname>Kong</keyname><forenames>Tao</forenames></author><author><keyname>Fang</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Wenchang</forenames></author></authors><title>Attention-based Transfer Learning for Brain-computer Interface</title><categories>eess.SP cs.AI cs.HC cs.LG</categories><comments>In Proceedings of IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP) 2019, 12 - 17 May, 2019, Brighton, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different functional areas of the human brain play different roles in brain
activity, which has not been paid sufficient research attention in the
brain-computer interface (BCI) field. This paper presents a new approach for
electroencephalography (EEG) classification that applies attention-based
transfer learning. Our approach considers the importance of different brain
functional areas to improve the accuracy of EEG classification, and provides an
additional way to automatically identify brain functional areas associated with
new activities without the involvement of a medical professional. We
demonstrate empirically that our approach out-performs state-of-the-art
approaches in the task of EEG classification, and the results of visualization
indicate that our approach can detect brain functional areas related to a
certain task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11951</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11951</id><created>2019-04-23</created><authors><author><keyname>Brajato</keyname><forenames>Giovanni</forenames></author><author><keyname>Lundberg</keyname><forenames>Lars</forenames></author><author><keyname>Torres-Company</keyname><forenames>Victor</forenames></author><author><keyname>Zibar</keyname><forenames>Darko</forenames></author></authors><title>Optical Frequency Comb Noise Characterization Using Machine Learning</title><categories>eess.SP physics.optics quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel tool, based on Bayesian filtering framework and expectation
maximization algorithm, is numerically and experimentally demonstrated for
accurate frequency comb noise characterization. The tool is statistically
optimum in a mean-square-error-sense, works at wide range of SNRs and offers
more accurate noise estimation compared to conventional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11952</identifier>
 <datestamp>2019-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11952</id><created>2019-04-22</created><updated>2019-05-03</updated><authors><author><keyname>Marin-Palomo</keyname><forenames>Pablo</forenames></author><author><keyname>Kemal</keyname><forenames>Juned N.</forenames></author><author><keyname>Trocha</keyname><forenames>Philipp</forenames></author><author><keyname>Wolf</keyname><forenames>Stefan</forenames></author><author><keyname>Merghem</keyname><forenames>Kamel</forenames></author><author><keyname>Lelarge</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Ramdane</keyname><forenames>Abderrahim</forenames></author><author><keyname>Freude</keyname><forenames>Wolfgang</forenames></author><author><keyname>Randel</keyname><forenames>Sebastian</forenames></author><author><keyname>Koos</keyname><forenames>Christian</forenames></author></authors><title>Comb-based WDM transmission at 10 Tbit/s using a DC-driven quantum-dash
  mode-locked laser diode</title><categories>eess.SP physics.optics</categories><doi>10.1364/OE.27.031110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chip-scale frequency comb generators have the potential to become key
building blocks of compact wavelength-division multiplexing (WDM) transceivers
in future metropolitan or campus-area networks. Among the various comb
generator concepts, quantum-dash (QD) mode-locked laser diodes (MLLD) stand out
as a particularly promising option, combining small footprint with simple
operation by a DC current and offering flat broadband comb spectra. However,
the data transmission performance achieved with QD-MLLD was so far limited by
strong phase noise of the individual comb tones, restricting experiments to
rather simple modulation formats such as quadrature phase shift keying (QPSK)
or requiring hard-ware-based compensation schemes. Here we demonstrate that
these limitations can be over-come by digital symbol-wise phase tracking
algorithms, avoiding any hardware-based phase-noise compensation. We
demonstrate 16QAM dual-polarization WDM transmission on 38 channels at an
aggregate net data rate of 10.68 Tbit/s over 75 km of standard single-mode
fiber. To the best of our knowledge, this corresponds to the highest data rate
achieved through a DC-driven chip-scale comb generator without any
hardware-based phase-noise reduction schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11953</identifier>
 <datestamp>2019-04-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11953</id><created>2019-04-19</created><authors><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Song</keyname><forenames>Yunpeng</forenames></author><author><keyname>Zhang</keyname><forenames>Jimuyang</forenames></author><author><keyname>Han</keyname><forenames>Jinsong</forenames></author><author><keyname>Huang</keyname><forenames>Dong</forenames></author></authors><title>Temporal Unet: Sample Level Human Action Recognition using WiFi</title><categories>eess.SP cs.CV cs.HC</categories><comments>14 pages, 14 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human doing actions will result in WiFi distortion, which is widely explored
for action recognition, such as the elderly fallen detection, hand sign
language recognition, and keystroke estimation. As our best survey, past work
recognizes human action by categorizing one complete distortion series into one
action, which we term as series-level action recognition. In this paper, we
introduce a much more fine-grained and challenging action recognition task into
WiFi sensing domain, i.e., sample-level action recognition. In this task, every
WiFi distortion sample in the whole series should be categorized into one
action, which is a critical technique in precise action localization,
continuous action segmentation, and real-time action recognition. To achieve
WiFi-based sample-level action recognition, we fully analyze approaches in
image-based semantic segmentation as well as in video-based frame-level action
recognition, then propose a simple yet efficient deep convolutional neural
network, i.e., Temporal Unet. Experimental results show that Temporal Unet
achieves this novel task well. Codes have been made publicly available at
https://github.com/geekfeiw/WiSLAR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11954</identifier>
 <datestamp>2020-02-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11954</id><created>2019-04-17</created><authors><author><keyname>Tarable</keyname><forenames>Alberto</forenames></author><author><keyname>Escribano</keyname><forenames>Francisco J.</forenames></author></authors><title>Chaos-Based Anytime Reliable Coded Communications</title><categories>eess.SP cs.IT math.IT nlin.CD</categories><comments>31 pages, 7 figures</comments><doi>10.1109/JSYST.2019.2919988</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anytime reliable communication systems are needed in contexts where the
property of vanishing error probability with time is critical. This is the case
of unstable real time systems that are to be controlled through the
transmission and processing of remotely sensed data. The most successful
anytime reliable transmission systems developed so far are based on channel
codes and channel coding theory. In this work, another focus is proposed,
placing the stress on the waveform level rather than just on the coding level.
This alleviates the coding and decoding complexity problems faced by other
proposals. To this purpose, chaos theory is successfully exploited in order to
design two different anytime reliable alternatives. The anytime reliability
property is formally demonstrated in each case for the AWGN channel, under
given conditions. The simulation results shown validate the theoretical
developments, and demonstrate that these systems can achieve anytime
reliability with affordable resource expenditure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11983</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11983</id><created>2019-04-26</created><updated>2019-07-13</updated><authors><author><keyname>An</keyname><forenames>Yi</forenames></author><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Huang</keyname><forenames>Liangjin</forenames></author><author><keyname>Leng</keyname><forenames>Jinyong</forenames></author><author><keyname>Yang</keyname><forenames>Lijia</forenames></author><author><keyname>Zhou</keyname><forenames>Pu</forenames></author></authors><title>Deep learning enabled superfast and accurate M^2 evaluation for fiber
  beams</title><categories>eess.IV physics.optics</categories><comments>12 pages, 10 figures</comments><journal-ref>Optics Express, 27, 18683-18694 (2019)</journal-ref><doi>10.1364/OE.27.018683</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce deep learning technique to predict the beam propagation factor
M^2 of the laser beams emitting from few-mode fiber for the first time, to the
best of our knowledge. The deep convolutional neural network (CNN) is trained
with paired data of simulated near-field beam patterns and their calculated M^2
value, aiming at learning a fast and accurate mapping from the former to the
latter. The trained deep CNN can then be utilized to evaluate M^2 of the fiber
beams from single beam patterns. The results of simulated testing samples have
shown that our scheme can achieve an averaged prediction error smaller than 2%
even when up to 10 eigenmodes are involved in the fiber. The error becomes
slightly larger when heavy noises are added into the input beam patterns but
still smaller than 2.5%, which further proves the accuracy and robustness of
our method. Furthermore, the M^2 estimation takes only about 5 ms for a
prepared beam pattern with one forward pass, which can be adopted for real-time
M^2 determination with only one supporting Charge-Coupled Device (CCD). The
experimental results further prove the feasibility of our scheme. Moreover, the
method we proposed can be confidently extended to other kinds of beams provided
that adequate training samples are accessible. Deep learning paves the way to
superfast and accurate M^2 evaluation with very low experimental efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.11985</identifier>
 <datestamp>2019-06-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.11985</id><created>2019-04-26</created><authors><author><keyname>Caramazza</keyname><forenames>Piergiorgio</forenames></author><author><keyname>Moran</keyname><forenames>Ois&#xed;n</forenames></author><author><keyname>Murray-Smith</keyname><forenames>Roderick</forenames></author><author><keyname>Faccio</keyname><forenames>Daniele</forenames></author></authors><title>Transmission of natural scene images through a multimode fibre</title><categories>eess.IV physics.optics</categories><doi>10.1038/s41467-019-10057-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The optical transport of images through a multimode fibre remains an
outstanding challenge with applications ranging from optical communications to
neuro-imaging. State of the art approaches either involve measurement and
control of the full complex field transmitted through the fibre or, more
recently, training of artificial neural networks that however, are typically
limited to image classes belong to the same class as the training data set.
Here we implement a method that statistically reconstructs the inverse
transformation matrix for the fibre. We demonstrate imaging at high frame
rates, high resolutions and in full colour of natural scenes, thus
demonstrating general-purpose imaging capability. Real-time imaging over long
fibre lengths opens alternative routes to exploitation for example for secure
communication systems, novel remote imaging devices, quantum state control
processing and endoscopy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12008</identifier>
 <datestamp>2019-08-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12008</id><created>2019-04-26</created><authors><author><keyname>Eshraghian</keyname><forenames>Jason K.</forenames></author><author><keyname>Kang</keyname><forenames>Sung-Mo</forenames></author><author><keyname>Baek</keyname><forenames>Seungbum</forenames></author><author><keyname>Orchard</keyname><forenames>Garrick</forenames></author><author><keyname>Iu</keyname><forenames>Herbert Ho-Ching</forenames></author><author><keyname>Lei</keyname><forenames>Wen</forenames></author></authors><title>Analog Weights in ReRAM DNN Accelerators</title><categories>eess.SP eess.IV</categories><comments>2019 IEEE International Conference on Artificial Intelligence
  Circuits and Systems, 5 pages, 4 figures</comments><doi>10.1109/AICAS.2019.8771550</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial neural networks have become ubiquitous in modern life, which has
triggered the emergence of a new class of application specific integrated
circuits for their acceleration. ReRAM-based accelerators have gained
significant traction due to their ability to leverage in-memory computations.
In a crossbar structure, they can perform multiply-and-accumulate operations
more efficiently than standard CMOS logic. By virtue of being resistive
switches, ReRAM switches can only reliably store one of two states. This is a
severe limitation on the range of values in a computational kernel. This paper
presents a novel scheme in alleviating the single-bit-per-device restriction by
exploiting frequency dependence of v-i plane hysteresis, and assigning kernel
information not only to the device conductance but also partially distributing
it to the frequency of a time-varying input. We show this approach reduces
average power consumption for a single crossbar convolution by up to a factor
of x16 for an unsigned 8-bit input image, where each convolutional process
consumes a worst-case of 1.1mW, and reduces area by a factor of x8, without
reducing accuracy to the level of binarized neural networks. This presents a
massive saving in computing cost when there are many simultaneous in-situ
multiply-and-accumulate processes occurring across different crossbars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12016</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12016</id><created>2019-04-26</created><authors><author><keyname>Pourjabar</keyname><forenames>Sina</forenames></author><author><keyname>Choi</keyname><forenames>Gwan S.</forenames></author></authors><title>CVR: A Continuously Variable Rate LDPC Decoder Using Parity Check
  Extension for Minimum Latency</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This brief presents a novel IEEE 802.16e (WiMAX) based decoder that performs
close to the 5G code but without the expensive hardware re-development cost.
The design uses an extension of the existing WiMAX parity check code to reduce
the processing latency and power consumption while keeping the decoder
throughput at maximum. It achieves similar Frame Error Rate (FER) compared to
5G (0.1dB off), and most notably the error curves trend down like 5G instead
flooring. At FER=10^-3 there is 0.1 dB gain in the FER code performance
compared to WiMAX. An implementation of the design is a modified version of the
existing fully-parallel WiMAX decoder that supports multi-rate codeword size
and reduces latency by 33%. Additionally, for SNR greater than 3dB, decoding
only the shorter code reduces the power consumption by 36%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12069</identifier>
 <datestamp>2020-02-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12069</id><created>2019-04-26</created><updated>2020-02-21</updated><authors><author><keyname>Alamdari</keyname><forenames>Nasim</forenames></author><author><keyname>Azarang</keyname><forenames>Arian</forenames></author><author><keyname>Kehtarnavaz</keyname><forenames>Nasser</forenames></author></authors><title>Improving Deep Speech Denoising by Noisy2Noisy Signal Mapping</title><categories>eess.AS cs.SD eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing deep learning-based speech denoising approaches require clean speech
signals to be available for training. This paper presents a deep learning-based
approach to improve speech denoising in real-world audio environments by not
requiring the availability of clean speech signals in a self-supervised manner.
A fully convolutional neural network is trained by using two noisy realizations
of the same speech signal, one used as the input and the other as the output of
the network. Extensive experimentations are conducted to show the superiority
of the developed deep speech denoising approach over the conventional
supervised deep speech denoising approach based on four commonly used
performance metrics and also based on actual field-testing outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12088</identifier>
 <datestamp>2019-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12088</id><created>2019-04-26</created><updated>2019-11-17</updated><authors><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Takaki</keyname><forenames>Shinji</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author></authors><title>Neural source-filter waveform models for statistical parametric speech
  synthesis</title><categories>eess.AS cs.SD stat.ML</categories><comments>Accepted to IEEE/ACM TASLP. Note: this paper is on a follow-up work
  of our ICASSP paper. Based on the h-NSF introduced in this work, we proposed
  a h-sinc-NSF model and published the third paper in SSW 10
  (https://www.isca-speech.org/archive/SSW_2019/pdfs/SSW10_O_1-1.pdf)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural waveform models such as WaveNet have demonstrated better performance
than conventional vocoders for statistical parametric speech synthesis. As an
autoregressive (AR) model, WaveNet is limited by a slow sequential waveform
generation process. Some new models that use the inverse-autoregressive flow
(IAF) can generate a whole waveform in a one-shot manner. However, these
IAF-based models require sequential transformation during training, which
severely slows down the training speed. Other models such as Parallel WaveNet
and ClariNet bring together the benefits of AR and IAF-based models and train
an IAF model by transferring the knowledge from a pre-trained AR teacher to an
IAF student without any sequential transformation. However, both models require
additional training criteria, and their implementation is prohibitively
complicated.
  We propose a framework for neural source-filter (NSF) waveform modeling
without AR nor IAF-based approaches. This framework requires only three
components for waveform generation: a source module that generates a sine-based
signal as excitation, a non-AR dilated-convolution-based filter module that
transforms the excitation into a waveform, and a conditional module that
pre-processes the acoustic features for the source and filer modules. This
framework minimizes spectral-amplitude distances for model training, which can
be efficiently implemented by using short-time Fourier transform routines.
Under this framework, we designed three NSF models and compared them with
WaveNet. It was demonstrated that the NSF models generated waveforms at least
100 times faster than WaveNet, and the quality of the synthetic speech from the
best NSF model was better than or equally good as that from WaveNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12101</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12101</id><created>2019-04-26</created><authors><author><keyname>Jog</keyname><forenames>Amod</forenames></author><author><keyname>Grant</keyname><forenames>P. Ellen</forenames></author><author><keyname>Jacobson</keyname><forenames>Joseph L.</forenames></author><author><keyname>van der Kouwe</keyname><forenames>Andre</forenames></author><author><keyname>Meintjes</keyname><forenames>Ernesta M.</forenames></author><author><keyname>Fischl</keyname><forenames>Bruce</forenames></author><author><keyname>Z&#xf6;llei</keyname><forenames>Lilla</forenames></author></authors><title>Fast Infant MRI Skullstripping with Multiview 2D Convolutional Neural
  Networks</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Skullstripping is defined as the task of segmenting brain tissue from a full
head magnetic resonance image~(MRI). It is a critical component in neuroimage
processing pipelines. Downstream deformable registration and whole brain
segmentation performance is highly dependent on accurate skullstripping.
Skullstripping is an especially challenging task for infant~(age range 0--18
months) head MRI images due to the significant size and shape variability of
the head and the brain in that age range. Infant brain tissue development also
changes the $T_1$-weighted image contrast over time, making consistent
skullstripping a difficult task. Existing tools for adult brain MRI
skullstripping are ill equipped to handle these variations and a specialized
infant MRI skullstripping algorithm is necessary. In this paper, we describe a
supervised skullstripping algorithm that utilizes three trained fully
convolutional neural networks~(CNN), each of which segments 2D $T_1$-weighted
slices in axial, coronal, and sagittal views respectively. The three
probabilistic segmentations in the three views are linearly fused and
thresholded to produce a final brain mask. We compared our method to existing
adult and infant skullstripping algorithms and showed significant improvement
based on Dice overlap metric~(average Dice of 0.97) with a manually labeled
ground truth data set. Label fusion experiments on multiple, unlabeled data
sets show that our method is consistent and has fewer failure modes. In
addition, our method is computationally very fast with a run time of 30 seconds
per image on NVidia P40/P100/Quadro 4000 GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12102</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12102</id><created>2019-04-26</created><authors><author><keyname>Hou</keyname><forenames>Yuanbo</forenames></author><author><keyname>Kong</keyname><forenames>Qiuqiang</forenames></author><author><keyname>Li</keyname><forenames>Shengchen</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Sound Event Detection with Sequentially Labelled Data Based on
  Connectionist Temporal Classification and Unsupervised Clustering</title><categories>cs.SD eess.AS</categories><journal-ref>ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</journal-ref><doi>10.1109/ICASSP.2019.8683627</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sound event detection (SED) methods typically rely on either strongly
labelled data or weakly labelled data. As an alternative, sequentially labelled
data (SLD) was proposed. In SLD, the events and the order of events in audio
clips are known, without knowing the occurrence time of events. This paper
proposes a connectionist temporal classification (CTC) based SED system that
uses SLD instead of strongly labelled data, with a novel unsupervised
clustering stage. Experiments on 41 classes of sound events show that the
proposed two-stage method trained on SLD achieves performance comparable to the
previous state-of-the-art SED system trained on strongly labelled data, and is
far better than another state-of-the-art SED system trained on weakly labelled
data, which indicates the effectiveness of the proposed two-stage method
trained on SLD without any onset/offset time of sound events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12146</identifier>
 <datestamp>2019-07-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12146</id><created>2019-04-27</created><updated>2019-07-18</updated><authors><author><keyname>Tonami</keyname><forenames>Noriyuki</forenames></author><author><keyname>Imoto</keyname><forenames>Keisuke</forenames></author><author><keyname>Niitsuma</keyname><forenames>Masahiro</forenames></author><author><keyname>Yamanishi</keyname><forenames>Ryosuke</forenames></author><author><keyname>Yamashita</keyname><forenames>Yoichi</forenames></author></authors><title>Joint Analysis of Acoustic Events and Scenes Based on Multitask Learning</title><categories>cs.SD eess.AS</categories><comments>Accepted to WASPAA 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustic event detection and scene classification are major research tasks in
environmental sound analysis, and many methods based on neural networks have
been proposed. Conventional methods have addressed these tasks separately;
however, acoustic events and scenes are closely related to each other. For
example, in the acoustic scene `office', the acoustic events `mouse clicking'
and `keyboard typing' are likely to occur. In this paper, we propose multitask
learning for joint analysis of acoustic events and scenes, which shares the
parts of the networks holding information on acoustic events and scenes in
common. By integrating the two networks, we expect that information on acoustic
scenes will improve the performance of acoustic event detection. Experimental
results obtained using TUT Sound Events 2016/2017 and TUT Acoustic Scenes 2016
datasets indicate that the proposed method improves the performance of acoustic
event detection by 10.66 percentage points in terms of the F-score, compared
with a conventional method based on a convolutional recurrent neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12175</identifier>
 <datestamp>2019-05-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12175</id><created>2019-04-27</created><updated>2019-05-10</updated><authors><author><keyname>Qu</keyname><forenames>Ying</forenames></author><author><keyname>Qi</keyname><forenames>Hairong</forenames></author><author><keyname>Kwan</keyname><forenames>Chiman</forenames></author></authors><title>Unsupervised and Unregistered Hyperspectral Image Super-Resolution with
  Mutual Dirichlet-Net</title><categories>cs.CV eess.IV</categories><comments>Submitted to IEEE Transactions on Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral images (HSI) provide rich spectral information that contributed
to the successful performance improvement of numerous computer vision tasks.
However, it can only be achieved at the expense of images' spatial resolution.
Hyperspectral image super-resolution (HSI-SR) addresses this problem by fusing
low resolution (LR) HSI with multispectral image (MSI) carrying much higher
spatial resolution (HR). All existing HSI-SR approaches require the LR HSI and
HR MSI to be well registered and the reconstruction accuracy of the HR HSI
relies heavily on the registration accuracy of different modalities. This paper
exploits the uncharted problem domain of HSI-SR without the requirement of
multi-modality registration. Given the unregistered LR HSI and HR MSI with
overlapped regions, we design a unique unsupervised learning structure linking
the two unregistered modalities by projecting them into the same statistical
space through the same encoder. The mutual information (MI) is further adopted
to capture the non-linear statistical dependencies between the representations
from two modalities (carrying spatial information) and their raw inputs. By
maximizing the MI, spatial correlations between different modalities can be
well characterized to further reduce the spectral distortion. A collaborative
$l_{2,1}$ norm is employed as the reconstruction error instead of the more
common $l_2$ norm, so that individual pixels can be recovered as accurately as
possible. With this design, the network allows to extract correlated spectral
and spatial information from unregistered images that better preserves the
spectral information. The proposed method is referred to as unregistered and
unsupervised mutual Dirichlet Net ($u^2$-MDN). Extensive experimental results
using benchmark HSI datasets demonstrate the superior performance of $u^2$-MDN
as compared to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12181</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12181</id><created>2019-04-27</created><authors><author><keyname>He</keyname><forenames>Xiang</forenames></author><author><keyname>Yang</keyname><forenames>Sibei</forenames></author><author><keyname>Li?</keyname><forenames>Guanbin</forenames></author><author><keyname>Li</keyname><forenames>Haofeng</forenames></author><author><keyname>Chang</keyname><forenames>Huiyou</forenames></author><author><keyname>Yu</keyname><forenames>Yizhou</forenames></author></authors><title>Non-Local Context Encoder: Robust Biomedical Image Segmentation against
  Adversarial Attacks</title><categories>cs.CV eess.IV</categories><comments>Accepted by AAAI2019 as oral presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in biomedical image segmentation based on deep convolutional
neural networks (CNNs) has drawn much attention. However, its vulnerability
towards adversarial samples cannot be overlooked. This paper is the first one
that discovers that all the CNN-based state-of-the-art biomedical image
segmentation models are sensitive to adversarial perturbations. This limits the
deployment of these methods in safety-critical biomedical fields. In this
paper, we discover that global spatial dependencies and global contextual
information in a biomedical image can be exploited to defend against
adversarial attacks. To this end, non-local context encoder (NLCE) is proposed
to model short- and long range spatial dependencies and encode global contexts
for strengthening feature activations by channel-wise attention. The NLCE
modules enhance the robustness and accuracy of the non-local context encoding
network (NLCEN), which learns robust enhanced pyramid feature representations
with NLCE modules, and then integrates the information across different levels.
Experiments on both lung and skin lesion segmentation datasets have
demonstrated that NLCEN outperforms any other state-of-the-art biomedical image
segmentation methods against adversarial attacks. In addition, NLCE modules can
be applied to improve the robustness of other CNN-based biomedical image
segmentation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12190</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12190</id><created>2019-04-27</created><authors><author><keyname>Avalos</keyname><forenames>Sebastian</forenames></author><author><keyname>Ortiz</keyname><forenames>Julian M.</forenames></author></authors><title>Geological modeling using a recursive convolutional neural networks
  approach</title><categories>eess.IV stat.OT</categories><comments>CIM convention 2019 - Conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Resource models are constrained by the extent of geological units that often
depend on the lithology, alteration and mineralization. A three dimensional
model of these geological units must be built from scarce information coming
from drillholes and limited understanding about the geological setting in which
the ore deposit is places. In this work, we present a new technique for
multiple-point geostatistical simulation based on a recursive convolutional
neural network approach (RCNN). The method requires conditioning data and a
training image that depicts the type of geological structures expected to be
found in the deposit. This training image is used to learn the patterns of
categories found and these are imposed in the final simulated model conditioned
by the categories found during logging at the actual drill-hole samples. A
lithological modeling process is carried out in a copper deposit in Chile to
demonstrate the method. Comparison with current techniques and spatial metrics
are used to clarify concepts and RCNN properties. Also, strengths and
weaknesses of the methodology are discussed by briefly reviewing the
theoretical perspective and looking into some of its practical aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12194</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12194</id><created>2019-04-27</created><authors><author><keyname>Deolekar</keyname><forenames>Subodh</forenames></author><author><keyname>Abraham</keyname><forenames>Siby</forenames></author></authors><title>Towards Automation of Creativity: A Machine Intelligence Approach</title><categories>cs.SD cs.AI eess.AS</categories><comments>31 pages, 24 figures, 12 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper demonstrates emergence of computational creativity in the field of
music. Different aspects of creativity such as producer, process, product and
press are studied and formulated. Different notions of computational creativity
such as novelty, quality and typicality of compositions as products are studied
and evaluated. We formulate an algorithmic perception on human creativity and
propose a prototype that is capable of demonstrating human-level creativity. We
then validate the proposed prototype by applying various creativity benchmarks
with the results obtained and compare the proposed prototype with the other
existing computational creative systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12200</identifier>
 <datestamp>2019-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12200</id><created>2019-04-27</created><updated>2019-10-01</updated><authors><author><keyname>Sharma</keyname><forenames>Anmol</forenames></author><author><keyname>Hamarneh</keyname><forenames>Ghassan</forenames></author></authors><title>Missing MRI Pulse Sequence Synthesis using Multi-Modal Generative
  Adversarial Network</title><categories>eess.IV cs.AI cs.CV cs.LG stat.ML</categories><comments>Accepted for publication in IEEE Transactions on Medical Imaging</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonance imaging (MRI) is being increasingly utilized to assess,
diagnose, and plan treatment for a variety of diseases. The ability to
visualize tissue in varied contrasts in the form of MR pulse sequences in a
single scan provides valuable insights to physicians, as well as enabling
automated systems performing downstream analysis. However many issues like
prohibitive scan time, image corruption, different acquisition protocols, or
allergies to certain contrast materials may hinder the process of acquiring
multiple sequences for a patient. This poses challenges to both physicians and
automated systems since complementary information provided by the missing
sequences is lost. In this paper, we propose a variant of generative
adversarial network (GAN) capable of leveraging redundant information contained
within multiple available sequences in order to generate one or more missing
sequences for a patient scan. The proposed network is designed as a
multi-input, multi-output network which combines information from all the
available pulse sequences, implicitly infers which sequences are missing, and
synthesizes the missing ones in a single forward pass. We demonstrate and
validate our method on two brain MRI datasets each with four sequences, and
show the applicability of the proposed method in simultaneously synthesizing
all missing sequences in any possible scenario where either one, two, or three
of the four sequences may be missing. We compare our approach with competing
unimodal and multi-modal methods, and show that we outperform both
quantitatively and qualitatively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12230</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12230</id><created>2019-04-27</created><authors><author><keyname>Kirkland</keyname><forenames>Paul</forenames></author></authors><title>UAV Detection: A STDP trained Deep Convolutional Spiking Neural Network
  Retina-Neuromorphic Approach</title><categories>eess.IV</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The Dynamic Vision Sensor (DVS) has many attributes that allow it to be well
suited to the task for UAV Detection. This paper is the first to look at
exploiting the features of an Event Camera solely for Drone Detection while
combining it with a Spiking Neural Network (SNN) trained using the unsupervised
approach of Spike-Time-Dependent Plasticity (STDP) for feature and pattern
recognition for detection. Highlighting the key features and current drawbacks
with the technology while comparing real and simulated data to show how future
devices could overcome these drawbacks to help tackle this current problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12237</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12237</id><created>2019-04-27</created><updated>2019-07-13</updated><authors><author><keyname>Ouyang</keyname><forenames>Chongjun</forenames></author><author><keyname>Ou</keyname><forenames>Zeliang</forenames></author><author><keyname>Yang</keyname><forenames>Pei</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Hongwen</forenames></author></authors><title>Ergodic Secrecy Rate of Antenna-Selection-Aided MIMOME Channels with
  BPSK/QPSK Modulations</title><categories>eess.SP</categories><comments>There are several errors in this article and the results are not very
  convincing. I hope to polish this paper. Because of this, I want to withdrawl
  it</comments><msc-class>45A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes transmit antenna selection (TAS) under Rayleigh flat
fading for BPSK/QPSK modulations in multiple-input multiple-output wiretap
channels, also termed as multiple-input multiple-output multiple-eavesdropper
(MIMOME) channels. In our protocol, a single antenna is selected to transmit
the secret message and selection combing (SC) or maximal-ratio combing (MRC) is
utilized at the legitimate receiver or the eavesdropper. Novel closed-form
expressions for the ergodic secrecy rates are derived to approximate the exact
values, which hold high precision and compact forms. Besides theoretical
derivations, simulations are provided to demonstrate the feasibility and
validity of the proposed formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12238</identifier>
 <datestamp>2019-07-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12238</id><created>2019-04-27</created><updated>2019-07-16</updated><authors><author><keyname>Ouyang</keyname><forenames>Chongjun</forenames></author><author><keyname>Wu</keyname><forenames>Sheng</forenames></author><author><keyname>Ou</keyname><forenames>Zeliang</forenames></author><author><keyname>Yang</keyname><forenames>Pei</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Hongwen</forenames></author></authors><title>Ergodic Mutual Information for Generalized Fadings</title><categories>eess.SP</categories><comments>4 pages</comments><msc-class>45A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, novel expressions are derived to evaluate the ergodic mutual
information (EMI) under BPSK modulation of single-input single-output (SISO)
systems operating in generalized fading channels, including $\eta$-$\mu$ fading
and $\kappa$-$\mu$ fading. To verify our derivation, we first investigate the
specific fading types, namely Rayleigh, Nakagami-$m$ and Rician, and then turn
to generalized fading scenarios. It is shown that all the expressions concluded
from the generalized cases can be specialized into those derived from the
specific ones. Different from the conventional results of the EMI, our
developed expressions contain only the simplest numerical calculations, without
any Meijer's G-functions which must be implemented in the particular computing
software. Additionally, it should be noted that our work provides a complete
analysis of the EMI in wireless channel under discrete inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12239</identifier>
 <datestamp>2019-06-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12239</id><created>2019-04-27</created><updated>2019-06-19</updated><authors><author><keyname>Ou</keyname><forenames>Zeliang</forenames></author><author><keyname>Ouyang</keyname><forenames>Chongjun</forenames></author><author><keyname>Yang</keyname><forenames>Pei</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Yang</keyname><forenames>Hongwen</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author></authors><title>Ergodic H-S/MRC Mutual Information</title><categories>eess.SP</categories><comments>5 pages. arXiv admin note: text overlap with arXiv:1903.03032</comments><msc-class>94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the ergodic mutual information of hybrid
selection/maximal-ratio combining (H-S/MRC) diversity system under BPSK/QPSK
modulations. We consider a simple single-input multiple-output (SIMO) channel,
where a subset of branches are selected and combined using maximal-ratio
combining (MRC) to maximize the instantaneous Signal to Noise Ratio (SNR) at
the receiver. For independent and identically distributed (i.i.d.) Rayleigh
flat fading, a general recursive expression is developed to estimate the
ergodic input-output mutual information of the whole system. Besides analytical
derivations, simulations are provided to demonstrate the feasibility and
validity of the derived results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12271</identifier>
 <datestamp>2019-05-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12271</id><created>2019-04-28</created><updated>2019-05-09</updated><authors><author><keyname>Sushmit</keyname><forenames>Asif Shahriyar</forenames></author><author><keyname>Zaman</keyname><forenames>Shakib Uz</forenames></author><author><keyname>Humayun</keyname><forenames>Ahmed Imtiaz</forenames></author><author><keyname>Hasan</keyname><forenames>Taufiq</forenames></author><author><keyname>Bhuiyan</keyname><forenames>Mohammed Imamul Hassan</forenames></author></authors><title>X-Ray Image Compression Using Convolutional Recurrent Neural Networks</title><categories>cs.CV eess.IV</categories><comments>4 pages, 2 figures, IEEE BHI 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the advent of a digital health revolution, vast amounts of clinical data
are being generated, stored and processed on a daily basis. This has made the
storage and retrieval of large volumes of health-care data, especially,
high-resolution medical images, particularly challenging. Effective image
compression for medical images thus plays a vital role in today's healthcare
information system, particularly in teleradiology. In this work, an X-ray image
compression method based on a Convolutional Recurrent Neural Networks RNN-Conv
is presented. The proposed architecture can provide variable compression rates
during deployment while it requires each network to be trained only once for a
specific dimension of X-ray images. The model uses a multi-level pooling scheme
that learns contextualized features for effective compression. We perform our
image compression experiments on the National Institute of Health (NIH)
ChestX-ray8 dataset and compare the performance of the proposed architecture
with a state-of-the-art RNN based technique and JPEG 2000. The experimental
results depict improved compression performance achieved by the proposed method
in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio
(PSNR) metrics. To the best of our knowledge, this is the first reported
evaluation on using a deep convolutional RNN for medical image compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12272</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12272</id><created>2019-04-28</created><authors><author><keyname>Wang</keyname><forenames>Mingjin</forenames></author><author><keyname>Gao</keyname><forenames>Feifei</forenames></author><author><keyname>Flanagan</keyname><forenames>Mark F.</forenames></author><author><keyname>Shlezinger</keyname><forenames>Nir</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>A Block Sparsity Based Estimator for mmWave Massive MIMO Channels with
  Beam Squint</title><categories>eess.SP cs.IT math.IT</categories><doi>10.1109/TSP.2019.2956677</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input multiple-output (MIMO) millimeter wave (mmWave) communication
is a key technology for next generation wireless networks. One of the
consequences of utilizing a large number of antennas with an increased
bandwidth is that array steering vectors vary among different subcarriers. Due
to this effect, known as beam squint, the conventional channel model is no
longer applicable for mmWave massive MIMO systems. In this paper, we study
channel estimation under the resulting non-standard model. To that aim, we
first analyze the beam squint effect from an array signal processing
perspective, resulting in a model which sheds light on the angle-delay sparsity
of mmWave transmission. We next design a compressive sensing based channel
estimation algorithm which utilizes the shift-invariant block-sparsity of this
channel model. The proposed algorithm jointly computes the off-grid angles, the
off-grid delays, and the complex gains of the multi-path channel. We show that
the newly proposed scheme reflects the mmWave channel more accurately and
results in improved performance compared to traditional approaches. We then
demonstrate how this approach can be applied to recover both the uplink as well
as the downlink channel in frequency division duplex (FDD) systems, by
exploiting the angle-delay reciprocity of mmWave channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12281</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12281</id><created>2019-04-28</created><authors><author><keyname>Zibaeenejad</keyname><forenames>Ali</forenames></author><author><keyname>Chen</keyname><forenames>Jun</forenames></author></authors><title>The Optimal Power Control Policy for an Energy Harvesting System with
  Look-Ahead: Bernoulli Energy Arrivals</title><categories>cs.IT eess.SP math.IT</categories><comments>8 pages, 2 figures, Extended Version, 2019 IEEE International
  Symposium on Information Theory (ISIT), Paris, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study power control for an energy harvesting communication system with
independent and identically distributed Bernoulli energy arrivals. It is
assumed that the transmitter is equipped with a finite-sized rechargeable
battery and is able to look ahead to observe a fixed number of future arrivals.
A complete characterization is provided for the optimal power control policy
that achieves the maximum long-term average throughput over an additive white
Gaussian noise channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12313</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12313</id><created>2019-04-28</created><authors><author><keyname>Cai</keyname><forenames>Qianqian</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaorong</forenames></author><author><keyname>Fu</keyname><forenames>Minyue</forenames></author></authors><title>A Fast Converging Distributed Solver for Linear Systems with Generalised
  Diagonal Dominance</title><categories>eess.SP</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new distributed algorithm for solving linear systems
associated with a sparse graph under a generalised diagonal dominance
assumption. The algorithm runs iteratively on each node of the graph, with low
complexities on local information exchange between neighbouring nodes, local
computation and local storage. For an acyclic graph under the condition of
diagonal dominance, the algorithm is shown to converge to the correct solution
in a finite number of iterations, equalling the diameter of the graph. For a
loopy graph, the algorithm is shown to converge to the correct solution
asymptotically. Simulations verify that the proposed algorithm significantly
outperforms the classical Jacobi method and a recent distributed linear system
solver based on average consensus and orthogonal projection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12323</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12323</id><created>2019-04-28</created><authors><author><keyname>Jena</keyname><forenames>Rohit</forenames></author></authors><title>An approach to image denoising using manifold approximation without
  clean images</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Image restoration has been an extensively researched topic in numerous
fields. With the advent of deep learning, a lot of the current algorithms were
replaced by algorithms that are more flexible and robust. Deep networks have
demonstrated impressive performance in a variety of tasks like blind denoising,
image enhancement, deblurring, super-resolution, inpainting, among others. Most
of these learning-based algorithms use a large amount of clean data during the
training process. However, in certain applications in medical image processing,
one may not have access to a large amount of clean data. In this paper, we
propose a method for denoising that attempts to learn the denoising process by
pushing the noisy data close to the clean data manifold, using only noisy
images during training. Furthermore, we use perceptual loss terms and an
iterative refinement step to further refine the clean images without losing
important features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12327</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12327</id><created>2019-04-28</created><authors><author><keyname>Srikanth</keyname><forenames>Goli</forenames></author><author><keyname>Chakka</keyname><forenames>Vijay Kumar</forenames></author><author><keyname>Shah</keyname><forenames>Shaik Basheeruddin</forenames></author></authors><title>Ramanujan Periodic Subspace Division Multiplexing (RPSDM)</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, a new modulation method defined as Ramanujan Periodic Subspace
Division Multiplexing (RPSDM) is proposed using Ramanujan subspaces. Each
subspace contains an integer valued Ramanujan Sum (RS) and its circular
downshifts as a basis. The proposed RPSDM decomposes the linear time-invariant
wireless channels into a Toeplitz stair block diagonal matrices, whereas
Orthogonal Frequency Division Multiplexing (OFDM) decompose the same into
diagonal. Advantages of such structured subspaces representation are studied
and compared with an OFDM representation in terms of Peak-Average Power Ratio
(PAPR) and Bit-Error-Rate (BER). Zero Forcing (ZF) and Minimum Mean Square
Error (MMSE) detectors are applied to evaluate the performance of OFDM and
RPSDM techniques. Finally, the simulation results show that the proposed design
(with an additional receiver complexity) outperforms OFDM under both detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12354</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12354</id><created>2019-04-28</created><authors><author><keyname>Teyhouee</keyname><forenames>Aydin</forenames></author><author><keyname>Osgood</keyname><forenames>Nathaniel D.</forenames></author></authors><title>Cough Detection Using Hidden Markov Models</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>SBP-BRiMS 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Respiratory infections and chronic respiratory diseases impose a heavy health
burden worldwide. Coughing is one of the most common symptoms of many such
infections, and can be indicative of flare-ups of chronic respiratory diseases.
Whether at a clinical or public health level, the capacity to identify bouts of
coughing can aid understanding of population and individual health status.
Developing health monitoring models in the context of respiratory diseases and
also seasonal diseases with symptoms such as cough has the potential to improve
quality of life, help clinicians and public health authorities with their
decisions and decrease the cost of health services. In this paper, we
investigated the ability to which a simple machine learning approach in the
form of Hidden Markov Models (HMMs) could be used to classify different states
of coughing using univariate (with a single energy band as the input feature)
and multivariate (with a multiple energy band as the input features) binned
time series using both of cough data. We further used the model to distinguish
cough events from other events and environmental noise. Our Hidden Markov
algorithm achieved 92% AUR (Area Under Receiver Operating Characteristic Curve)
in classifying coughing events in noisy environments. Moreover, comparison of
univariate with multivariate HMMs suggest a high accuracy of multivariate HMMs
for cough event classifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12399</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12399</id><created>2019-04-28</created><authors><author><keyname>Meng</keyname><forenames>Zhong</forenames></author><author><keyname>Li</keyname><forenames>Jinyu</forenames></author><author><keyname>Zhao</keyname><forenames>Yong</forenames></author><author><keyname>Gong</keyname><forenames>Yifan</forenames></author></authors><title>Conditional Teacher-Student Learning</title><categories>cs.LG cs.CL cs.SD eess.AS stat.ML</categories><comments>5 pages, 1 figure, ICASSP 2019</comments><journal-ref>2019 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Brighton, UK</journal-ref><doi>10.1109/ICASSP.2019.8683438</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The teacher-student (T/S) learning has been shown to be effective for a
variety of problems such as domain adaptation and model compression. One
shortcoming of the T/S learning is that a teacher model, not always perfect,
sporadically produces wrong guidance in form of posterior probabilities that
misleads the student model towards a suboptimal performance. To overcome this
problem, we propose a conditional T/S learning scheme, in which a &quot;smart&quot;
student model selectively chooses to learn from either the teacher model or the
ground truth labels conditioned on whether the teacher can correctly predict
the ground truth. Unlike a naive linear combination of the two knowledge
sources, the conditional learning is exclusively engaged with the teacher model
when the teacher model's prediction is correct, and otherwise backs off to the
ground truth. Thus, the student model is able to learn effectively from the
teacher and even potentially surpass the teacher. We examine the proposed
learning scheme on two tasks: domain adaptation on CHiME-3 dataset and speaker
adaptation on Microsoft short message dictation dataset. The proposed method
achieves 9.8% and 12.8% relative word error rate reductions, respectively, over
T/S learning for environment adaptation and speaker-independent model for
speaker adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12400</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12400</id><created>2019-04-28</created><authors><author><keyname>Meng</keyname><forenames>Zhong</forenames></author><author><keyname>Li</keyname><forenames>Jinyu</forenames></author><author><keyname>Gong</keyname><forenames>Yifan</forenames></author></authors><title>Attentive Adversarial Learning for Domain-Invariant Training</title><categories>cs.LG cs.CL cs.SD eess.AS stat.ML</categories><comments>5 pages, 1 figure, ICASSP 2019</comments><journal-ref>2019 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Brighton, United Kingdom</journal-ref><doi>10.1109/ICASSP.2019.8683486</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial domain-invariant training (ADIT) proves to be effective in
suppressing the effects of domain variability in acoustic modeling and has led
to improved performance in automatic speech recognition (ASR). In ADIT, an
auxiliary domain classifier takes in equally-weighted deep features from a deep
neural network (DNN) acoustic model and is trained to improve their
domain-invariance by optimizing an adversarial loss function. In this work, we
propose an attentive ADIT (AADIT) in which we advance the domain classifier
with an attention mechanism to automatically weight the input deep features
according to their importance in domain classification. With this attentive
re-weighting, AADIT can focus on the domain normalization of phonetic
components that are more susceptible to domain variability and generates deep
features with improved domain-invariance and senone-discriminativity over ADIT.
Most importantly, the attention block serves only as an external component to
the DNN acoustic model and is not involved in ASR, so AADIT can be used to
improve the acoustic modeling with any DNN architectures. More generally, the
same methodology can improve any adversarial learning system with an auxiliary
discriminator. Evaluated on CHiME-3 dataset, the AADIT achieves 13.6% and 9.3%
relative WER improvements, respectively, over a multi-conditional model and a
strong ADIT baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12403</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12403</id><created>2019-04-28</created><authors><author><keyname>Kim</keyname><forenames>Joshua Y.</forenames></author><author><keyname>Liu</keyname><forenames>Chunfeng</forenames></author><author><keyname>Calvo</keyname><forenames>Rafael A.</forenames></author><author><keyname>McCabe</keyname><forenames>Kathryn</forenames></author><author><keyname>Taylor</keyname><forenames>Silas C. R.</forenames></author><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn W.</forenames></author><author><keyname>Wu</keyname><forenames>Kaihang</forenames></author></authors><title>A Comparison of Online Automatic Speech Recognition Systems and the
  Nonverbal Responses to Unintelligible Speech</title><categories>cs.SD eess.AS</categories><comments>10 pages, 2 figures</comments><acm-class>H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic Speech Recognition (ASR) systems have proliferated over the recent
years to the point that free platforms such as YouTube now provide speech
recognition services. Given the wide selection of ASR systems, we contribute to
the field of automatic speech recognition by comparing the relative performance
of two sets of manual transcriptions and five sets of automatic transcriptions
(Google Cloud, IBM Watson, Microsoft Azure, Trint, and YouTube) to help
researchers to select accurate transcription services. In addition, we identify
nonverbal behaviors that are associated with unintelligible speech, as
indicated by high word error rates. We show that manual transcriptions remain
superior to current automatic transcriptions. Amongst the automatic
transcription services, YouTube offers the most accurate transcription service.
For non-verbal behavioral involvement, we provide evidence that the variability
of smile intensities from the listener is high (low) when the speaker is clear
(unintelligible). These findings are derived from videoconferencing
interactions between student doctors and simulated patients; therefore, we
contribute towards both the ASR literature and the healthcare communication
skills teaching community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12406</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12406</id><created>2019-04-28</created><authors><author><keyname>Meng</keyname><forenames>Zhong</forenames></author><author><keyname>Zhao</keyname><forenames>Yong</forenames></author><author><keyname>Li</keyname><forenames>Jinyu</forenames></author><author><keyname>Gong</keyname><forenames>Yifan</forenames></author></authors><title>Adversarial Speaker Verification</title><categories>cs.SD cs.CL cs.LG eess.AS stat.ML</categories><comments>5 pages, 1 figure, ICASSP 2019</comments><journal-ref>2019 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Brighton, United Kingdom</journal-ref><doi>10.1109/ICASSP.2019.8682488</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of deep networks to extract embeddings for speaker recognition has
proven successfully. However, such embeddings are susceptible to performance
degradation due to the mismatches among the training, enrollment, and test
conditions. In this work, we propose an adversarial speaker verification (ASV)
scheme to learn the condition-invariant deep embedding via adversarial
multi-task training. In ASV, a speaker classification network and a condition
identification network are jointly optimized to minimize the speaker
classification loss and simultaneously mini-maximize the condition loss. The
target labels of the condition network can be categorical (environment types)
and continuous (SNR values). We further propose multi-factorial ASV to
simultaneously suppress multiple factors that constitute the condition
variability. Evaluated on a Microsoft Cortana text-dependent speaker
verification task, the ASV achieves 8.8% and 14.5% relative improvements in
equal error rates (EER) for known and unknown conditions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12407</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12407</id><created>2019-04-28</created><authors><author><keyname>Meng</keyname><forenames>Zhong</forenames></author><author><keyname>Li</keyname><forenames>Jinyu</forenames></author><author><keyname>Gong</keyname><forenames>Yifan</forenames></author></authors><title>Adversarial Speaker Adaptation</title><categories>cs.LG cs.CL cs.SD eess.AS stat.ML</categories><comments>5 pages, 2 figures, ICASSP 2019</comments><journal-ref>2019 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Brighton, United Kingdom</journal-ref><doi>10.1109/ICASSP.2019.8682510</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel adversarial speaker adaptation (ASA) scheme, in which
adversarial learning is applied to regularize the distribution of deep hidden
features in a speaker-dependent (SD) deep neural network (DNN) acoustic model
to be close to that of a fixed speaker-independent (SI) DNN acoustic model
during adaptation. An additional discriminator network is introduced to
distinguish the deep features generated by the SD model from those produced by
the SI model. In ASA, with a fixed SI model as the reference, an SD model is
jointly optimized with the discriminator network to minimize the senone
classification loss, and simultaneously to mini-maximize the SI/SD
discrimination loss on the adaptation data. With ASA, a senone-discriminative
deep feature is learned in the SD model with a similar distribution to that of
the SI model. With such a regularized and adapted deep feature, the SD model
can perform improved automatic speech recognition on the target speaker's
speech. Evaluated on the Microsoft short message dictation dataset, ASA
achieves 14.4% and 7.9% relative word error rate improvements for supervised
and unsupervised adaptation, respectively, over an SI model trained from 2600
hours data, with 200 adaptation utterances per speaker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12434</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12434</id><created>2019-04-28</created><authors><author><keyname>Kitayama</keyname><forenames>Masaki</forenames></author><author><keyname>Kiya</keyname><forenames>Hitoshi</forenames></author></authors><title>HOG feature extraction from encrypted images for privacy-preserving
  machine learning</title><categories>cs.CV eess.IV</categories><comments>To appear in The 4th IEEE International Conference on Consumer
  Electronics (ICCE) Asia, Bankok, Thailand</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an extraction method of HOG
(histograms-of-oriented-gradients) features from encryption-then-compression
(EtC) images for privacy-preserving machine learning, where EtC images are
images encrypted by a block-based encryption method proposed for EtC systems
with JPEG compression, and HOG is a feature descriptor used in computer vision
for the purpose of object detection and image classification. Recently, cloud
computing and machine learning have been spreading in many fields. However, the
cloud computing has serious privacy issues for end users, due to unreliability
of providers and some accidents. Accordingly, we propose a novel block-based
extraction method of HOG features, and the proposed method enables us to carry
out any machine learning algorithms without any influence, under some
conditions. In an experiment, the proposed method is applied to a face image
recognition problem under the use of two kinds of classifiers: linear support
vector machine (SVM), gaussian SVM, to demonstrate the effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12462</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12462</id><created>2019-04-29</created><authors><author><keyname>Liu</keyname><forenames>Dong</forenames></author><author><keyname>Li</keyname><forenames>Yue</forenames></author><author><keyname>Lin</keyname><forenames>Jianping</forenames></author><author><keyname>Li</keyname><forenames>Houqiang</forenames></author><author><keyname>Wu</keyname><forenames>Feng</forenames></author></authors><title>Deep Learning-Based Video Coding: A Review and A Case Study</title><categories>cs.MM eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past decade has witnessed great success of deep learning technology in
many disciplines, especially in computer vision and image processing. However,
deep learning-based video coding remains in its infancy. This paper reviews the
representative works about using deep learning for image/video coding, which
has been an actively developing research area since the year of 2015. We divide
the related works into two categories: new coding schemes that are built
primarily upon deep networks (deep schemes), and deep network-based coding
tools (deep tools) that shall be used within traditional coding schemes or
together with traditional coding tools. For deep schemes, pixel probability
modeling and auto-encoder are the two approaches, that can be viewed as
predictive coding scheme and transform coding scheme, respectively. For deep
tools, there have been several proposed techniques using deep learning to
perform intra-picture prediction, inter-picture prediction, cross-channel
prediction, probability distribution prediction, transform, post- or in-loop
filtering, down- and up-sampling, as well as encoding optimizations. In the
hope of advocating the research of deep learning-based video coding, we present
a case study of our developed prototype video codec, namely Deep Learning Video
Coding (DLVC). DLVC features two deep tools that are both based on
convolutional neural network (CNN), namely CNN-based in-loop filter (CNN-ILF)
and CNN-based block adaptive resolution coding (CNN-BARC). Both tools help
improve the compression efficiency by a significant margin. With the two deep
tools as well as other non-deep coding tools, DLVC is able to achieve on
average 39.6\% and 33.0\% bits saving than HEVC, under random-access and
low-delay configurations, respectively. The source code of DLVC has been
released for future researches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12475</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12475</id><created>2019-04-29</created><updated>2019-04-30</updated><authors><author><keyname>Jiang</keyname><forenames>Tao</forenames></author><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author></authors><title>Over-the-Air Computation via Intelligent Reflecting Surfaces</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over-the-air computation (AirComp) becomes a promising approach for fast
wireless data aggregation via exploiting the superposition property in a
multiple access channel. To further overcome the unfavorable signal propagation
conditions for AirComp, in this paper, we propose an intelligent reflecting
surface (IRS) aided AirComp system to build controllable wireless environments,
thereby boosting the received signal power significantly. This is achieved by
smartly tuning the phase shifts for the incoming electromagnetic waves at IRS,
resulting in reconfigurable signal propagations. Unfortunately, it turns out
that the joint design problem for AirComp transceivers and IRS phase shifts
becomes a highly intractable nonconvex bi-quadratic programming problem, for
which a novel alternating difference-of-convex (DC) programming algorithm is
developed. This is achieved by providing a novel DC function representation for
the rank-one constraint in the low-rank matrix optimization problem via matrix
lifting. Simulation results demonstrate the algorithmic advantages and
admirable performance of the proposed approaches compared with the state-of-art
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12522</identifier>
 <datestamp>2019-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12522</id><created>2019-04-29</created><updated>2019-09-19</updated><authors><author><keyname>Lee</keyname><forenames>Jieun</forenames></author><author><keyname>Lee</keyname><forenames>Doohee</forenames></author><author><keyname>Choi</keyname><forenames>Joon Yul</forenames></author><author><keyname>Shin</keyname><forenames>Dongmyung</forenames></author><author><keyname>Shin</keyname><forenames>Hyeong-Geol</forenames></author><author><keyname>Lee</keyname><forenames>Jongho</forenames></author></authors><title>Artificial neural network for myelin water imaging</title><categories>eess.IV</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: To demonstrate the application of artificial-neural-network (ANN)
for real-time processing of myelin water imaging (MWI). Methods: Three neural
networks, ANN-IMWF, ANN-IGMT2, and ANN-II, were developed to generate MWI.
ANN-IMWF and ANN-IGMT2 were designed to output myelin water fraction (MWF) and
geometric mean T2 (GMT2,IEW), respectively whereas ANN-II generates a T2
distribution. For the networks, gradient and spin echo data from 18 healthy
controls (HC) and 26 multiple sclerosis patients (MS) were utilized. Among
them, 10 HC and 12 MS had the same scan parameters and were used for training
(6 HC and 6 MS), validation (1 HC and 1 MS), and test sets (3 HC and 5 HC). The
remaining data had different scan parameters and were applied to exam the
effects of the scan parameters. The network results were compared with those of
conventional MWI in the white matter mask and regions of interest (ROI).
Results: The networks produced highly accurate results, showing averaged
normalized root-mean-squared error under 3% for MWF and 0.4% for GMT2,IEW in
the white matter mask of the test set. In the ROI analysis, the differences
between ANNs and conventional MWI were less than 0.1% in MWF and 0.1 ms in
GMT2,IEW (no statistical difference and R2 &gt; 0.97). Datasets with different
scan parameters showed increased errors. The average processing time was 0.68
sec in ANNs, gaining 11,702 times acceleration in the computational speed
(conventional MWI: 7,958 sec). Conclusion: The proposed neural networks
demonstrate the feasibility of real-time processing for MWI with high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12561</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12561</id><created>2019-04-29</created><authors><author><keyname>van der Heide</keyname><forenames>Sjoerd</forenames></author><author><keyname>Chen</keyname><forenames>Bin</forenames></author><author><keyname>Hout</keyname><forenames>Menno van den</forenames></author><author><keyname>Liga</keyname><forenames>Gabriele</forenames></author><author><keyname>Koonen</keyname><forenames>Ton</forenames></author><author><keyname>Hafermann</keyname><forenames>Hartmut</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Okonkwo</keyname><forenames>Chigo</forenames></author></authors><title>11,700 km Transmission at 4.8 bit/4D-sym via Four-dimensional
  Geometrically-shaped Polarization-Ring-Switching Modulation</title><categories>eess.SP</categories><comments>Contributed paper for OECC/PSC. Tue. Jul 9, 2019 9:00 AM - 10:30 AM
  JST</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Using a novel geometrically-shaped four-dimensional modulation format, we
transmitted 11x200 Gbit/s DWDM at 4.8 bit/4D-sym over 7,925 km and 11,700 km
using EDFA-only and hybrid amplification, respectively. A reach increase of 16%
is achieved over PM-8QAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12585</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12585</id><created>2019-04-01</created><updated>2019-05-06</updated><authors><author><keyname>Ziabari</keyname><forenames>Amirkoushyar</forenames></author><author><keyname>Kirka</keyname><forenames>Michael</forenames></author><author><keyname>Paquit</keyname><forenames>Vincent</forenames></author><author><keyname>Bingham</keyname><forenames>Philip</forenames></author><author><keyname>Venkatakrishnan</keyname><forenames>Singanallur</forenames></author></authors><title>X-Ray CT Reconstruction of Additively Manufactured Parts using 2.5D Deep
  Learning MBIR</title><categories>cs.CV eess.IV</categories><doi>10.1017/S1431927619002617</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a deep learning algorithm to rapidly obtain high
quality CT reconstructions for AM parts. In particular, we propose to use CAD
models of the parts that are to be manufactured, introduce typical defects and
simulate XCT measurements. These simulated measurements were processed using
FBP (computationally simple but result in noisy images) and the MBIR technique.
We then train a 2.5D deep convolutional neural network [4], deemed 2.5D Deep
Learning MBIR (2.5D DL-MBIR), on these pairs of noisy and high-quality 3D
volumes to learn a fast, non-linear mapping function. The 2.5D DL-MBIR
reconstructs a 3D volume in a 2.5D scheme where each slice is reconstructed
from multiple inputs slices of the FBP input. Given this trained system, we can
take a small set of measurements on an actual part, process it using a
combination of FBP followed by 2.5D DL-MBIR. Both steps can be rapidly
performed using GPUs, resulting in a real-time algorithm that achieves the
high-quality of MBIR as fast as standard techniques. Intuitively, since CAD
models are typically available for parts to be manufactured, this provides a
strong constraint &quot;prior&quot; which can be leveraged to improve the reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12603</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12603</id><created>2019-04-17</created><authors><author><keyname>Khan</keyname><forenames>Zohaib</forenames></author><author><keyname>Shafait</keyname><forenames>Faisal</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author></authors><title>Converting a Common Document Scanner to a Multispectral Scanner</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We propose the construction of a prototype scanner designed to capture
multispectral images of documents. A standard sheet-feed scanner is modified by
disconnecting its internal light source and connecting an external
multispectral light source comprising of narrow band light emitting diodes
(LED). A document is scanned by illuminating the scanner light guide
successively with different LEDs and capturing a scan of the document. The
system is portable and can be used for potential applications in verification
of questioned documents, cheques, receipts and bank notes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12614</identifier>
 <datestamp>2019-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12614</id><created>2019-04-21</created><updated>2019-09-10</updated><authors><author><keyname>Brody</keyname><forenames>Dorje C</forenames></author></authors><title>Modelling election dynamics and the impact of disinformation</title><categories>physics.soc-ph eess.SP math.DS math.PR q-fin.MF</categories><comments>20 pages, 5 figures</comments><journal-ref>Information Geometry, 2019</journal-ref><doi>10.1007/s41884-019-00021-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex dynamical systems driven by the unravelling of information can be
modelled effectively by treating the underlying flow of information as the
model input. Complicated dynamical behaviour of the system is then derived as
an output. Such an information-based approach is in sharp contrast to the
conventional mathematical modelling of information-driven systems whereby one
attempts to come up with essentially {\it ad hoc} models for the outputs. Here,
dynamics of electoral competition is modelled by the specification of the flow
of information relevant to election. The seemingly random evolution of the
election poll statistics are then derived as model outputs, which in turn are
used to study election prediction, impact of disinformation, and the optimal
strategy for information management in an election campaign.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12687</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12687</id><created>2019-04-26</created><authors><author><keyname>Al-Hameed</keyname><forenames>Aubida A.</forenames></author><author><keyname>Younus</keyname><forenames>Safwan Hafeedh</forenames></author><author><keyname>Hussein</keyname><forenames>Ahmed Taha</forenames></author><author><keyname>Alresheedi</keyname><forenames>Mohammed T.</forenames></author><author><keyname>Elmirghani</keyname><forenames>Jaafar M. H.</forenames></author></authors><title>Artificial Neural Network for LiDAL Systems</title><categories>eess.SP</categories><comments>arXiv admin note: substantial text overlap with arXiv:1903.09896</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an intelligent light detection and localization
(LiDAL) system that uses artificial neural networks (ANN). The LiDAL systems of
interest are MIMO LiDAL and MISO IMG LiDAL systems. A trained ANN with the
LiDAL system of interest is used to distinguish a human (target) from the
background obstacles (furniture) in a realistic indoor environment. In the
LiDAL systems, the received reflected signals in the time domain have different
patterns corresponding to the number of targets and their locations in an
indoor environment. The indoor environment with background obstacles
(furniture) appears as a set of patterns in the time domain when the
transmitted optical signals are reflected from objects in LiDAL systems. Hence,
a trained neural network that has the ability to classify and recognize the
received signal patterns can distinguish the targets from the background
obstacles in a realistic environment. The LiDAL systems with ANN are evaluated
in a realistic indoor environment through computer simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12732</identifier>
 <datestamp>2019-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12732</id><created>2019-04-18</created><updated>2019-08-14</updated><authors><author><keyname>Sarhan</keyname><forenames>Mhd Hasan</forenames></author><author><keyname>Albarqouni</keyname><forenames>Shadi</forenames></author><author><keyname>Yigitsoy</keyname><forenames>Mehmet</forenames></author><author><keyname>Navab</keyname><forenames>Nassir</forenames></author><author><keyname>Eslami</keyname><forenames>Abouzar</forenames></author></authors><title>Multi-scale Microaneurysms Segmentation Using Embedding Triplet Loss</title><categories>eess.IV cs.CV physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning techniques are recently being used in fundus image analysis and
diabetic retinopathy detection. Microaneurysms are an important indicator of
diabetic retinopathy progression. We introduce a two-stage deep learning
approach for microaneurysms segmentation using multiple scales of the input
with selective sampling and embedding triplet loss. The model first segments on
two scales and then the segmentations are refined with a classification model.
To enhance the discriminative power of the classification model, we incorporate
triplet embedding loss with a selective sampling routine. The model is
evaluated quantitatively to assess the segmentation performance and
qualitatively to analyze the model predictions. This approach introduces a
30.29% relative improvement over the fully convolutional neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12733</identifier>
 <datestamp>2019-07-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12733</id><created>2019-04-17</created><authors><author><keyname>Noyel</keyname><forenames>Guillaume</forenames><affiliation>IPRI, SIGPH@iPRI</affiliation></author><author><keyname>Thomas</keyname><forenames>R</forenames><affiliation>DESW</affiliation></author><author><keyname>Iles</keyname><forenames>S</forenames><affiliation>DESW</affiliation></author><author><keyname>Bhakta</keyname><forenames>G</forenames><affiliation>DESW</affiliation></author><author><keyname>Crowder</keyname><forenames>A</forenames><affiliation>DESW</affiliation></author><author><keyname>Owens</keyname><forenames>D.</forenames><affiliation>IPRI, SIGPH@iPRI</affiliation></author><author><keyname>Boyle</keyname><forenames>P.</forenames><affiliation>IPRI, SIGPH@iPRI</affiliation></author></authors><title>Registration of retinal images from Public Health by minimising an error
  between vessels using an affine model with radial distortions</title><categories>physics.med-ph cs.CV eess.IV</categories><proxy>ccsd</proxy><journal-ref>IEEE 16th International Symposium on Biomedical Imaging (ISBI
  2019), IEEE, Apr 2019, Venice, Italy. pp.561-564</journal-ref><doi>10.1109/ISBI.2019.8759415</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to estimate a registration model of eye fundus images made of an
affinity and two radial distortions, we introduce an estimation criterion based
on an error between the vessels. In [1], we estimated this model by minimising
the error between characteristics points. In this paper, the detected vessels
are selected using the circle and ellipse equations of the overlap area
boundaries deduced from our model. Our method successfully registers 96 % of
the 271 pairs in a Public Health dataset acquired mostly with different
cameras. This is better than our previous method [1] and better than three
other state-of-the-art methods. On a publicly available dataset, ours still
better register the images than the reference method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12743</identifier>
 <datestamp>2019-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12743</id><created>2019-04-29</created><authors><author><keyname>Morales</keyname><forenames>Giorgio</forenames></author><author><keyname>Ram&#xed;rez</keyname><forenames>Alejandro</forenames></author><author><keyname>Telles</keyname><forenames>Joel</forenames></author></authors><title>End-to-end Cloud Segmentation in High-Resolution Multispectral Satellite
  Imagery Using Deep Learning</title><categories>cs.CV eess.IV</categories><comments>Submitted to INTERCON2019 conference. Lima, Peru</comments><journal-ref>2019 IEEE XXVI International Conference on Electronics, Electrical
  Engineering and Computing (INTERCON)</journal-ref><doi>10.1109/INTERCON.2019.8853549</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Segmenting clouds in high-resolution satellite images is an arduous and
challenging task due to the many types of geographies and clouds a satellite
can capture. Therefore, it needs to be automated and optimized, specially for
those who regularly process great amounts of satellite images, such as
governmental institutions. In that sense, the contribution of this work is
twofold: We present the CloudPeru2 dataset, consisting of 22,400 images of
512x512 pixels and their respective hand-drawn cloud masks, as well as the
proposal of an end-to-end segmentation method for clouds using a Convolutional
Neural Network (CNN) based on the Deeplab v3+ architecture. The results over
the test set achieved an accuracy of 96.62%, precision of 96.46%, specificity
of 98.53%, and sensitivity of 96.72% which is superior to the compared methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12769</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12769</id><created>2019-04-29</created><authors><author><keyname>Adavanne</keyname><forenames>Sharath</forenames></author><author><keyname>Politis</keyname><forenames>Archontis</forenames></author><author><keyname>Virtanen</keyname><forenames>Tuomas</forenames></author></authors><title>Localization, Detection and Tracking of Multiple Moving Sound Sources
  with a Convolutional Recurrent Neural Network</title><categories>cs.SD cs.LG eess.AS</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper investigates the joint localization, detection, and tracking of
sound events using a convolutional recurrent neural network (CRNN). We use a
CRNN previously proposed for the localization and detection of stationary
sources, and show that the recurrent layers enable the spatial tracking of
moving sources when trained with dynamic scenes. The tracking performance of
the CRNN is compared with a stand-alone tracking method that combines a
multi-source (DOA) estimator and a particle filter. Their respective
performance is evaluated in various acoustic conditions such as anechoic and
reverberant scenarios, stationary and moving sources at several angular
velocities, and with a varying number of overlapping sources. The results show
that the CRNN manages to track multiple sources more consistently than the
parametric method across acoustic scenarios, but at the cost of higher
localization error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12810</identifier>
 <datestamp>2020-01-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12810</id><created>2019-04-29</created><updated>2020-01-06</updated><authors><author><keyname>Chatelain</keyname><forenames>Florent</forenames></author><author><keyname>Bihan</keyname><forenames>Nicolas Le</forenames></author><author><keyname>Manton</keyname><forenames>Jonathan H.</forenames></author></authors><title>Asymptotic regime for impropriety tests of complex random vectors</title><categories>eess.SP math.ST stat.TH</categories><comments>11 pages, 8 figures, submitted to IEEE TSP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Impropriety testing for complex-valued vector has been considered lately due
to potential applications ranging from digital communications to complex media
imaging. This paper provides new results for such tests in the asymptotic
regime, i.e. when the vector dimension and sample size grow commensurately to
infinity. The studied tests are based on invariant statistics named impropriety
coefficients. Limiting distributions for these statistics are derived, together
with those of the Generalized Likelihood Ratio Test (GLRT) and Roy's test, in
the Gaussian case. This characterization in the asymptotic regime allows also
to identify a phase transition in Roy's test with potential application in
detection of complex-valued low-rank subspace corrupted by proper noise in
large datasets. Simulations illustrate the accuracy of the proposed asymptotic
approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12824</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12824</id><created>2019-04-29</created><authors><author><keyname>Goossens</keyname><forenames>Jan-Willem</forenames></author><author><keyname>Jaou&#xeb;n</keyname><forenames>Yves</forenames></author><author><keyname>Hafermann</keyname><forenames>Hartmut</forenames></author></authors><title>Experimental Demonstration of Data Transmission Based on the Exact
  Inverse Periodic Nonlinear Fourier Transform</title><categories>cs.IT eess.SP math.IT</categories><comments>Accepted in the Optical Fiber Communications Conference (OFC) 2019</comments><doi>10.1364/OFC.2019.M1I.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a two-dimensional signal constellation based on the exact periodic
inverse nonlinear Fourier transform. Feasibility of continuous transmission
with periodic signals is experimentally demonstrated over more than 2000 km.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12835</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12835</id><created>2019-04-29</created><authors><author><keyname>Grossi</keyname><forenames>Emanuele</forenames></author><author><keyname>Lops</keyname><forenames>Marco</forenames></author><author><keyname>Venturino</keyname><forenames>Luca</forenames></author></authors><title>Adaptive detection and localization exploiting the IEEE 802.11ad
  standard</title><categories>eess.SP</categories><comments>Journal paper submitted to the IEEE Transactions on Wireless
  Communications on April 19, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we exploit the sector level sweep of the IEEE 802.11ad
communication standard to implement an opportunistic radar at mmWaves and
derive an adaptive procedure for detecting multiple targets (echoes) and
estimating their parameters. The proposed detector/estimator extracts the
prospective echoes one-by-one from the received signal, after removing the
interference caused by the previously detected (stronger) targets. Examples are
provided to assess the system performance, also in comparison with the
canonical matched-filter peak-detector and the Cram\'er-Rao bounds. Results
indicate that the proposed method is robust against the signal spillover and
the near-far problem caused by the imperfect auto-correlation of the probing
signal and, for the same probability of false alarm, grants detection and
localization performances close to those previously obtained in a simplified
single-target scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12926</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12926</id><created>2019-04-29</created><authors><author><keyname>Shi</keyname><forenames>Bowen</forenames></author><author><keyname>Sun</keyname><forenames>Ming</forenames></author><author><keyname>Kao</keyname><forenames>Chieh-Chi</forenames></author><author><keyname>Rozgic</keyname><forenames>Viktor</forenames></author><author><keyname>Matsoukas</keyname><forenames>Spyros</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author></authors><title>Semi-supervised Acoustic Event Detection based on tri-training</title><categories>eess.AS cs.LG cs.SD</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our work of training acoustic event detection (AED)
models using unlabeled dataset. Recent acoustic event detectors are based on
large-scale neural networks, which are typically trained with huge amounts of
labeled data. Labels for acoustic events are expensive to obtain, and relevant
acoustic event audios can be limited, especially for rare events. In this paper
we leverage an Internet-scale unlabeled dataset with potential domain shift to
improve the detection of acoustic events. Based on the classic tri-training
approach, our proposed method shows accuracy improvement over both the
supervised training baseline, and semisupervised self-training set-up, in all
pre-defined acoustic event detection tasks. As our approach relies on ensemble
models, we further show the improvements can be distilled to a single model via
knowledge distillation, with the resulting single student model maintaining
high accuracy of teacher ensemble models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12945</identifier>
 <datestamp>2019-08-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12945</id><created>2019-04-29</created><updated>2019-07-30</updated><authors><author><keyname>Liu</keyname><forenames>Jiaming</forenames></author><author><keyname>Wu</keyname><forenames>Chi-Hao</forenames></author><author><keyname>Wang</keyname><forenames>Yuzhi</forenames></author><author><keyname>Xu</keyname><forenames>Qin</forenames></author><author><keyname>Zhou</keyname><forenames>Yuqian</forenames></author><author><keyname>Huang</keyname><forenames>Haibin</forenames></author><author><keyname>Wang</keyname><forenames>Chuan</forenames></author><author><keyname>Cai</keyname><forenames>Shaofan</forenames></author><author><keyname>Ding</keyname><forenames>Yifan</forenames></author><author><keyname>Fan</keyname><forenames>Haoqiang</forenames></author><author><keyname>Wang</keyname><forenames>Jue</forenames></author></authors><title>Learning Raw Image Denoising with Bayer Pattern Unification and Bayer
  Preserving Augmentation</title><categories>cs.CV eess.IV</categories><comments>Accepted by CVPRW 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present new data pre-processing and augmentation techniques
for DNN-based raw image denoising. Compared with traditional RGB image
denoising, performing this task on direct camera sensor readings presents new
challenges such as how to effectively handle various Bayer patterns from
different data sources, and subsequently how to perform valid data augmentation
with raw images. To address the first problem, we propose a Bayer pattern
unification (BayerUnify) method to unify different Bayer patterns. This allows
us to fully utilize a heterogeneous dataset to train a single denoising model
instead of training one model for each pattern. Furthermore, while it is
essential to augment the dataset to improve model generalization and
performance, we discovered that it is error-prone to modify raw images by
adapting augmentation methods designed for RGB images. Towards this end, we
present a Bayer preserving augmentation (BayerAug) method as an effective
approach for raw image augmentation. Combining these data processing technqiues
with a modified U-Net, our method achieves a PSNR of 52.11 and a SSIM of 0.9969
in NTIRE 2019 Real Image Denoising Challenge, demonstrating the
state-of-the-art performance. Our code is available at
https://github.com/Jiaming-Liu/BayerUnifyAug.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12953</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12953</id><created>2019-04-29</created><authors><author><keyname>Tetzlaff</keyname><forenames>Thomas</forenames></author></authors><title>Low-complexity prediction of complex-valued sequences using a novel
  &quot;residual-as-prediction&quot; method</title><categories>eess.SP</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method of prediction is presented to aid compression of sequences of
complex-valued samples. The focus is on using prediction to reduce the average
magnitude of residual values after prediction (not on the subsequent
compression of the residual sequence). The prediction method has low
computational complexity, so as to keep power consumption in implementations of
the method low. The new method presented applies specifically to sequences that
occupy a significant percentage of the sampling bandwidth; something that
existing, simple prediction methods fail to adequately address. The new method,
labeled &quot;residual-as-prediction&quot; here, produces residual sequences with reduced
mean magnitude compared to the original sequence, even for sequences whose
bandwidth is up to 85% of the sampling bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.12999</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.12999</id><created>2019-04-29</created><authors><author><keyname>Shafiee</keyname><forenames>Maryam</forenames></author><author><keyname>Ozev</keyname><forenames>Sule</forenames></author></authors><title>An In-Field Programmable Adaptive CMOS LNA for Intelligent IOT Sensor
  Node Applications</title><categories>eess.SP cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the Internet of Things (IOT) is growing rapidly, there is an emerging need
to facilitate development of IOT devices in the design cycle while optimized
performance is obtained in the field of operation. This paper develops
reconfiguration approaches that enable post-production adaptation of circuit
performance to enable RF IC re-use across different IOT applications. An
adaptable low noise amplifier is designed and fabricated in 130nm CMOS
technology to investigate the post-production reconfiguration concept. A
statistical model that relates circuit-level reconfiguration parameters to
circuit performances is generated by characterizing a limited number of
samples. This model is used to predict the performance parameters of the device
in the field. The estimation error for LNA performance parameters are obtained
in the simulation environment as well as chip measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13010</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13010</id><created>2019-04-29</created><authors><author><keyname>Zhang</keyname><forenames>Zezhong</forenames></author><author><keyname>Ko</keyname><forenames>Seung-Woo</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author></authors><title>Realizing Multi-Point Vehicular Positioning via Millimeter-wave
  Transmission</title><categories>cs.IT eess.SP math.IT</categories><comments>9 pages, 6 figures, conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-point detection of the full-scale environment is an important issue in
autonomous driving. The state-of-the-art positioning technologies (such as
RADAR and LIDAR) are incapable of real-time detection without line-of-sight. To
address this issue, this paper presents a novel multi-point vehicular
positioning technology via \emph{millimeter-wave} (mmWave) transmission that
exploits multi-path reflection from a \emph{target vehicle} (TV) to a
\emph{sensing vehicle} (SV), which enables the SV to fast capture both the
shape and location information of the TV in \emph{non-line-of-sight} (NLoS)
under the assistance of multi-path reflections. A
\emph{phase-difference-of-arrival} (PDoA) based hyperbolic positioning
algorithm is designed to achieve the synchronization between the TV and SV. The
\emph{stepped-frequency-continuous-wave} (SFCW) is utilized as signals for
multi-point detection of the TVs.
  Transceiver separation enables our approach to work in NLoS conditions and
achieve much lower latency compared with conventional positioning techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13017</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13017</id><created>2019-04-29</created><authors><author><keyname>Zhao</keyname><forenames>Min</forenames></author><author><keyname>Wang</keyname><forenames>Mou</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Rahardja</keyname><forenames>Susanto</forenames></author></authors><title>Hyperspectral Unmixing via Deep Autoencoder Networks for a Generalized
  Linear-Mixture/Nonlinear-Fluctuation Model</title><categories>eess.IV</categories><comments>11 pages, 9 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral unmixing is an important task in hyperspectral image processing for
separating the mixed spectral data pertaining to various materials observed
individual pixels. Recently, nonlinear spectral unmixing has received
particular attention because a linear mixture is not appropriate under many
conditions. However, existing nonlinear unmixing approaches are often based on
specific assumptions regarding the inherent nonlinearity, and they can be
ineffective when applied to conditions deviating from the original assumptions.
Therefore, these approaches are not well suited to scenes with unknown
nonlinearity characteristics. This paper presents an unsupervised nonlinear
spectral unmixing method based on a deep autoencoder network that applies to a
generalized linear-mixture/nonlinear fluctuation model, consisting of a linear
mixture component and an additive nonlinear mixture component that depends on
both endmembers and abundances. The proposed approach benefits from the
universal modeling ability of deep neural networks to learn the inherent
nonlinearity of the nonlinear mixture component from the data itself via the
autoencoder network, rather than relying on an assumed form. Extensive
experiments with numerically synthetic, labeled laboratory-created data and
real airborne data, illustrate the generality and effectiveness of this
approach compared with state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13028</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13028</id><created>2019-04-29</created><authors><author><keyname>Bai</keyname><forenames>Jinqiang</forenames></author><author><keyname>Lian</keyname><forenames>Shiguo</forenames></author><author><keyname>Liu</keyname><forenames>Zhaoxiang</forenames></author><author><keyname>Wang</keyname><forenames>Kai</forenames></author><author><keyname>Liu</keyname><forenames>Dijun</forenames></author></authors><title>Virtual-Blind-Road Following Based Wearable Navigation Device for Blind
  People</title><categories>cs.CV eess.IV</categories><comments>8 pages, 9 figures, TCE accepted</comments><doi>10.1109/TCE.2018.2812498</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To help the blind people walk to the destination efficiently and safely in
indoor environment, a novel wearable navigation device is presented in this
paper. The locating, way-finding, route following and obstacle avoiding modules
are the essential components in a navigation system, while it remains a
challenging task to consider obstacle avoiding during route following, as the
indoor environment is complex, changeable and possibly with dynamic objects. To
address this issue, we propose a novel scheme which utilizes a dynamic sub-goal
selecting strategy to guide the users to the destination and help them bypass
obstacles at the same time. This scheme serves as the key component of a
complete navigation system deployed on a pair of wearable optical see-through
glasses for the ease of use of blind people's daily walks. The proposed
navigation device has been tested on a collection of individuals and proved to
be effective on indoor navigation tasks. The sensors embedded are of low cost,
small volume and easy integration, making it possible for the glasses to be
widely used as a wearable consumer device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13032</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13032</id><created>2019-04-29</created><authors><author><keyname>Ahmed</keyname><forenames>Kazi Ishfaq</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>A Deep Q-Learning Method for Downlink Power Allocation in Multi-Cell
  Networks</title><categories>cs.NI cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal resource allocation is a fundamental challenge for dense and
heterogeneous wireless networks with massive wireless connections. Because of
the non-convex nature of the optimization problem, it is computationally
demanding to obtain the optimal resource allocation. Recently, deep
reinforcement learning (DRL) has emerged as a promising technique in solving
non-convex optimization problems. Unlike deep learning (DL), DRL does not
require any optimal/ near-optimal training dataset which is either unavailable
or computationally expensive in generating synthetic data. In this paper, we
propose a novel centralized DRL based downlink power allocation scheme for a
multi-cell system intending to maximize the total network throughput.
Specifically, we apply a deep Q-learning (DQL) approach to achieve near-optimal
power allocation policy. For benchmarking the proposed approach, we use a
Genetic Algorithm (GA) to obtain near-optimal power allocation solution.
Simulation results show that the proposed DRL-based power allocation scheme
performs better compared to the conventional power allocation schemes in a
multi-cell scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13036</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13036</id><created>2019-04-29</created><authors><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Zhang</keyname><forenames>Fahong</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>Optimal Clustering Framework for Hyperspectral Band Selection</title><categories>eess.IV cs.LG stat.ML</categories><journal-ref>IEEE Trans. Geoscience and Remote Sensing, vol. 56, no. 10, pp.
  5910-5922, 2018</journal-ref><doi>10.1109/TGRS.2018.2828161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Band selection, by choosing a set of representative bands in hyperspectral
image (HSI), is an effective method to reduce the redundant information without
compromising the original contents. Recently, various unsupervised band
selection methods have been proposed, but most of them are based on
approximation algorithms which can only obtain suboptimal solutions toward a
specific objective function. This paper focuses on clustering-based band
selection, and proposes a new framework to solve the above dilemma, claiming
the following contributions: 1) An optimal clustering framework (OCF), which
can obtain the optimal clustering result for a particular form of objective
function under a reasonable constraint. 2) A rank on clusters strategy (RCS),
which provides an effective criterion to select bands on existing clustering
structure. 3) An automatic method to determine the number of the required
bands, which can better evaluate the distinctive information produced by
certain number of bands. In experiments, the proposed algorithm is compared to
some state-of-the-art competitors. According to the experimental results, the
proposed algorithm is robust and significantly outperform the other methods on
various data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13038</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13038</id><created>2019-04-29</created><updated>2019-05-04</updated><authors><author><keyname>Singh</keyname><forenames>Rishabh</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>A New Uncertainty Framework for Stochastic Signal Processing</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fields of signal processing and information theory have evolved with the
goal of developing formulations to extract intrinsic information from limited
amount of data. When one considers the modeling of unpredictably varying
processes and complex dynamical signals with a large number of unknowns (such
as those encountered in the fields of finance, NLP, communications, etc.),
there is a need for algorithms to have increased sensitivity with short spans
of data, while maintaining stochastic generalization ability. This naturally
calls for an increased focus on localized stochastic representation. So far,
most metrics developed for characterizing signals envision data from entropic
and probabilistic points of view that lack sensitivity towards quick changes in
signal dynamics. We hypothesize that models that work with the intrinsic
uncertainties associated with local data induced metric spaces would be
significantly more sensitive towards signal characterization. To this end, we
develop a new framework for stochastic signal processing that is based on
decomposing the local metric space of the signal in a Gaussian Reproducing
Kernel Hilbert Space (RKHS). A major advantage of our framework is that we are
able to implement this decomposition on a sample-by-sample basis. The key
aspects of our framework are the following: (1) We use a data defined metric
related to Parzen density estimation for quantifying the local structure of
data in the Gaussian RKHS. (2) We use a quantum description of this metric
which consequently introduces uncertainty in the structure of the local kernel
space. Since the RKHS has been well established and known for providing
universal data fitting capabilities, we submit that local quantifiers of the
kernel space data projection could significantly improve acquisition of signal
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13142</identifier>
 <datestamp>2019-07-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13142</id><created>2019-04-30</created><updated>2019-07-01</updated><authors><author><keyname>Liao</keyname><forenames>Chien-Feng</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Lu</keyname><forenames>Xugang</forenames></author><author><keyname>Kawai</keyname><forenames>Hisashi</forenames></author></authors><title>Incorporating Symbolic Sequential Modeling for Speech Enhancement</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Accepted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a noisy environment, a lossy speech signal can be automatically restored
by a listener if he/she knows the language well. That is, with the built-in
knowledge of a &quot;language model&quot;, a listener may effectively suppress noise
interference and retrieve the target speech signals. Accordingly, we argue that
familiarity with the underlying linguistic content of spoken utterances
benefits speech enhancement (SE) in noisy environments. In this study, in
addition to the conventional modeling for learning the acoustic noisy-clean
speech mapping, an abstract symbolic sequential modeling is incorporated into
the SE framework. This symbolic sequential modeling can be regarded as a
&quot;linguistic constraint&quot; in learning the acoustic noisy-clean speech mapping
function. In this study, the symbolic sequences for acoustic signals are
obtained as discrete representations with a Vector Quantized Variational
Autoencoder algorithm. The obtained symbols are able to capture high-level
phoneme-like content from speech signals. The experimental results demonstrate
that the proposed framework can obtain notable performance improvement in terms
of perceptual evaluation of speech quality (PESQ) and short-time objective
intelligibility (STOI) on the TIMIT dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13204</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13204</id><created>2019-04-30</created><authors><author><keyname>Alekseev</keyname><forenames>Andrey</forenames></author><author><keyname>Bobe</keyname><forenames>Anatoly</forenames></author></authors><title>GaborNet: Gabor filters with learnable parameters in deep convolutional
  neural networks</title><categories>cs.CV cs.LG eess.IV</categories><comments>10 pages, 6 figures, 3 tables, preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article describes a system for image recognition using deep convolutional
neural networks. Modified network architecture is proposed that focuses on
improving convergence and reducing training complexity. The filters in the
first layer of the network are constrained to fit the Gabor function. The
parameters of Gabor functions are learnable and are updated by standard
backpropagation techniques. The system was implemented on Python, tested on
several datasets and outperformed the common convolutional networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13216</identifier>
 <datestamp>2019-08-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13216</id><created>2019-04-18</created><updated>2019-08-01</updated><authors><author><keyname>Bizopoulos</keyname><forenames>Paschalis</forenames></author><author><keyname>Lambrou</keyname><forenames>George I</forenames></author><author><keyname>Koutsouris</keyname><forenames>Dimitrios</forenames></author></authors><title>Signal2Image Modules in Deep Neural Networks for EEG Classification</title><categories>eess.SP cs.CV eess.IV</categories><comments>4 pages, 2 figures, 1 table, EMBC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has revolutionized computer vision utilizing the increased
availability of big data and the power of parallel computational units such as
graphical processing units. The vast majority of deep learning research is
conducted using images as training data, however the biomedical domain is rich
in physiological signals that are used for diagnosis and prediction problems.
It is still an open research question how to best utilize signals to train deep
neural networks.
  In this paper we define the term Signal2Image (S2Is) as trainable or
non-trainable prefix modules that convert signals, such as
Electroencephalography (EEG), to image-like representations making them
suitable for training image-based deep neural networks defined as `base
models'. We compare the accuracy and time performance of four S2Is (`signal as
image', spectrogram, one and two layer Convolutional Neural Networks (CNNs))
combined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)
along with the depth-wise and 1D variations of the latter. We also provide
empirical evidence that the one layer CNN S2I performs better in eleven out of
fifteen tested models than non-trainable S2Is for classifying EEG signals and
we present visual comparisons of the outputs of the S2Is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13221</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13221</id><created>2019-04-30</created><authors><author><keyname>Bamatraf</keyname><forenames>Saeed</forenames></author><author><keyname>Hussain</keyname><forenames>Muhammad</forenames></author><author><keyname>Qazi</keyname><forenames>Emad-ul-Haq</forenames></author><author><keyname>Aboalsamh</keyname><forenames>Hatim</forenames></author></authors><title>Eigen Values Features for the Classification of Brain Signals
  corresponding to 2D and 3D Educational Contents</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have proposed a brain signal classification method, which
uses eigenvalues of the covariance matrix as features to classify images
(topomaps) created from the brain signals. The signals are recorded during the
answering of 2D and 3D questions. The system is used to classify the correct
and incorrect answers for both 2D and 3D questions. Using the classification
technique, the impacts of 2D and 3D multimedia educational contents on
learning, memory retention and recall will be compared. The subjects learn
similar 2D and 3D educational contents. Afterwards, subjects are asked 20
multiple-choice questions (MCQs) associated with the contents after thirty
minutes (Short-Term Memory) and two months (Long-Term Memory). Eigenvalues
features extracted from topomaps images are given to K-Nearest Neighbor (KNN)
and Support Vector Machine (SVM) classifiers, in order to identify the states
of the brain related to incorrect and correct answers. Excellent accuracies
obtained by both classifiers and by applying statistical analysis on the
results, no significant difference is indicated between 2D and 3D multimedia
educational contents on learning, memory retention and recall in both STM and
LTM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13228</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13228</id><created>2019-04-30</created><authors><author><keyname>Qazi</keyname><forenames>Emad-ul-Haq</forenames></author><author><keyname>Hussain</keyname><forenames>Muhammad</forenames></author><author><keyname>Aboalsamh</keyname><forenames>Hatim</forenames></author></authors><title>An Efficient Intelligent System for the Classification of
  Electroencephalography (EEG) Brain Signals using Nuclear Features for Human
  Cognitive Tasks</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation and classification of Electroencephalography (EEG) brain
signals are critical processes for their analysis in cognitive tasks.
Particularly, extraction of discriminative features from raw EEG signals,
without any pre-processing, is a challenging task. Motivated by nuclear norm,
we observed that there is a significant difference between the variances of EEG
signals captured from the same brain region when a subject performs different
tasks. This observation lead us to use singular value decomposition for
computing dominant variances of EEG signals captured from a certain brain
region while performing a certain task and use them as features (nuclear
features). A simple and efficient class means based minimum distance classifier
(CMMDC) is enough to predict brain states. This approach results in the feature
space of significantly small dimension and gives equally good classification
results on clean as well as raw data. We validated the effectiveness and
robustness of the technique using four datasets of different tasks: fluid
intelligence clean data (FICD), fluid intelligence raw data (FIRD), memory
recall task (MRT), and eyes open / eyes closed task (EOEC). For each task, we
analyzed EEG signals over six (06) different brain regions with 8, 16, 20, 18,
18 and 100 electrodes. The nuclear features from frontal brain region gave the
100% prediction accuracy. The discriminant analysis of the nuclear features has
been conducted using intra-class and inter-class variations. Comparisons with
the state-of-the-art techniques showed the superiority of the proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13231</identifier>
 <datestamp>2019-06-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13231</id><created>2019-04-26</created><updated>2019-06-18</updated><authors><author><keyname>Pham</keyname><forenames>Quang Long</forenames></author><author><keyname>Chege</keyname><forenames>David</forenames></author><author><keyname>Dijamco</keyname><forenames>Timothy</forenames></author><author><keyname>Surblyte</keyname><forenames>Migle</forenames></author><author><keyname>Naik</keyname><forenames>Akshay</forenames></author><author><keyname>Campbell</keyname><forenames>Kolawole</forenames></author><author><keyname>Tong</keyname><forenames>Nhat-Anh-Nguyen</forenames></author><author><keyname>Voronov</keyname><forenames>Roman</forenames></author></authors><title>Open-Source Matlab-Based Graphical User Interface (GUI) For Computer
  Control of Microscopes Using Micro-Manager</title><categories>eess.IV physics.ins-det</categories><comments>21 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Live time-lapse microscopy is essential for a wide range of biological
applications. Software-based automation is the gold standard for the operation
of hardware accessories necessary for image acquisition. Given that current
software packages are neither affordable nor open to complex structured
programming, we have developed a Matlab-based graphical user interface (GUI)
that is fundamentally accessible while providing limitless avenues for further
customization. The GUI simultaneously communicates with the open-source,
cross-functional platform micro-Manager for controlling hardware for time-lapse
image acquisition and with other software for image processing. The use of the
GUI is demonstrated through an 18-hour cell migration experiment. The results
suggest that the GUI can generate high-quality, high-throughput time-lapse
images. The core code behind the GUI is open-source so that it can be modified
and upgraded. Therefore, it benefits the researchers worldwide by providing
them a fundamental yet functional template to add in advanced features specific
to their needs, such as additional non-microscopy hardware parts and software
packages supported by the Matlab software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13234</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13234</id><created>2019-04-30</created><authors><author><keyname>Qazi</keyname><forenames>Emad-ul-Haq</forenames></author><author><keyname>Hussain</keyname><forenames>Muhammad</forenames></author><author><keyname>AboAlsamh</keyname><forenames>Hatim</forenames></author><author><keyname>Ullah</keyname><forenames>Ihsan</forenames></author></authors><title>Automatic Emotion Recognition (AER) System based on Two-Level Ensemble
  of Lightweight Deep CNN Models</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotions play a crucial role in human interaction, health care and security
investigations and monitoring. Automatic emotion recognition (AER) using
electroencephalogram (EEG) signals is an effective method for decoding the real
emotions, which are independent of body gestures, but it is a challenging
problem. Several automatic emotion recognition systems have been proposed,
which are based on traditional hand-engineered approaches and their
performances are very poor. Motivated by the outstanding performance of deep
learning (DL) in many recognition tasks, we introduce an AER system (Deep-AER)
based on EEG brain signals using DL. A DL model involves a large number of
learnable parameters, and its training needs a large dataset of EEG signals,
which is difficult to acquire for AER problem. To overcome this problem, we
proposed a lightweight pyramidal one-dimensional convolutional neural network
(LP-1D-CNN) model, which involves a small number of learnable parameters. Using
LP-1D-CNN, we build a two level ensemble model. In the first level of the
ensemble, each channel is scanned incrementally by LP-1D-CNN to generate
predictions, which are fused using majority vote. The second level of the
ensemble combines the predictions of all channels of an EEG signal using
majority vote for detecting the emotion state. We validated the effectiveness
and robustness of Deep-AER using DEAP, a benchmark dataset for emotion
recognition research. The results indicate that FRONT plays dominant role in
AER and over this region, Deep-AER achieved the accuracies of 98.43% and 97.65%
for two AER problems, i.e., high valence vs low valence (HV vs LV) and high
arousal vs low arousal (HA vs LA), respectively. The comparison reveals that
Deep-AER outperforms the state-of-the-art systems with large margin. The
Deep-AER system will be helpful in monitoring for health care and security
investigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13258</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13258</id><created>2019-04-30</created><authors><author><keyname>Thomas</keyname><forenames>Samuel</forenames></author><author><keyname>Suzuki</keyname><forenames>Masayuki</forenames></author><author><keyname>Huang</keyname><forenames>Yinghui</forenames></author><author><keyname>Kurata</keyname><forenames>Gakuto</forenames></author><author><keyname>Tuske</keyname><forenames>Zoltan</forenames></author><author><keyname>Saon</keyname><forenames>George</forenames></author><author><keyname>Kingsbury</keyname><forenames>Brian</forenames></author><author><keyname>Picheny</keyname><forenames>Michael</forenames></author><author><keyname>Dibert</keyname><forenames>Tom</forenames></author><author><keyname>Kaiser-Schatzlein</keyname><forenames>Alice</forenames></author><author><keyname>Samko</keyname><forenames>Bern</forenames></author></authors><title>English Broadcast News Speech Recognition by Humans and Machines</title><categories>cs.CL cs.SD eess.AS</categories><comments>\copyright 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><doi>10.1109/ICASSP.2019.8683211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent advances in deep learning, considerable attention has been given
to achieving automatic speech recognition performance close to human
performance on tasks like conversational telephone speech (CTS) recognition. In
this paper we evaluate the usefulness of these proposed techniques on broadcast
news (BN), a similar challenging task. We also perform a set of recognition
measurements to understand how close the achieved automatic speech recognition
results are to human performance on this task. On two publicly available BN
test sets, DEV04F and RT04, our speech recognition system using LSTM and
residual network based acoustic models with a combination of n-gram and neural
network language models performs at 6.5% and 5.9% word error rate. By achieving
new performance milestones on these test sets, our experiments show that
techniques developed on other related tasks, like CTS, can be transferred to
achieve similar performance. In contrast, the best measured human recognition
performance on these test sets is much lower, at 3.6% and 2.8% respectively,
indicating that there is still room for new techniques and improvements in this
space, to reach human performance levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13270</identifier>
 <datestamp>2019-08-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13270</id><created>2019-04-30</created><updated>2019-08-14</updated><authors><author><keyname>Lang</keyname><forenames>Nico</forenames></author><author><keyname>Schindler</keyname><forenames>Konrad</forenames></author><author><keyname>Wegner</keyname><forenames>Jan Dirk</forenames></author></authors><title>Country-wide high-resolution vegetation height mapping with Sentinel-2</title><categories>eess.IV cs.CV cs.LG</categories><journal-ref>Remote Sensing of Environment 233 (2019) 111347</journal-ref><doi>10.1016/j.rse.2019.111347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentinel-2 multi-spectral images collected over periods of several months
were used to estimate vegetation height for Gabon and Switzerland. A deep
convolutional neural network (CNN) was trained to extract suitable spectral and
textural features from reflectance images and to regress per-pixel vegetation
height. In Gabon, reference heights for training and validation were derived
from airborne LiDAR measurements. In Switzerland, reference heights were taken
from an existing canopy height model derived via photogrammetric surface
reconstruction. The resulting maps have a mean absolute error (MAE) of 1.7 m in
Switzerland and 4.3 m in Gabon (a root mean square error (RMSE) of 3.4 m and
5.6 m, respectively), and correctly estimate vegetation heights up to &gt;50 m.
They also show good qualitative agreement with existing vegetation height maps.
Our work demonstrates that, given a moderate amount of reference data (i.e.,
2000 km$^2$ in Gabon and $\approx$5800 km$^2$ in Switzerland), high-resolution
vegetation height maps with 10 m ground sampling distance (GSD) can be derived
at country scale from Sentinel-2 imagery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13279</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13279</id><created>2019-04-30</created><authors><author><keyname>Pfeifer</keyname><forenames>Tim</forenames></author><author><keyname>Protzel</keyname><forenames>Peter</forenames></author></authors><title>Incrementally Learned Mixture Models for GNSS Localization</title><categories>cs.RO eess.SP</categories><comments>8 pages, 5 figures, accepted by IEEE Intelligent Vehicles Symposium
  (IV) 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GNSS localization is an important part of today's autonomous systems,
although it suffers from non-Gaussian errors caused by non-line-of-sight
effects. Recent methods are able to mitigate these effects by including the
corresponding distributions in the sensor fusion algorithm. However, these
approaches require prior knowledge about the sensor's distribution, which is
often not available. We introduce a novel sensor fusion algorithm based on
variational Bayesian inference, that is able to approximate the true
distribution with a Gaussian mixture model and to learn its parametrization
online. The proposed Incremental Variational Mixture algorithm automatically
adapts the number of mixture components to the complexity of the measurement's
error distribution. We compare the proposed algorithm against current
state-of-the-art approaches using a collection of open access real world
datasets and demonstrate its superior localization accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13281</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13281</id><created>2019-04-30</created><authors><author><keyname>Rubin</keyname><forenames>Jonathan</forenames></author><author><keyname>Abulnaga</keyname><forenames>S. Mazdak</forenames></author></authors><title>CT-To-MR Conditional Generative Adversarial Networks for Ischemic Stroke
  Lesion Segmentation</title><categories>eess.IV cs.CV cs.LG</categories><comments>Seventh IEEE International Conference on Healthcare Informatics (ICHI
  2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infarcted brain tissue resulting from acute stroke readily shows up as
hyperintense regions within diffusion-weighted magnetic resonance imaging
(DWI). It has also been proposed that computed tomography perfusion (CTP) could
alternatively be used to triage stroke patients, given improvements in speed
and availability, as well as reduced cost. However, CTP has a lower signal to
noise ratio compared to MR. In this work, we investigate whether a conditional
mapping can be learned by a generative adversarial network to map CTP inputs to
generated MR DWI that more clearly delineates hyperintense regions due to
ischemic stroke. We detail the architectures of the generator and discriminator
and describe the training process used to perform image-to-image translation
from multi-modal CT perfusion maps to diffusion weighted MR outputs. We
evaluate the results both qualitatively by visual comparison of generated MR to
ground truth, as well as quantitatively by training fully convolutional neural
networks that make use of generated MR data inputs to perform ischemic stroke
lesion segmentation. Segmentation networks trained using generated CT-to-MR
inputs result in at least some improvement on all metrics used for evaluation,
compared with networks that only use CT perfusion input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13285</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13285</id><created>2019-04-30</created><authors><author><keyname>Castro</keyname><forenames>Pablo Samuel</forenames></author></authors><title>Performing Structured Improvisations with pre-trained Deep Learning
  Models</title><categories>cs.SD cs.LG eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of outputs produced by deep generative models for music have seen
a dramatic improvement in the last few years. However, most deep learning
models perform in &quot;offline&quot; mode, with few restrictions on the processing time.
Integrating these types of models into a live structured performance poses a
challenge because of the necessity to respect the beat and harmony. Further,
these deep models tend to be agnostic to the style of a performer, which often
renders them impractical for live performance. In this paper we propose a
system which enables the integration of out-of-the-box generative models by
leveraging the musician's creativity and expertise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13296</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13296</id><created>2019-04-30</created><authors><author><keyname>Wu</keyname><forenames>Shangbin</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaoqing</forenames></author></authors><title>A Low-Complexity Antenna-Layout-Aware Spatial Covariance Matrix
  Estimation Method</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed a low-complexity antenna layout-aware (ALA) covariance
matrix estimation method. In the estimation process, antenna layout is assumed
known at the estimator. Using this information, the estimator finds antenna
pairs with statistically equivalent covariance values and sets their covariance
values to the average of covariance values of all these antenna pairs. ALA for
both uniform linear array (ULA) and uniform planar array (UPA) is discussed.
This method takes the benefit that covariance matrices do not have full degrees
of freedom. Then, the proposed ALA covariance matrix method is applied to a
multi-cell network. Simulations have demonstrated that the proposed method can
provide better performance than the widely used viaQ method, with respect to
mean square errors and downlink spectral efficiencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13304</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13304</id><created>2019-04-29</created><authors><author><keyname>Kim</keyname><forenames>Youngjin</forenames></author></authors><title>A supervised-learning-based strategy for optimal demand response of an
  HVAC System</title><categories>cs.LG cs.SY eess.SP stat.ML</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large thermal capacity of buildings enables heating, ventilating, and
air-conditioning (HVAC) systems to be exploited as demand response (DR)
resources. Optimal DR of HVAC units is challenging, particularly for multi-zone
buildings, because this requires detailed physics-based models of zonal
temperature variations for HVAC system operation and building thermal
conditions. This paper proposes a new strategy for optimal DR of an HVAC system
in a multi-zone building, based on supervised learning (SL). Artificial neural
networks (ANNs) are trained with data obtained under normal building operating
conditions. The ANNs are replicated using piecewise linear equations, which are
explicitly integrated into an optimal scheduling problem for price-based DR.
The optimization problem is solved for various electricity prices and building
thermal conditions. The solutions are further used to train a deep neural
network (DNN) to directly determine the optimal DR schedule, referred to here
as supervised-learning-aided meta-prediction (SLAMP). Case studies are
performed using three different methods: explicit ANN replication (EAR), SLAMP,
and physics-based modeling. The case study results verify the effectiveness of
the proposed SL-based strategy, in terms of both practical applicability and
computational time, while also ensuring the thermal comfort of occupants and
cost-effective operation of the HVAC system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13307</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13307</id><created>2019-04-26</created><authors><author><keyname>Vemuri</keyname><forenames>Anant S.</forenames></author></authors><title>Survey of Computer Vision and Machine Learning in Gastrointestinal
  Endoscopy</title><categories>physics.med-ph cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper attempts to provide the reader a place to begin studying the
application of computer vision and machine learning to gastrointestinal (GI)
endoscopy. They have been classified into 18 categories. It should be be noted
by the reader that this is a review from pre-deep learning era. A lot of deep
learning based applications have not been covered in this thesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13328</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13328</id><created>2019-04-30</created><authors><author><keyname>Messina</keyname><forenames>Francisco</forenames></author><author><keyname>Marchi</keyname><forenames>Pablo</forenames></author><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author><author><keyname>Galarza</keyname><forenames>Cecilia</forenames></author></authors><title>A Self-Adaptive Contractive Algorithm for Enhanced Dynamic Phasor
  Estimation</title><categories>eess.SP</categories><comments>8 pages, 4 figures. Submitted to IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a self-adaptive contractive (SAC) algorithm is proposed for
enhanced dynamic phasor estimation in the diverse operating conditions of
modern power systems. At a high-level, the method is composed of three stages:
parameter shifting, filtering and parameter unshifting. The goal of the first
stage is to transform the input signal phasor so that it is approximately
mapped to nominal conditions. The second stage provides estimates of the
phasor, frequency, rate of change of frequency (ROCOF), damping and rate of
change of damping (ROCOD) of the parameter shifted phasor by using a
differentiator filter bank (DFB). The final stage recovers the original signal
phasor parameters while rejecting misleading estimates. The most important
features of the algorithm are that it offers convergence guarantees in a set of
desired conditions, and also great harmonic rejection. Numerical examples,
including the IEEE C37.118.1 standard tests with realistic noise levels, as
well as fault conditions, validate the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13350</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13350</id><created>2019-04-30</created><authors><author><keyname>Yu</keyname><forenames>Wen-Kai</forenames></author></authors><title>Three-dimensional imaging with single-frame jigsaw-puzzle-reorganized
  sinusoidal fringe using multi-pixel axial flat brush scanning</title><categories>eess.IV physics.optics</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured-light three-dimensional (3D) imaging can achieve 3D shape of a
stationary object via one or more pixelated array cameras with phase-shifting
illumination. In order to extend 3D imaging to moving scenarios, we propose a
3D imaging method with double projection of a single-frame modulated light
pattern and a sampling pattern. It can continuously image the moving 3D scene
by making multi-pixel detector axial flat brush scan along the motion axis.
Utilizing spatial multiplexing for multiple single-pixel imaging, each
single-pixel does not need to keep staring at some part of the object, avoiding
motion blur problem. The performance of our method has been demonstrated by
numerical simulations. Given this, we believe that the technique paves the way
to practical applications including product line 3D monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13358</identifier>
 <datestamp>2019-05-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13358</id><created>2019-04-30</created><authors><author><keyname>Mahmood</keyname><forenames>Faisal</forenames></author><author><keyname>Xu</keyname><forenames>Wenhao</forenames></author><author><keyname>Durr</keyname><forenames>Nicholas J.</forenames></author><author><keyname>Johnson</keyname><forenames>Jeremiah W.</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author></authors><title>Structured Prediction using cGANs with Fusion Discriminator</title><categories>cs.CV cs.LG eess.IV</categories><comments>13 pages, 5 figures, 3 tables</comments><journal-ref>Workshop on Deep Generative Models for Structured Prediction at
  ICLR 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the fusion discriminator, a single unified framework for
incorporating conditional information into a generative adversarial network
(GAN) for a variety of distinct structured prediction tasks, including image
synthesis, semantic segmentation, and depth estimation. Much like commonly used
convolutional neural network -- conditional Markov random field (CNN-CRF)
models, the proposed method is able to enforce higher-order consistency in the
model, but without being limited to a very specific class of potentials. The
method is conceptually simple and flexible, and our experimental results
demonstrate improvement on several diverse structured prediction tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1904.13377</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1904.13377</id><created>2019-04-30</created><updated>2019-05-03</updated><authors><author><keyname>Pham</keyname><forenames>Ngoc-Quan</forenames></author><author><keyname>Nguyen</keyname><forenames>Thai-Son</forenames></author><author><keyname>Niehues</keyname><forenames>Jan</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Markus</forenames></author><author><keyname>St&#xfc;ker</keyname><forenames>Sebastian</forenames></author><author><keyname>Waibel</keyname><forenames>Alexander</forenames></author></authors><title>Very Deep Self-Attention Networks for End-to-End Speech Recognition</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Submitted to INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, end-to-end sequence-to-sequence models for speech recognition have
gained significant interest in the research community. While previous
architecture choices revolve around time-delay neural networks (TDNN) and long
short-term memory (LSTM) recurrent neural networks, we propose to use
self-attention via the Transformer architecture as an alternative. Our analysis
shows that deep Transformer networks with high learning capacity are able to
exceed performance from previous end-to-end approaches and even match the
conventional hybrid systems. Moreover, we trained very deep models with up to
48 Transformer layers for both encoder and decoders combined with stochastic
residual connections, which greatly improve generalizability and training
efficiency. The resulting models outperform all previous end-to-end ASR
approaches on the Switchboard benchmark. An ensemble of these models achieve
9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This
finding brings our end-to-end models to competitive levels with previous hybrid
systems. Further, with model ensembling the Transformers can outperform certain
hybrid systems, which are more complicated in terms of both structure and
training procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00005</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00005</id><created>2019-04-29</created><authors><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Qu</keyname><forenames>Daiming</forenames></author><author><keyname>Jiang</keyname><forenames>Hao</forenames></author></authors><title>Optimal Preamble Length for Spectral Efficiency in Grant-Free RA with
  Massive MIMO</title><categories>eess.SP cs.IT math.IT</categories><comments>Accepted By IEEE ICEIC 2019. arXiv admin note: text overlap with
  arXiv:1805.08345</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grant-free random access (RA) with massive MIMO is a promising RA technique
for massive access with low signaling overhead. In the grant-free RA with
massive MIMO, preamble length has a critical impact on the performance of the
system. In this paper, the optimal preamble length is investigated to maximize
spectral efficiency (SE) of the grant-free RA with massive MIMO, where effects
of the preamble length on the preamble collision and preamble overhead as well
as channel estimation accuracy are taken into account. Simulation results agree
well with our analyses and confirm the existence of optimal preamble length for
SE maximization in the grant-free RA with massive MIMO. Moreover, properties of
the optimal preamble length with respect to system parameters are revealed.
Compared to the granted access, it is shown that longer preamble length is
required for SE maximization in the grant-free RA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00065</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00065</id><created>2019-04-30</created><authors><author><keyname>Garcia-Corrales</keyname><forenames>Celia</forenames></author><author><keyname>Fernandez-Plazaola</keyname><forenames>Unai</forenames></author><author><keyname>Ca&#xf1;ete</keyname><forenames>Francisco J.</forenames></author><author><keyname>Paris</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Lopez-Martinez</keyname><forenames>F. Javier</forenames></author></authors><title>Unveiling the Hyper-Rayleigh Regime of the Fluctuating Two-Ray Fading
  Model</title><categories>cs.IT eess.SP math.IT</categories><comments>This work has been submitted to the IEEE for publication. Copyright
  may be transferred without notice, after which this version may no longer be
  accesible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed Fluctuating Two-Ray (FTR) model is gaining momentum as
a reference fading model in scenarios where two dominant specular waves are
present. Despite the numerous research works devoted to the performance
analysis under FTR fading, little attention has been paid to effectively
understanding the interplay between the fading model parameters and the fading
severity. According to a new scale defined in this work, which measures the
hyper-Rayleigh character of a fading channel in terms of the Amount of Fading,
the outage probability and the average capacity, we see that the FTR fading
model exhibits a full hyper-Rayleigh behavior. However, the Two-Wave with
Diffuse Power fading model from which the former is derived has only strong
hyper-Rayleigh behavior, which constitutes an interesting new insight. We also
identify that the random fluctuations in the dominant specular waves are
ultimately responsible for the full hyper-Rayleigh behavior of this class of
fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00069</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00069</id><created>2019-04-30</created><authors><author><keyname>Ram&#xed;rez-Espinosa</keyname><forenames>Pablo</forenames></author><author><keyname>Lopez-Martinez</keyname><forenames>F. Javier</forenames></author></authors><title>On the Utility of the Inverse Gamma Distribution in Modeling Composite
  Fading Channels</title><categories>cs.IT eess.SP math.IT</categories><comments>This work has been submitted to the IEEE for publication. Copyright
  may be transferred without notice, after which this version may no longer be
  accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general approach to characterize composite fading models based
on inverse gamma (IG) shadowing. We first determine to what extent the IG
distribution is an adequate choice for modeling shadow fading, by means of a
comprehensive test with field measurements and other distributions
conventionally used for this purpose. Then, we prove that the probability
density function and cumulative density function of any IG-based composite
fading model are directly expressed in terms of a Laplace-domain statistic of
the underlying fast fading model, and in some relevant cases, as a mixture of
well-known state-of-the-art distributions. We exemplify our approach by
presenting a composite IG/two-wave with diffuse power fading model, for which
its statistical characterization is directly attained in a simple form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00078</identifier>
 <datestamp>2019-05-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00078</id><created>2019-04-30</created><updated>2019-05-25</updated><authors><author><keyname>Purwins</keyname><forenames>Hendrik</forenames><affiliation>Aalborg University Copenhagen</affiliation></author><author><keyname>Li</keyname><forenames>Bo</forenames><affiliation>Google</affiliation></author><author><keyname>Virtanen</keyname><forenames>Tuomas</forenames><affiliation>Tampere University</affiliation></author><author><keyname>Schl&#xfc;ter</keyname><forenames>Jan</forenames><affiliation>Universit&#xe9; de Toulon</affiliation><affiliation>Austrian Research Institute for Artificial Intelligence</affiliation></author><author><keyname>Chang</keyname><forenames>Shuo-yiin</forenames><affiliation>Google</affiliation></author><author><keyname>Sainath</keyname><forenames>Tara</forenames><affiliation>Google</affiliation></author></authors><title>Deep Learning for Audio Signal Processing</title><categories>cs.SD eess.AS stat.ML</categories><comments>15 pages, 2 pdf figures</comments><acm-class>I.2.6; H.5.1</acm-class><journal-ref>Journal of Selected Topics of Signal Processing 14, No. 8 (2019)</journal-ref><doi>10.1109/JSTSP.2019.2908700</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the recent surge in developments of deep learning, this article
provides a review of the state-of-the-art deep learning techniques for audio
signal processing. Speech, music, and environmental sound processing are
considered side-by-side, in order to point out similarities and differences
between the domains, highlighting general methods, problems, key references,
and potential for cross-fertilization between areas. The dominant feature
representations (in particular, log-mel spectra and raw waveform) and deep
learning models are reviewed, including convolutional neural networks, variants
of the long short-term memory architecture, as well as more audio-specific
neural network models. Subsequently, prominent deep learning application areas
are covered, i.e. audio recognition (automatic speech recognition, music
information retrieval, environmental sound detection, localization and
tracking) and synthesis and transformation (source separation, audio
enhancement, generative models for speech, sound, and music synthesis).
Finally, key issues and future questions regarding deep learning applied to
audio signal processing are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00125</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00125</id><created>2019-04-30</created><authors><author><keyname>Singh</keyname><forenames>Bhanu Pratap</forenames></author><author><keyname>Deznabi</keyname><forenames>Iman</forenames></author><author><keyname>Narasimhan</keyname><forenames>Bharath</forenames></author><author><keyname>Kucharski</keyname><forenames>Bryon</forenames></author><author><keyname>Uppaal</keyname><forenames>Rheeya</forenames></author><author><keyname>Josyula</keyname><forenames>Akhila</forenames></author><author><keyname>Fiterau</keyname><forenames>Madalina</forenames></author></authors><title>Multi-resolution Networks For Flexible Irregular Time Series Modeling
  (Multi-FIT)</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Missing values, irregularly collected samples, and multi-resolution signals
commonly occur in multivariate time series data, making predictive tasks
difficult. These challenges are especially prevalent in the healthcare domain,
where patients' vital signs and electronic records are collected at different
frequencies and have occasionally missing information due to the imperfections
in equipment or patient circumstances. Researchers have handled each of these
issues differently, often handling missing data through mean value imputation
and then using sequence models over the multivariate signals while ignoring the
different resolution of signals. We propose a unified model named
Multi-resolution Flexible Irregular Time series Network (Multi-FIT). The
building block for Multi-FIT is the FIT network. The FIT network creates an
informative dense representation at each time step using signal information
such as last observed value, time difference since the last observed time stamp
and overall mean for the signal. Vertical FIT (FIT-V) is a variant of FIT which
also models the relationship between different temporal signals while creating
the informative dense representations for the signal. The multi-FIT model uses
multiple FIT networks for sets of signals with different resolutions, further
facilitating the construction of flexible representations. Our model has three
main contributions: a.) it does not impute values but rather creates
informative representations to provide flexibility to the model for creating
task-specific representations b.) it models the relationship between different
signals in the form of support signals c.) it models different resolutions in
parallel before merging them for the final prediction task. The FIT, FIT-V and
Multi-FIT networks improve upon the state-of-the-art models for three
predictive tasks, including the forecasting of patient survival.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00151</identifier>
 <datestamp>2019-05-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00151</id><created>2019-04-30</created><updated>2019-05-09</updated><authors><author><keyname>Venkataramani</keyname><forenames>Shrikant</forenames></author><author><keyname>Tzinis</keyname><forenames>Efthymios</forenames></author><author><keyname>Smaragdis</keyname><forenames>Paris</forenames></author></authors><title>A Style Transfer Approach to Source Separation</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training neural networks for source separation involves presenting a mixture
recording at the input of the network and updating network parameters in order
to produce an output that resembles the clean source. Consequently, supervised
source separation depends on the availability of paired mixture-clean training
examples. In this paper, we interpret source separation as a style transfer
problem. We present a variational auto-encoder network that exploits the
commonality across the domain of mixtures and the domain of clean sounds and
learns a shared latent representation across the two domains. Using these
cycle-consistent variational auto-encoders, we learn a mapping from the mixture
domain to the domain of clean sounds and perform source separation without
explicitly supervising with paired training examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00161</identifier>
 <datestamp>2019-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00161</id><created>2019-04-30</created><updated>2019-10-28</updated><authors><author><keyname>Li</keyname><forenames>Chen</forenames></author><author><keyname>Xu</keyname><forenames>Mai</forenames></author><author><keyname>Zhang</keyname><forenames>Shanyi</forenames></author><author><keyname>Callet</keyname><forenames>Patrick Le</forenames></author></authors><title>State-of-the-art in 360{\deg} Video/Image Processing: Perception,
  Assessment and Compression</title><categories>eess.IV cs.MM</categories><comments>Submitted to IEEE J-STSP SI of Perception-driven 360-degree video
  processing as an Invited Overview Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, 360{\deg} video/image has been increasingly popular and drawn great
attention. The spherical viewing range of 360{\deg} video/image accounts for
huge data, which pose the challenges to 360{\deg} video/image processing in
solving the bottleneck of storage, transmission, etc. Accordingly, the recent
years have witnessed the explosive emergence of works on 360{\deg} video/image
processing. In this paper, we review the state-of-the-art works on 360{\deg}
video/image processing from the aspects of perception, assessment and
compression. First, this paper reviews both datasets and visual attention
modelling approaches for 360{\deg} video/image. Second, we survey the related
works on both subjective and objective visual quality assessment (VQA) of
360{\deg} video/image. Third, we overview the compression approaches for
360{\deg} video/image, which either utilize the spherical characteristics or
visual attention models. Finally, we summarize this overview paper and outlook
the future research trends on 360{\deg} video/image processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00162</identifier>
 <datestamp>2019-05-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00162</id><created>2019-04-30</created><authors><author><keyname>Song</keyname><forenames>Pengming</forenames></author><author><keyname>Jiang</keyname><forenames>Shaowei</forenames></author><author><keyname>Zhang</keyname><forenames>He</forenames></author><author><keyname>Huang</keyname><forenames>Xizhi</forenames></author><author><keyname>Zhang</keyname><forenames>Yongbing</forenames></author><author><keyname>Zheng</keyname><forenames>Guoan</forenames></author></authors><title>Full-field Fourier ptychography (FFP): spatially varying pupil modeling
  and its application for rapid field-dependent aberration metrology</title><categories>physics.optics eess.IV</categories><journal-ref>APL Photonics 4, 050802 (2019)</journal-ref><doi>10.1063/1.5090552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital aberration measurement and removal play a prominent role in
computational imaging platforms aimed at achieving simple and compact optical
arrangements. A recent important class of such platforms is Fourier
ptychography, which is geared towards efficiently creating gigapixel images
with high resolution and large field of view (FOV). In current FP
implementations, pupil aberration is often recovered at each small segment of
the entire FOV. This reconstruction strategy fails to consider the
field-dependent nature of the optical pupil. Given the power series expansion
of the wavefront aberration, the spatially varying pupil can be fully
characterized by tens of coefficients over the entire FOV. With this
observation, we report a Full-field Fourier Ptychography (FFP) scheme for rapid
and robust aberration metrology. The meaning of 'full-field' in FFP is referred
to the recovering of the 'full-field' coefficients that govern the
field-dependent pupil over the entire FOV. The optimization degrees of freedom
are at least two orders of magnitude lower than the previous implementations.
We show that the image acquisition process of FFP can be completed in ~1s and
the spatially varying aberration of the entire FOV can be recovered in ~35s
using a CPU. The reported approach may facilitate the further development of
Fourier ptychography. Since no moving part or calibration target is needed in
this approach, it may find important applications in aberration metrology. The
derivation of the full-field coefficients and its extension for Zernike modes
also provide a general tool for analyzing spatially varying aberrations in
computational imaging systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00190</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00190</id><created>2019-05-01</created><authors><author><keyname>Alexandre</keyname><forenames>David</forenames></author><author><keyname>Chang</keyname><forenames>Chih-Peng</forenames></author><author><keyname>Peng</keyname><forenames>Wen-Hsiao</forenames></author><author><keyname>Hang</keyname><forenames>Hsueh-Ming</forenames></author></authors><title>Learned Image Compression with Soft Bit-based Rate-Distortion
  Optimization</title><categories>eess.IV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the notion of soft bits to address the rate-distortion
optimization for learning-based image compression. Recent methods for such
compression train an autoencoder end-to-end with an objective to strike a
balance between distortion and rate. They are faced with the zero gradient
issue due to quantization and the difficulty of estimating the rate accurately.
Inspired by soft quantization, we represent quantization indices of feature
maps with differentiable soft bits. This allows us to couple tightly the rate
estimation with context-adaptive binary arithmetic coding. It also provides a
differentiable distortion objective function. Experimental results show that
our approach achieves the state-of-the-art compression performance among the
learning-based schemes in terms of MS-SSIM and PSNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00197</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00197</id><created>2019-05-01</created><authors><author><keyname>Dang</keyname><forenames>Shuping</forenames></author><author><keyname>Ma</keyname><forenames>Guoqing</forenames></author><author><keyname>Shihada</keyname><forenames>Basem</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Enhanced Orthogonal Frequency-Division Multiplexing with Subcarrier
  Number Modulation</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel modulation scheme termed orthogonal frequency-division multiplexing
with subcarrier number modulation (OFDM-SNM) has been proposed and regarded as
one of the promising candidate modulation schemes for next generation networks.
Although OFDM-SNM is capable of having a higher spectral efficiency (SE) than
OFDM with index modulation (OFDM-IM) and plain OFDM under certain conditions,
its reliability is relatively inferior to these existing schemes, because the
number of active subcarriers varies. In this regard, we propose an enhanced
OFDM-SNM scheme in this paper, which utilizes the flexibility of placing
subcarriers to harvest a coding gain in the high signal-to-noise ratio (SNR)
region. In particular, we stipulate a methodology that optimizes the subcarrier
activation pattern (SAP) by subcarrier assignment using instantaneous channel
state information (CSI) and therefore the subcarriers with higher channel power
gains will be granted the priority to be activated, given the number of
subcarriers is fixed. We also analyze the proposed enhanced OFDM-SNM system in
terms of outage and error performance. The average outage probability and block
error rate (BLER) are derived and approximated in closed-form expressions,
which are further verified by numerical results generated by Monte Carlo
simulations. The high-reliability nature of the enhanced OFDM-SNM makes it a
promising candidate for implementing in the Internet of Things (IoT) with
stationary machine-type devices (MTDs), which are subject to slow fading and
supported by proper power supply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00230</identifier>
 <datestamp>2019-05-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00230</id><created>2019-05-01</created><authors><author><keyname>Sorinasa</keyname><forenames>Jennifer</forenames></author><author><keyname>Fernandez-Troyano</keyname><forenames>Juan C.</forenames></author><author><keyname>Val-Calvo</keyname><forenames>Mikel</forenames></author><author><keyname>Ferr&#xe1;ndez</keyname><forenames>Jose Manuel</forenames></author><author><keyname>Fernandez</keyname><forenames>Eduardo</forenames></author></authors><title>A new model for the implementation of positive and negative emotion
  recognition</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large range of potential applications, not only for patients but also for
healthy people, that could be achieved by affective BCI (aBCI) makes more
latent the necessity of finding a commonly accepted protocol for real-time
EEG-based emotion recognition. Based on wavelet package for spectral feature
extraction, attending to the nature of the EEG signal, we have specified some
of the main parameters needed for the implementation of robust positive and
negative emotion classification. 12 seconds has resulted as the most
appropriate sliding window size; from that, a set of 20 target
frequency-location variables have been proposed as the most relevant features
that carry the emotional information. Lastly, QDA and KNN classifiers and
population rating criterion for stimuli labeling have been suggested as the
most suitable approaches for EEG-base emotion recognition. The proposed model
reached a mean accuracy of 98% (s.d. 1.4) and 98.96% (s.d. 1.28) in a
subject-dependent approach for QDA and KNN classifier, respectively. This new
model represents a step forward towards real-time classification. Moreover,
although results were not conclusive, new insights regarding
subject-independent approximation have been discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00237</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00237</id><created>2019-05-01</created><updated>2019-09-03</updated><authors><author><keyname>Gupta</keyname><forenames>Vaibhav Kumar</forenames></author><author><keyname>Kasbekar</keyname><forenames>Gaurav S</forenames></author></authors><title>Stability Analysis of Simple and Online User Association Policies for
  Millimeter Wave Networks</title><categories>eess.SP</categories><comments>15 pages, Submitted to JSAC millimeter Wave Networking 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a millimeter wave (mmWave) network, user association, the process of
deciding as to which base station (BS) a given user should associate with, is a
crucial process which affects the throughput and delay performance seen by
users in the network and the amount of load at each BS. In the existing
research literature, the stability region of a user association policy, i.e.,
the set of user arrival rates for which the user association policy stabilizes
the network, has not been analytically characterized for any user association
policy for mmWave networks. In this paper, we study the user association
problem in mmWave networks and compare the performance of four user association
policies: Signal to Noise Ratio (SNR) based, Throughput based, Load based and
Mixed. All these policies are simple, easy to implement, distributed and
online. We use a Continuous Time Markov Chain (CTMC) model and Lyapunov
function techniques to analytically characterize the stability region of each
of the above four user association policies. We also evaluate the performance
of the above four user association policies in a large mmWave network, in which
link qualities fluctuate with time and users are mobile, via detailed
simulations. Our results show that the Throughput based policy outperforms the
other three user association policies in terms of stability region as well as
average throughput, average delay and fairness performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00267</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00267</id><created>2019-05-01</created><authors><author><keyname>Bright</keyname><forenames>Curtis</forenames></author><author><keyname>Kotsireas</keyname><forenames>Ilias</forenames></author><author><keyname>Ganesh</keyname><forenames>Vijay</forenames></author></authors><title>New Infinite Families of Perfect Quaternion Sequences and Williamson
  Sequences</title><categories>cs.IT eess.SP math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new constructions for perfect and odd perfect sequences over the
quaternion group $Q_8$. In particular, we show for the first time that perfect
and odd perfect quaternion sequences exist in all lengths $2^t$ for $t\geq0$.
In doing so we disprove the quaternionic form of Mow's conjecture that the
longest perfect $Q_8$-sequence that can be constructed from an orthogonal array
construction is of length 64. Furthermore, we use a connection to combinatorial
design theory to prove the existence of a new infinite class of Williamson
sequences, showing that Williamson sequences of length $2^t n$ exist for all
$t\geq0$ when Williamson sequences of odd length $n$ exist. Our constructions
explain the abundance of Williamson sequences in lengths that are multiples of
a large power of two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00268</identifier>
 <datestamp>2019-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00268</id><created>2019-05-01</created><updated>2019-11-05</updated><authors><author><keyname>Cao</keyname><forenames>Yin</forenames></author><author><keyname>Kong</keyname><forenames>Qiuqiang</forenames></author><author><keyname>Iqbal</keyname><forenames>Turab</forenames></author><author><keyname>An</keyname><forenames>Fengyan</forenames></author><author><keyname>Wang</keyname><forenames>Wenwu</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Polyphonic Sound Event Detection and Localization using a Two-Stage
  Strategy</title><categories>cs.SD eess.AS</categories><comments>6 pages, 2 figures, conference</comments><doi>10.33682/4jhy-bj81</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sound event detection (SED) and localization refer to recognizing sound
events and estimating their spatial and temporal locations. Using neural
networks has become the prevailing method for SED. In the area of sound
localization, which is usually performed by estimating the direction of arrival
(DOA), learning-based methods have recently been developed. In this paper, it
is experimentally shown that the trained SED model is able to contribute to the
direction of arrival estimation (DOAE). However, joint training of SED and DOAE
degrades the performance of both. Based on these results, a two-stage
polyphonic sound event detection and localization method is proposed. The
method learns SED first, after which the learned feature layers are transferred
for DOAE. It then uses the SED ground truth as a mask to train DOAE. The
proposed method is evaluated on the DCASE 2019 Task 3 dataset, which contains
different overlapping sound events in different environments. Experimental
results show that the proposed method is able to improve the performance of
both SED and DOAE, and also performs significantly better than the baseline
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00273</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00273</id><created>2019-05-01</created><authors><author><keyname>Kwon</keyname><forenames>Soon-Won</forenames></author><author><keyname>Choi</keyname><forenames>Hanho</forenames></author><author><keyname>Jeon</keyname><forenames>Younho</forenames></author><author><keyname>Kim</keyname><forenames>Bongjin</forenames></author><author><keyname>Kwon</keyname><forenames>WooHyun</forenames></author><author><keyname>Park</keyname><forenames>Homin</forenames></author><author><keyname>Kwon</keyname><forenames>Kyeongha</forenames></author><author><keyname>Kim</keyname><forenames>Gain</forenames></author><author><keyname>Bae</keyname><forenames>Hyeon-Min</forenames></author></authors><title>A fully-digital semi-rotational frequency detection algorithm for
  bang-bang CDRs</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a new frequency acquisition method using semi-rotational
frequency detection (SRFD) algorithm for a reference-less clock and data
recovery (CDR) in a serial-link receiver. The proposed SRFD algorithm
classifies the bang-bang phase detector(BBPD) outputs to estimate the current
phase state, and detects the frequency mismatch between the input data and the
sampling clock. The VCO-track path in a digital loop filter (DLF) enables
online calibration of a drifted frequency of VCO caused by temperature or
voltage variation after a frequency acquisition. The proposed algorithm can be
implemented as a digitally-synthesized circuit, lowering design efforts for
referenceless CDRs. A 10 Gbps transceiver IC with the proposed algorithm,
fabricated in a 65nm CMOS process, demonstrates successful recovery of the
input phase without any reference clock.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00274</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00274</id><created>2019-05-01</created><authors><author><keyname>Kwon</keyname><forenames>Soon-Won</forenames></author><author><keyname>Bae</keyname><forenames>Hyeon-Min</forenames></author></authors><title>An efficient coding algorithm for general Framed Pulse Width Modulations</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new coding algorithm for Framed Pulse Width
Modulation (FPWM). The proposed algorithm requires 93% fewer look-up tables
(LUTs) than the previous FPWM coding algorithm and increases a bitrate by 25%.
The proposed algorithm is compatible with general FPWM with various frame
lengths and pulse width resolutions. Theoretical bitrates and the sizes of LUT
required for coding various FPWMs are also provided. The MATLAB simulation
demonstrates the proposed FPWM signal which contains 14-bit information in 8 UI
frame length, showing 75% higher bitrate than the NRZ signal with the same baud
rate. The decoding algorithm restores the original bit without any bit error
and validates the proposed FPWM and its coding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00310</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00310</id><created>2019-05-01</created><authors><author><keyname>Pfisterer</keyname><forenames>Kaylen J.</forenames></author><author><keyname>Amelard</keyname><forenames>Robert</forenames></author><author><keyname>Syrnyk</keyname><forenames>Braeden</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Towards computer vision powered color-nutrient assessment of pureed food</title><categories>cs.CV cs.NE eess.IV</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With one in four individuals afflicted with malnutrition, computer vision may
provide a way of introducing a new level of automation in the nutrition field
to reliably monitor food and nutrient intake. In this study, we present a novel
approach to modeling the link between color and vitamin A content using
transmittance imaging of a pureed foods dilution series in a computer vision
powered nutrient sensing system via a fine-tuned deep autoencoder network,
which in this case was trained to predict the relative concentration of sweet
potato purees. Experimental results show the deep autoencoder network can
achieve an accuracy of 80% across beginner (6 month) and intermediate (8 month)
commercially prepared pureed sweet potato samples. Prediction errors may be
explained by fundamental differences in optical properties which are further
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00322</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00322</id><created>2019-05-01</created><updated>2019-05-06</updated><authors><author><keyname>Mastan</keyname><forenames>Indra Deep</forenames></author><author><keyname>Raman</keyname><forenames>Shanmuganathan</forenames></author></authors><title>Multi-level Encoder-Decoder Architectures for Image Restoration</title><categories>eess.IV cs.CV</categories><comments>Accepted in the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR) Workshop: &quot;New Trends in Image Restoration and Enhancement
  workshop (NTIRE) 2019&quot;</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many real-world solutions for image restoration are learning-free and based
on handcrafted image priors such as self-similarity. Recently, deep-learning
methods that use training data have achieved state-of-the-art results in
various image restoration tasks (e.g., super-resolution and inpainting).
Ulyanov et al. bridge the gap between these two families of methods (CVPR 18).
They have shown that learning-free methods perform close to the
state-of-the-art learning-based methods (approximately 1 PSNR). Their approach
benefits from the encoder-decoder network. In this paper, we propose a
framework based on the multi-level extensions of the encoder-decoder network,
to investigate interesting aspects of the relationship between image
restoration and network construction independent of learning. Our framework
allows various network structures by modifying the following network
components: skip links, cascading of the network input into intermediate
layers, a composition of the encoder-decoder subnetworks, and network depth.
These handcrafted network structures illustrate how the construction of
untrained networks influence the following image restoration tasks: denoising,
super-resolution, and inpainting. We also demonstrate image reconstruction
using flash and no-flash image pairs. We provide performance comparisons with
the state-of-the-art methods for all the restoration tasks above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00329</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00329</id><created>2019-05-01</created><authors><author><keyname>Wagner</keyname><forenames>Martin R.</forenames></author><author><keyname>Jereminov</keyname><forenames>Marko</forenames></author><author><keyname>Pandey</keyname><forenames>Amritanshu</forenames></author><author><keyname>Pileggi</keyname><forenames>Larry</forenames></author></authors><title>A Probabilistic Approach to Power System State Estimation using a Linear
  Algorithm</title><categories>eess.SP</categories><comments>19th IEEE International Conference on Environment and Electrical
  Engineering, 2019, Genova, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An equivalent circuit formulation for power system analysis was demonstrated
to improve robustness of Power Flow and enable more generalized modeling,
including that for RTUs (Remote Terminal Units) and PMUs (Phasor Measurement
Units). These measurement device models, together with an adjoint circuit based
optimization framework, enable an alternative formulation to Power System State
Estimation (SE) that can be solved within the equivalent circuit formulation.
In this paper, we utilize a linear RTU model to create a fully linear SE
algorithm that includes PMU and RTU measurements to enable a probabilistic
approach to SE. Results demonstrate that this is a practical approach that is
well suited for real-world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00349</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00349</id><created>2019-05-01</created><updated>2020-01-06</updated><authors><author><keyname>Dean</keyname><forenames>Thomas</forenames></author><author><keyname>Chowdhury</keyname><forenames>Mainak</forenames></author><author><keyname>Grimwood</keyname><forenames>Nicole</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Rethinking Modulation and Detection for High Doppler Channels</title><categories>eess.SP</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two modulation and detection techniques that are designed to allow
for efficient equalization for channels that exhibit an arbitrary Doppler
spread but no delay spread. These techniques are based on principles similar to
techniques designed for time-invariant delay spread channels (e.g., Orthogonal
Frequency Division Multiplexing or OFDM) and have the same computational
complexity. Through numerical simulations, we show that effective equalization
is possible for channels that exhibit a high Doppler spread and even a modest
delay spread, whereas equalized OFDM exhibits a strictly worse performance in
these environments. Our results indicate that, in rapidly time-varying
channels, such as those found in high-mobility or mmWave deployments, new
modulation coupled with appropriate channel estimation and equalization
techniques may significantly outperform modulation and detection schemes that
are designed for static or slowly time varying multipath channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00377</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00377</id><created>2019-05-01</created><authors><author><keyname>Arora</keyname><forenames>Siddharth</forenames></author><author><keyname>Baghai-Ravary</keyname><forenames>Ladan</forenames></author><author><keyname>Tsanas</keyname><forenames>Athanasios</forenames></author></authors><title>Developing a large scale population screening tool for the assessment of
  Parkinson's disease using telephone-quality voice</title><categories>stat.AP cs.SD eess.AS</categories><comments>43 pages, 5 figures, 6 tables</comments><doi>10.1121/1.5100272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have demonstrated that analysis of laboratory-quality voice
recordings can be used to accurately differentiate people diagnosed with
Parkinson's disease (PD) from healthy controls (HC). These findings could help
facilitate the development of remote screening and monitoring tools for PD. In
this study, we analyzed 2759 telephone-quality voice recordings from 1483 PD
and 15321 recordings from 8300 HC participants. To account for variations in
phonetic backgrounds, we acquired data from seven countries. We developed a
statistical framework for analyzing voice, whereby we computed 307 dysphonia
measures that quantify different properties of voice impairment, such as,
breathiness, roughness, monopitch, hoarse voice quality, and exaggerated vocal
tremor. We used feature selection algorithms to identify robust parsimonious
feature subsets, which were used in combination with a Random Forests (RF)
classifier to accurately distinguish PD from HC. The best 10-fold
cross-validation performance was obtained using Gram-Schmidt Orthogonalization
(GSO) and RF, leading to mean sensitivity of 64.90% (standard deviation, SD
2.90%) and mean specificity of 67.96% (SD 2.90%). This large-scale study is a
step forward towards assessing the development of a reliable, cost-effective
and practical clinical decision support tool for screening the population at
large for PD using telephone-quality voice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00390</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00390</id><created>2019-04-30</created><authors><author><keyname>Jimenez-Fernandez</keyname><forenames>Angel</forenames></author><author><keyname>Dominguez-Morales</keyname><forenames>Juan Pedro</forenames></author><author><keyname>Gutierrez-Galan</keyname><forenames>Daniel</forenames></author><author><keyname>Rios-Navarro</keyname><forenames>Antonio</forenames></author><author><keyname>Tapiador-Morales</keyname><forenames>Ricardo</forenames></author><author><keyname>Linares-Barranco</keyname><forenames>Alejandro</forenames></author></authors><title>Interfacing PDM sensors with PFM spiking systems: application for
  Neuromorphic Auditory Sensors</title><categories>eess.AS cs.SD eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a sub-system to convert audio information from
low-power MEMS microphones with pulse density modulation (PDM) output into rate
coded spike streams. These spikes represent the input signal of a Neuromorphic
Auditory Sensor (NAS), which is implemented with Spike Signal Processing (SSP)
building blocks. For this conversion, we have designed a HDL component for FPGA
able to interface with PDM microphones and converts their pulses to temporal
distributed spikes following a pulse frequency modulation (PFM) scheme with an
accurate configurable Inter-Spike-Interval. The new FPGA component has been
tested in two scenarios, first as a stand-alone circuit for its
characterization, and then it has been integrated with a full NAS design to
verify its behavior. This PDM interface demands less than 1% of a Spartan 6
FPGA resources and has a power consumption below 5mW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00391</identifier>
 <datestamp>2019-05-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00391</id><created>2019-05-01</created><updated>2019-05-16</updated><authors><author><keyname>Li</keyname><forenames>Qingbiao</forenames></author><author><keyname>Lin</keyname><forenames>Jianyu</forenames></author><author><keyname>Clancy</keyname><forenames>Neil T.</forenames></author><author><keyname>Elson</keyname><forenames>Daniel S.</forenames></author></authors><title>Estimation of Tissue Oxygen Saturation from RGB images and Sparse
  Hyperspectral Signals based on Conditional Generative Adversarial Network</title><categories>cs.CV eess.IV</categories><journal-ref>International journal of computer assisted radiology and surgery
  (2019)</journal-ref><doi>10.1007/s11548-019-01940-2</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Purpose: Intra-operative measurement of tissue oxygen saturation (StO2) is
important in the detection of ischemia, monitoring perfusion and identifying
disease. Hyperspectral imaging (HSI) measures the optical reflectance spectrum
of the tissue and uses this information to quantify its composition, including
StO2. However, real-time monitoring is difficult due to the capture rate and
data processing time. Methods: An endoscopic system based on a multi-fiber
probe was previously developed to sparsely capture HSI data (sHSI). These were
combined with RGB images, via a deep neural network, to generate
high-resolution hypercubes and calculate StO2. To improve accuracy and
processing speed, we propose a dual-input conditional generative adversarial
network (cGAN), Dual2StO2, to directly estimate StO2 by fusing features from
both RGB and sHSI. Results: Validation experiments were carried out on in vivo
porcine bowel data, where the ground truth StO2 was generated from the HSI
camera. The performance was also compared to our previous
super-spectral-resolution network, SSRNet in terms of mean StO2 prediction
accuracy and structural similarity metrics. Dual2StO2 was also tested using
simulated probe data with varying fiber number. Conclusions: StO2 estimation by
Dual2StO2 is visually closer to ground truth in general structure, achieves
higher prediction accuracy and faster processing speed than SSRNet. Simulations
showed that results improved when a greater number of fibers are used in the
probe. Future work will include refinement of the network architecture,
hardware optimization based on simulation results, and evaluation of the
technique in clinical applications beyond StO2 estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00418</identifier>
 <datestamp>2019-08-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00418</id><created>2019-05-01</created><updated>2019-08-15</updated><authors><author><keyname>Hashmi</keyname><forenames>Md Umar</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Arpan</forenames></author><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Elias</keyname><forenames>Jocelyne</forenames></author><author><keyname>Kiedanski</keyname><forenames>Diego</forenames></author></authors><title>Optimal Storage Arbitrage under Net Metering using Linear Programming</title><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We formulate the optimal energy arbitrage problem for a piecewise linear cost
function for energy storage devices using linear programming (LP). The LP
formulation is based on the equivalent minimization of the epigraph. This
formulation considers ramping and capacity constraints, charging and
discharging efficiency losses of the storage, inelastic consumer load and local
renewable generation in presence of net-metering which facilitates selling of
energy to the grid and incentivizes consumers to install renewable generation
and energy storage. We consider the case where the consumer loads, electricity
prices, and renewable generations at different instances are uncertain. These
uncertain quantities are predicted using an Auto-Regressive Moving Average
(ARMA) model and used in a model predictive control (MPC) framework to obtain
the arbitrage decision at each instance. In numerical results we present the
sensitivity analysis of storage performing arbitrage with varying ramping
batteries and different ratio of selling and buying price of electricity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00439</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00439</id><created>2019-05-01</created><authors><author><keyname>Afisiadis</keyname><forenames>Orion</forenames></author><author><keyname>Cotting</keyname><forenames>Matthieu</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author></authors><title>LoRa Symbol Error Rate Under Non-Chip- and Non-Phase-Aligned
  Interference</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we examine the performance of the LoRa chirp spread spectrum
modulation in the presence of both additive white Gaussian noise and
interference from another LoRa user. To this end, we extend an existing
interference model to the more realistic case where the interfering user is
neither chip- nor phase-aligned with the signal of interest and we derive an
expression for the SER. We show that the existing interference model
overestimates the effect of interference on the error rate. Moreover, we derive
a low-complexity approximate formula that can significantly reduce the
complexity of computing the symbol error rate compared to the complete
expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00454</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00454</id><created>2019-05-01</created><authors><author><keyname>Addabbo</keyname><forenames>Pia</forenames></author><author><keyname>Orlando</keyname><forenames>Danilo</forenames></author><author><keyname>Ricci</keyname><forenames>Giuseppe</forenames></author></authors><title>Adaptive Radar Detection of Dim Moving Targets in Presence of Range
  Migration</title><categories>eess.SP</categories><comments>5 pages, 2 figures, submitted to IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2019.2936650</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses adaptive radar detection of dim moving targets. To
circumvent range migration, the detection problem is formulated as a multiple
hypothesis test and solved applying model order selection rules which allow to
estimate the &quot;position&quot; of the target within the CPI and eventually detect it.
The performance analysis proves the effectiveness of the proposed approach also
in comparison to existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00469</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00469</id><created>2019-05-01</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Cheng</keyname><forenames>Irene</forenames></author><author><keyname>Basu</keyname><forenames>Anup</forenames></author></authors><title>Fully Automatic Brain Tumor Segmentation using a Normalized Gaussian
  Bayesian Classifier and 3D Fluid Vector Flow</title><categories>eess.IV cs.CV cs.LG cs.MM stat.ML</categories><comments>ICIP 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain tumor segmentation from Magnetic Resonance Images (MRIs) is an
important task to measure tumor responses to treatments. However, automatic
segmentation is very challenging. This paper presents an automatic brain tumor
segmentation method based on a Normalized Gaussian Bayesian classification and
a new 3D Fluid Vector Flow (FVF) algorithm. In our method, a Normalized
Gaussian Mixture Model (NGMM) is proposed and used to model the healthy brain
tissues. Gaussian Bayesian Classifier is exploited to acquire a Gaussian
Bayesian Brain Map (GBBM) from the test brain MR images. GBBM is further
processed to initialize the 3D FVF algorithm, which segments the brain tumor.
This algorithm has two major contributions. First, we present a NGMM to model
healthy brains. Second, we extend our 2D FVF algorithm to 3D space and use it
for brain tumor segmentation. The proposed method is validated on a publicly
available dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00479</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00479</id><created>2019-05-01</created><authors><author><keyname>Trigui</keyname><forenames>Imene</forenames></author><author><keyname>Diamantoulakis</keyname><forenames>Panagiotis D.</forenames></author><author><keyname>Affes</keyname><forenames>Sofiene</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Shadowed FSO/mmWave Systems with Interference</title><categories>cs.IT cs.NI eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the performance of mixed free space optical
(FSO)/millimeter-wave (mmWave) relay networks with interference at the
destination. The FSO/mmWave channels are assumed to follow
Malaga-M/Generalized-K fading models with pointing errors in the FSO link. The
H-transform theory, wherein integral transforms involve Fox's H-functions as
kernels, is embodied to unifying the performance analysis framework that
encompasses closed-form expressions for the outage probability, the average bit
error rate (BER) and the average capacity. By virtue of some H-transform
asymptotic expansions, the high signal-to-interference-plus-noise ratio (SINR)
analysis reduces to easy-to-compute expressions for the outage probability and
BER, which reveals inside information for the system design. We finally
investigate the optimal power allocation strategy, which minimizes the outage
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00492</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00492</id><created>2019-04-15</created><authors><author><keyname>Afghah</keyname><forenames>Fatemeh</forenames></author><author><keyname>Razi</keyname><forenames>Abolfazl</forenames></author><author><keyname>Chakareski</keyname><forenames>Jacob</forenames></author><author><keyname>Ashdown</keyname><forenames>Jonathan</forenames></author></authors><title>Wildfire Monitoring in Remote Areas using Autonomous Unmanned Aerial
  Vehicles</title><categories>cs.CY eess.SP</categories><comments>9 pages, 4 figures, accepted in IEEE INFOCOM workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a drone-based wildfire monitoring system for remote
and hard-to-reach areas. This system utilizes autonomous unmanned aerial
vehicles (UAVs) with the main advantage of providing on-demand monitoring
service faster than the current approaches of using satellite images, manned
aircraft and remotely controlled drones. Furthermore, using autonomous drones
facilitates minimizing human intervention in risky wildfire zones. In
particular, to develop a fully autonomous system, we propose a distributed
leader-follower coalition formation model to cluster a set of drones into
multiple coalitions that collectively cover the designated monitoring field.
The coalition leader is a drone %with longer communication range that employs
observer drones potentially with different sensing and imaging %actuation
capabilities to hover in circular paths and collect imagery information from
the impacted areas. The objectives of the proposed system include i) to cover
the entire fire zone with a minimum number of drones, and ii) to minimize the
energy consumption and latency of the available drones to fly to the fire zone.
Simulation results confirm that the performance of the proposed system --
without the need for inter-coalition communications -- approaches that of a
centrally-optimized system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00503</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00503</id><created>2019-05-01</created><updated>2019-05-03</updated><authors><author><keyname>Siddharth</keyname></author><author><keyname>Trivedi</keyname><forenames>Mohan M.</forenames></author></authors><title>Attention Monitoring and Hazard Assessment with Bio-Sensing and Vision:
  Empirical Analysis Utilizing CNNs on the KITTI Dataset</title><categories>cs.HC eess.SP</categories><comments>Accepted for publication at IEEE Intelligent Vehicles Symposium 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assessing the driver's attention and detecting various hazardous and
non-hazardous events during a drive are critical for driver's safety. Attention
monitoring in driving scenarios has mostly been carried out using vision
(camera-based) modality by tracking the driver's gaze and facial expressions.
It is only recently that bio-sensing modalities such as Electroencephalogram
(EEG) are being explored. But, there is another open problem which has not been
explored sufficiently yet in this paradigm. This is the detection of specific
events, hazardous and non-hazardous, during driving that affects the driver's
mental and physiological states. The other challenge in evaluating multi-modal
sensory applications is the absence of very large scale EEG data because of the
various limitations of using EEG in the real world. In this paper, we use both
of the above sensor modalities and compare them against the two tasks of
assessing the driver's attention and detecting hazardous vs. non-hazardous
driving events. We collect user data on twelve subjects and show how in the
absence of very large-scale datasets, we can still use pre-trained deep
learning convolution networks to extract meaningful features from both of the
above modalities. We used the publicly available KITTI dataset for evaluating
our platform and to compare it with previous studies. Finally, we show that the
results presented in this paper surpass the previous benchmark set up in the
above driver awareness-related applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00510</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00510</id><created>2019-05-01</created><authors><author><keyname>Uba</keyname><forenames>Nagesh Kumar</forenames></author></authors><title>Land Use and Land Cover Classification Using Deep Learning Techniques</title><categories>eess.IV cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large datasets of sub-meter aerial imagery represented as orthophoto mosaics
are widely available today, and these data sets may hold a great deal of
untapped information. This imagery has a potential to locate several types of
features; for example, forests, parking lots, airports, residential areas, or
freeways in the imagery. However, the appearances of these things vary based on
many things including the time that the image is captured, the sensor settings,
processing done to rectify the image, and the geographical and cultural context
of the region captured by the image. This thesis explores the use of deep
convolutional neural networks to classify land use from very high spatial
resolution (VHR), orthorectified, visible band multispectral imagery. Recent
technological and commercial applications have driven the collection a massive
amount of VHR images in the visible red, green, blue (RGB) spectral bands, this
work explores the potential for deep learning algorithms to exploit this
imagery for automatic land use/ land cover (LULC) classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00550</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00550</id><created>2019-05-01</created><authors><author><keyname>Gupta</keyname><forenames>Riten</forenames></author><author><keyname>Yan</keyname><forenames>Han</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Joint Precoder and Combiner Design for MMSE Distributed Beamforming with
  Per-Antenna Power Constraints</title><categories>eess.SP</categories><doi>10.1109/ICCNC.2019.8685511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider minimum mean square error (MMSE) joint precoder and combiner
design for single and multi carrier distributed beamforming systems with
nonuniform per-antenna transmit power constraints. We show that, similar to the
maximum-gain problem, an iterative Gauss-Seidel algorithm can be used for
minimizing MSE which alternately optimizes the transmitter and receiver
coefficients. In a single carrier system the optimum transmit coefficients are
obtained by a simple projection of the effective MISO channel. In the
multicarrier case with a sum-MSE objective, the Gauss-Seidel approach is once
again applicable, but the transmit coefficients must be found by solving a
quadratically constrained quadratic problem for which we apply a dual gradient
algorithm. A numerical example is presented which shows improvement of 0.7 dB
in carrier signal-to-noise ratio (SNR) relative to a projected eigenvector
method for a multicarrier DBF system with Rayleigh-faded multipath channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00590</identifier>
 <datestamp>2019-06-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00590</id><created>2019-05-02</created><updated>2019-06-26</updated><authors><author><keyname>Kons</keyname><forenames>Zvi</forenames></author><author><keyname>Shechtman</keyname><forenames>Slava</forenames></author><author><keyname>Sorin</keyname><forenames>Alex</forenames></author><author><keyname>Rabinovitz</keyname><forenames>Carmel</forenames></author><author><keyname>Hoory</keyname><forenames>Ron</forenames></author></authors><title>High quality, lightweight and adaptable TTS using LPCNet</title><categories>eess.AS cs.SD</categories><comments>Accepted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a lightweight adaptable neural TTS system with high quality
output. The system is composed of three separate neural network blocks: prosody
prediction, acoustic feature prediction and Linear Prediction Coding Net as a
neural vocoder. This system can synthesize speech with close to natural quality
while running 3 times faster than real-time on a standard CPU. The modular
setup of the system allows for simple adaptation to new voices with a small
amount of data. We first demonstrate the ability of the system to produce high
quality speech when trained on large, high quality datasets. Following that, we
demonstrate its adaptability by mimicking unseen voices using 5 to 20 minutes
long datasets with lower recording quality. Large scale Mean Opinion Score
quality and similarity tests are presented, showing that the system can adapt
to unseen voices with quality gap of 0.12 and similarity gap of 3% compared to
natural speech for male voices and quality gap of 0.35 and similarity of gap of
9 % for female voices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00594</identifier>
 <datestamp>2019-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00594</id><created>2019-05-02</created><updated>2019-11-12</updated><authors><author><keyname>Ali</keyname><forenames>Ziad</forenames></author><author><keyname>Duel-Hallen</keyname><forenames>Alexandra</forenames></author><author><keyname>Hallen</keyname><forenames>Hans</forenames></author></authors><title>Early Warning of mmWave Signal Blockage and AoA Transition Using sub-6
  GHz Observations</title><categories>eess.SP</categories><comments>in IEEE Communications Letters</comments><doi>10.1109/LCOMM.2019.2952602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The susceptibility of millimeter-wave (mmWave) signals to physical blockage
and abrupt signal strength variations presents a challenge to reliable 5G
communication. This work proposes and examines the feasibility of utilizing
lower-frequency signals as early-warning indicators of mobile mmWave signal
blockage or recovery. A physics-based channel simulation tool incorporating
Fresnel diffraction and image sources is employed to demonstrate that sub-6 GHz
signals &quot;lead&quot; mmWave signals in reaching a specific signal-strength threshold
by several to tens of milliseconds at mobile speeds, suggesting early-warning
systems are viable. This predictive approach stems from frequency-dependent
properties of diffraction and does not assume a specific topology or mobile and
obstacle speeds. Realistic simulations that include transitions from line of
sight (LoS) to non-line of sight (NLoS) and reflection scenarios are employed
to verify the proposed prediction capabilities. Moreover, prediction of the
strongest multipath component and its angle of arrival (AoA) using sub-6 GHz
observations is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00596</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00596</id><created>2019-05-02</created><authors><author><keyname>Brusin</keyname><forenames>Ann Margareth Rosa</forenames></author><author><keyname>Curri</keyname><forenames>Vittorio</forenames></author><author><keyname>Zibar</keyname><forenames>Darko</forenames></author><author><keyname>Carena</keyname><forenames>Andrea</forenames></author></authors><title>An ultra-fast method for gain and noise prediction of Raman amplifiers</title><categories>eess.SP physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A machine learning method for prediction of Raman gain and noise spectra is
presented: it guarantees high-accuracy (RMSE &lt; 0.4 dB) and low computational
complexity making it suitable for real-time implementation in future optical
networks controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00599</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00599</id><created>2019-05-02</created><authors><author><keyname>Pienaar</keyname><forenames>Schalk Wilhelm</forenames></author><author><keyname>Malekian</keyname><forenames>Reza</forenames></author></authors><title>Human Activity Recognition Using LSTM-RNN Deep Neural Network
  Architecture</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using raw sensor data to model and train networks for Human Activity
Recognition can be used in many different applications, from fitness tracking
to safety monitoring applications. These models can be easily extended to be
trained with different data sources for increased accuracies or an extension of
classifications for different prediction classes. This paper goes into the
discussion on the available dataset provided by WISDM and the unique features
of each class for the different axes. Furthermore, the design of a Long Short
Term Memory (LSTM) architecture model is outlined for the application of human
activity recognition. An accuracy of above 94% and a loss of less than 30% has
been reached in the first 500 epochs of training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00603</identifier>
 <datestamp>2019-05-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00603</id><created>2019-05-02</created><updated>2019-05-16</updated><authors><author><keyname>O'Lone</keyname><forenames>Christopher E.</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Buehrer</keyname><forenames>R. Michael</forenames></author></authors><title>A Mathematical Justification for Exponentially Distributed NLOS Bias</title><categories>cs.IT eess.SP math.IT</categories><comments>Submitted to the 2019 IEEE Global Communications Conference,
  Waikoloa, HI, USA; [v2, comments] Minor grammatical changes made to improve
  clarity</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few decades, the localization literature has seen many models
attempting to characterize the non-line-of-sight (NLOS) bias error commonly
experienced in range measurements. These models have either been based on
specific measurement data or chosen due to attractive features of a particular
distribution, yet to date, none have been backed by rigorous analysis.
Leveraging tools from stochastic geometry, this paper attempts to fill this
void by providing the first analytical backing for an NLOS bias error model.
Using a Boolean model to statistically characterize the random locations,
orientations, and sizes of reflectors, and assuming first-order (i.e.,
single-bounce) reflections, the distance traversed by the first-arriving NLOS
path is characterized. Under these assumptions, this analysis reveals that NLOS
bias exhibits an exponential form and can in fact be well approximated by an
exponential distribution -- a result consistent with previous NLOS bias error
models in the literature. This analytically derived distribution is then
compared to a common exponential model from the literature, revealing this
distribution to be a close match in some cases and a lower bound in others.
Lastly, the assumptions under which these results were derived suggest this
model is aptly suited to characterize NLOS bias in 5G millimeter wave systems
as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00604</identifier>
 <datestamp>2019-08-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00604</id><created>2019-05-02</created><updated>2019-08-21</updated><authors><author><keyname>Yang</keyname><forenames>Yifei</forenames></author><author><keyname>Zhang</keyname><forenames>Shuowen</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>IRS-Enhanced OFDM: Power Allocation and Passive Array Optimization</title><categories>eess.SP cs.IT math.IT</categories><comments>to appear in IEEE GLOBECOM 2019. arXiv admin note: substantial text
  overlap with arXiv:1906.09956</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent reflecting surface (IRS) is a promising new technology for
achieving spectrum and energy efficient wireless communication systems in the
future. By adaptively varying the incident signals' phases/amplitudes and
thereby establishing favorable channel responses through a large number of
reconfigurable passive reflecting elements, IRS is able to enhance the
communication performance of mobile users in its vicinity cost-effectively. In
this paper, we study an IRS-enhanced orthogonal frequency division multiplexing
(OFDM) system in which an IRS is deployed to assist the communication between a
nearby user and its associated base station (BS). We aim to maximize the
downlink achievable rate for the user by jointly optimizing the transmit power
allocation at the BS and the passive array reflection coefficients at the IRS.
Although the formulated problem is non-convex and thus difficult to solve, we
propose an efficient algorithm to obtain a high-quality suboptimal solution for
it, by alternately optimizing the BS's power allocation and the IRS's passive
array coefficients in an iterative manner, along with a customized method for
the initialization. Simulation results show that the proposed design
significantly improves the OFDM link rate performance as compared to the cases
without the IRS or with other heuristic IRS designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00615</identifier>
 <datestamp>2019-07-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00615</id><created>2019-05-02</created><updated>2019-07-08</updated><authors><author><keyname>Huang</keyname><forenames>Wen-Chin</forenames></author><author><keyname>Wu</keyname><forenames>Yi-Chiao</forenames></author><author><keyname>Lo</keyname><forenames>Chen-Chou</forenames></author><author><keyname>Tobing</keyname><forenames>Patrick Lumban</forenames></author><author><keyname>Hayashi</keyname><forenames>Tomoki</forenames></author><author><keyname>Kobayashi</keyname><forenames>Kazuhiro</forenames></author><author><keyname>Toda</keyname><forenames>Tomoki</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Hsin-Min</forenames></author></authors><title>Investigation of F0 conditioning and Fully Convolutional Networks in
  Variational Autoencoder based Voice Conversion</title><categories>eess.AS cs.SD</categories><comments>5 pages, 6 figures, 3 tables; Accepted to Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the effectiveness of two techniques for
improving variational autoencoder (VAE) based voice conversion (VC). First, we
reconsider the relationship between vocoder features extracted using the high
quality vocoders adopted in conventional VC systems, and hypothesize that the
spectral features are in fact F0 dependent. Such hypothesis implies that during
the conversion phase, the latent codes and the converted features in VAE based
VC are in fact source F0 dependent. To this end, we propose to utilize the F0
as an additional input of the decoder. The model can learn to disentangle the
latent code from the F0 and thus generates converted F0 dependent converted
features. Second, to better capture temporal dependencies of the spectral
features and the F0 pattern, we replace the frame wise conversion structure in
the original VAE based VC framework with a fully convolutional network
structure. Our experiments demonstrate that the degree of disentanglement as
well as the naturalness of the converted speech are indeed improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00628</identifier>
 <datestamp>2020-01-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00628</id><created>2019-05-02</created><authors><author><keyname>Z&#xe1;vi&#x161;ka</keyname><forenames>Pavel</forenames></author><author><keyname>Rajmic</keyname><forenames>Pavel</forenames></author><author><keyname>Schimmel</keyname><forenames>J&#xed;&#x159;&#xed;</forenames></author></authors><title>Psychoacoustically Motivated Declipping Based on Weighted l1
  Minimization</title><categories>eess.AS cs.SD</categories><journal-ref>2019 42nd International Conference on Telecommunications and
  Signal Processing (TSP)</journal-ref><doi>10.1109/TSP.2019.8769109</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A novel method for audio declipping based on sparsity is presented. The
method incorporates psychoacoustic information by weighting the transform
coefficients in the $\ell_1$ minimization. Weighting leads to an improved
quality of restoration while retaining a low complexity of the algorithm. Three
possible constructions of the weights are proposed, based on the absolute
threshold of hearing, the global masking threshold and on a quadratic curve.
Experiments compare the restoration quality according to the
signal-to-distortion ratio (SDR) and PEMO-Q objective difference grade (ODG)
and indicate that with correctly chosen weights, the presented method is able
to compete, or even outperform, the current state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00637</identifier>
 <datestamp>2019-08-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00637</id><created>2019-05-02</created><updated>2019-08-05</updated><authors><author><keyname>Son</keyname><forenames>Chang-Hwan</forenames></author></authors><title>Inverse Halftoning Through Structure-Aware Deep Convolutional Neural
  Networks</title><categories>eess.IV cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary issue in inverse halftoning is removing noisy dots on flat areas
and restoring image structures (e.g., lines, patterns) on textured areas.
Hence, a new structure-aware deep convolutional neural network that
incorporates two subnetworks is proposed in this paper. One subnetwork is for
image structure prediction while the other is for continuous-tone image
reconstruction. First, to predict image structures, patch pairs comprising
continuous-tone patches and the corresponding halftoned patches generated
through digital halftoning are trained. Subsequently, gradient patches are
generated by convolving gradient filters with the continuous-tone patches. The
subnetwork for the image structure prediction is trained using the mini-batch
gradient descent algorithm given the halftoned patches and gradient patches,
which are fed into the input and loss layers of the subnetwork, respectively.
Next, the predicted map including the image structures is stacked on the top of
the input halftoned image through a fusion layer and fed into the image
reconstruction subnetwork such that the entire network is trained adaptively to
the image structures. The experimental results confirm that the proposed
structure-aware network can remove noisy dot-patterns well on flat areas and
restore details clearly on textured areas. Furthermore, it is demonstrated that
the proposed method surpasses the conventional state-of-the-art methods based
on deep convolutional neural networks and locally learned dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00645</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00645</id><created>2019-05-02</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author></authors><title>Reproducible Research: Best Practices and Potential Misuse</title><categories>eess.SP</categories><comments>10 pages, 1 figure</comments><journal-ref>IEEE Signal Processing Magazine, vol. 36, no. 3, pp. 106-123, May
  2019</journal-ref><doi>10.1109/MSP.2019.2898421</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scientific world is becoming more open to the public and fellow
researchers. Open access publishing is becoming accepted, even if some
publishers are resisting. The next step is the open code and data paradigm,
which was briefly discussed in the &quot;From the Editor&quot; column in the November
2018 issue of IEEE Signal Processing Magazine (SPM) [1]. In this column, I
follow up on this topic by sharing my experiences, best practices, and thoughts
about reproducible research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00689</identifier>
 <datestamp>2019-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00689</id><created>2019-05-02</created><updated>2019-10-30</updated><authors><author><keyname>Kouris</keyname><forenames>Alexandros</forenames></author><author><keyname>Venieris</keyname><forenames>Stylianos I.</forenames></author><author><keyname>Rizakis</keyname><forenames>Michail</forenames></author><author><keyname>Bouganis</keyname><forenames>Christos-Savvas</forenames></author></authors><title>Approximate LSTMs for Time-Constrained Inference: Enabling Fast Reaction
  in Self-Driving Cars</title><categories>eess.SP cs.LG cs.RO</categories><comments>PREPRINT: Accepted for publication at the IEEE Consumer Electronics
  Magazine (CEM). [Acceptance Date: 28-Oct-2019]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to recognise long-term dependencies in sequential data such as video
streams has made Long Short-Term Memory (LSTM) networks a prominent Artificial
Intelligence model for many emerging applications. However, the high
computational and memory demands of LSTMs introduce challenges in their
deployment on latency-critical systems such as self-driving cars which are
equipped with limited computational resources on-board. In this paper, we
introduce a progressive inference computing scheme that combines model pruning
and computation restructuring leading to the best possible approximation of the
result given the available latency budget of the target application. The
proposed methodology enables mission-critical systems to make informed
decisions even in early stages of the computation, based on approximate LSTM
inference, meeting their specifications on safety and robustness. Our
experiments on a state-of-the-art driving model for autonomous vehicle
navigation demonstrate that the proposed approach can yield outputs with
similar quality of result compared to a faithful LSTM baseline, up to 415x
faster (198x on average, 76x geo. mean).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00690</identifier>
 <datestamp>2019-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00690</id><created>2019-05-02</created><updated>2019-05-18</updated><authors><author><keyname>Mishra</keyname><forenames>Kumar Vijay</forenames></author><author><keyname>R.</keyname><forenames>Bhavani Shankar M.</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>Toward Millimeter Wave Joint Radar-Communications: A Signal Processing
  Perspective</title><categories>eess.SP</categories><comments>24 pages, 6 figures, IEEE Signal Processing Magazine</comments><journal-ref>IEEE Signal Processing Magazine, vol. 36, no. 5, pp. 100-114,
  Sept. 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synergistic design of communications and radar systems with common spectral
and hardware resources is heralding a new era of efficiently utilizing a
limited radio-frequency spectrum. Such a joint radar-communications (JRC) model
has advantages of low-cost, compact size, less power consumption, spectrum
sharing, improved performance, and safety due to enhanced information sharing.
Today, millimeter-wave (mm-wave) communications have emerged as the preferred
technology for short distance wireless links because they provide transmission
bandwidth that is several gigahertz wide. This band is also promising for
short-range radar applications, which benefit from the high-range resolution
arising from large transmit signal bandwidths. Signal processing techniques are
critical in implementation of mmWave JRC systems. Major challenges are joint
waveform design and performance criteria that would optimally trade-off between
communications and radar functionalities. Novel multiple-input-multiple-output
(MIMO) signal processing techniques are required because mmWave JRC systems
employ large antenna arrays. There are opportunities to exploit recent advances
in cognition, compressed sensing, and machine learning to reduce required
resources and dynamically allocate them with low overheads. This article
provides a signal processing perspective of mmWave JRC systems with an emphasis
on waveform design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00723</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00723</id><created>2019-04-17</created><authors><author><keyname>Ruymbeek</keyname><forenames>Koen</forenames></author><author><keyname>Vanroose</keyname><forenames>Wim</forenames></author></authors><title>Algorithm for the reconstruction of dynamic objects in CT-scanning using
  optical flow</title><categories>eess.IV math.DS math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computed Tomography is a powerful imaging technique that allows
non-destructive visualization of the interior of physical objects in different
scientific areas. In traditional reconstruction techniques the object of
interest is mostly considered to be static, which gives artefacts if the object
is moving during the data acquisition. In this paper we present a method that,
given only scan results of multiple successive scans, can estimate the motion
and correct the CT-images for this motion assuming that the motion field is
smooth over the complete domain using optical flow. The proposed method is
validated on simulated scan data. The main contribution is that we show we can
use the optical flow technique from imaging to correct CT-scan images for
motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00726</identifier>
 <datestamp>2019-08-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00726</id><created>2019-05-02</created><updated>2019-08-04</updated><authors><author><keyname>Mankar</keyname><forenames>Praful D.</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author></authors><title>Meta Distribution for Downlink NOMA in Cellular Networks with
  3GPP-inspired User Ranking</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the meta distribution analysis of the downlink two-user
non-orthogonal multiple access (NOMA) in cellular networks. We propose a novel
user ranking technique wherein the users from the cell center (CC) and cell
edge (CE) regions are paired for the non-orthogonal transmission. Inspired by
how users are partitioned in 3GPP cellular models, the CC and CE users are
characterized based on the mean powers received from the serving and the
dominant interfering BSs. We demonstrate that the proposed technique ranks
users in an accurate order with distinct link qualities, which is imperative
for the performance of NOMA system. The exact moments of the meta distributions
for the CC and CE users under NOMA and orthogonal multiple access (OMA) are
derived. In addition, we provide tight beta distribution approximations for the
meta distributions and exact expressions of the mean local delays and the cell
throughputs for the NOMA and OMA cases. To the best of our knowledge, this is
the first comprehensive analysis of NOMA using stochastic geometry with
3GPP-inspired user ranking scheme that depends upon both of the link qualities
from the serving and dominant interfering BSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00756</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00756</id><created>2019-05-02</created><authors><author><keyname>Dias</keyname><forenames>Wheberth</forenames></author><author><keyname>Gaspar</keyname><forenames>Danilo</forenames></author><author><keyname>Mendes</keyname><forenames>Luciano</forenames></author><author><keyname>Chafii</keyname><forenames>Marwa</forenames></author><author><keyname>Matth&#xe9;</keyname><forenames>Maximilian</forenames></author><author><keyname>Neuhaus</keyname><forenames>Peter</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Performance Analysis of a 5G Transceiver Implementation for Remote Areas
  Scenarios</title><categories>eess.SP</categories><comments>Presented in 2018 European Conference on Networks and Communications
  (EuCNC),18-21 June, 2018, Ljubljana, Slovenia</comments><doi>10.1109/EuCNC.2018.8443268</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fifth generation of mobile communication networks will support a large
set of new services and applications. One important use case is the remote area
coverage for broadband Internet access. This use case ha significant social and
economic impact, since a considerable percentage of the global population
living in low populated area does not have Internet access and the
communication infrastructure in rural areas can be used to improve agribusiness
productivity. The aim of this paper is to analyze the performance of a 5G for
Remote Areas transceiver, implemented on field programmable gate array based
hardware for real-time processing. This transceiver employs the latest digital
communication techniques, such as generalized frequency division multiplexing
waveform combined with 2 by 2 multiple-input multiple-output diversity scheme
and polar channel coding. The performance of the prototype is evaluated
regarding its out-of-band emissions and bit error rate under AWGN channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00768</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00768</id><created>2019-05-02</created><authors><author><keyname>Kara</keyname><forenames>Ferdi</forenames></author><author><keyname>Kaya</keyname><forenames>Hakan</forenames></author></authors><title>Threshold-based Selective Cooperative-NOMA</title><categories>cs.IT eess.SP math.IT</categories><doi>10.1109/LCOMM.2019.2914918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we propose threshold-based selective cooperative-NOMA
(TBS-C-NOMA) to increase the data reliability of conventional cooperative-NOMA
(C-NOMA) networks. In TBS-C-NOMA, the intra-cell user forwards the symbols of
cell-edge user after successive interference canceler (SIC) only if the
signal-to-interference plus noise ratio (SINR) is greater than the
pre-determined threshold value. Hence, the data reliability of the cell-edge
user is increased by eliminating the effect of the error propagation. We derive
closed-form end-to-end exact bit error probability (BEP) of proposed system for
various modulation constellations. Then, the optimum threshold value is
analyzed in order to minimize BEP. The obtained expressions are validated via
simulations and it is revealed that TBS-C-NOMA outperforms C-NOMA and full
diversity order is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00777</identifier>
 <datestamp>2019-10-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00777</id><created>2019-05-02</created><authors><author><keyname>Kara</keyname><forenames>Ferdi</forenames></author><author><keyname>Kaya</keyname><forenames>Hakan</forenames></author></authors><title>Performance Analysis of SSK-NOMA</title><categories>cs.IT eess.SP math.IT</categories><journal-ref>in IEEE Transactions on Vehicular Technology, vol. 68, no. 7, pp.
  6231-6242, July 2019</journal-ref><doi>10.1109/TVT.2019.2914315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the combination between two promising techniques:
space-shift keying (SSK) and non-orthogonal multiple access (NOMA) for future
radio access networks. We analyze the performance of SSK-NOMA networks and
provide a comprehensive analytical framework of SSK-NOMA regarding bit error
probability (BEP), ergodic capacity and outage probability. It is worth
pointing out all analysis also stand for conventional SIMO-NOMA networks. We
derive closed-form exact average BEP (ABEP) expressions when the number of
users in a resource block is equal to i.e., $L=3$. Nevertheless, we analyze the
ABEP of users when the number of users is more than i.e., $L\geq3$, and derive
bit-error-rate (BER) union bound since the error propagation due to iterative
successive interference canceler (SIC) makes the exact analysis intractable.
Then, we analyze the achievable rate of users and derive exact ergodic capacity
of the users so the ergodic sum rate of the system in closed-forms. Moreover,
we provide the average outage probability of the users exactly in the
closed-form. All derived expressions are validated via Monte Carlo simulations
and it is proved that SSK-NOMA outperforms conventional NOMA networks in terms
of all performance metrics (i.e., BER, sum rate, outage). Finally, the effect
of the power allocation (PA) on the performance of SSK-NOMA networks is
investigated and the optimum PA is discussed under BER and outage constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00824</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00824</id><created>2019-05-02</created><authors><author><keyname>Sun</keyname><forenames>Tiancheng</forenames></author><author><keyname>Barron</keyname><forenames>Jonathan T.</forenames></author><author><keyname>Tsai</keyname><forenames>Yun-Ta</forenames></author><author><keyname>Xu</keyname><forenames>Zexiang</forenames></author><author><keyname>Yu</keyname><forenames>Xueming</forenames></author><author><keyname>Fyffe</keyname><forenames>Graham</forenames></author><author><keyname>Rhemann</keyname><forenames>Christoph</forenames></author><author><keyname>Busch</keyname><forenames>Jay</forenames></author><author><keyname>Debevec</keyname><forenames>Paul</forenames></author><author><keyname>Ramamoorthi</keyname><forenames>Ravi</forenames></author></authors><title>Single Image Portrait Relighting</title><categories>cs.GR cs.CV eess.IV</categories><comments>SIGGRAPH 2019 Technical Paper accepted</comments><journal-ref>ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)</journal-ref><doi>10.1145/3306346.3323008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lighting plays a central role in conveying the essence and depth of the
subject in a portrait photograph. Professional photographers will carefully
control the lighting in their studio to manipulate the appearance of their
subject, while consumer photographers are usually constrained to the
illumination of their environment. Though prior works have explored techniques
for relighting an image, their utility is usually limited due to requirements
of specialized hardware, multiple images of the subject under controlled or
known illuminations, or accurate models of geometry and reflectance. To this
end, we present a system for portrait relighting: a neural network that takes
as input a single RGB image of a portrait taken with a standard cellphone
camera in an unconstrained environment, and from that image produces a relit
image of that subject as though it were illuminated according to any provided
environment map. Our method is trained on a small database of 18 individuals
captured under different directional light sources in a controlled light stage
setup consisting of a densely sampled sphere of lights. Our proposed technique
produces quantitatively superior results on our dataset's validation set
compared to prior works, and produces convincing qualitative relighting results
on a dataset of hundreds of real-world cellphone portraits. Because our
technique can produce a 640 $\times$ 640 image in only 160 milliseconds, it may
enable interactive user-facing photographic applications in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00851</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00851</id><created>2019-05-02</created><authors><author><keyname>M&#xf6;llenhoff</keyname><forenames>Thomas</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author></authors><title>Lifting Vectorial Variational Problems: A Natural Formulation based on
  Geometric Measure Theory and Discrete Exterior Calculus</title><categories>cs.CV eess.IV</categories><comments>Oral presentation at CVPR 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous tasks in imaging and vision can be formulated as variational
problems over vector-valued maps. We approach the relaxation and
convexification of such vectorial variational problems via a lifting to the
space of currents. To that end, we recall that functionals with polyconvex
Lagrangians can be reparametrized as convex one-homogeneous functionals on the
graph of the function. This leads to an equivalent shape optimization problem
over oriented surfaces in the product space of domain and codomain. A convex
formulation is then obtained by relaxing the search space from oriented
surfaces to more general currents. We propose a discretization of the resulting
infinite-dimensional optimization problem using Whitney forms, which also
generalizes recent &quot;sublabel-accurate&quot; multilabeling approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00855</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00855</id><created>2019-05-02</created><authors><author><keyname>Shi</keyname><forenames>Bowen</forenames></author><author><keyname>Sun</keyname><forenames>Ming</forenames></author><author><keyname>Kao</keyname><forenames>Chieh-Chi</forenames></author><author><keyname>Rozgic</keyname><forenames>Viktor</forenames></author><author><keyname>Matsoukas</keyname><forenames>Spyros</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author></authors><title>Compression of Acoustic Event Detection Models with Low-rank Matrix
  Factorization and Quantization Training</title><categories>eess.AS cs.CL cs.SD</categories><comments>NeuralPS 2018 CDNNRIA workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a compression approach based on the combination of
low-rank matrix factorization and quantization training, to reduce complexity
for neural network based acoustic event detection (AED) models. Our
experimental results show this combined compression approach is very effective.
For a three-layer long short-term memory (LSTM) based AED model, the original
model size can be reduced to 1% with negligible loss of accuracy. Our approach
enables the feasibility of deploying AED for resource-constraint applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00931</identifier>
 <datestamp>2019-08-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00931</id><created>2019-05-02</created><updated>2019-08-20</updated><authors><author><keyname>Jo</keyname><forenames>Taeho</forenames></author><author><keyname>Nho</keyname><forenames>Kwangsik</forenames></author><author><keyname>Saykin</keyname><forenames>Andrew J.</forenames></author></authors><title>Deep Learning in Alzheimer's disease: Diagnostic Classification and
  Prognostic Prediction using Neuroimaging Data</title><categories>eess.IV cs.LG stat.ML</categories><journal-ref>Frontiers in Aging Neuroscience 11 (2019): 220</journal-ref><doi>10.3389/fnagi.2019.00220</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has shown outstanding performance in identifying intricate
structures in complex high-dimensional data, especially in the domain of
computer vision. The application of deep learning to early detection and
automated classification of Alzheimer's disease (AD) has recently gained
considerable attention, as rapid progress in neuroimaging techniques has
generated large-scale multimodal neuroimaging data. A systematic review of
publications using deep learning approaches and neuroimaging data for
diagnostic classification of AD was performed. A PubMed and Google Scholar
search was used to identify deep learning papers on AD published between
January 2013 and July 2018. These papers were reviewed, evaluated, and
classified by algorithm and neuroimaging type, and the findings were
summarized. Of 16 studies meeting full inclusion criteria, 4 used a combination
of deep learning and traditional machine learning approaches, and 12 used only
deep learning approaches. The combination of traditional machine learning for
classification and stacked auto-encoder (SAE) for feature selection produced
accuracies of up to 98.8% for AD classification and 83.7% for prediction of
conversion from mild cognitive impairment (MCI), a prodromal stage of AD, to
AD. Deep learning approaches, such as convolutional neural network (CNN) or
recurrent neural network (RNN), that use neuroimaging data without
preprocessing for feature selection have yielded accuracies of up to 96.0% for
AD classification and 84.2% for MCI conversion prediction. The best
classification performance was obtained when multimodal neuroimaging and fluid
biomarkers were combined. AD research that uses deep learning is still
evolving, improving performance by incorporating additional hybrid data types,
increasing transparency with explainable approaches that add knowledge of
specific disease-related features and mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00933</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00933</id><created>2019-05-02</created><authors><author><keyname>Soh</keyname><forenames>Jae Woong</forenames></author><author><keyname>Park</keyname><forenames>Jae Sung</forenames></author><author><keyname>Cho</keyname><forenames>Nam Ik</forenames></author></authors><title>Joint High Dynamic Range Imaging and Super-Resolution from a Single
  Image</title><categories>eess.IV cs.CV</categories><comments>11 pages</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new framework for jointly enhancing the resolution and
the dynamic range of an image, i.e., simultaneous super-resolution (SR) and
high dynamic range imaging (HDRI), based on a convolutional neural network
(CNN). From the common trends of both tasks, we train a CNN for the joint HDRI
and SR by focusing on the reconstruction of high-frequency details.
Specifically, the high-frequency component in our work is the reflectance
component according to the Retinex-based image decomposition, and only the
reflectance component is manipulated by the CNN while another component
(illumination) is processed in a conventional way. In training the CNN, we
devise an appropriate loss function that contributes to the naturalness quality
of resulting images. Experiments show that our algorithm outperforms the
cascade implementation of CNN-based SR and HDRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00934</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00934</id><created>2019-05-02</created><authors><author><keyname>Li</keyname><forenames>Fangda</forenames></author><author><keyname>Manerikar</keyname><forenames>Ankit</forenames></author><author><keyname>Prakash</keyname><forenames>Tanmay</forenames></author><author><keyname>Kak</keyname><forenames>Avinash</forenames></author></authors><title>A Splitting-Based Iterative Algorithm for GPU-Accelerated Statistical
  Dual-Energy X-Ray CT Reconstruction</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When dealing with material classification in baggage at airports, Dual-Energy
Computed Tomography (DECT) allows characterization of any given material with
coefficients based on two attenuative effects: Compton scattering and
photoelectric absorption. However, straightforward projection-domain
decomposition methods for this characterization often yield poor
reconstructions due to the high dynamic range of material properties
encountered in an actual luggage scan. Hence, for better reconstruction quality
under a timing constraint, we propose a splitting-based, GPU-accelerated,
statistical DECT reconstruction algorithm. Compared to prior art, our main
contribution lies in the significant acceleration made possible by separating
reconstruction and decomposition within an ADMM framework. Experimental
results, on both synthetic and real-world baggage phantoms, demonstrate a
significant reduction in time required for convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00943</identifier>
 <datestamp>2019-05-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00943</id><created>2019-05-02</created><updated>2019-05-16</updated><authors><author><keyname>Sadeghzadehyazdi</keyname><forenames>Nasrin</forenames></author><author><keyname>Batabyal</keyname><forenames>Tamal</forenames></author><author><keyname>Glandon</keyname><forenames>A.</forenames></author><author><keyname>Dhar</keyname><forenames>Nibir K.</forenames></author><author><keyname>Familoni</keyname><forenames>B. O.</forenames></author><author><keyname>Iftekharuddin</keyname><forenames>K. M.</forenames></author><author><keyname>Acton</keyname><forenames>Scott T.</forenames></author></authors><title>Glidar3DJ: A View-Invariant gait identification via flash lidar data
  correction</title><categories>eess.IV</categories><comments>This paper is accepted to be published in: 2019 IEEE International
  Conference on Image Processing, Sept 22-25, 2019, Taipei, Taiwan</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gait recognition is a leading remote-based identification method, suitable
for real-world surveillance and medical applications. Model-based gait
recognition methods have been particularly recognized due to their scale and
view-invariant properties. We present the first model-based gait recognition
methodology, $\mathcal{G}$lidar3DJ using a skeleton model extracted from
sequences generated by a single flash lidar camera. Existing successful
model-based approaches take advantage of high quality skeleton data collected
by Kinect and Mocap, for example, are not practicable for applications outside
the laboratory. The low resolution and noisy imaging process of lidar
negatively affects the performance of state-of-the-art skeleton-based systems,
generating a significant number of outlier skeletons. We propose a rule-based
filtering mechanism that adopts robust statistics to correct for skeleton joint
measurements. Quantitative measurements validate the efficacy of the proposed
method in improving gait recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00963</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00963</id><created>2019-05-01</created><authors><author><keyname>Kasper</keyname><forenames>Manuel</forenames></author><author><keyname>Ragulskis</keyname><forenames>Mykolas</forenames></author><author><keyname>Gramse</keyname><forenames>Georg</forenames></author><author><keyname>Kienberger</keyname><forenames>Ferry</forenames></author></authors><title>S-parameter calibration procedure for multiport microwave imaging
  systems</title><categories>eess.SP</categories><comments>Funding by H2020 GA No. 764479 (Emerald) and GA No. 761036 (MMAMA)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multitude of antennas are typically used in microwave imaging systems. Here
we outline a simple and effective calibration method for multiport imaging
systems. By using only one additional component, an electronic calibration
module (ECal), one port is calibrated and the calibration plane is thereby
moved to the antenna connector. Assuming all antennas interact with the test
phantom in the same way, the one-port calibration is transferred to all other
antennas. For full calibration including the transmission path between
antennas, the unknown thru technique is used. This calibration procedure is
simple and it can be fully automated, and no RF components are perturbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00972</identifier>
 <datestamp>2019-05-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00972</id><created>2019-05-02</created><updated>2019-05-08</updated><authors><author><keyname>Banagar</keyname><forenames>Morteza</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author></authors><title>3GPP-inspired Stochastic Geometry-based Mobility Model for a Drone
  Cellular Network</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the stochastic geometry-based characterization of the
time-varying performance of a drone cellular network in which the initial
locations of drone base stations (DBSs) are modeled as a Poisson point process
(PPP) and each DBS is assumed to move on a straight line in a random direction.
This drone placement and trajectory model closely emulates the one used by the
third generation partnership project (3GPP) for drone-related studies. Assuming
the nearest neighbor association policy for a typical user equipment (UE) on
the ground, we consider two models for the mobility of the serving DBS: (i) UE
independent model, and (ii) UE dependent model. Using displacement theorem from
stochastic geometry, we characterize the time-varying interference field as
seen by the typical UE, using which we derive the time-varying coverage
probability and data rate at the typical UE. We also compare our model with
more sophisticated mobility models where the DBSs may move in nonlinear
trajectories and demonstrate that the coverage probability and rate estimated
by our model act as lower bounds to these more general models. To the best of
our knowledge, this is the first work to perform a rigorous analysis of the
3GPP-inspired drone mobility model and establish connection between this model
and the more general non-linear mobility models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00979</identifier>
 <datestamp>2019-07-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00979</id><created>2019-05-02</created><updated>2019-07-29</updated><authors><author><keyname>Bear</keyname><forenames>Helen L.</forenames></author><author><keyname>Heittola</keyname><forenames>Toni</forenames></author><author><keyname>Mesaros</keyname><forenames>Annamaria</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author><author><keyname>Virtanen</keyname><forenames>Tuomas</forenames></author></authors><title>City classification from multiple real-world sound scenes</title><categories>eess.AS cs.SD</categories><comments>Accepted to WASPAA 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of sound scene analysis work focuses on one of two clearly
defined tasks: acoustic scene classification or sound event detection. Whilst
this separation of tasks is useful for problem definition, they inherently
ignore some subtleties of the real-world, in particular how humans vary in how
they describe a scene. Some will describe the weather and features within it,
others will use a holistic descriptor like `park', and others still will use
unique identifiers such as cities or names. In this paper, we undertake the
task of automatic city classification to ask whether we can recognize a city
from a set of sound scenes? In this problem each city has recordings from
multiple scenes. We test a series of methods for this novel task and show that
a simple convolutional neural network (CNN) can achieve accuracy of 50%. This
is less than the acoustic scene classification task baseline in the DCASE 2018
ASC challenge on the same data. A simple adaptation to the class labels of
pairing city labels with grouped scenes, accuracy increases to 52%, closer to
the simpler scene classification task. Finally we also formulate the problem in
a multi-task learning framework and achieve an accuracy of 56%, outperforming
the aforementioned approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.00985</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.00985</id><created>2019-05-02</created><authors><author><keyname>Malkiel</keyname><forenames>Itzik</forenames></author><author><keyname>Ahn</keyname><forenames>Sangtae</forenames></author><author><keyname>Taviani</keyname><forenames>Valentina</forenames></author><author><keyname>Menini</keyname><forenames>Anne</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author><author><keyname>Hardy</keyname><forenames>Christopher J.</forenames></author></authors><title>Conditional WGANs with Adaptive Gradient Balancing for Sparse MRI
  Reconstruction</title><categories>cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent sparse MRI reconstruction models have used Deep Neural Networks (DNNs)
to reconstruct relatively high-quality images from highly undersampled k-space
data, enabling much faster MRI scanning. However, these techniques sometimes
struggle to reconstruct sharp images that preserve fine detail while
maintaining a natural appearance. In this work, we enhance the image quality by
using a Conditional Wasserstein Generative Adversarial Network combined with a
novel Adaptive Gradient Balancing technique that stabilizes the training and
minimizes the degree of artifacts, while maintaining a high-quality
reconstruction that produces sharper images than other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01000</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01000</id><created>2019-05-02</created><authors><author><keyname>AlHajri</keyname><forenames>Mohamed I.</forenames></author><author><keyname>Ali</keyname><forenames>Nazar T.</forenames></author><author><keyname>Shubair</keyname><forenames>Raed M.</forenames></author></authors><title>Indoor Localization for IoT Using Adaptive Feature Selection: A Cascaded
  Machine Learning Approach</title><categories>eess.SP cs.AI</categories><comments>13 pages</comments><doi>10.1109/LAWP.2019.2915047</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Evolving Internet-of-Things (IoT) applications often require the use of
sensor-based indoor tracking and positioning, for which the performance is
significantly improved by identifying the type of the surrounding indoor
environment. This identification is of high importance since it leads to higher
localization accuracy. This paper presents a novel method based on a cascaded
two-stage machine learning approach for highly-accurate and robust localization
in indoor environments using adaptive selection and combination of RF features.
In the proposed method, machine learning is first used to identify the type of
the surrounding indoor environment. Then, in the second stage, machine learning
is employed to identify the most appropriate selection and combination of RF
features that yield the highest localization accuracy. Analysis is based on
k-Nearest Neighbor (k-NN) machine learning algorithm applied on a real dataset
generated from practical measurements of the RF signal in realistic indoor
environments. Received Signal Strength, Channel Transfer Function, and
Frequency Coherence Function are the primary RF features being explored and
combined. Numerical investigations demonstrate that prediction based on the
concatenation of primary RF features enhanced significantly as the localization
accuracy improved by at least 50% to more than 70%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01008</identifier>
 <datestamp>2019-05-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01008</id><created>2019-05-02</created><updated>2019-05-07</updated><authors><author><keyname>Shi</keyname><forenames>Yi</forenames></author><author><keyname>Davaslioglu</keyname><forenames>Kemal</forenames></author><author><keyname>Sagduyu</keyname><forenames>Yalin E.</forenames></author></authors><title>Generative Adversarial Network for Wireless Signal Spoofing</title><categories>eess.SP cs.LG cs.NI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a novel approach of spoofing wireless signals by using a
general adversarial network (GAN) to generate and transmit synthetic signals
that cannot be reliably distinguished from intended signals. It is of paramount
importance to authenticate wireless signals at the PHY layer before they
proceed through the receiver chain. For that purpose, various waveform,
channel, and radio hardware features that are inherent to original wireless
signals need to be captured. In the meantime, adversaries become sophisticated
with the cognitive radio capability to record, analyze, and manipulate signals
before spoofing. Building upon deep learning techniques, this paper introduces
a spoofing attack by an adversary pair of a transmitter and a receiver that
assume the generator and discriminator roles in the GAN and play a minimax game
to generate the best spoofing signals that aim to fool the best trained defense
mechanism. The output of this approach is two-fold. From the attacker point of
view, a deep learning-based spoofing mechanism is trained to potentially fool a
defense mechanism such as RF fingerprinting. From the defender point of view, a
deep learning-based defense mechanism is trained against potential spoofing
attacks when an adversary pair of a transmitter and a receiver cooperates. The
probability that the spoofing signal is misclassified as the intended signal is
measured for random signal, replay, and GAN-based spoofing attacks. Results
show that the GAN-based spoofing attack provides a major increase in the
success probability of wireless signal spoofing even when a deep learning
classifier is used as the defense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01022</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01022</id><created>2019-05-01</created><authors><author><keyname>Sheng</keyname><forenames>Di</forenames></author><author><keyname>Fazekas</keyname><forenames>Gy&#xf6;rgy</forenames></author></authors><title>A Feature Learning Siamese Model for Intelligent Control of the Dynamic
  Range Compressor</title><categories>eess.AS cs.LG cs.SD</categories><comments>8 pages, accepted in IJCNN 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a siamese DNN model is proposed to learn the characteristics
of the audio dynamic range compressor (DRC). This facilitates an intelligent
control system that uses audio examples to configure the DRC, a widely used
non-linear audio signal conditioning technique in the areas of music
production, speech communication and broadcasting. Several alternative siamese
DNN architectures are proposed to learn feature embeddings that can
characterise subtle effects due to dynamic range compression. These models are
compared with each other as well as handcrafted features proposed in previous
work. The evaluation of the relations between the hyperparameters of DNN and
DRC parameters are also provided. The best model is able to produce a universal
feature embedding that is capable of predicting multiple DRC parameters
simultaneously, which is a significant improvement from our previous research.
The feature embedding shows better performance than handcrafted audio features
when predicting DRC parameters for both mono-instrument audio loops and
polyphonic music pieces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01025</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01025</id><created>2019-05-02</created><authors><author><keyname>Lu</keyname><forenames>Ming</forenames></author><author><keyname>Cheng</keyname><forenames>Ming</forenames></author><author><keyname>Xu</keyname><forenames>Yiling</forenames></author><author><keyname>Pu</keyname><forenames>Shiliang</forenames></author><author><keyname>Shen</keyname><forenames>Qiu</forenames></author><author><keyname>Ma</keyname><forenames>Zhan</forenames></author></authors><title>Learned Quality Enhancement via Multi-Frame Priors for HEVC Compliant
  Low-Delay Applications</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networked video applications, e.g., video conferencing, often suffer from
poor visual quality due to unexpected network fluctuation and limited
bandwidth. In this paper, we have developed a Quality Enhancement Network
(QENet) to reduce the video compression artifacts, leveraging the spatial and
temporal priors generated by respective multi-scale convolutions spatially and
warped temporal predictions in a recurrent fashion temporally. We have
integrated this QENet as a standard-alone post-processing subsystem to the High
Efficiency Video Coding (HEVC) compliant decoder. Experimental results show
that our QENet demonstrates the state-of-the-art performance against default
in-loop filters in HEVC and other deep learning based methods with noticeable
objective gains in Peak-Signal-to-Noise Ratio (PSNR) and subjective gains
visually.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01030</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01030</id><created>2019-05-03</created><authors><author><keyname>Amirsoleimani</keyname><forenames>Shervin</forenames></author><author><keyname>Olfat</keyname><forenames>Ali</forenames></author></authors><title>Single Stage DOA-Frequency Representation of the Array Data with Source
  Reconstruction Capability</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new signal processing framework is proposed, in which the
array time samples are represented in DOA-frequency domain through a single
stage problem. It is shown that concatenated array data is well represented in
a $\mathbf{G}$ dictionary atoms space, where $\mathbf{G}$ columns correspond to
pixels in the DOA-frequency image. We present two approaches for the
$\mathbf{G}$ formation and compare the benefits and disadvantages of them. A
mutual coherence guaranteed $\mathbf{G}$ manipulation technique is also
proposed. Furthermore, unlike most of the existing methods, the proposed
problem is reversible into the time domain, therefore, source recovery from the
resulted DOA-frequency image is possible. The proposed representation in
DOA-frequency domain can be simply transformed into a group sparse problem, in
the case of non-multitone sources in a given bandwidth. Therefore, it can also
be utilized as an effective wideband DOA estimator. In the simulation part, two
scenarios of multitone sources with unknown frequency and DOA locations and
non-multitone wideband sources with assumed frequency region are examined. In
multitone scenario, sparse solvers yield more accurate DOA-frequency
representation compared to some noncoherent approaches. At the latter scenario,
the proposed method with group sparse solver outperforms some existing wideband
DOA estimators in low SNR regime. In addition, sources' recovery simultaneous
with DOA estimation shows significant improvement compared to the conventional
delay and sum beamformer and without prerequisites required in sophisticated
wideband beamformers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01093</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01093</id><created>2019-05-03</created><authors><author><keyname>Burrello</keyname><forenames>Alessio</forenames></author><author><keyname>Marchioni</keyname><forenames>Alex</forenames></author><author><keyname>Brunelli</keyname><forenames>Davide</forenames></author><author><keyname>Benini</keyname><forenames>Luca</forenames></author></authors><title>Embedding Principal Component Analysis for Data Reductionin Structural
  Health Monitoring on Low-Cost IoT Gateways</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis (PCA) is a powerful data reductionmethod for
Structural Health Monitoring. However, its computa-tional cost and data memory
footprint pose a significant challengewhen PCA has to run on limited capability
embedded platformsin low-cost IoT gateways. This paper presents a
memory-efficientparallel implementation of the streaming History PCA
algorithm.On our dataset, it achieves 10x compression factor and 59x
memoryreduction with less than 0.15 dB degradation in the
reconstructedsignal-to-noise ratio (RSNR) compared to standard PCA. More-over,
the algorithm benefits from parallelization on multiple cores,achieving a
maximum speedup of 4.8x on Samsung ARTIK 710.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01123</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01123</id><created>2019-05-03</created><authors><author><keyname>Kibria</keyname><forenames>Mirza Golam</forenames></author><author><keyname>Lagunas</keyname><forenames>Eva</forenames></author><author><keyname>Maturo</keyname><forenames>Nicola</forenames></author><author><keyname>Spano</keyname><forenames>Danilo</forenames></author><author><keyname>Al-Hraishawi</keyname><forenames>Hayder</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author></authors><title>Carrier Aggregation in Multi-Beam High Throughput Satellite Systems</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carrier Aggregation (CA) is an integral part of current terrestrial networks.
Its ability to enhance the peak data rate, to efficiently utilize the limited
available spectrum resources and to satisfy the demand for data-hungry
applications has drawn large attention from different wireless network
communities. Given the benefits of CA in the terrestrial wireless environment,
it is of great interest to analyze and evaluate the potential impact of CA in
the satellite domain. In this paper, we study CA in multibeam high throughput
satellite systems. We consider both inter-transponder and intra-transponder CA
at the satellite payload level of the communication stack, and we address the
problem of carrier-user assignment assuming that multiple users can be
multiplexed in each carrier. The transmission parameters of different carriers
are generated considering the transmission characteristics of carriers in
different transponders. In particular, we propose a flexible carrier allocation
approach for a CA-enabled multibeam satellite system targeting a proportionally
fair user demand satisfaction. Simulation results and analysis shed some light
on this rather unexplored scenario and demonstrate the feasibility of the CA in
satellite communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01139</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01139</id><created>2019-04-15</created><authors><author><keyname>Sardar</keyname><forenames>Santu</forenames></author><author><keyname>Mishra</keyname><forenames>Amit K.</forenames></author><author><keyname>Khan</keyname><forenames>Mohammed Zafar Ali</forenames></author></authors><title>Performance Evaluation of LTE-CommSense System for Discrimination of
  Presence of Multiple Objects in Outdoor Environment</title><categories>cs.NI eess.SP</categories><comments>This paper is a postprint of a paper submitted to and accepted for
  publication in IEEE Transactions on Instrumentation and Measurement and is
  subject to IEEE Transactions on Instrumentation and Measurement Copyright</comments><report-no>TIM2904332</report-no><journal-ref>IEEE Transactions on Instrumentation and Measurement. year 2019,
  Print ISSN: 0018-9456, Online ISSN: 1557-9662</journal-ref><doi>10.1109/TIM.2019.2904332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE-CommSense is a novel instrumentation scheme which analyzes channel
affected reference signals of LTE downlink signal to obtain knowledge about the
environmental change. This work presents the characterization of LTE-CommSense
instrument to detect presence or absence of objects in outdoor environment.
Additionally, we analyze its capability of detecting and distinguishing when
multiple objects are present. For performance evaluation and characterization
of this instrument, we derive object detection accuracy, FAR, FRR and
resolution which we believe are the most important figures of merit in this
case. As the operation of LTE-CommSense is to detect events instead of objects,
we redefine the concept of resolution for LTE-CommSense. Two different
proposals to represent the redefined resolution viz. Neyman Pearson principle
based and Cramer Rao principle based resolution are presented here. All the
performance metrics are derived using practical data captured using an SDR
platform modeled as a LTE-CommSense receiver. We observe that, LTE-CommSense
provides better performance in detecting presence or absence of objects at near
range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01152</identifier>
 <datestamp>2019-08-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01152</id><created>2019-04-30</created><updated>2019-08-20</updated><authors><author><keyname>Baskar</keyname><forenames>Murali Karthick</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Astudillo</keyname><forenames>Ramon</forenames></author><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author><author><keyname>Burget</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>&#x10c;ernock&#xfd;</keyname><forenames>Jan</forenames></author></authors><title>Semi-supervised Sequence-to-sequence ASR using Unpaired Speech and Text</title><categories>eess.AS cs.CL cs.IR cs.LG cs.SD</categories><comments>INTERSPEECH 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sequence-to-sequence automatic speech recognition (ASR) models require large
quantities of data to attain high performance. For this reason, there has been
a recent surge in interest for unsupervised and semi-supervised training in
such models. This work builds upon recent results showing notable improvements
in semi-supervised training using cycle-consistency and related techniques.
Such techniques derive training procedures and losses able to leverage unpaired
speech and/or text data by combining ASR with Text-to-Speech (TTS) models. In
particular, this work proposes a new semi-supervised loss combining an
end-to-end differentiable ASR$\rightarrow$TTS loss with TTS$\rightarrow$ASR
loss. The method is able to leverage both unpaired speech and text data to
outperform recently proposed related techniques in terms of \%WER. We provide
extensive results analyzing the impact of data quantity and speech and text
modalities and show consistent gains across WSJ and Librispeech corpora. Our
code is provided in ESPnet to reproduce the experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01154</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01154</id><created>2019-04-29</created><updated>2019-05-06</updated><authors><author><keyname>Talvitie</keyname><forenames>Jukka</forenames></author><author><keyname>Levanen</keyname><forenames>Toni</forenames></author><author><keyname>Koivisto</keyname><forenames>Mike</forenames></author><author><keyname>Ihalainen</keyname><forenames>Tero</forenames></author><author><keyname>Pajukoski</keyname><forenames>Kari</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Positioning and Location-Aware Communications for Modern Railways with
  5G New Radio</title><categories>cs.NI eess.SP</categories><comments>This article has been submitted to IEEE Communications Magazine for a
  publication. This is the first revised version of the original article and is
  now under second review round</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing high-capacity radio connectivity for high-speed trains (HSTs) is
one of the most important use cases of emerging 5G New Radio (NR) networks. In
this article, we show that 5G NR technology can also facilitate high-accuracy
continuous localization and tracking of HSTs. Furthermore, we describe and
demonstrate how the NR network can utilize the continuous location information
for efficient beam-management and beamforming, as well as for downlink Doppler
precompensation in the single-frequency network context. Additionally, with
particular focus on millimeter wave networks, novel concepts for low-latency
intercarrier interference (ICI) estimation and compensation, due to residual
Doppler and oscillator phase noise, are described and demonstrated. The
provided numerical results at 30 GHz operating band show that sub-meter
positioning and sub-degree beam-direction accuracies can be obtained with very
high probabilities in the order of 95-99%. The results also show that the
described Doppler precompensation and ICI estimation and cancellation methods
substantially improve the throughput of the single-frequency HST network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01199</identifier>
 <datestamp>2019-05-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01199</id><created>2019-05-03</created><authors><author><keyname>Churchill</keyname><forenames>Victor</forenames></author><author><keyname>Gelb</keyname><forenames>Anne</forenames></author></authors><title>Total Variation Bayesian Learning via Synthesis</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a sparse Bayesian learning algorithm for inverse problems
in signal and image processing with a total variation (TV) sparsity prior.
Because of the prior used, and the fact that the prior parameters are estimated
directly from the data, sparse Bayesian learning often produces more accurate
results than the typical maximum a posteriori Bayesian estimates for sparse
signal recovery. It also provides a full posterior distribution. However,
sparse Bayesian learning is only available to problems with a direct sparsity
prior or those formed via synthesis. This paper demonstrates how a problem with
a TV sparsity prior can be formulated in a synthesis approach. We then develop
a method that combines this synthesis-based TV with the sparse Bayesian
learning algorithm and provide numerical examples to demonstrate how our new
technique is effectively employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01209</identifier>
 <datestamp>2019-05-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01209</id><created>2019-05-03</created><updated>2019-05-14</updated><authors><author><keyname>Pariente</keyname><forenames>Manuel</forenames><affiliation>MULTISPEECH</affiliation></author><author><keyname>Deleforge</keyname><forenames>Antoine</forenames><affiliation>MULTISPEECH</affiliation></author><author><keyname>Vincent</keyname><forenames>Emmanuel</forenames><affiliation>MULTISPEECH</affiliation></author></authors><title>A Statistically Principled and Computationally Efficient Approach to
  Speech Enhancement using Variational Autoencoders</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Submitted to INTERSPEECH 2019</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have explored the use of deep generative models of speech
spectra based of variational autoencoders (VAEs), combined with unsupervised
noise models, to perform speech enhancement. These studies developed iterative
algorithms involving either Gibbs sampling or gradient descent at each step,
making them computationally expensive. This paper proposes a variational
inference method to iteratively estimate the power spectrogram of the clean
speech. Our main contribution is the analytical derivation of the variational
steps in which the en-coder of the pre-learned VAE can be used to estimate the
varia-tional approximation of the true posterior distribution, using the very
same assumption made to train VAEs. Experiments show that the proposed method
produces results on par with the afore-mentioned iterative methods using
sampling, while decreasing the computational cost by a factor 36 to reach a
given performance .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01344</identifier>
 <datestamp>2019-06-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01344</id><created>2019-05-03</created><authors><author><keyname>Carnahan</keyname><forenames>Patrick</forenames></author><author><keyname>Ginty</keyname><forenames>Olivia</forenames></author><author><keyname>Moore</keyname><forenames>John</forenames></author><author><keyname>Lasso</keyname><forenames>Andras</forenames></author><author><keyname>Jolley</keyname><forenames>Matthew A.</forenames></author><author><keyname>Herz</keyname><forenames>Christian</forenames></author><author><keyname>Eskandari</keyname><forenames>Mehdi</forenames></author><author><keyname>Bainbridge</keyname><forenames>Daniel</forenames></author><author><keyname>Peters</keyname><forenames>Terry M.</forenames></author></authors><title>Interactive-Automatic Segmentation and Modelling of the Mitral Valve</title><categories>eess.IV</categories><comments>Accepted for publication in Lecture Notes in Computer Science,
  Functional Imaging and Modelling of the Heart 2019</comments><doi>10.1007/978-3-030-21949-9_43</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mitral valve regurgitation is the most common valvular disease, affecting 10%
of the population over 75 years old. Left untreated, patients with mitral valve
regurgitation can suffer declining cardiac health until cardiac failure and
death. Mitral valve repair is generally preferred over valve replacement.
However, there is a direct correlation between the volume of cases performed
and surgical outcomes, therefore there is a demand for the ability of surgeons
to practice repairs on patient specific models in advance of surgery. This work
demonstrates a semi-automated segmentation method to enable fast and accurate
modelling of the mitral valve that captures patient-specific valve geometry.
This modelling approach utilizes 3D active contours in a user-in-the-loop
system which segments first the atrial blood pool, then the mitral leaflets. In
a group of 15 mitral valve repair patients, valve segmentation and modelling
attains an overall accuracy (mean absolute surface distance) of 1.40+-0.26 mm,
and an accuracy of 1.01+-0.13 mm when only comparing the extracted leaflet
surface proximal to the ultrasound probe. Thus this image-based segmentation
tool has the potential to improve the workflow for extracting patient-specific
mitral valve geometry for 3D modelling of the valve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01375</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01375</id><created>2019-05-03</created><authors><author><keyname>Covert</keyname><forenames>Ian</forenames></author><author><keyname>Krishnan</keyname><forenames>Balu</forenames></author><author><keyname>Najm</keyname><forenames>Imad</forenames></author><author><keyname>Zhan</keyname><forenames>Jiening</forenames></author><author><keyname>Shore</keyname><forenames>Matthew</forenames></author><author><keyname>Hixson</keyname><forenames>John</forenames></author><author><keyname>Po</keyname><forenames>Ming Jack</forenames></author></authors><title>Temporal Graph Convolutional Networks for Automatic Seizure Detection</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seizure detection from EEGs is a challenging and time consuming clinical
problem that would benefit from the development of automated algorithms. EEGs
can be viewed as structural time series, because they are multivariate time
series where the placement of leads on a patient's scalp provides prior
information about the structure of interactions. Commonly used deep learning
models for time series don't offer a way to leverage structural information,
but this would be desirable in a model for structural time series. To address
this challenge, we propose the temporal graph convolutional network (TGCN), a
model that leverages structural information and has relatively few parameters.
TGCNs apply feature extraction operations that are localized and shared over
both time and space, thereby providing a useful inductive bias in tasks where
one expects similar features to be discriminative across the different
sequences. In our experiments we focus on metrics that are most important to
seizure detection, and demonstrate that TGCN matches the performance of related
models that have been shown to be state of the art in other tasks.
Additionally, we investigate interpretability advantages of TGCN by exploring
approaches for helping clinicians determine when precisely seizures occur, and
the parts of the brain that are most involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01376</identifier>
 <datestamp>2019-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01376</id><created>2019-05-03</created><authors><author><keyname>Chen</keyname><forenames>Yicheng</forenames></author><author><keyname>Blum</keyname><forenames>Rick S.</forenames></author><author><keyname>Sadler</keyname><forenames>Brian M.</forenames></author><author><keyname>Zhang</keyname><forenames>Jiangfan</forenames></author></authors><title>Testing the Structure of a Gaussian Graphical Model with Reduced
  Transmissions in a Distributed Setting</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2940119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing a covariance matrix following a Gaussian graphical model (GGM) is
considered in this paper based on observations made at a set of distributed
sensors grouped into clusters. Ordered transmissions are proposed to achieve
the same Bayes risk as the optimum centralized energy unconstrained approach
but with fewer transmissions and a completely distributed approach. In this
approach, we represent the Bayes optimum test statistic as a sum of local test
statistics which can be calculated by only utilizing the observations available
at one cluster. We select one sensor to be the cluster head (CH) to collect and
summarize the observed data in each cluster and intercluster communications are
assumed to be inexpensive. The CHs with more informative observations transmit
their data to the fusion center (FC) first. By halting before all transmissions
have taken place, transmissions can be saved without performance loss. It is
shown that this ordering approach can guarantee a lower bound on the average
number of transmissions saved for any given GGM and the lower bound can
approach approximately half the number of clusters when the minimum eigenvalue
of the covariance matrix under the alternative hypothesis in each cluster
becomes sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01378</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01378</id><created>2019-05-03</created><authors><author><keyname>Liu</keyname><forenames>Zhentao</forenames></author><author><keyname>Mock</keyname><forenames>Jeffrey</forenames></author><author><keyname>Huang</keyname><forenames>Yufei</forenames></author><author><keyname>Golob</keyname><forenames>Edward</forenames></author></authors><title>Predicting Auditory Spatial Attention from EEG using Single- and
  Multi-task Convolutional Neural Networks</title><categories>eess.SP</categories><comments>6 pages, 5 tables, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent behavioral and electroencephalograph (EEG) studies have defined ways
that auditory spatial attention can be allocated over large regions of space.
As with most experimental studies, behavior EEG was averaged over 10s of
minutes because identifying abstract feature spatial codes from raw EEG data is
extremely challenging. The goal of this study is to design a deep learning
model that can learn from raw EEG data and predict auditory spatial information
on a trial-by-trial basis. We designed a convolutional neural networks (CNN)
model to predict the attended location or other stimulus locations relative to
the attended location. A multi-task model was also used to predict the attended
and stimulus locations at the same time. Based on the visualization of our
models, we investigated features of individual classification tasks and joint
feature of the multi-task model. Our model achieved an average 72.4% in
relative location prediction and 90.0% in attended location prediction
individually. The multi-task model improved the performance of attended
location prediction by 3%. Our results suggest a strong correlation between
attended location and relative location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01389</identifier>
 <datestamp>2019-05-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01389</id><created>2019-05-03</created><updated>2019-05-10</updated><authors><author><keyname>Cai</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Xiaoguang</forenames></author><author><keyname>Liu</keyname><forenames>Lizuo</forenames></author></authors><title>PhaseDNN - A Parallel Phase Shift Deep Neural Network for Adaptive
  Wideband Learning</title><categories>eess.SP cs.LG stat.ML</categories><msc-class>68Q32, 68T01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a phase shift deep neural network (PhaseDNN) which
provides a wideband convergence in approximating a high dimensional function
during its training of the network. The PhaseDNN utilizes the fact that many
DNN achieves convergence in the low frequency range first, thus, a series of
moderately-sized of DNNs are constructed and trained in parallel for ranges of
higher frequencies. With the help of phase shifts in the frequency domain,
implemented through a simple phase factor multiplication on the training data,
each DNN in the series will be trained to approximate the target function's
higher frequency content over a specific range. Due to the phase shift, each
DNN achieves the speed of convergence as in the low frequency range. As a
result, the proposed PhaseDNN system is able to convert wideband frequency
learning to low frequency learning, thus allowing a uniform learning to
wideband high dimensional functions with frequency adaptive training. Numerical
results have demonstrated the capability of PhaseDNN in learning information of
a target function from low to high frequency uniformly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01391</identifier>
 <datestamp>2019-09-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01391</id><created>2019-05-03</created><updated>2019-09-26</updated><authors><author><keyname>Casebeer</keyname><forenames>Jonah</forenames></author><author><keyname>Colomb</keyname><forenames>Michael</forenames></author><author><keyname>Smaragdis</keyname><forenames>Paris</forenames></author></authors><title>Deep Tensor Factorization for Spatially-Aware Scene Decomposition</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>5 pages, 5 figures, accepted to WASPAA 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a completely unsupervised method to understand audio scenes
observed with random microphone arrangements by decomposing the scene into its
constituent sources and their relative presence in each microphone. To this
end, we formulate a neural network architecture that can be interpreted as a
nonnegative tensor factorization of a multi-channel audio recording. By
clustering on the learned network parameters corresponding to channel content,
we can learn sources' individual spectral dictionaries and their activation
patterns over time. Our method allows us to leverage deep learning advances
like end-to-end training, while also allowing stochastic minibatch training so
that we can feasibly decompose realistic audio scenes that are intractable to
decompose using standard methods. This neural network architecture is easily
extensible to other kinds of tensor factorizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01465</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01465</id><created>2019-05-04</created><authors><author><keyname>Cisotto</keyname><forenames>Giulia</forenames></author><author><keyname>Pupolin</keyname><forenames>Silvano</forenames></author><author><keyname>Piccione</keyname><forenames>Francesco</forenames></author></authors><title>Comparison About EEG Signals Processing in BCI Applications</title><categories>eess.SP q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of a Brain Computer Interface platform implemented for the arm
rehabilitation of mildly impaired stroke patients, two methods of EEG signals
processing are compared in terms of (i) their identification performance rate
and (ii) their computational complexity with the overall goal to select the
most efficient and feasible real-time procedure. An effective signal processing
is, indeed, one of the most critical issue for such kind of technology which
aims to establish a real-time communication between the subject's brain and a
machine, i.e. a computer, a robotic arm or another device, that should
implement his/her intention to move in place of his/her impaired arm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01472</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01472</id><created>2019-05-04</created><authors><author><keyname>Illi</keyname><forenames>Elmehdi</forenames></author><author><keyname>Bouanani</keyname><forenames>Faissal El</forenames></author><author><keyname>Park</keyname><forenames>Ki-Hong</forenames></author><author><keyname>Ayoub</keyname><forenames>Fouad</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>An Improved Accurate Solver for the Time-Dependent RTE in Underwater
  Optical Wireless Communications</title><categories>eess.SP cs.CE cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an improved numerical solver to evaluate the time-dependent
radiative transfer equation (RTE) for underwater optical wireless
communications (UOWC) is investigated. The RTE evaluates the optical path-loss
of light wave in an underwater channel in terms of the inherent optical
properties related to the environments, namely the absorption and scattering
coefficients as well as the phase scattering function (PSF). The proposed
numerical algorithm was improved based on the ones proposed in [1]-[4], by
modifying the finite difference scheme proposed in [1] as well as an
enhancement of the quadrature method proposed in [2] by involving a more
accurate 7-points quadrature scheme in order to calculate the quadrature weight
coefficients corresponding to the integral term of the RTE. Furthermore, the
scattering angular discretization algorithm used in [3] and [4] was modified,
based on which the receiver's field of view discretization was adapted
correspondingly. Interestingly, the RTE solver has been applied to three volume
scattering functions, namely: the single-term HG phase function, the two-term
HG phase function [5], and the Fournier-Forand phase function [6], over
Harbor-I and Harbor-II water types. Based on the normalized received power
evaluated through the proposed algorithm, the bit error rate performance of the
UOWC system is investigated in terms of system and channel parameters. The
enhanced algorithm gives a tightly close performance to its Monte Carlo
counterpart improved based on the simulations provided in [7], by adjusting the
numerical cumulative distribution function computation method as well as
optimizing the number of scattering angles. Matlab codes for the proposed RTE
solver are presented in [8].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01477</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01477</id><created>2019-05-04</created><authors><author><keyname>Dabiri</keyname><forenames>Mohammad Taghi</forenames></author><author><keyname>Safi</keyname><forenames>Hossein</forenames></author><author><keyname>Parsaeefard</keyname><forenames>Saeedeh</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author></authors><title>Analytical Channel Models for Millimeter Wave UAV Networks under
  Hovering Fluctuations</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of unmanned aerial vehicles (UAVs) and millimeter wave
(mmWave) wireless systems has been recently proposed to provide high data rate
aerial links for next generation wireless networks. However, establishing
UAV-based mmWave links is quite challenging due to the random fluctuations of
hovering UAVs which can induce antenna gain mismatch between transmitter and
receiver. To assess the benefit of UAV-based mmWave links, in this paper,
tractable, closed-form statistical channel models are derived for three UAV
communication scenarios: (i) a direct UAV-to-UAV link, (ii) an aerial relay
link in which source, relay, and destination are hovering UAVs, and (iii) a
relay link in which a hovering UAV connects a ground source to a ground
destination. The accuracy of the derived analytical expressions is corroborated
by performing Monte-Carlo simulations. Numerical results are then used to study
the effect of antenna directivity gain under different channel conditions for
establishing reliable UAV-based mmWave links in terms of achieving minimum
outage probability. It is shown that the performance of such links is largely
dependent on the random fluctuations of hovering UAVs. Moreover, higher antenna
directivity gains achieve better performance at low SNR regime. Nevertheless,
at the high SNR regime, lower antenna directivity gains result in a more
reliable communication link. The developed results can therefore be applied as
a benchmark for finding the optimal antenna directivity gain of UAVs under the
different levels of instability without resorting to time-consuming
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01491</identifier>
 <datestamp>2019-08-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01491</id><created>2019-05-04</created><updated>2019-08-07</updated><authors><author><keyname>Yan</keyname><forenames>Wenjing</forenames><affiliation>Senior Member IEEE</affiliation></author><author><keyname>Kuai</keyname><forenames>Xiaoyan</forenames><affiliation>Senior Member IEEE</affiliation></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames><affiliation>Senior Member IEEE</affiliation></author></authors><title>Passive Beamforming and Information Transfer via Large Intelligent
  Surface</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large intelligent surface (LIS) has emerged as a promising new solution to
improve the energy and spectrum efficiency of wireless networks. A LIS,
composed of a large number of low-cost and energy-efficient reconfigurable
passive reflecting elements, enhances wireless communications by reflecting
impinging electro-magnetic waves. In this paper, we propose a novel passive
beamforming and information transfer (PBIT) technique, in which the LIS
simultaneously enhances the primary communication and sends information to the
receiver. We develop a passive beamforming method to improve the average
receive signal-to-noise ratio (SNR).We also establish a two-step approach at
the receiver to retrieve the information from both the transmitter and the LIS.
Numerical results show that the proposed PBIT system, especially with the
optimized passive beamforming, significantly outperforms the system without LIS
enhancement. Furthermore, a tradeoff between the passive-beamforming gain and
the information rate of the LIS has been demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01588</identifier>
 <datestamp>2019-07-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01588</id><created>2019-05-04</created><updated>2019-05-12</updated><authors><author><keyname>Dong</keyname><forenames>Ming</forenames></author><author><keyname>Sun</keyname><forenames>Jessie</forenames></author><author><keyname>Wang</keyname><forenames>Carl</forenames></author></authors><title>A Pattern Recognition Method for Partial Discharge Detection on
  Insulated Overhead Conductors</title><categories>eess.SP cs.AI</categories><comments>4 pages,6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today,insulated overhead conductors are increasingly used in many places of
the world due to the higher operational reliability, elimination of
phase-to-phase contact, closer distances between phases and stronger protection
for animals. However, the standard protection devices are often not able to
detect the conductor phase-to-ground fault and the more frequent tree/tree
branch hitting conductor events as these events only lead to partial discharge
(PD) activities instead of causing overcurrent seen on bare conductors. To
solve this problem, in recent years, Technical University of Ostrava (VSB)
devised a special meter to measure the voltage signal of the stray electrical
field along the insulated overhead conductors, hoping to detect the above
hazardous PD activities. In 2018, VSB published a large amount of waveform data
recorded by their meter on Kaggle, the world's largest data science
collaboration platform, looking for promising pattern recognition methods for
this application. To tackle this challenge, we developed a unique method based
on Seasonal and Trend decomposition using Loess (STL) and Support Vector
Machine (SVM) to recognize PD activities on insulated overhead conductors.
Different SVM kernels were tested and compared. Satisfactory classification
rates on VSB dataset were achieved with the use of Gaussian radial basis
kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01635</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01635</id><created>2019-05-05</created><authors><author><keyname>Lim</keyname><forenames>Kai Li</forenames></author><author><keyname>Br&#xe4;unl</keyname><forenames>Thomas</forenames></author></authors><title>A Methodological Review of Visual Road Recognition Procedures for
  Autonomous Driving Applications</title><categories>cs.CV eess.IV</categories><comments>14 pages, 6 Figures, 2 Tables. Permission to reprint granted from
  original figure authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current research interest in autonomous driving is growing at a rapid
pace, attracting great investments from both the academic and corporate
sectors. In order for vehicles to be fully autonomous, it is imperative that
the driver assistance system is adapt in road and lane keeping. In this paper,
we present a methodological review of techniques with a focus on visual road
detection and recognition. We adopt a pragmatic outlook in presenting this
review, whereby the procedures of road recognition is emphasised with respect
to its practical implementations. The contribution of this review hence covers
the topic in two parts -- the first part describes the methodological approach
to conventional road detection, which covers the algorithms and approaches
involved to classify and segregate roads from non-road regions; and the other
part focuses on recent state-of-the-art machine learning techniques that are
applied to visual road recognition, with an emphasis on methods that
incorporate convolutional neural networks and semantic segmentation. A
subsequent overview of recent implementations in the commercial sector is also
presented, along with some recent research works pertaining to road detections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01654</identifier>
 <datestamp>2019-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01654</id><created>2019-05-05</created><updated>2019-11-10</updated><authors><author><keyname>Liu</keyname><forenames>Chengxiao</forenames></author><author><keyname>Feng</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Yunfei</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang</forenames></author><author><keyname>Ge</keyname><forenames>Ning</forenames></author></authors><title>Optimal Beamforming for Hybrid Satellite Terrestrial Networks with
  Nonlinear PA and Imperfect CSIT</title><categories>cs.NI eess.SP</categories><comments>5 pages, 5 figures, journal</comments><journal-ref>IEEE Wireless Communications Letters, 2019</journal-ref><doi>10.1109/LWC.2019.2952124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In hybrid satellite-terrestrial networks (HSTNs), spectrum sharing is crucial
to alleviate the &quot;spectrum scarcity&quot; problem. Therein, the transmit beams
should be carefully designed to mitigate the inter-satellite-terrestrial
interference. Different from previous studies, this work considers the impact
of both nonlinear power amplifier (PA) and large-scale channel state
information at the transmitter (CSIT) on beamforming. These phenomena are
usually inevitable in a practical HSTN. Based on the Saleh model of PA
nonlinearity and the large-scale multi-beam satellite channel parameters, we
formulate a beamforming optimization problem to maximize the achievable rate of
the satellite system while ensuring that the inter-satellite-terrestrial
interference is below a given threshold. The optimal amplitude and phase of
desired beams are derived in a decoupled manner. Simulation results demonstrate
the superiority of the proposed beamforming scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01662</identifier>
 <datestamp>2019-05-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01662</id><created>2019-05-05</created><authors><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Member</keyname><forenames>Senior</forenames></author><author><keyname>IEEE</keyname></author><author><keyname>Yuan</keyname><forenames>Zhenghang</forenames></author><author><keyname>Du</keyname><forenames>Qian</forenames></author><author><keyname>Fellow</keyname></author><author><keyname>IEEE</keyname></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author><author><keyname>Fellow</keyname></author><author><keyname>IEEE</keyname></author></authors><title>GETNET: A General End-to-end Two-dimensional CNN Framework for
  Hyperspectral Image Change Detection</title><categories>cs.CV eess.IV</categories><doi>10.1109/TGRS.2018.2849692</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change detection (CD) is an important application of remote sensing, which
provides timely change information about large-scale Earth surface. With the
emergence of hyperspectral imagery, CD technology has been greatly promoted, as
hyperspectral data with the highspectral resolution are capable of detecting
finer changes than using the traditional multispectral imagery. Nevertheless,
the high dimension of hyperspectral data makes it difficult to implement
traditional CD algorithms. Besides, endmember abundance information at subpixel
level is often not fully utilized. In order to better handle high dimension
problem and explore abundance information, this paper presents a General
End-to-end Two-dimensional CNN (GETNET) framework for hyperspectral image
change detection (HSI-CD). The main contributions of this work are threefold:
1) Mixed-affinity matrix that integrates subpixel representation is introduced
to mine more cross-channel gradient features and fuse multi-source information;
2) 2-D CNN is designed to learn the discriminative features effectively from
multi-source data at a higher level and enhance the generalization ability of
the proposed CD algorithm; 3) A new HSI-CD data set is designed for the
objective comparison of different methods. Experimental results on real
hyperspectral data sets demonstrate the proposed method outperforms most of the
state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01663</identifier>
 <datestamp>2019-05-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01663</id><created>2019-05-05</created><authors><author><keyname>Wan</keyname><forenames>Shuo</forenames></author><author><keyname>Lu</keyname><forenames>Jiaxun</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Towards Big data processing in IoT: network management for online edge
  data processing</title><categories>cs.NI cs.AI cs.DC cs.IT eess.IV math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heavy data load and wide cover range have always been crucial problems for
internet of things (IoT). However, in mobile-edge computing (MEC) network, the
huge data can be partly processed at the edge. In this paper, a MEC-based big
data analysis network is discussed. The raw data generated by distributed
network terminals are collected and processed by edge servers. The edge servers
split out a large sum of redundant data and transmit extracted information to
the center cloud for further analysis. However, for consideration of limited
edge computation ability, part of the raw data in huge data sources may be
directly transmitted to the cloud. To manage limited resources online, we
propose an algorithm based on Lyapunov optimization to jointly optimize the
policy of edge processor frequency, transmission power and bandwidth
allocation. The algorithm aims at stabilizing data processing delay and saving
energy without knowing probability distributions of data sources. The proposed
network management algorithm may contribute to big data processing in future
IoT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01727</identifier>
 <datestamp>2020-01-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01727</id><created>2019-05-05</created><authors><author><keyname>Canessa</keyname><forenames>Enrique</forenames></author><author><keyname>Tenze</keyname><forenames>Livio</forenames></author></authors><title>Morphing a Stereogram into Hologram</title><categories>eess.IV physics.optics</categories><comments>PDF, 8 pages, 4 Figs</comments><journal-ref>J. Imaging 2020, 6(1), 1</journal-ref><doi>10.3390/jimaging6010001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a simple and fast method to reconstruct reality from
stereoscopic images. We bring together ideas from robust optical flow
techniques, morphing deformations and lightfield 3D rendering in order to
create unsupervised multiview images of a scene. The reconstruction algorithm
provides a good visualization of the virtual 3D imagery behind stereograms upon
display on a headset-free Looking Glass 3D monitor. We discuss the possibility
of applying the method for live 3D streaming optimized via an associated lookup
table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01743</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01743</id><created>2019-05-05</created><updated>2019-09-03</updated><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Tiulpin</keyname><forenames>Aleksei</forenames></author><author><keyname>Shvets</keyname><forenames>Alexey A.</forenames></author><author><keyname>Kalinin</keyname><forenames>Alexandr A.</forenames></author><author><keyname>Iglovikov</keyname><forenames>Vladimir I.</forenames></author><author><keyname>Nikolenko</keyname><forenames>Sergey</forenames></author></authors><title>Breast Tumor Cellularity Assessment using Deep Neural Networks</title><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Breast cancer is one of the main causes of death worldwide. Histopathological
cellularity assessment of residual tumors in post-surgical tissues is used to
analyze a tumor's response to a therapy. Correct cellularity assessment
increases the chances of getting an appropriate treatment and facilitates the
patient's survival. In current clinical practice, tumor cellularity is manually
estimated by pathologists; this process is tedious and prone to errors or low
agreement rates between assessors. In this work, we evaluated three strong
novel Deep Learning-based approaches for automatic assessment of tumor
cellularity from post-treated breast surgical specimens stained with
hematoxylin and eosin. We validated the proposed methods on the BreastPathQ
SPIE challenge dataset that consisted of 2395 image patches selected from whole
slide images acquired from 64 patients. Compared to expert pathologist scoring,
our best performing method yielded the Cohen's kappa coefficient of 0.70 (vs.
0.42 previously known in literature) and the intra-class correlation
coefficient of 0.89 (vs. 0.83). Our results suggest that Deep Learning-based
methods have a significant potential to alleviate the burden on pathologists,
enhance the diagnostic workflow, and, thereby, facilitate better clinical
outcomes in breast cancer treatment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01787</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01787</id><created>2019-05-05</created><authors><author><keyname>Yao</keyname><forenames>Yiwu</forenames></author><author><keyname>Yang</keyname><forenames>Weiqiang</forenames></author><author><keyname>Zhu</keyname><forenames>Haoqi</forenames></author></authors><title>Creating Lightweight Object Detectors with Model Compression for
  Deployment on Edge Devices</title><categories>cs.CV eess.IV</categories><comments>lightweight detector, automatic channel pruning, fixed channel
  deletion, knowledge distillation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve lightweight object detectors for deployment on the edge devices,
an effective model compression pipeline is proposed in this paper. The
compression pipeline consists of automatic channel pruning for the backbone,
fixed channel deletion for the branch layers and knowledge distillation for the
guidance learning. As results, the Resnet50-v1d is auto-pruned and fine-tuned
on ImageNet to attain a compact base model as the backbone of object detector.
Then, lightweight object detectors are implemented with proposed compression
pipeline. For instance, the SSD-300 with model size=16.3MB, FLOPS=2.31G, and
mAP=71.2 is created, revealing a better result than SSD-300-MobileNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01827</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01827</id><created>2019-05-06</created><authors><author><keyname>Sirichotedumrong</keyname><forenames>Warit</forenames></author><author><keyname>Maekawa</keyname><forenames>Takahiro</forenames></author><author><keyname>Kinoshita</keyname><forenames>Yuma</forenames></author><author><keyname>Kiya</keyname><forenames>Hitoshi</forenames></author></authors><title>Privacy-Preserving Deep Neural Networks with Pixel-based Image
  Encryption Considering Data Augmentation in the Encrypted Domain</title><categories>cs.CR eess.IV</categories><comments>Accepted in the 26th IEEE International Conference on Image
  Processing (ICIP2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel privacy-preserving scheme for deep neural networks (DNNs)
that enables us not to only apply images without visual information to DNNs for
both training and testing but to also consider data augmentation in the
encrypted domain for the first time. In this paper, a novel pixel-based image
encryption method is first proposed for privacy-preserving DNNs. In addition, a
novel adaptation network is considered that reduces the influence of image
encryption. In an experiment, the proposed method is applied to a well-known
network, ResNet-18, for image classification. The experimental results
demonstrate that conventional privacy-preserving machine learning methods
including the state-of-the-arts cannot be applied to data augmentation in the
encrypted domain and that the proposed method outperforms them in terms of
classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01842</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01842</id><created>2019-05-06</created><authors><author><keyname>Nardelli</keyname><forenames>Marco Buongiorno</forenames></author></authors><title>Topology of Networks in Generalized Musical Spaces</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The abstraction of musical structures (notes, melodies, chords, harmonic or
rhythmic progressions, etc.) as mathematical objects in a geometrical space is
one of the great accomplishments of contemporary music theory. Building on this
foundation, I generalize the concept of musical spaces as networks and derive
functional principles of compositional design by the direct analysis of the
network topology. This approach provides a novel framework for the analysis and
quantification of similarity of musical objects and structures, and suggests a
way to relate such measures to the human perception of different musical
entities. Finally, the analysis of a single work or a corpus of compositions as
complex networks provides alternative ways of interpreting the compositional
process of a composer by quantifying emergent behaviors with well-established
statistical mechanics techniques. Interpreting the latter as probabilistic
randomness in the network, I develop novel compositional design frameworks that
are central to my own artistic research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01898</identifier>
 <datestamp>2020-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01898</id><created>2019-05-06</created><updated>2019-11-14</updated><authors><author><keyname>Fu</keyname><forenames>Szu-Wei</forenames></author><author><keyname>Liao</keyname><forenames>Chien-Feng</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author></authors><title>Learning with Learned Loss Function: Speech Enhancement with Quality-Net
  to Improve Perceptual Evaluation of Speech Quality</title><categories>cs.SD cs.LG eess.AS</categories><comments>Accepted by IEEE Signal Processing Letters (SPL)</comments><doi>10.1109/LSP.2019.2953810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilizing a human-perception-related objective function to train a speech
enhancement model has become a popular topic recently. The main reason is that
the conventional mean squared error (MSE) loss cannot represent auditory
perception well. One of the typical hu-man-perception-related metrics, which is
the perceptual evaluation of speech quality (PESQ), has been proven to provide
a high correlation to the quality scores rated by humans. Owing to its complex
and non-differentiable properties, however, the PESQ function may not be used
to optimize speech enhancement models directly. In this study, we propose
optimizing the enhancement model with an approximated PESQ function, which is
differentiable and learned from the training data. The experimental results
show that the learned surrogate function can guide the enhancement model to
further boost the PESQ score (in-crease of 0.18 points compared to the results
trained with MSE loss) and maintain the speech intelligibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01899</identifier>
 <datestamp>2019-07-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01899</id><created>2019-05-06</created><updated>2019-07-30</updated><authors><author><keyname>Lordelo</keyname><forenames>Carlos</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author><author><keyname>Dixon</keyname><forenames>Simon</forenames></author><author><keyname>Ahlb&#xe4;ck</keyname><forenames>Sven</forenames></author></authors><title>Investigating kernel shapes and skip connections for deep learning-based
  harmonic-percussive separation</title><categories>cs.SD eess.AS</categories><comments>Accepted for publication at WASPAA 2019, 5 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper we propose an efficient deep learning encoder-decoder network
for performing Harmonic-Percussive Source Separation (HPSS). It is shown that
we are able to greatly reduce the number of model trainable parameters by using
a dense arrangement of skip connections between the model layers. We also
explore the utilisation of different kernel sizes for the 2D filters of the
convolutional layers with the objective of allowing the network to learn the
different time-frequency patterns associated with percussive and harmonic
sources more efficiently. The training and evaluation of the separation has
been done using the training and test sets of the MUSDB18 dataset. Results show
that the proposed deep network achieves automatic learning of high-level
features and maintains HPSS performance at a state-of-the-art level while
reducing the number of parameters and training time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01902</identifier>
 <datestamp>2019-05-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01902</id><created>2019-05-06</created><updated>2019-05-26</updated><authors><author><keyname>Xing</keyname><forenames>Jie</forenames></author><author><keyname>Li</keyname><forenames>Zheren</forenames></author><author><keyname>Wang</keyname><forenames>Biyuan</forenames></author><author><keyname>Yu</keyname><forenames>Bingbin</forenames></author><author><keyname>Zanjani</keyname><forenames>Farhad G.</forenames></author><author><keyname>Zheng</keyname><forenames>Aiwen</forenames></author><author><keyname>Duits</keyname><forenames>Remco</forenames></author><author><keyname>Tan</keyname><forenames>Tao</forenames></author></authors><title>Automated Segmentation of Lesions in Ultrasound Using Semi-pixel-wise
  Cycle Generative Adversarial Nets</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breast cancer is the most common invasive cancer with the highest cancer
occurrence in females. Handheld ultrasound is one of the most efficient ways to
identify and diagnose the breast cancer. The area and the shape information of
a lesion is very helpful for clinicians to make diagnostic decisions. In this
study we propose a new deep-learning scheme, semi-pixel-wise cycle generative
adversarial net (SPCGAN) for segmenting the lesion in 2D ultrasound. The method
takes the advantage of a fully connected convolutional neural network (FCN) and
a generative adversarial net to segment a lesion by using prior knowledge. We
compared the proposed method to a fully connected neural network and the level
set segmentation method on a test dataset consisting of 32 malignant lesions
and 109 benign lesions. Our proposed method achieved a Dice similarity
coefficient (DSC) of 0.92 while FCN and the level set achieved 0.90 and 0.79
respectively. Particularly, for malignant lesions, our method increases the DSC
(0.90) of the fully connected neural network to 0.93 significantly (p$&lt;$0.001).
The results show that our SPCGAN can obtain robust segmentation results and may
be used to relieve the radiologists' burden for annotation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01916</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01916</id><created>2019-05-06</created><authors><author><keyname>Detlefs</keyname><forenames>Carsten</forenames></author><author><keyname>Beltran</keyname><forenames>Mario A.</forenames></author><author><keyname>Guigay</keyname><forenames>Jean-Pierre</forenames></author><author><keyname>Simons</keyname><forenames>Hugh</forenames></author></authors><title>Translative lens-based full field coherent X-ray imaging</title><categories>physics.optics eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a full-field coherent imaging approach suitable for hard X-rays
based on a classical (i.e. Galilean) X-ray microscope. The method combines a
series of low-resolution images acquired at different transverse lens positions
into a single high-resolution image, overcoming the spatial resolution limit
set by the numerical aperture of the objective lens. We describe the optical
principles of the approach, demonstrate the successful reconstruction of
simulated phantom data, and discuss aspects of the reconstruction. We believe
this approach offers some potential benefits over conventional scanning X-ray
ptychography in terms of acquisition speed, spatial bandwidth and radiation
dose rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01926</identifier>
 <datestamp>2019-08-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01926</id><created>2019-05-06</created><updated>2019-08-07</updated><authors><author><keyname>Xie</keyname><forenames>Huang</forenames></author><author><keyname>Virtanen</keyname><forenames>Tuomas</forenames></author></authors><title>Zero-Shot Audio Classification Based on Class Label Embeddings</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>2019 IEEE Workshop on Applications of Signal Processing to Audio and
  Acoustics (WASPAA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a zero-shot learning approach for audio classification
based on the textual information about class labels without any audio samples
from target classes. We propose an audio classification system built on the
bilinear model, which takes audio feature embeddings and semantic class label
embeddings as input, and measures the compatibility between an audio feature
embedding and a class label embedding. We use VGGish to extract audio feature
embeddings from audio recordings. We treat textual labels as semantic side
information of audio classes, and use Word2Vec to generate class label
embeddings. Results on the ESC-50 dataset show that the proposed system can
perform zero-shot audio classification with small training dataset. It can
achieve accuracy (26 % on average) better than random guess (10 %) on each
audio category. Particularly, it reaches up to 39.7 % for the category of
natural audio classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1905.01982</identifier>
 <datestamp>2020-01-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1905.01982</id><created>2019-05-03</created><updated>2020-01-17</updated><authors><author><keyname>Yesilli</keyname><forenames>Melih C.</forenames></author><author><keyname>Khasawneh</keyname><forenames>Firas A.</forenames></author><author><keyname>Otto</keyname><forenames>Andreas</forenames></author></authors><title>On Transfer Learning For Chatter Detection in Turning Using Wavelet
  Packet Transform and Empirical Mode Decomposition</title><categories>eess.SP cs.CE cs.LG stat.ML</categories><comments>Informative wavelet packet numbers were edited with respect to
  frequency ordering in section 3. Three more supervised learning algorithms
  were added to compare performance of both method (see section 5 and 6). For
  transfer learning, results of the application where classifiers are trained
  with two different stickout size data and tested on remaining cases were
  added into section 6.3</comments><doi>10.1016/j.cirpj.2019.11.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing availability of sensor data at machine tools makes automatic
chatter detection algorithms a trending topic in metal cutting. Two prominent
and advanced methods for feature extraction via signal decomposition are
Wavelet Packet Transform (WPT) and Ensemble Empirical Mode Decomposition
(EEMD). We apply these two methods to time series acquired from an acceleration
sensor at the tool holder of a lathe. Different turning experiments with
varying dynamic behavior of the machine tool structure were performed. We
compare the performance of these two methods with Support Vector Machine (SVM),
Logistic Regression, Random Forest Classification and Gradient Boosting
combined with Recursive Feature Elimination (RFE). We also show that the common
WPT-based approach of choosing wavelet packets with the highest energy ratios
as representative features for chatter does not always result in packets that
enclose the chatter frequency, thus reducing the classification accuracy.
Further, we test the transfer learning capability of each of these methods by
training the classifier on one of the cutting configurations and then testing
it on the other cases. It is found that when training and testing on data from
the same cutting configuration both methods yield high accuracies reaching in
one of the cases as high as 94% and 95%, respectively, for WPT and EEMD.
However, our experimental results show that EEMD can outperform WPT in transfer
learning applications with accuracy of up to 95%.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="5000" completeListSize="16166">4250076|6001</resumptionToken>
</ListRecords>
</OAI-PMH>
