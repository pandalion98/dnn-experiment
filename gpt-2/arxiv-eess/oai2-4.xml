<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2020-03-01T06:59:12Z</responseDate>
<request verb="ListRecords" resumptionToken="4250076|3001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00358</identifier>
 <datestamp>2018-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00358</id><created>2018-09-02</created><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Allsop</keyname><forenames>Stephen</forenames></author><author><keyname>Tye</keyname><forenames>Kay M.</forenames></author><author><keyname>Ba</keyname><forenames>Demba</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>Sequential Detection of Regime Changes in Neural Data</title><categories>eess.SP q-bio.NC stat.AP stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting changes in firing patterns in neural data is
studied. The problem is formulated as a quickest change detection problem.
Important algorithms from the literature are reviewed. A new algorithmic
technique is discussed to detect deviations from learned baseline behavior. The
algorithms studied can be applied to both spike and local field potential data.
The algorithms are applied to mice spike data to verify the presence of
behavioral learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00381</identifier>
 <datestamp>2018-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00381</id><created>2018-09-02</created><authors><author><keyname>Bittner</keyname><forenames>Rachel M.</forenames></author><author><keyname>McFee</keyname><forenames>Brian</forenames></author><author><keyname>Bello</keyname><forenames>Juan P.</forenames></author></authors><title>Multitask Learning for Fundamental Frequency Estimation in Music</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental frequency (f0) estimation from polyphonic music includes the
tasks of multiple-f0, melody, vocal, and bass line estimation. Historically
these problems have been approached separately, and only recently, using
learning-based approaches. We present a multitask deep learning architecture
that jointly estimates outputs for various tasks including multiple-f0, melody,
vocal and bass line estimation, and is trained using a large,
semi-automatically annotated dataset. We show that the multitask model
outperforms its single-task counterparts, and explore the effect of various
design decisions in our approach, and show that it performs better or at least
competitively when compared against strong baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00386</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00386</id><created>2018-09-02</created><authors><author><keyname>Bashar</keyname><forenames>Manijeh</forenames></author><author><keyname>Haneda</keyname><forenames>Katsuyuki</forenames></author><author><keyname>Burr</keyname><forenames>Alister G.</forenames></author><author><keyname>Cumanan</keyname><forenames>Kanapathippillai</forenames></author></authors><title>A Study of Dynamic Multipath Clusters at 60 GHz in a Large Indoor
  Environment</title><categories>eess.SP cs.IT math.IT</categories><comments>7 pages, 12 figures, The IEEE GLOBECOM 2018 Workshops: Channel models
  and measurements for mmWave bands</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The available geometry-based stochastic channel models (GSCMs) at
millimetre-wave (mmWave) frequencies do not necessarily retain spatial
consistency for simulated channels, which is essential for small cells with
ultra-dense users. In this paper, we work on cluster parameterization for the
COST 2100 channel model using mobile channel simulations at 61 GHz in Helsinki
Airport. The paper considers a ray-tracer which has been optimized to match
measurements, to obtain double-directional channels at mmWave frequencies. A
joint clustering-tracking framework is used to determine cluster parameters for
the COST 2100 channel model. The KPowerMeans algorithm and the Kalman filter
are exploited to identify the cluster positions and to predict and track
cluster positions respectively. The results confirm that the joint
clustering-and-tracking is a suitable tool for cluster identification and
tracking for our ray-tracer results. The movement of cluster centroids, cluster
lifetime and number of clusters per snapshot are investigated for this set of
ray-tracer results. Simulation results show that the multipath components
(MPCs) are grouped into clusters at mmWave frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00446</identifier>
 <datestamp>2018-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00446</id><created>2018-09-03</created><authors><author><keyname>Kachroo</keyname><forenames>Amit</forenames></author><author><keyname>Ekin</keyname><forenames>Sabit</forenames></author></authors><title>Impact of Secondary User Interference on Primary Network in Cognitive
  Radio Systems</title><categories>eess.SP</categories><comments>5 pages,8 figures, This work is accepted for publication at IEEE 88th
  Vehicular Technology Conference (VTC 2018-Fall). Copyright IEEE 2018</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Most of the research in cognitive radio field is primarily focused on finding
and improving secondary user (SU) performance parameters such as bit error
rate, outage probability and capacity etc. Less attention is being paid towards
the other side of the network that is the primary network which is under
interference from SU. Also, it is the primary user (PU) that decides upon the
interference temperature constraint for power adaptation to maintain a certain
level of quality of service while providing access to SUs. However, given the
random nature of wireless communication, interference temperature can be
regulated dynamically to overcome the bottlenecks in entire network
performance. In order to do so, we need to analyze the primary network
carefully. This study tries to fill this gap by analytically finding the closed
form theoretical expressions for signal to interference and noise ratio (SINR),
mean SINR, instantaneous capacity, mean capacity and outage probability of PU,
while taking peak transmit power adaptation at SU into picture. Furthermore,
the expressions generated are validated with the simulation results and it is
found that our theoretical derivations are in perfect accord with the
simulation outcomes
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00502</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00502</id><created>2018-09-03</created><authors><author><keyname>Yu</keyname><forenames>Yi</forenames></author><author><keyname>Beuret</keyname><forenames>Samuel</forenames></author><author><keyname>Zeng</keyname><forenames>Donghuo</forenames></author><author><keyname>Oyama</keyname><forenames>Keizo</forenames></author></authors><title>Deep Learning of Human Perception in Audio Event Classification</title><categories>cs.SD cs.MM eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce our recent studies on human perception in audio
event classification by different deep learning models. In particular, the
pre-trained model VGGish is used as feature extractor to process audio data,
and DenseNet is trained by and used as feature extractor for our
electroencephalography (EEG) data. The correlation between audio stimuli and
EEG is learned in a shared space. In the experiments, we record brain
activities (EEG signals) of several subjects while they are listening to music
events of 8 audio categories selected from Google AudioSet, using a 16-channel
EEG headset with active electrodes. Our experimental results demonstrate that
i) audio event classification can be improved by exploiting the power of human
perception, and ii) the correlation between audio stimuli and EEG can be
learned to complement audio event understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00531</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00531</id><created>2018-09-03</created><updated>2018-09-07</updated><authors><author><keyname>Song</keyname><forenames>Qun</forenames></author><author><keyname>Gu</keyname><forenames>Chaojie</forenames></author><author><keyname>Tan</keyname><forenames>Rui</forenames></author></authors><title>Deep Room Recognition Using Inaudible Echos</title><categories>cs.SD eess.AS</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen the increasing need of location awareness by mobile
applications. This paper presents a room-level indoor localization approach
based on the measured room's echos in response to a two-millisecond single-tone
inaudible chirp emitted by a smartphone's loudspeaker. Different from other
acoustics-based room recognition systems that record full-spectrum audio for up
to ten seconds, our approach records audio in a narrow inaudible band for 0.1
seconds only to preserve the user's privacy. However, the short-time and
narrowband audio signal carries limited information about the room's
characteristics, presenting challenges to accurate room recognition. This paper
applies deep learning to effectively capture the subtle fingerprints in the
rooms' acoustic responses. Our extensive experiments show that a two-layer
convolutional neural network fed with the spectrogram of the inaudible echos
achieve the best performance, compared with alternative designs using other raw
data formats and deep models. Based on this result, we design a RoomRecognize
cloud service and its mobile client library that enable the mobile application
developers to readily implement the room recognition functionality without
resorting to any existing infrastructures and add-on hardware.
  Extensive evaluation shows that RoomRecognize achieves 99.7%, 97.7%, 99%, and
89% accuracy in differentiating 22 and 50 residential/office rooms, 19 spots in
a quiet museum, and 15 spots in a crowded museum, respectively. Compared with
the state-of-the-art approaches based on support vector machine, RoomRecognize
significantly improves the Pareto frontier of recognition accuracy versus
robustness against interfering sounds (e.g., ambient music).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00576</identifier>
 <datestamp>2019-05-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00576</id><created>2018-09-03</created><updated>2019-05-27</updated><authors><author><keyname>Rafi</keyname><forenames>Abdul Muntakim</forenames></author><author><keyname>Kamal</keyname><forenames>Uday</forenames></author><author><keyname>Hoque</keyname><forenames>Rakibul</forenames></author><author><keyname>Abrar</keyname><forenames>Abid</forenames></author><author><keyname>Das</keyname><forenames>Sowmitra</forenames></author><author><keyname>Lagani&#xe8;re</keyname><forenames>Robert</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Kamrul</forenames></author></authors><title>Application of DenseNet in Camera Model Identification and
  Post-processing Detection</title><categories>eess.IV eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Camera model identification has earned paramount importance in the field of
image forensics with an upsurge of digitally altered images which are
constantly being shared through websites, media, and social applications. But,
the task of identification becomes quite challenging if metadata are absent
from the image and/or if the image has been post-processed. In this paper, we
present a DenseNet pipeline to solve the problem of identifying the source
camera-model of an image. Our approach is to extract patches of 256*256 from a
labeled image dataset and apply augmentations, i.e., Empirical Mode
Decomposition (EMD). We use this extended dataset to train a Neural Network
with the DenseNet-201 architecture. We concatenate the output features for 3
different sizes (64*64, 128*128, 256*256) and pass them to a secondary network
to make the final prediction. This strategy proves to be very robust for
identifying the source camera model, even when the original image is
post-processed. Our model has been trained and tested on the Forensic
Camera-Model Identification Dataset provided for the IEEE Signal Processing
(SP) Cup 2018. During testing we achieved an overall accuracy of 98.37%, which
is the current state-of-the-art on this dataset using a single model. We used
transfer learning and tested our model on the Dresden Database for Camera Model
Identification, with an overall test accuracy of over 99% for 19 models. In
addition, we demonstrate that the proposed pipeline is suitable for other
image-forensic classification tasks, such as, detecting the type of
post-processing applied to an image with an accuracy of 96.66% -- which
indicates the generality of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00750</identifier>
 <datestamp>2018-10-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00750</id><created>2018-09-03</created><authors><author><keyname>Ying</keyname><forenames>Jiaxi</forenames></author><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Guo</keyname><forenames>Di</forenames></author><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Chen</keyname><forenames>Zhong</forenames></author><author><keyname>Qu</keyname><forenames>Xiaobo</forenames></author></authors><title>Vandermonde Factorization of Hankel Matrix for Complex Exponential
  Signal Recovery -- Application in Fast NMR Spectroscopy</title><categories>eess.SP</categories><comments>14 pages, 9 figures, 3 tables, 63 references</comments><doi>10.1109/TSP.2018.2869122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many signals are modeled as a superposition of exponential functions in
spectroscopy of chemistry, biology and medical imaging. This paper studies the
problem of recovering exponential signals from a random subset of samples. We
exploit the Vandermonde structure of the Hankel matrix formed by the
exponential signal and formulate signal recovery as Hankel matrix completion
with Vandermonde factorization (HVaF). A numerical algorithm is developed to
solve the proposed model and its sequence convergence is analyzed
theoretically. Experiments on synthetic data demonstrate that HVaF succeeds
over a wider regime than the state-of-the-art nuclear-normminimization-based
Hankel matrix completion method, while has a less restriction on frequency
separation than the state-of-the-art atomic norm minimization and fast
iterative hard thresholding methods. The effectiveness of HVaF is further
validated on biological magnetic resonance spectroscopy data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00758</identifier>
 <datestamp>2018-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00758</id><created>2018-09-03</created><updated>2018-10-02</updated><authors><author><keyname>Chae</keyname><forenames>Myungsu</forenames></author><author><keyname>Kim</keyname><forenames>Tae-Ho</forenames></author><author><keyname>Shin</keyname><forenames>Young Hoon</forenames></author><author><keyname>Kim</keyname><forenames>June-Woo</forenames></author><author><keyname>Lee</keyname><forenames>Soo-Young</forenames></author></authors><title>End-to-end Multimodal Emotion and Gender Recognition with Dynamic Joint
  Loss Weights</title><categories>cs.LG cs.CV cs.SD eess.AS stat.ML</categories><comments>IROS 2018 Workshop on Crossmodal Learning for Intelligent Robotics</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task learning is a method for improving the generalizability of
multiple tasks. In order to perform multiple classification tasks with one
neural network model, the losses of each task should be combined. Previous
studies have mostly focused on multiple prediction tasks using joint loss with
static weights for training models, choosing the weights between tasks without
making sufficient considerations by setting them uniformly or empirically. In
this study, we propose a method to calculate joint loss using dynamic weights
to improve the total performance, instead of the individual performance, of
tasks. We apply this method to design an end-to-end multimodal emotion and
gender recognition model using audio and video data. This approach provides
proper weights for the loss of each task when the training process ends. In our
experiments, emotion and gender recognition with the proposed method yielded a
lower joint loss, which is computed as the negative log-likelihood, than using
static weights for joint loss. Moreover, our proposed model has better
generalizability than other models. To the best of our knowledge, this research
is the first to demonstrate the strength of using dynamic weights for joint
loss for maximizing overall performance in emotion and gender recognition
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00774</identifier>
 <datestamp>2018-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00774</id><created>2018-09-03</created><authors><author><keyname>Yuan</keyname><forenames>Feiniu</forenames></author><author><keyname>Zhang</keyname><forenames>Lin</forenames></author><author><keyname>Xia</keyname><forenames>Xue</forenames></author><author><keyname>Wan</keyname><forenames>Boyang</forenames></author><author><keyname>Huang</keyname><forenames>Qinghua</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>Deep Smoke Segmentation</title><categories>cs.CV eess.IV</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the recent success of fully convolutional networks (FCN) in
semantic segmentation, we propose a deep smoke segmentation network to infer
high quality segmentation masks from blurry smoke images. To overcome large
variations in texture, color and shape of smoke appearance, we divide the
proposed network into a coarse path and a fine path. The first path is an
encoder-decoder FCN with skip structures, which extracts global context
information of smoke and accordingly generates a coarse segmentation mask. To
retain fine spatial details of smoke, the second path is also designed as an
encoder-decoder FCN with skip structures, but it is shallower than the first
path network. Finally, we propose a very small network containing only add,
convolution and activation layers to fuse the results of the two paths. Thus,
we can easily train the proposed network end to end for simultaneous
optimization of network parameters. To avoid the difficulty in manually
labelling fuzzy smoke objects, we propose a method to generate synthetic smoke
images. According to results of our deep segmentation method, we can easily and
accurately perform smoke detection from videos. Experiments on three synthetic
smoke datasets and a realistic smoke dataset show that our method achieves much
better performance than state-of-the-art segmentation algorithms based on FCNs.
Test results of our method on videos are also appealing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00806</identifier>
 <datestamp>2019-02-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00806</id><created>2018-09-04</created><updated>2019-02-04</updated><authors><author><keyname>Santos</keyname><forenames>Irene</forenames></author><author><keyname>Murillo-Fuentes</keyname><forenames>Juan Jos&#xe9;</forenames></author><author><keyname>Arias-de-Reyna</keyname><forenames>Eva</forenames></author></authors><title>Equalization with Expectation Propagation at Smoothing Level</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a smoothing turbo equalizer based on the expectation
propagation (EP) algorithm with quite improved performance compared to the
Kalman smoother, at similar complexity. In scenarios where high-order
modulations or/and large memory channels are employed, the optimal BCJR
algorithm is computationally unfeasible. In this situation, low-cost but
suboptimal solutions, such as the linear minimum mean square error (LMMSE), are
commonly used. Recently, EP has been proposed as a tool to improve the Kalman
smoothing performance. In this paper we review these solutions to apply the EP
at the smoothing level, rather than at the forward and backwards stages. Also,
we better exploit the information coming from the channel decoder in the turbo
equalization schemes. With these improvements we reduce the computational
complexity, speed up convergence and outperform previous approaches. We
included some simulation results to show the robust behavior of the proposed
method regardless of the scenario, and its improvement in terms of performance
in comparison with other EP-based solutions in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00888</identifier>
 <datestamp>2018-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00888</id><created>2018-09-04</created><authors><author><keyname>Afchar</keyname><forenames>Darius</forenames></author><author><keyname>Nozick</keyname><forenames>Vincent</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Echizen</keyname><forenames>Isao</forenames></author></authors><title>MesoNet: a Compact Facial Video Forgery Detection Network</title><categories>cs.CV eess.IV</categories><comments>accepted to WIFS 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method to automatically and efficiently detect face
tampering in videos, and particularly focuses on two recent techniques used to
generate hyper-realistic forged videos: Deepfake and Face2Face. Traditional
image forensics techniques are usually not well suited to videos due to the
compression that strongly degrades the data. Thus, this paper follows a deep
learning approach and presents two networks, both with a low number of layers
to focus on the mesoscopic properties of images. We evaluate those fast
networks on both an existing dataset and a dataset we have constituted from
online videos. The tests demonstrate a very successful detection rate with more
than 98% for Deepfake and 95% for Face2Face.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.00962</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.00962</id><created>2018-08-26</created><updated>2018-10-30</updated><authors><author><keyname>Fannjiang</keyname><forenames>A.</forenames></author><author><keyname>Zhang</keyname><forenames>Z.</forenames></author></authors><title>Blind Ptychography by Douglas-Rachford Splitting</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind ptychography is the scanning version of coherent diffractive imaging
which seeks to recover both the object and the probe simultaneously. Based on
alternating minimization by Douglas-Rachford splitting, AMDRS is a blind
ptychographic algorithm informed by the uniqueness theory, the Poisson noise
model and the stability analysis. Enhanced by the initialization method and the
use of a randomly phased mask, AMDRS converges globally and geometrically.
Three boundary conditions are considered in the simulations: periodic,
dark-field and bright-field boundary conditions. The dark-field boundary
condition is suited for isolated objects while the bright-field boundary
condition is for non-isolated objects. The periodic boundary condition is a
mathematically convenient reference point. Depending on the avail- ability of
the boundary prior the dark-field and the bright-field boundary conditions may
or may not be enforced in the reconstruction. Not surprisingly, enforcing the
boundary condition improves the rate of convergence, sometimes in a significant
way. Enforcing the bright-field condition in the reconstruction can also remove
the linear phase ambiguity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01030</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01030</id><created>2018-09-04</created><authors><author><keyname>Fontanesi</keyname><forenames>Gianluca</forenames></author><author><keyname>Zhu</keyname><forenames>Anding</forenames></author><author><keyname>Ahmadi</keyname><forenames>Hamed</forenames></author></authors><title>Saving Lives at Sea with UAV-assisted Wireless Networks</title><categories>eess.SP cs.NI</categories><comments>3 pages, 2 figures, 1 tab</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate traits and trade-offs of a system combining
UAVs with BS or C-RAN for extending the terrestrial wireless coverage over the
sea in emergency situations. Results for an over the sea deployment link budget
show the trade-off between power consumption and throughput to meet the Search
and Rescue targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01046</identifier>
 <datestamp>2018-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01046</id><created>2018-08-29</created><authors><author><keyname>Iyer</keyname><forenames>Aditi</forenames></author><author><keyname>Tang</keyname><forenames>Bingjing</forenames></author><author><keyname>Rao</keyname><forenames>Vinayak</forenames></author><author><keyname>Kong</keyname><forenames>Nan</forenames></author></authors><title>Group-Representative Functional Network Estimation from Multi-Subject
  fMRI Data via MRF-based Image Segmentation</title><categories>stat.CO eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel two-phase approach to functional network estimation of
multi-subject functional Magnetic Resonance Imaging (fMRI) data, which applies
model-based image segmentation to determine a group-representative connectivity
map. In our approach, we first improve clustering-based Independent Component
Analysis (ICA) to generate maps of components occurring consistently across
subjects, and then estimate the group-representative map through MAP-MRF
(Maximum a priori - Markov random field) labeling. For the latter, we provide a
novel and efficient variational Bayes algorithm. We study the performance of
the proposed method using synthesized data following a theoretical model, and
demonstrate its viability in blind extraction of group-representative
functional networks using simulated fMRI data. We anticipate the proposed
method will be applied in identifying common neuronal characteristics in a
population, and could be further extended to real-world clinical diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01084</identifier>
 <datestamp>2019-02-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01084</id><created>2018-09-04</created><updated>2019-02-14</updated><authors><author><keyname>Yang</keyname><forenames>Zhaohui</forenames></author><author><keyname>Hou</keyname><forenames>Jiancao</forenames></author><author><keyname>Shikh-Bahaei</keyname><forenames>Mohammad</forenames></author></authors><title>Energy Efficient Resource Allocation for Mobile-Edge Computation
  Networks with NOMA</title><categories>eess.SP</categories><comments>7 pages 5 figures. arXiv admin note: text overlap with
  arXiv:1807.11846</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper investigates an uplink non-orthogonal multiple access (NOMA)-based
mobile-edge computing (MEC) network. Our objective is to minimize the total
energy consumption of all users including transmission energy and local
computation energy subject to computation latency and cloud computation
capacity constraints. We first prove that the total energy minimization problem
is a convex problem, and it is optimal to transmit with maximal time. Then, we
accordingly proposed an iterative algorithm with low complexity, where
closed-form solutions are obtained in each step. The proposed algorithm is
successfully shown to be globally optimal. Numerical results show that the
proposed algorithm achieves better performance than the conventional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01096</identifier>
 <datestamp>2018-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01096</id><created>2018-09-04</created><authors><author><keyname>Mazin</keyname><forenames>Asim</forenames></author><author><keyname>Elkourdi</keyname><forenames>Mohamed</forenames></author><author><keyname>Gitlin</keyname><forenames>Richard D.</forenames></author></authors><title>Accelerating Beam Sweeping in mmWave Standalone 5G New Radios using
  Recurrent Neural Networks</title><categories>eess.SP cs.LG</categories><comments>4 pages and 4 Figures. It was presented at VTC 2018-Fall</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) is a key technology to support high data rate
demands for 5G applications. Highly directional transmissions are crucial at
these frequencies to compensate for high isotropic pathloss. This reliance on
di- rectional beamforming, however, makes the cell discovery (cell search)
challenging since both base station (gNB) and user equipment (UE) jointly
perform a search over angular space to locate potential beams to initiate
communication. In the cell discovery phase, sequential beam sweeping is
performed through the angular coverage region in order to transmit
synchronization signals. The sweeping pattern can either be a linear rotation
or a hopping pattern that makes use of additional information. This paper
proposes beam sweeping pattern prediction, based on the dynamic distribution of
user traffic, using a form of recurrent neural networks (RNNs) called Gated
Recurrent Unit (GRU). The spatial distribution of users is inferred from data
in call detail records (CDRs) of the cellular network. Results show that the
users spatial distribution and their approximate location (direction) can be
accurately predicted based on CDRs data using GRU, which is then used to
calculate the sweeping pattern in the angular domain during cell search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01133</identifier>
 <datestamp>2018-09-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01133</id><created>2018-09-04</created><authors><author><keyname>Papadopoulos</keyname><forenames>Timos</forenames></author><author><keyname>Roberts</keyname><forenames>Stephen J.</forenames></author><author><keyname>Willis</keyname><forenames>Katherine J.</forenames></author></authors><title>Automated bird sound recognition in realistic settings</title><categories>cs.SD cs.CY cs.LG eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluated the effectiveness of an automated bird sound identification
system in a situation that emulates a realistic, typical application. We
trained classification algorithms on a crowd-sourced collection of bird audio
recording data and restricted our training methods to be completely free of
manual intervention. The approach is hence directly applicable to the analysis
of multiple species collections, with labelling provided by crowd-sourced
collection. We evaluated the performance of the bird sound recognition system
on a realistic number of candidate classes, corresponding to real conditions.
We investigated the use of two canonical classification methods, chosen due to
their widespread use and ease of interpretation, namely a k Nearest Neighbour
(kNN) classifier with histogram-based features and a Support Vector Machine
(SVM) with time-summarisation features. We further investigated the use of a
certainty measure, derived from the output probabilities of the classifiers, to
enhance the interpretability and reliability of the class decisions. Our
results demonstrate that both identification methods achieved similar
performance, but we argue that the use of the kNN classifier offers somewhat
more flexibility. Furthermore, we show that employing an outcome certainty
measure provides a valuable and consistent indicator of the reliability of
classification results. Our use of generic training data and our investigation
of probabilistic classification methodologies that can flexibly address the
variable number of candidate species/classes that are expected to be
encountered in the field, directly contribute to the development of a practical
bird sound identification system with potentially global application. Further,
we show that certainty measures associated with identification outcomes can
significantly contribute to the practical usability of the overall system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01265</identifier>
 <datestamp>2018-09-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01265</id><created>2018-09-04</created><authors><author><keyname>Hawkins</keyname><forenames>Cole</forenames></author><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author></authors><title>Robust Factorization and Completion of Streaming Tensor Data via
  Variational Bayesian Inference</title><categories>eess.SP eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming tensor factorization is a powerful tool for processing high-volume
and multi-way temporal data in Internet networks, recommender systems and
image/video data analysis. In many applications the full tensor is not known,
but instead received in a slice-by-slice manner over time. Streaming
factorizations aim to take advantage of inherent temporal relationships in data
analytics. Existing streaming tensor factorization algorithms rely on
least-squares data fitting and they do not possess a mechanism for tensor rank
determination. This leaves them susceptible to outliers and vulnerable to
over-fitting. This paper presents the first Bayesian robust streaming tensor
factorization model. Our model successfully identifies sparse outliers,
automatically determines the underlying tensor rank and accurately fits
low-rank structure. We implement our model in Matlab and compare it to existing
algorithms. Our algorithm is applied to factorize and complete various
streaming tensors including synthetic data, dynamic MRI, video sequences, and
Internet traffic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01410</identifier>
 <datestamp>2018-09-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01410</id><created>2018-09-05</created><updated>2018-09-06</updated><authors><author><keyname>Baur</keyname><forenames>Christoph</forenames></author><author><keyname>Albarqouni</keyname><forenames>Shadi</forenames></author><author><keyname>Navab</keyname><forenames>Nassir</forenames></author></authors><title>Generating Highly Realistic Images of Skin Lesions with GANs</title><categories>cs.CV eess.IV</categories><comments>Accepted at the MICCAI 2018 ISIC Skin Lesion Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As many other machine learning driven medical image analysis tasks, skin
image analysis suffers from a chronic lack of labeled data and skewed class
distributions, which poses problems for the training of robust and
well-generalizing models. The ability to synthesize realistic looking images of
skin lesions could act as a reliever for the aforementioned problems.
Generative Adversarial Networks (GANs) have been successfully used to
synthesize realistically looking medical images, however limited to low
resolution, whereas machine learning models for challenging tasks such as skin
lesion segmentation or classification benefit from much higher resolution data.
In this work, we successfully synthesize realistically looking images of skin
lesions with GANs at such high resolution. Therefore, we utilize the concept of
progressive growing, which we both quantitatively and qualitatively compare to
other GAN architectures such as the DCGAN and the LAPGAN. Our results show that
with the help of progressive growing, we can synthesize highly realistic
dermoscopic images of skin lesions that even expert dermatologists find hard to
distinguish from real ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01485</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01485</id><created>2018-09-05</created><updated>2019-04-12</updated><authors><author><keyname>Wai</keyname><forenames>Hoi-To</forenames></author><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman E.</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Blind Community Detection from Low-rank Excitations of a Graph Filter</title><categories>cs.SI eess.SP stat.ML</categories><comments>Single column format, 32 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper considers a new framework to detect communities in a graph from
the observation of signals at its nodes. We model the observed signals as noisy
outputs of an unknown network process, represented as a graph filter that is
excited by a set of unknown low-rank inputs/excitations. Application scenarios
of this model include diffusion dynamics, pricing experiments, and opinion
dynamics. Rather than learning the precise parameters of the graph itself, we
aim at retrieving the community structure directly. The paper shows that
communities can be detected by applying a spectral method to the covariance
matrix of graph signals. Our analysis indicates that the community detection
performance depends on a `low-pass' property of the graph filter. We also show
that the performance can be improved via a low-rank matrix plus sparse
decomposition method when the latent parameter vectors are known. Numerical
experiments demonstrate that our approach is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01518</identifier>
 <datestamp>2018-09-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01518</id><created>2018-09-02</created><authors><author><keyname>Zhou</keyname><forenames>Yifan</forenames></author><author><keyname>Zhou</keyname><forenames>Huilin</forenames></author><author><keyname>Zhou</keyname><forenames>Fuhui</forenames></author><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Resource Allocation for a Wireless Powered Integrated Radar and
  Communication System</title><categories>eess.SP</categories><comments>This paper was accepted by IEEE Wireless Communications Letters</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The integrated radar and communication system is promising in the next
generation wireless communication networks. However, its performance is
confined by the limited energy. In order to overcome it, a wireless powered
integrated radar and communication system is proposed. An energy minimization
problem is formulated subject to constraints on the radar and communication
performances. The energy beamforming and radar-communication waveform are
jointly optimized to minimize the consumption energy. The challenging
non-convex problem is solved by using semidefinite relaxation and auxiliary
variable methods. It is proved that the optimal solution can be obtained.
Simulation results demonstrate that our proposed optimal design outperforms the
benchmark scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01536</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01536</id><created>2018-09-03</created><updated>2018-09-06</updated><authors><author><keyname>Bai</keyname><forenames>Lin</forenames></author><author><keyname>Zhao</keyname><forenames>Yiming</forenames></author><author><keyname>Huang</keyname><forenames>Xinming</forenames></author></authors><title>A CNN Accelerator on FPGA Using Depthwise Separable Convolution</title><categories>eess.SP cs.AR</categories><comments>Accepted by IEEE Transaction on Circuits and Systems II: Express
  Briefs, Volume 65, Issue 10, Oct 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) have been widely deployed in the fields
of computer vision and pattern recognition because of their high accuracy.
However, large convolution operations are computing-intensive that often
requires a powerful computing platform such as Graphics Processing Unit (GPU).
This makes it difficult to apply CNNs to portable devices. The state-of-the-art
CNNs, such as MobileNetV2 and Xception, adopt depthwise separable convolution
to replace the standard convolution for embedded platforms. That significantly
reduces operations and parameters with only limited loss in accuracy. This
highly structured model is very suitable for Field-Programmable Gate Array
(FPGA) implementation. In this paper, a scalable high performance depthwise
separable convolution optimized CNN accelerator is proposed. The accelerator
can be fit into an FPGA of different sizes, provided the balancing between
hardware resources and processing speed. As an example, MobileNetV2 is
implemented on Arria 10 SoC FPGA, and the results show this accelerator can
classify each picture from ImageNet in 3.75ms, which is about 266.6 frames per
second. This achieves 20x speedup if compared to CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01569</identifier>
 <datestamp>2019-12-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01569</id><created>2018-09-05</created><updated>2019-12-07</updated><authors><author><keyname>Jereminov</keyname><forenames>Marko</forenames></author><author><keyname>Bromberg</keyname><forenames>David M.</forenames></author><author><keyname>Pandey</keyname><forenames>Amritanshu</forenames></author><author><keyname>Wagner</keyname><forenames>Martin R.</forenames></author><author><keyname>Pileggi</keyname><forenames>Larry</forenames></author></authors><title>Evaluating Feasibility within Power Flow</title><categories>eess.SP</categories><comments>Submitted manuscript for IEEE Transactions onSmart Grids (under
  review)</comments><doi>10.1109/TSG-00394-2019.R2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent development of techniques that improve the convergence properties of
power flow simulation have been demonstrated to facilitate scaling to large
system sizes (80k+ buses). However, the problem remains to reliably identify
cases that are infeasible, system configurations that have no solution. In this
paper, we use the circuit theoretic approach based on adjoint networks to
evaluate the feasibility of a power flow test case and further locate and
quantify the source of infeasibility in the cases operating beyond the tip of
the nose curve. By creating infeasibility current source models that are added
to each node of the system model and further coupling each source to its
corresponding node of the adjoint network, any locations of insufficient real
or reactive power are captured by a non-zero response of the adjoint network.
Furthermore, it is shown that the proposed joint simulation of power flow and
its adjoint network models provide the optimally minimized currents that can be
later utilized to inform corrective actions to restore the feasibility of power
flow problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01579</identifier>
 <datestamp>2019-03-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01579</id><created>2018-09-05</created><updated>2019-02-28</updated><authors><author><keyname>Samuylov</keyname><forenames>Denis K.</forenames></author><author><keyname>Purwar</keyname><forenames>Prateek</forenames></author><author><keyname>Sz&#xe9;kely</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Paul</keyname><forenames>Gr&#xe9;gory</forenames></author></authors><title>Modelling Point Spread Function in Fluorescence Microscopy with a Sparse
  Combination of Gaussian Mixture: Trade-off between Accuracy and Efficiency</title><categories>eess.IV cs.CV</categories><comments>This paper has been accepted in the IEEE Transactions on Image
  Processing</comments><doi>10.1109/TIP.2019.2898843</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deblurring is a fundamental inverse problem in bioimaging. It requires
modelling the point spread function (PSF), which captures the optical
distortions entailed by the image formation process. The PSF limits the spatial
resolution attainable for a given microscope. However, recent applications
require a higher resolution, and have prompted the development of
super-resolution techniques to achieve sub-pixel accuracy. This requirement
restricts the class of suitable PSF models to analog ones. In addition,
deblurring is computationally intensive, hence further requiring
computationally efficient models. A custom candidate fitting both requirements
is the Gaussian model. However, this model cannot capture the rich tail
structures found in both theoretical and empirical PSFs. In this paper, we aim
at improving the reconstruction accuracy beyond the Gaussian model, while
preserving its computational efficiency. We introduce a new class of analog PSF
models based on Gaussian mixtures. The number of Gaussian kernels controls both
the modelling accuracy and the computational efficiency of the model: the lower
the number of kernels, the lower accuracy and the higher efficiency. To explore
the accuracy--efficiency trade-off, we propose a variational formulation of the
PSF calibration problem, where a convex sparsity-inducing penalty on the number
of Gaussian kernels allows trading accuracy for efficiency. We derive an
efficient algorithm based on a fully-split formulation of alternating split
Bregman. We assess our framework on synthetic and real data and demonstrate a
better reconstruction accuracy in both geometry and photometry in point source
localisation---a fundamental inverse problem in fluorescence microscopy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01590</identifier>
 <datestamp>2018-09-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01590</id><created>2018-09-05</created><authors><author><keyname>Samuylov</keyname><forenames>Denis K.</forenames></author><author><keyname>Sz&#xe9;kely</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Paul</keyname><forenames>Gr&#xe9;gory</forenames></author></authors><title>A Bayesian framework for the analog reconstruction of kymographs from
  fluorescence microscopy data</title><categories>eess.IV cs.CV</categories><comments>This paper has been accepted in the IEEE Transactions on Image
  Processing</comments><doi>10.1109/TIP.2018.2867946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kymographs are widely used to represent and anal- yse spatio-temporal
dynamics of fluorescence markers along curvilinear biological compartments.
These objects have a sin- gular geometry, thus kymograph reconstruction is
inherently an analog image processing task. However, the existing approaches
are essentially digital: the kymograph photometry is sampled directly from the
time-lapse images. As a result, such kymographs rely on raw image data that
suffer from the degradations entailed by the image formation process and the
spatio-temporal resolution of the imaging setup. In this work, we address these
limitations and introduce a well-grounded Bayesian framework for the analog
reconstruction of kymographs. To handle the movement of the object, we
introduce an intrinsic description of kymographs using differential geometry: a
kymograph is a photometry defined on a parameter space that is embedded in
physical space by a time-varying map that follows the object geometry. We model
the kymograph photometry as a L\'evy innovation process, a flexible class of
non-parametric signal priors. We account for the image formation process using
the virtual microscope framework. We formulate a computationally tractable
representation of the associated maximum a posteriori problem and solve it
using a class of efficient and modular algorithms based on the alternating
split Bregman. We assess the performance of our Bayesian framework on synthetic
data and apply it to reconstruct the fluorescence dynamics along microtubules
in vivo in the budding yeast S. cerevisiae. We demonstrate that our framework
allows revealing patterns from single time-lapse data that are invisible on
standard digital kymographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01653</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01653</id><created>2018-09-05</created><updated>2018-12-26</updated><authors><author><keyname>Yoshida</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>Hierarchical Distribution Matching for Probabilistically Shaped Coded
  Modulation</title><categories>eess.SP cs.IT math.IT</categories><comments>11 pages, 7 figures</comments><doi>10.1109/JLT.2019.2895065</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation difficulties of combining distribution matching (DM) and
dematching (invDM) for probabilistic shaping (PS) with soft-decision forward
error correction (FEC) coding can be relaxed by reverse concatenation, for
which the FEC coding and decoding lies inside the shaping algorithms. PS can
seemingly achieve performance close to the Shannon limit, although there are
practical implementation challenges that need to be carefully addressed. We
propose a hierarchical DM (HiDM) scheme, having fully parallelized input/output
interfaces and a pipelined architecture that can efficiently perform the
DM/invDM without the complex operations of previously proposed methods such as
constant composition DM (CCDM). Furthermore, HiDM can operate at a
significantly larger post-FEC bit error rate (BER) for the same post-invDM BER
performance, which facilitates simulations. These benefits come at the cost of
a slightly larger rate loss and required signal-to-noise ratio at a given
post-FEC BER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01674</identifier>
 <datestamp>2020-02-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01674</id><created>2018-09-05</created><updated>2020-02-21</updated><authors><author><keyname>Nozari</keyname><forenames>Erfan</forenames></author><author><keyname>Cort&#xe9;s</keyname><forenames>Jorge</forenames></author></authors><title>Hierarchical Selective Recruitment in Linear-Threshold Brain Networks --
  Part I: Single-Layer Dynamics and Selective Inhibition</title><categories>eess.SY cs.NE cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goal-driven selective attention (GDSA) refers to the brain's function of
prioritizing, according to one's internal goals and desires, the activity of a
task-relevant subset of its overall network to efficiently process relevant
information while inhibiting the effects of distractions. Despite decades of
research in neuroscience, a comprehensive understanding of GDSA is still
lacking. We propose a novel framework for GDSA using concepts and tools from
control theory as well as insights and structures from neuroscience. Central to
this framework is an information-processing hierarchy with two main components:
selective inhibition of task-irrelevant activity and top-down recruitment of
task-relevant activity. We analyze the internal dynamics of each layer of the
hierarchy described as a network with linear-threshold dynamics and derive
conditions on its structure to guarantee existence and uniqueness of
equilibria, asymptotic stability, and boundedness of trajectories. We also
provide mechanisms that enforce selective inhibition using the
biologically-inspired schemes of feedforward and feedback inhibition. Despite
their differences, both schemes lead to the same conclusion: the intrinsic
dynamical properties of the (not-inhibited) task-relevant subnetworks are the
sole determiner of the dynamical properties that are achievable under selective
inhibition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01697</identifier>
 <datestamp>2018-09-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01697</id><created>2018-09-03</created><authors><author><keyname>Xu</keyname><forenames>Zirui</forenames></author><author><keyname>Yu</keyname><forenames>Fuxun</forenames></author><author><keyname>Liu</keyname><forenames>Chenchen</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author></authors><title>HASP: A High-Performance Adaptive Mobile Security Enhancement Against
  Malicious Speech Recognition</title><categories>cs.CR cs.LG cs.SD eess.AS eess.SP stat.ML</categories><comments>8 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, machine learning based Automatic Speech Recognition (ASR) technique
has widely spread in smartphones, home devices, and public facilities. As
convenient as this technology can be, a considerable security issue also raises
-- the users' speech content might be exposed to malicious ASR monitoring and
cause severe privacy leakage. In this work, we propose HASP -- a
high-performance security enhancement approach to solve this security issue on
mobile devices. Leveraging ASR systems' vulnerability to the adversarial
examples, HASP is designed to cast human imperceptible adversarial noises to
real-time speech and effectively perturb malicious ASR monitoring by increasing
the Word Error Rate (WER). To enhance the practical performance on mobile
devices, HASP is also optimized for effective adaptation to the human speech
characteristics, environmental noises, and mobile computation scenarios. The
experiments show that HASP can achieve optimal real-time security enhancement:
it can lead an average WER of 84.55% for perturbing the malicious ASR
monitoring, and the data processing speed is 15x to 40x faster compared to the
state-of-the-art methods. Moreover, HASP can effectively perturb various ASR
systems, demonstrating a strong transferability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01721</identifier>
 <datestamp>2018-09-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01721</id><created>2018-09-03</created><authors><author><keyname>Shahin</keyname><forenames>Ismail</forenames></author><author><keyname>Nassif</keyname><forenames>Ali Bou</forenames></author></authors><title>Three-Stage Speaker Verification Architecture in Emotional Talking
  Environments</title><categories>cs.SD cs.AI eess.AS</categories><comments>18 pages. arXiv admin note: substantial text overlap with
  arXiv:1804.00155, arXiv:1707.00137</comments><journal-ref>International Journal of Speech Technology, 2018</journal-ref><doi>10.1007/s10772-018-9543-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speaker verification performance in neutral talking environment is usually
high, while it is sharply decreased in emotional talking environments. This
performance degradation in emotional environments is due to the problem of
mismatch between training in neutral environment while testing in emotional
environments. In this work, a three-stage speaker verification architecture has
been proposed to enhance speaker verification performance in emotional
environments. This architecture is comprised of three cascaded stages: gender
identification stage followed by an emotion identification stage followed by a
speaker verification stage. The proposed framework has been evaluated on two
distinct and independent emotional speech datasets: in-house dataset and
Emotional Prosody Speech and Transcripts dataset. Our results show that speaker
verification based on both gender information and emotion information is
superior to each of speaker verification based on gender information only,
emotion information only, and neither gender information nor emotion
information. The attained average speaker verification performance based on the
proposed framework is very alike to that attained in subjective assessment by
human listeners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01722</identifier>
 <datestamp>2019-09-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01722</id><created>2018-08-27</created><updated>2019-08-09</updated><authors><author><keyname>Lin</keyname><forenames>Yu-Ting</forenames></author><author><keyname>Lo</keyname><forenames>Yu-Lun</forenames></author><author><keyname>Lin</keyname><forenames>Chen-Yun</forenames></author><author><keyname>Wu</keyname><forenames>Hau-Tieng</forenames></author><author><keyname>Frasch</keyname><forenames>Martin G.</forenames></author></authors><title>Unexpected sawtooth artifact in beat-to-beat pulse transit time measured
  from patient monitor data</title><categories>q-bio.QM cs.LG eess.SP physics.data-an stat.AP</categories><doi>10.1371/journal.pone.0221319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object: It is increasingly popular to collect as much data as possible in the
hospital setting from clinical monitors for research purposes. However, in this
setup the data calibration issue is often not discussed and, rather, implicitly
assumed, while the clinical monitors might not be designed for the data
analysis purpose. We hypothesize that this calibration issue for a secondary
analysis may become an important source of artifacts in patient monitor data.
We test an off-the-shelf integrated photoplethysmography (PPG) and
electrocardiogram (ECG) monitoring device for its ability to yield a reliable
pulse transit time (PTT) signal. Approach: This is a retrospective clinical
study using two databases: one containing 35 subjects who underwent
laparoscopic cholecystectomy, another containing 22 subjects who underwent
spontaneous breathing test in the intensive care unit. All data sets include
recordings of PPG and ECG using a commonly deployed patient monitor. We
calculated the PTT signal offline. Main Results: We report a novel constant
oscillatory pattern in the PTT signal and identify this pattern as a sawtooth
artifact. We apply an approach based on the de-shape method to visualize,
quantify and validate this sawtooth artifact. Significance: The PPG and ECG
signals not designed for the PTT evaluation may contain unwanted artifacts. The
PTT signal should be calibrated before analysis to avoid erroneous
interpretation of its physiological meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01728</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01728</id><created>2018-09-05</created><updated>2019-05-01</updated><authors><author><keyname>Sterpu</keyname><forenames>George</forenames></author><author><keyname>Saam</keyname><forenames>Christian</forenames></author><author><keyname>Harte</keyname><forenames>Naomi</forenames></author></authors><title>Attention-based Audio-Visual Fusion for Robust Automatic Speech
  Recognition</title><categories>eess.AS cs.LG cs.SD eess.IV stat.ML</categories><comments>In ICMI'18, October 16-20, 2018, Boulder, CO, USA. Equation (2)
  corrected on this version</comments><doi>10.1145/3242969.3243014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic speech recognition can potentially benefit from the lip motion
patterns, complementing acoustic speech to improve the overall recognition
performance, particularly in noise. In this paper we propose an audio-visual
fusion strategy that goes beyond simple feature concatenation and learns to
automatically align the two modalities, leading to enhanced representations
which increase the recognition accuracy in both clean and noisy conditions. We
test our strategy on the TCD-TIMIT and LRS2 datasets, designed for large
vocabulary continuous speech recognition, applying three types of noise at
different power ratios. We also exploit state of the art Sequence-to-Sequence
architectures, showing that our method can be easily integrated. Results show
relative improvements from 7% up to 30% on TCD-TIMIT over the acoustic modality
alone, depending on the acoustic noise level. We anticipate that the fusion
strategy can easily generalise to many other multimodal tasks which involve
correlated modalities. Code available online on GitHub:
https://github.com/georgesterpu/Sigmedia-AVSR
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01733</identifier>
 <datestamp>2019-06-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01733</id><created>2018-09-04</created><updated>2019-06-17</updated><authors><author><keyname>Bourtsoulatze</keyname><forenames>Eirina</forenames></author><author><keyname>Kurka</keyname><forenames>David Burth</forenames></author><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author></authors><title>Deep Joint Source-Channel Coding for Wireless Image Transmission</title><categories>cs.IT cs.LG eess.SP math.IT stat.ML</categories><comments>To appear in IEEE Transactions on Cognitive Communications and
  Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a joint source and channel coding (JSCC) technique for wireless
image transmission that does not rely on explicit codes for either compression
or error correction; instead, it directly maps the image pixel values to the
complex-valued channel input symbols. We parameterize the encoder and decoder
functions by two convolutional neural networks (CNNs), which are trained
jointly, and can be considered as an autoencoder with a non-trainable layer in
the middle that represents the noisy communication channel. Our results show
that the proposed deep JSCC scheme outperforms digital transmission
concatenating JPEG or JPEG2000 compression with a capacity achieving channel
code at low signal-to-noise ratio (SNR) and channel bandwidth values in the
presence of additive white Gaussian noise (AWGN). More strikingly, deep JSCC
does not suffer from the ``cliff effect'', and it provides a graceful
performance degradation as the channel SNR varies with respect to the SNR value
assumed during training. In the case of a slow Rayleigh fading channel, deep
JSCC learns noise resilient coded representations and significantly outperforms
separation-based digital communication at all SNR and channel bandwidth values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01827</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01827</id><created>2018-09-06</created><authors><author><keyname>Sakiyama</keyname><forenames>Akie</forenames></author><author><keyname>Tanaka</keyname><forenames>Yuichi</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshihisa</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Eigendecomposition-Free Sampling Set Selection for Graph Signals</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2908129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of selecting an optimal sampling set for
signals on graphs. The proposed sampling set selection (SSS) is based on a
localization operator that can consider both vertex domain and spectral domain
localization. We clarify the relationships among the proposed method, sensor
position selection methods in machine learning, and conventional SSS methods
based on graph frequency. In contrast to the conventional graph signal
processing-based approaches, the proposed method does not need to compute the
eigendecomposition of a variation operator, while still considering (graph)
frequency information. We evaluate the performance of our approach through
comparisons of prediction errors and execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01859</identifier>
 <datestamp>2018-09-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01859</id><created>2018-09-06</created><authors><author><keyname>Cao</keyname><forenames>Congzhe</forenames></author><author><keyname>Li</keyname><forenames>Duanshun</forenames></author><author><keyname>Fair</keyname><forenames>Ivan</forenames></author></authors><title>Deep Learning-Based Decoding for Constrained Sequence Codes</title><categories>cs.IT cs.LG eess.SP math.IT stat.ML</categories><comments>7 pages, 6 figures, accepted by IEEE Global Communications Conference
  Workshop - Machine learning for communications</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Constrained sequence codes have been widely used in modern communication and
data storage systems. Sequences encoded with constrained sequence codes satisfy
constraints imposed by the physical channel, hence enabling efficient and
reliable transmission of coded symbols. Traditional encoding and decoding of
constrained sequence codes rely on table look-up, which is prone to errors that
occur during transmission. In this paper, we introduce constrained sequence
decoding based on deep learning. With multiple layer perception (MLP) networks
and convolutional neural networks (CNNs), we are able to achieve low bit error
rates that are close to maximum a posteriori probability (MAP) decoding as well
as improve the system throughput. Moreover, implementation of
capacity-achieving fixed-length codes, where the complexity is prohibitively
high with table look-up decoding, becomes practical with deep learning-based
decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01926</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01926</id><created>2018-09-06</created><authors><author><keyname>Burrello</keyname><forenames>Alessio</forenames></author><author><keyname>Schindler</keyname><forenames>Kaspar</forenames></author><author><keyname>Benini</keyname><forenames>Luca</forenames></author><author><keyname>Rahimi</keyname><forenames>Abbas</forenames></author></authors><title>One-shot Learning for iEEG Seizure Detection Using End-to-end Binary
  Operations: Local Binary Patterns with Hyperdimensional Computing</title><categories>eess.SP cs.LG q-bio.NC stat.ML</categories><comments>Published as a conference paper at the IEEE BioCAS 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an efficient binarized algorithm for both learning and
classification of human epileptic seizures from intracranial
electroencephalography (iEEG). The algorithm combines local binary patterns
with brain-inspired hyperdimensional computing to enable end-to-end learning
and inference with binary operations. The algorithm first transforms iEEG time
series from each electrode into local binary pattern codes. Then atomic
high-dimensional binary vectors are used to construct composite representations
of seizures across all electrodes. For the majority of our patients (10 out of
16), the algorithm quickly learns from one or two seizures (i.e., one-/few-shot
learning) and perfectly generalizes on 27 further seizures. For other patients,
the algorithm requires three to six seizures for learning. Overall, our
algorithm surpasses the state-of-the-art methods for detecting 65 novel
seizures with higher specificity and sensitivity, and lower memory footprint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.01930</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.01930</id><created>2018-09-06</created><updated>2018-11-14</updated><authors><author><keyname>Al-Ahmadi</keyname><forenames>Saad</forenames></author><author><keyname>Maraqa</keyname><forenames>Omar</forenames></author><author><keyname>Uysal</keyname><forenames>Murat</forenames></author><author><keyname>Sait</keyname><forenames>Sadiq M.</forenames></author></authors><title>Multi-User Visible Light Communications: State-of-the-Art and Future
  Directions</title><categories>eess.SP</categories><comments>Version 3: Accepted for publication in IEEE Access, Nov. 2018</comments><doi>10.1109/ACCESS.2018.2879885</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible light communication (VLC) builds upon the dual use of existing
lighting infrastructure for wireless data transmission. VLC has recently gained
interest as cost-effective, secure, and energy-efficient wireless access
technology particularly for indoor user-dense environments. While initial
studies in this area are mainly limited to single-user point-to-point links,
more recent efforts have focused on multi-user VLC systems in an effort to
transform VLC into a scalable and fully networked wireless technology. In this
paper, we provide a comprehensive overview of multi-user VLC systems discussing
the recent advances on multi-user precoding, multiple access, resource
allocation, and mobility management. We further provide possible directions of
future research in this emerging topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02153</identifier>
 <datestamp>2019-01-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02153</id><created>2018-09-06</created><updated>2018-12-31</updated><authors><author><keyname>Hawkins</keyname><forenames>Cole</forenames></author><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author></authors><title>Variational Bayesian Inference for Robust Streaming Tensor Factorization
  and Completion</title><categories>stat.ML cs.LG eess.SP</categories><comments>ICDM 2018. arXiv admin note: substantial text overlap with
  arXiv:1809.01265</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming tensor factorization is a powerful tool for processing high-volume
and multi-way temporal data in Internet networks, recommender systems and
image/video data analysis. Existing streaming tensor factorization algorithms
rely on least-squares data fitting and they do not possess a mechanism for
tensor rank determination. This leaves them susceptible to outliers and
vulnerable to over-fitting. This paper presents a Bayesian robust streaming
tensor factorization model to identify sparse outliers, automatically determine
the underlying tensor rank and accurately fit low-rank structure. We implement
our model in Matlab and compare it with existing algorithms on tensor datasets
generated from dynamic MRI and Internet traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02196</identifier>
 <datestamp>2019-01-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02196</id><created>2018-09-06</created><updated>2019-01-12</updated><authors><author><keyname>Tobar</keyname><forenames>Felipe</forenames></author></authors><title>Bayesian Nonparametric Spectral Estimation</title><categories>stat.ML cs.LG eess.SP</categories><comments>11 pages. In Advances in Neural Information Processing Systems, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral estimation (SE) aims to identify how the energy of a signal (e.g., a
time series) is distributed across different frequencies. This can become
particularly challenging when only partial and noisy observations of the signal
are available, where current methods fail to handle uncertainty appropriately.
In this context, we propose a joint probabilistic model for signals,
observations and spectra, where SE is addressed as an exact inference problem.
Assuming a Gaussian process prior over the signal, we apply Bayes' rule to find
the analytic posterior distribution of the spectrum given a set of
observations. Besides its expressiveness and natural account of spectral
uncertainty, the proposed model also provides a functional-form representation
of the power spectral density, which can be optimised efficiently. Comparison
with previous approaches, in particular against Lomb-Scargle, is addressed
theoretically and also experimentally in three different scenarios. Code and
demo available at https://github.com/GAMES-UChile/BayesianSpectralEstimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02211</identifier>
 <datestamp>2019-12-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02211</id><created>2018-09-06</created><authors><author><keyname>Kantor</keyname><forenames>M. Yu.</forenames></author><author><keyname>Sidorov</keyname><forenames>A. V.</forenames></author></authors><title>True Gaussian shaping for high count rate measurements of pulse
  amplitudes</title><categories>physics.ins-det eess.SP</categories><comments>32 pages, 14 figures</comments><journal-ref>JINST 14 P01004 (2019)</journal-ref><doi>10.1088/1748-0221/14/01/P01004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A digital shaper for high-count-rate detection and amplitude measurement of
pulses is proposed and analysed in this paper. The proposed shaper converts
pulses with a short leading edge and a long exponential tail into a true
Gaussian form. The width of Gaussian pulses can be several times smaller than
the rise time of the input pulses, i.e. considerably shorter than the
undistorted output pulses provided by standard shapers. Therefore, the proposed
true Gaussian shaper resolves strongly overlapped pulses better and provides a
higher output count rate. The capabilities of the proposed true Gaussian shaper
are analysed with real and simulated output signals of a silicon drift detector
of soft X-ray radiation, operating at a high count rate of the collected
quanta. Our analysis shows that true Gaussian shapers can increase the count
rate of spectrometer systems several times compared with the widely used
trapezoidal shapers, while maintaining their amplitude resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02251</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02251</id><created>2018-09-06</created><updated>2019-04-30</updated><authors><author><keyname>Meng</keyname><forenames>Zhong</forenames><affiliation>Fred</affiliation></author><author><keyname>Li</keyname><forenames>Jinyu</forenames><affiliation>Fred</affiliation></author><author><keyname>Gong</keyname><forenames>Yifan</forenames><affiliation>Fred</affiliation></author><author><keyname>Biing-Hwang</keyname><affiliation>Fred</affiliation></author><author><keyname>Juang</keyname></author></authors><title>Adversarial Feature-Mapping for Speech Enhancement</title><categories>eess.AS cs.AI cs.CL cs.SD</categories><comments>5 pages, 2 figures, Interspeech 2018</comments><journal-ref>Interspeech 2018</journal-ref><doi>10.21437/Interspeech.2018-2461</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature-mapping with deep neural networks is commonly used for single-channel
speech enhancement, in which a feature-mapping network directly transforms the
noisy features to the corresponding enhanced ones and is trained to minimize
the mean square errors between the enhanced and clean features. In this paper,
we propose an adversarial feature-mapping (AFM) method for speech enhancement
which advances the feature-mapping approach with adversarial learning. An
additional discriminator network is introduced to distinguish the enhanced
features from the real clean ones. The two networks are jointly optimized to
minimize the feature-mapping loss and simultaneously mini-maximize the
discrimination loss. The distribution of the enhanced features is further
pushed towards that of the clean features through this adversarial multi-task
training. To achieve better performance on ASR task, senone-aware (SA) AFM is
further proposed in which an acoustic model network is jointly trained with the
feature-mapping and discriminator networks to optimize the senone
classification loss in addition to the AFM losses. Evaluated on the CHiME-3
dataset, the proposed AFM achieves 16.95% and 5.27% relative word error rate
(WER) improvements over the real noisy data and the feature-mapping baseline
respectively and the SA-AFM achieves 9.85% relative WER improvement over the
multi-conditional acoustic model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02253</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02253</id><created>2018-09-06</created><updated>2019-04-30</updated><authors><author><keyname>Meng</keyname><forenames>Zhong</forenames><affiliation>Fred</affiliation></author><author><keyname>Li</keyname><forenames>Jinyu</forenames><affiliation>Fred</affiliation></author><author><keyname>Gong</keyname><forenames>Yifan</forenames><affiliation>Fred</affiliation></author><author><keyname>Biing-Hwang</keyname><affiliation>Fred</affiliation></author><author><keyname>Juang</keyname></author></authors><title>Cycle-Consistent Speech Enhancement</title><categories>eess.AS cs.CL cs.SD</categories><comments>5 pages, 2 figures. Interspeech 2018. arXiv admin note: text overlap
  with arXiv:1809.02251</comments><journal-ref>Interspeech 2018</journal-ref><doi>10.21437/Interspeech.2018-2409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature mapping using deep neural networks is an effective approach for
single-channel speech enhancement. Noisy features are transformed to the
enhanced ones through a mapping network and the mean square errors between the
enhanced and clean features are minimized. In this paper, we propose a
cycle-consistent speech enhancement (CSE) in which an additional inverse
mapping network is introduced to reconstruct the noisy features from the
enhanced ones. A cycle-consistent constraint is enforced to minimize the
reconstruction loss. Similarly, a backward cycle of mappings is performed in
the opposite direction with the same networks and losses. With
cycle-consistency, the speech structure is well preserved in the enhanced
features while noise is effectively reduced such that the feature-mapping
network generalizes better to unseen data. In cases where only unparalleled
noisy and clean data is available for training, two discriminator networks are
used to distinguish the enhanced and noised features from the clean and noisy
ones. The discrimination losses are jointly optimized with reconstruction
losses through adversarial multi-task learning. Evaluated on the CHiME-3
dataset, the proposed CSE achieves 19.60% and 6.69% relative word error rate
improvements respectively when using or without using parallel clean and noisy
speech data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02342</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02342</id><created>2018-09-07</created><authors><author><keyname>Bian</keyname><forenames>Chong</forenames></author><author><keyname>Yang</keyname><forenames>Shunkun</forenames></author><author><keyname>Huang</keyname><forenames>Tingting</forenames></author><author><keyname>Xu</keyname><forenames>Qingyang</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Zio</keyname><forenames>Enrico</forenames></author></authors><title>Performance Degradation Assessment for Electrical Machines Based on SOM
  and Hybrid DHMM</title><categories>eess.SP</categories><comments>19 pages, 21 figures and 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aimed at the timely detection of the degradation of electrical machines and
the organization of active maintenance, numerous studies on performance
degradation assessment have been conducted. However, previous research still
suffers from two deficiencies: 1) determining the relevant relationship among
diverse machine degradation states and assessing the specific degree of
deterioration and 2) determining the evolutionary relationships among
degradation and failure modes and assessing the failure modes corresponding to
different degradation scenarios. To address these two deficiencies, a novel
performance degradation assessment method is proposed. First, the
self-organizing feature map (SOM) network is used to mine the latent
degradation states of electrical machines. Second, the latent states are
quantified according to established statistical health indexes, and by
analyzing the distribution of extracted health indexes corresponding to
different degradation states, the relevant transition relationships of the
valid degradation states and the final evolving fault types are determined.
Third, a hybrid discrete HMM is developed to fully describe the transition
process among different states and assess the degradation scenario of a machine
in an online manner. The results of a real application of an electric point
machine show that the proposed method can identify valid degradation states and
obtain a superior assessment accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02349</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02349</id><created>2018-09-07</created><authors><author><keyname>Bian</keyname><forenames>Chong</forenames></author><author><keyname>Yang</keyname><forenames>Shunkun</forenames></author><author><keyname>Huang</keyname><forenames>Tingting</forenames></author><author><keyname>Xu</keyname><forenames>Qingyang</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Zio</keyname><forenames>Enrico</forenames></author></authors><title>Degradation Detection Method for Railway Point Machines</title><categories>eess.SP</categories><comments>25 pages, 20 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point machines (PMs) are used for switching and locking railway turnouts, and
are considered one of the most critical elements of a railway signal system.
The failure of the point mechanism directly affects the operation of the
railway and may cause serious safety accidents. Hence, there is a need for
early detection of the anomalies in PMs. From normal operation to complete
failure, the machine usually undergoes a series of degradation states. If the
degradation states are detected in time, maintenance can be organized in
advance to prevent the malfunction. This paper presents a degradation detection
method that can effectively mine and identify the degradation state of the PM.
First, power data is processed to obtain the feature set that can describe the
PM characteristics effectively. Then, a clustering analysis of the feature set
is carried out by self-organizing feature-mapping network, and various
degradation states are mined. Finally, the optimized support vector machine is
used to build the state classifier to identify the degradation state of the PM.
The experimental results obtained with the Siemens S700K PM show that the
proposed method could not only mine the effective degradation states, but also
obtain high identification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02493</identifier>
 <datestamp>2020-02-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02493</id><created>2018-09-05</created><updated>2020-02-21</updated><authors><author><keyname>Nozari</keyname><forenames>Erfan</forenames></author><author><keyname>Cort&#xe9;s</keyname><forenames>Jorge</forenames></author></authors><title>Hierarchical Selective Recruitment in Linear-Threshold Brain Networks --
  Part II: Multi-Layer Dynamics and Top-Down Recruitment</title><categories>eess.SY cs.NE cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goal-driven selective attention (GDSA) is a remarkable function that allows
the complex dynamical networks of the brain to support coherent perception and
cognition. Part I of this two-part paper proposes a new control-theoretic
framework, termed hierarchical selective recruitment (HSR), to rigorously
explain the emergence of GDSA from the brain's network structure and dynamics.
This part completes the development of HSR by deriving conditions on the joint
structure of the hierarchical subnetworks that guarantee top-down recruitment
of the task-relevant part of each subnetwork by the subnetwork at the layer
immediately above, while inhibiting the activity of task-irrelevant subnetworks
at all the hierarchical layers. To further verify the merit and applicability
of this framework, we carry out a comprehensive case study of selective
listening in rodents and show that a small network with HSR-based structure and
minimal size can explain the data with remarkable accuracy while satisfying the
theoretical requirements of HSR. Our technical approach relies on the theory of
switched systems and provides a novel converse Lyapunov theorem for
state-dependent switched affine systems that is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02587</identifier>
 <datestamp>2018-09-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02587</id><created>2018-09-07</created><authors><author><keyname>Morgado</keyname><forenames>Pedro</forenames></author><author><keyname>Vasconcelos</keyname><forenames>Nuno</forenames></author><author><keyname>Langlois</keyname><forenames>Timothy</forenames></author><author><keyname>Wang</keyname><forenames>Oliver</forenames></author></authors><title>Self-Supervised Generation of Spatial Audio for 360 Video</title><categories>cs.SD cs.CV cs.LG cs.MM eess.AS</categories><comments>To appear in NIPS 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an approach to convert mono audio recorded by a 360 video camera
into spatial audio, a representation of the distribution of sound over the full
viewing sphere. Spatial audio is an important component of immersive 360 video
viewing, but spatial audio microphones are still rare in current 360 video
production. Our system consists of end-to-end trainable neural networks that
separate individual sound sources and localize them on the viewing sphere,
conditioned on multi-modal analysis of audio and 360 video frames. We introduce
several datasets, including one filmed ourselves, and one collected in-the-wild
from YouTube, consisting of 360 videos uploaded with spatial audio. During
training, ground-truth spatial audio serves as self-supervision and a mixed
down mono track forms the input to our network. Using our approach, we show
that it is possible to infer the spatial location of sound sources based only
on 360 video and a mono audio track.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02665</identifier>
 <datestamp>2019-08-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02665</id><created>2018-08-25</created><authors><author><keyname>Choi</keyname><forenames>Sanghyun</forenames></author><author><keyname>Ivkin</keyname><forenames>Nikita</forenames></author><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Jacobs</keyname><forenames>Michael A.</forenames></author></authors><title>DreamNLP: Novel NLP System for Clinical Report Metadata Extraction using
  Count Sketch Data Streaming Algorithm: Preliminary Results</title><categories>cs.LG eess.AS stat.ML</categories><comments>13 pages, 3 figures, US patent</comments><acm-class>E.1; E.2; F.2.2; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting information from electronic health records (EHR) is a challenging
task since it requires prior knowledge of the reports and some natural language
processing algorithm (NLP). With the growing number of EHR implementations,
such knowledge is increasingly challenging to obtain in an efficient manner. We
address this challenge by proposing a novel methodology to analyze large sets
of EHRs using a modified Count Sketch data streaming algorithm termed DreamNLP.
By using DreamNLP, we generate a dictionary of frequently occurring terms or
heavy hitters in the EHRs using low computational memory compared to
conventional counting approach other NLP programs use. We demonstrate the
extraction of the most important breast diagnosis features from the EHRs in a
set of patients that underwent breast imaging. Based on the analysis,
extraction of these terms would be useful for defining important features for
downstream tasks such as machine learning for precision medicine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02777</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02777</id><created>2018-09-08</created><authors><author><keyname>Ahmed</keyname><forenames>I. Zakir</forenames></author><author><keyname>Sadjadpour</keyname><forenames>Hamid</forenames></author><author><keyname>Yousefi</keyname><forenames>Shahram</forenames></author></authors><title>Capacity analysis and bit allocation design for variable-resolution ADCs
  in Massive MIMO</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive an expression for the capacity of massive multiple-input
multiple-output Millimeter wave (mmWave) channel where the receiver is equipped
with a variable-resolution Analog to Digital Converter (ADC) and a hybrid
combiner. The capacity is shown to be a function of Cramer-Rao Lower Bound
(CRLB) for a given bit-allocation matrix and hybrid combiner. The condition for
optimal ADC bit-allocation under a receiver power constraint is derived. This
is derived based on the maximization of capacity with respect to bit-allocation
matrix for a given channel, hybrid precoder, and hybrid combiner. It is shown
that this condition coincides with that obtained using the CRLB minimization
proposed by Ahmed et al. Monte-carlo simulations show that the capacity
calculated using the proposed condition matches very closely with the capacity
obtained using the Exhaustive Search bit allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02855</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02855</id><created>2018-09-08</created><authors><author><keyname>El-Wakeel</keyname><forenames>Amr S.</forenames></author><author><keyname>Noureldin</keyname><forenames>Aboelmagd</forenames></author><author><keyname>Hassanein</keyname><forenames>Hossam S.</forenames></author><author><keyname>Zorba</keyname><forenames>Nizar</forenames></author></authors><title>iDriveSense: Dynamic Route Planning Involving Roads Quality Information</title><categories>cs.CY cs.AI cs.LG eess.SP stat.ML</categories><comments>Globecom 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Owing to the expeditious growth in the information and communication
technologies, smart cities have raised the expectations in terms of efficient
functioning and management. One key aspect of residents' daily comfort is
assured through affording reliable traffic management and route planning.
Comprehensively, the majority of the present trip planning applications and
service providers are enabling their trip planning recommendations relying on
shortest paths and/or fastest routes. However, such suggestions may discount
drivers' preferences with respect to safe and less disturbing trips. Road
anomalies such as cracks, potholes, and manholes induce risky driving scenarios
and can lead to vehicles damages and costly repairs. Accordingly, in this
paper, we propose a crowdsensing based dynamic route planning system.
Leveraging both the vehicle motion sensors and the inertial sensors within the
smart devices, road surface types and anomalies have been detected and
categorized. In addition, the monitored events are geo-referenced utilizing GPS
receivers on both vehicles and smart devices. Consequently, road segments
assessments are conducted using fuzzy system models based on aspects such as
the number of anomalies and their severity levels in each road segment.
Afterward, another fuzzy model is adopted to recommend the best trip routes
based on the road segments quality in each potential route. Extensive road
experiments are held to build and show the potential of the proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02859</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02859</id><created>2018-09-08</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Yang</keyname><forenames>Zhen</forenames></author></authors><title>Application of EOS-ELM with binary Jaya-based feature selection to
  real-time transient stability assessment using PMU data</title><categories>eess.SP</categories><comments>Accepted by IEEE Access</comments><journal-ref>IEEE Access 5 (2017) 23092-23101</journal-ref><doi>10.1109/ACCESS.2017.2765626</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies show that pattern-recognition-based transient stability
assessment (PRTSA) is a promising approach for predicting the transient
stability status of power systems. However, many of the current well-known
PRTSA methods suffer from excessive training time and complex tuning of
parameters, resulting in inefficiency for real-time implementation and lacking
the online model updating ability. In this paper, a novel PRTSA approach based
on an ensemble of OS-extreme learning machine (EOSELM) with binary Jaya
(BinJaya)-based feature selection is proposed with the use of phasor
measurement units (PMUs) data. After briefly describing the principles of
OS-ELM, an EOS-ELM-based PRTSA model is built to predict the post-fault
transient stability status of power systems in real time by integrating OS-ELM
and an online boosting algorithm, respectively, as a weak classifier and an
ensemble learning algorithm. Furthermore, a BinJaya-based feature selection
approach is put forward for selecting an optimal feature subset from the entire
feature space constituted by a group of system-level classification features
extracted from PMU data. The application results on the IEEE 39-bus system and
a real provincial system show that the proposal has superior computation speed
and prediction accuracy than other state-of-the-art sequential learning
algorithms. In addition, without sacrificing the classification performance,
the dimension of the input space has been reduced to about one-third of its
initial value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02906</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02906</id><created>2018-09-08</created><authors><author><keyname>Chen</keyname><forenames>Jinkun</forenames></author><author><keyname>Cai</keyname><forenames>Weicheng</forenames></author><author><keyname>Cai</keyname><forenames>Danwei</forenames></author><author><keyname>Cai</keyname><forenames>Zexin</forenames></author><author><keyname>Zhong</keyname><forenames>Haibin</forenames></author><author><keyname>Li</keyname><forenames>Ming</forenames></author></authors><title>End-to-end Language Identification using NetFV and NetVLAD</title><categories>eess.AS cs.AI cs.SD eess.SP</categories><comments>Accepted for ISCSLP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply the NetFV and NetVLAD layers for the end-to-end
language identification task. NetFV and NetVLAD layers are the differentiable
implementations of the standard Fisher Vector and Vector of Locally Aggregated
Descriptors (VLAD) methods, respectively. Both of them can encode a sequence of
feature vectors into a fixed dimensional vector which is very important to
process those variable-length utterances. We first present the relevances and
differences between the classical i-vector and the aforementioned encoding
schemes. Then, we construct a flexible end-to-end framework including a
convolutional neural network (CNN) architecture and an encoding layer (NetFV or
NetVLAD) for the language identification task. Experimental results on the NIST
LRE 2007 close-set task show that the proposed system achieves significant EER
reductions against the conventional i-vector baseline and the CNN temporal
average pooling system, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02929</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02929</id><created>2018-09-09</created><authors><author><keyname>Islam</keyname><forenames>Md Tauhidul</forenames></author><author><keyname>Tang</keyname><forenames>Songyuan</forenames></author><author><keyname>Liverani</keyname><forenames>Chiara</forenames></author><author><keyname>Tasciotti</keyname><forenames>Ennio</forenames></author><author><keyname>Righetti</keyname><forenames>Raffaella</forenames></author></authors><title>Non-invasive imaging of Young's modulus and Poisson's ratio in cancers
  in vivo</title><categories>eess.IV physics.med-ph</categories><comments>38 pages, 34 figures, Submitted to IEEE Transactions on Biomedical
  Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: Alterations of Young's modulus (YM) and Poisson's ratio (PR) in
biological tissues are often early indicators of the onset of pathological
conditions. Knowledge of these parameters has been proven to be of great
clinical significance for the diagnosis, prognosis and treatment of cancers.
Currently, however, there are no non-invasive modalities that can be used to
image and quantify these parameters in vivo without assuming incompressibility
of the tissue, an assumption that is rarely justified in human tissues.
Methods: In this paper, we develop a new method to simultaneously reconstruct
YM and PR of a tumor and of its surrounding tissues, irrespective of the
boundary conditions and the shape of the tumor based on ellipsoidal
approximation. This new non-invasive method allows the generation of high
spatial resolution YM and PR maps from axial and lateral strain data obtained
via ultrasound elastography. The method was validated using finite element (FE)
simulations and controlled experiments performed on phantoms with known
mechanical properties. The clinical feasibility of the developed method was
also demonstrated in an orthotopic mouse model of breast cancer. Results: Our
results from simulations and controlled experiments demonstrate that the
proposed reconstruction technique is accurate and robust. Conclusion:
Availability of the proposed technique could address the clinical need of a
non-invasive modality capable of imaging, quantifying and monitoring the
mechanical properties of tumors with high spatial resolution and in real time.
Significance: This technique can have a significant impact on the clinical
translation of elasticity imaging methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02934</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02934</id><created>2018-09-09</created><authors><author><keyname>Hu</keyname><forenames>Jingzhi</forenames></author><author><keyname>Zhang</keyname><forenames>Hongliang</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author></authors><title>Reinforcement Learning for Decentralized Trajectory Design in Cellular
  UAV Networks with Sense-and-Send Protocol</title><categories>eess.SP</categories><comments>13 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the unmanned aerial vehicles (UAVs) have been widely used in
real-time sensing applications over cellular networks, which sense the
conditions of the tasks and transmit the real-time sensory data to the base
station (BS). The performance of a UAV is determined by the performance of both
its sensing and transmission processes, which are influenced by the trajectory
of the UAV. However, it is challenging for UAVs to design their trajectories
efficiently, since they work in a dynamic environment. To tackle this
challenge, in this paper, we adopt the reinforcement learning framework to
solve the UAV trajectory design problem in a decentralized manner. To
coordinate multiple UAVs performing the real-time sensing tasks, we first
propose a sense-and-send protocol, and analyze the probability for successful
valid data transmission using nested Markov chains. Then, we formulate the
decentralized trajectory design problem and propose an enhanced multi-UAV
Q-learning algorithm to solve this problem. Simulation results show that the
proposed enhanced multi-UAV Q-learning algorithm converges faster and achieves
higher utilities for the UAVs in the real-time task-sensing scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.02946</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.02946</id><created>2018-09-09</created><authors><author><keyname>Xie</keyname><forenames>Huiqiang</forenames></author><author><keyname>Xu</keyname><forenames>Weiyang</forenames></author><author><keyname>Xiang</keyname><forenames>Wei</forenames></author><author><keyname>Shao</keyname><forenames>Ke</forenames></author><author><keyname>Xu</keyname><forenames>Shengbo</forenames></author></authors><title>Non-coherent Massive SIMO Systems in ISI Channels: Constellation Design
  and Performance Analysis</title><categories>eess.SP</categories><doi>10.1109/JSYST.2018.2870842</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A massive single-input multiple-output (SIMO) system with a single transmit
antenna and a large number of receive antennas in intersymbol interference
(ISI) channels is considered. Contrast to existing energy detection (ED)-based
non-coherent receiver where conventional pulse amplitude modulation (PAM) is
employed, we propose a constellation design which minimizes the symbol-error
rate (SER) with the knowledge of channel statistics. To make a comparison, we
derive the SERs of the ED-based receiver with both the proposed constellation
and PAM, namely $P_{e\_opt}$ and $P_{e\_pam}$. Specifically, asymptotic
behaviors of the SER in regimes of a large number of receive antennas and high
signal-to-noise ratio (SNR) are investigated. Analytical results demonstrate
that the logarithms of both $P_{e\_opt}$ and $P_{e\_pam}$ decrease
approximately linearly with the number of receive antennas, while $P_{e\_opt}$
degrades faster. It is also shown that the proposed design is of less cost,
because compared with PAM, less antennas are required to achieve the same error
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03140</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03140</id><created>2018-09-10</created><authors><author><keyname>Cherukuri</keyname><forenames>Venkateswararao</forenames></author><author><keyname>Guo</keyname><forenames>Tiantong</forenames></author><author><keyname>Schiff</keyname><forenames>Steven J.</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author></authors><title>Deep MR Image Super-Resolution Using Structural Priors</title><categories>cs.LG cs.CV eess.IV stat.ML</categories><comments>Accepted to IEEE ICIP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High resolution magnetic resonance (MR) images are desired for accurate
diagnostics. In practice, image resolution is restricted by factors like
hardware, cost and processing constraints. Recently, deep learning methods have
been shown to produce compelling state of the art results for image
super-resolution. Paying particular attention to desired hi-resolution MR image
structure, we propose a new regularized network that exploits image priors,
namely a low-rank structure and a sharpness prior to enhance deep MR image
superresolution. Our contributions are then incorporating these priors in an
analytically tractable fashion in the learning of a convolutional neural
network (CNN) that accomplishes the super-resolution task. This is particularly
challenging for the low rank prior, since the rank is not a differentiable
function of the image matrix (and hence the network parameters), an issue we
address by pursuing differentiable approximations of the rank. Sharpness is
emphasized by the variance of the Laplacian which we show can be implemented by
a fixed {\em feedback} layer at the output of the network. Experiments
performed on two publicly available MR brain image databases exhibit promising
results particularly when training imagery is limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03174</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03174</id><created>2018-09-10</created><authors><author><keyname>Xie</keyname><forenames>Qingsong</forenames></author><author><keyname>Wang</keyname><forenames>Guoxing</forenames></author><author><keyname>Lian</keyname><forenames>Yong</forenames></author></authors><title>Heart Rate Estimation from Ballistocardiography Based on Hilbert
  Transform and Phase Vocoder</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a robust method to monitor heart rate (HR) from BCG
(Ballistocardiography) signal, which is acquired from the sensor embedded in a
chair or a mattress. The proposed algorithm addresses the shortfalls in
traditional Fast Fourier Transform (FFT) based approaches by introducing
Hilbert Transform to extract the pulse envelope that models the repetition of
J-peaks in BCG signal. The frequency resolution is further enhanced by applying
FFT and phase vocoder to the pulse envelope. The performance of the proposed
algorithm is verified by experiment from 7 subjects. For HR estimation, mean
absolute error (MAE) of 0.90 beats per minute (BPM) and standard deviation of
absolute error (STD) of 1.14 BPM are obtained. Pearson correlation coefficient
between estimated HR and ground truth HR of 0.98 is also achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03192</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03192</id><created>2018-09-10</created><authors><author><keyname>Szajnowski</keyname><forenames>W. J.</forenames></author></authors><title>Zero-Crossing Waveform Interferometry: an Alternative to Correlation in
  Signal Processing</title><categories>eess.SP</categories><comments>14 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that multiple representations (such as replicas or Hilbert
transforms) of a random waveform can interfere constructively to form a compact
pattern, akin to a wave packet, when the representations are created in
synchrony with zero crossings of the waveform. A function of such 'engineered'
zero-crossing interferograms can exhibit time-delay resolution superior to that
associated with a conventional correlation function, especially for waveforms
with slowly-decaying power spectra. A phenomenon of local slew rate at zero
crossings is exploited to substantially reduce the Cram\'er-Rao bound on
time-delay estimators. A system, based on a concept of elapsed time, is
proposed to determine zero-crossing interferograms in real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03300</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03300</id><created>2018-09-10</created><authors><author><keyname>Cisotto</keyname><forenames>Giulia</forenames></author><author><keyname>Guglielmi</keyname><forenames>Anna V.</forenames></author><author><keyname>Badia</keyname><forenames>Leonardo</forenames></author><author><keyname>Zanella</keyname><forenames>Andrea</forenames></author></authors><title>Classification of grasping tasks based on EEG-EMG coherence</title><categories>eess.SP q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an innovative application of the well-known concept of
cortico-muscular coherence for the classification of various motor tasks, i.e.,
grasps of different kinds of objects. Our approach can classify objects with
different weights (motor-related features) and different surface frictions
(haptics-related features) with high accuracy (over 0:8). The outcomes
presented here provide information about the synchronization existing between
the brain and the muscles during specific activities; thus, this may represent
a new effective way to perform activity recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03389</identifier>
 <datestamp>2019-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03389</id><created>2018-09-10</created><authors><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author></authors><title>Joint Beamforming and Association Design for MIMO Radar</title><categories>eess.SP cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, Vol. 67, pp. 3663 - 3675,
  July 2019</journal-ref><doi>10.1109/TSP.2019.2918998</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A critical task of a radar receiver is data association, which assigns radar
target detections to target filter tracks. Motivated by its importance, this
paper introduces the problem of jointly designing multiple-input
multiple-output (MIMO) radar transmit beam patterns and the corresponding data
association schemes. We show that the coupling of the beamforming and the
association subproblems can be conveniently parameterized by what we term an
ambiguity graph, which prescribes if two targets are to be disambiguated by the
beamforming design or by the data association scheme. The choice of ambiguity
graph determines which of the two subproblems is more difficult and therefore
allows to trade performance of one versus the other, resulting in a
detection-association trade-off. This paper shows how to design both the beam
pattern and the association scheme for a given ambiguity graph. It then
discusses how to choose an ambiguity graph achieving close to the optimal
detection-association trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03393</identifier>
 <datestamp>2018-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03393</id><created>2018-08-30</created><updated>2018-09-18</updated><authors><author><keyname>Kalyakulina</keyname><forenames>Alena I.</forenames></author><author><keyname>Yusipov</keyname><forenames>Igor I.</forenames></author><author><keyname>Moskalenko</keyname><forenames>Victor A.</forenames></author><author><keyname>Nikolskiy</keyname><forenames>Alexander V.</forenames></author><author><keyname>Kozlov</keyname><forenames>Artem A.</forenames></author><author><keyname>Kosonogov</keyname><forenames>Konstantin A.</forenames></author><author><keyname>Zolotykh</keyname><forenames>Nikolay Yu.</forenames></author><author><keyname>Ivanchenko</keyname><forenames>Mikhail V.</forenames></author></authors><title>LU electrocardiography database: a new open-access validation tool for
  delineation algorithms</title><categories>q-bio.QM eess.SP physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report a new Lobachevsky University database (LUDB) of ECG signals that
contains 200 samples of 10-second 12-lead electrocardiograms (ECG) from
different subjects. The boundaries of the ECG signal complexes are manually
annotated by cardiologists for all samples and independently for each lead. The
database is representative of a variety of signal morphologies. In addition,
all records have an attributed diagnosis. These features make LUDB a promising
tool for validating ECG delineation algorithms across a broad range of ECG
signal shapes and patient diagnoses. A case study for the recently proposed
wavelet-based algorithm is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03395</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03395</id><created>2018-09-10</created><authors><author><keyname>Noman</keyname><forenames>Fuad</forenames></author><author><keyname>Salleh</keyname><forenames>Sh-Hussain</forenames></author><author><keyname>Ting</keyname><forenames>Chee-Ming</forenames></author><author><keyname>Samdin</keyname><forenames>S. Balqis</forenames></author><author><keyname>Ombao</keyname><forenames>Hernando</forenames></author><author><keyname>Hussain</keyname><forenames>Hadri</forenames></author></authors><title>A Markov-Switching Model Approach to Heart Sound Segmentation and
  Classification</title><categories>eess.SP cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: This paper considers challenges in developing algorithms for
accurate segmentation and classification of heart sound (HS) signals. Methods:
We propose an approach based on Markov switching autoregressive model (MSAR) to
segmenting the HS into four fundamental components each with distinct
second-order structure. The identified boundaries are then utilized for
automated classification of pathological HS using the continuous density hidden
Markov model (CD-HMM). The MSAR formulated in a state-space form is able to
capture simultaneously both the continuous hidden dynamics in HS, and the
regime switching in the dynamics using a discrete Markov chain. This overcomes
the limitation of HMM which uses a single-layer of discrete states. We
introduce three schemes for model estimation: (1.) switching Kalman filter
(SKF); (2.) refined SKF; (3.) fusion of SKF and the duration-dependent Viterbi
algorithm (SKF-Viterbi). Results: The proposed methods are evaluated on
Physionet/CinC Challenge 2016 database. The SKF-Viterbi significantly
outperforms SKF by improvement of segmentation accuracy from 71% to 84.2%. The
use of CD-HMM as a classifier and Mel-frequency cepstral coefficients (MFCCs)
as features can characterize not only the normal and abnormal morphologies of
HS signals but also morphologies considered as unclassifiable (denoted as
X-Factor). It gives classification rates with best gross F1 score of 90.19
(without X-Factor) and 82.7 (with X-Factor) for abnormal beats. Conclusion: The
proposed MSAR approach for automatic localization and detection of pathological
HS shows a noticeable performance on large HS dataset. Significance: It has
potential applications in heart monitoring systems to assist cardiologists for
pre-screening of heart pathologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03414</identifier>
 <datestamp>2018-09-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03414</id><created>2018-09-07</created><authors><author><keyname>Wu</keyname><forenames>Shangbin</forenames></author><author><keyname>Qi</keyname><forenames>Yinan</forenames></author></authors><title>Centralized and distributed schedulers for non-coherent joint
  transmission</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performance of three typical network coordination
schemes, i.e., dynamic point selection, fully overlapped non-coherent joint
transmission (F-NCJT), and nonfully overlapped NCJT (NF-NCJT), in 3GPP new
radio (NR) in indoor scenarios via system level simulation. Each of these
schemes requires a different level of user data and channel state information
(CSI) report exchange among coordinated transmission reception points (TRPs)
depending on centralized or distributed schedulers. Scheduling strategies of
these network coordination schemes are briefly discussed. It has been
demonstrated that distributed network coordination schemes (e.g., NFNCJT) can
still perform reasonably well; a result which has important implications to the
design of the fifth generation (5G) cellular network architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03479</identifier>
 <datestamp>2019-05-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03479</id><created>2018-09-10</created><updated>2019-01-11</updated><authors><author><keyname>Arafa</keyname><forenames>Ahmed</forenames></author><author><keyname>Panayirci</keyname><forenames>Erdal</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Relay-Aided Secure Broadcasting for Visible Light Communications</title><categories>cs.IT cs.NI eess.SP math.IT</categories><doi>10.1109/TCOMM.2019.2900632</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A visible light communication broadcast channel is considered, in which a
transmitter luminaire communicates with two legitimate receivers in the
presence of an external eavesdropper. A number of trusted cooperative
half-duplex relay luminaires are deployed to aid with securing the transmitted
data. Transmitters are equipped with single light fixtures, containing multiple
light emitting diodes, and receiving nodes are equipped with single
photo-detectors, rendering the considered setting as a single-input
single-output system. Transmission is amplitude-constrained to maintain
operation within the light emitting diodes' dynamic range. Achievable secrecy
rate regions are derived under such amplitude constraints for this
multi-receiver wiretap channel, first for direct transmission without the
relays, and then for multiple relaying schemes: cooperative jamming,
decode-and-forward, and amplify-and-forward. Superposition coding with uniform
signaling is used at the transmitter and the relays. Further, for each relaying
scheme, secure beamforming vectors are carefully designed at the relay nodes in
order to hurt the eavesdropper and/or benefit the legitimate receivers.
Superiority of the proposed relaying schemes, with secure beamforming, is shown
over direct transmission. It is also shown that the best relaying scheme
depends on how far the eavesdropper is located from the transmitter and the
relays, the number of relays, and their geometric layout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03547</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03547</id><created>2018-09-10</created><authors><author><keyname>Hellbourg</keyname><forenames>Gregory</forenames></author><author><keyname>Xu</keyname><forenames>Andrew</forenames></author></authors><title>Comparison of signal detectors for time domain radio SETI</title><categories>eess.SP astro-ph.IM</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The radio Search for Extra Terrestrial Intelligence (SETI) aims at
identifying intelligent and communicative civilizations in the Universe through
the detection of engineered transmissions. In the absence of prior knowledge
concerning the expected signal, SETI detection pipelines necessitate high
sensitivity, versatility, and limited computational complexity to maximize the
search parameter space and minimize the probability of misses. This paper
addresses the SETI detection problem as a binary hypothesis testing problem,
and compares four detection schemes exploiting artificial features of the data
collected by a single receiver radio telescope. After a theoretical comparison,
those detectors are applied to real data collected with the Green Bank
Telescope in West Virginia (USA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03583</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03583</id><created>2018-09-10</created><authors><author><keyname>Koivisto</keyname><forenames>Mike</forenames></author><author><keyname>Galinina</keyname><forenames>Olga</forenames></author><author><keyname>Andreev</keyname><forenames>Sergey</forenames></author><author><keyname>T&#xf6;lli</keyname><forenames>Antti</forenames></author><author><keyname>Destino</keyname><forenames>Giuseppe</forenames></author><author><keyname>Costa</keyname><forenames>M&#xe1;rio</forenames></author><author><keyname>Lepp&#xe4;nen</keyname><forenames>Kari</forenames></author><author><keyname>Koucheryavy</keyname><forenames>Yevgeni</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Benefits of Positioning-Aided Communication Technology in High-Frequency
  Industrial IoT</title><categories>eess.SP</categories><comments>7 pages, 5 figures, 1 table. This work has been submitted to the IEEE
  for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The future of industrial applications is shaped by intelligent moving IoT
devices, such as flying drones, advanced factory robots, and connected
vehicles, which may operate (semi-)autonomously. In these challenging
scenarios, dynamic radio connectivity at high frequencies -- augmented with
timely positioning-related information -- becomes instrumental to improve
communication performance and facilitate efficient computation offloading. Our
work reviews the main research challenges and reveals open implementation gaps
in Industrial IoT (IIoT) applications that rely on location awareness and
multi-connectivity in super high and extremely high frequency bands. It further
conducts a rigorous numerical investigation to confirm the potential of precise
device localization in the emerging IIoT systems. We focus on positioning-aided
benefits made available to multi-connectivity IIoT device operation at 28 GHz,
which notably improve data transfer rates, communication latency, and extent of
control overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03620</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03620</id><created>2018-09-10</created><authors><author><keyname>Hellbourg</keyname><forenames>Gregory</forenames></author></authors><title>RFI subspace smearing and projection for array radio telescopes</title><categories>eess.SP astro-ph.IM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active Radio Frequency Interference (RFI) mitigation becomes a necessity for
radio astronomy. The solution commonly applied by the community consists in
monitoring the statistics of the received signal, and flag out the detected
corrupted data. Subspace projection with array radio telescopes has been
suggested as an alternative to data excision to avoid important losses of data
and overcome its inherent ineffectiveness with continuous interference. Spatial
filtering relies on the estimation of the RFI spatial contribution, and the
projection of the subspace spanned by the RFI out of the observed data vector
space. To perform well, the dimensionality of the RFI subspace is constrained.
RFI subspace estimation techniques assume the source of RFI to be spatially
stationary over the sample covariance matrix evaluation. When the relative
movement between the telescope and the interferer becomes significant, the RFI
subspace gets smeared over the whole data vector space. The subspace projection
can then no longer be applied without affecting the source of interest
recovery. This paper addresses the effect of RFI subspace smearing on the
subspace projection approach, and suggests an alternative technique based on a
covariance matrix subtraction, improving the performance of spatial filtering
in the case of high subspace smearing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03663</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03663</id><created>2018-09-10</created><authors><author><keyname>Islam</keyname><forenames>Md Tauhidul</forenames></author><author><keyname>Tasciotti</keyname><forenames>Ennio</forenames></author><author><keyname>Righetti</keyname><forenames>Raffaella</forenames></author></authors><title>Non-invasive assessment of the spatial and temporal distributions of
  interstitial fluid pressure, fluid velocity and fluid flow in cancers in vivo</title><categories>eess.IV physics.med-ph q-bio.TO</categories><comments>18 pages, 10 figures, Submitted to IEE Transactions on Medical
  Imaging for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interstitial fluid pressure (IFP), interstitial fluid velocity (IFV),
interstitial permeability (IP) and vascular permeability (VP) are cancer
mechanopathological parameters of great clinical significance. To date, there
is a lack of non-invasive techniques that can be used to estimate these
parameters in vivo. In this study, we designed and tested new ultrasound
poroelastography methods capable of estimating the magnitude and spatial
distribution of fluid pressure, fluid velocity and fluid flow inside tumors. We
theoretically proved that fluid pressure, velocity and flow estimated using
poroelastography from a tumor under creep compression are directly related to
the underlying IFP, IFV and fluid flow, respectively, differing only in peak
values. We also proved that, from the spatial distribution of the fluid
pressure estimated using poroelastography, it is possible to derive: the
parameter alpha, which quantifies the spatial distribution of the IFP; the
ratio between VP and IP and the ratio between the peak IFP and effective
vascular pressure in the tumor. Finally, we demonstrated that axial strain time
constant (TC) elastograms are directly related to VP and IP in tumors. Our
techniques were validated using finite element and ultrasound simulations,
while experiments on a human breast cancer animal model were used to show the
feasibility of these methods in vivo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03682</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03682</id><created>2018-09-11</created><authors><author><keyname>Fan</keyname><forenames>Congmin</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Zhang</keyname><forenames>Ying-Jun Angela</forenames></author></authors><title>CNN-Based Signal Detection for Banded Linear Systems</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Banded linear systems arise in many communication scenarios, e.g., those
involving inter-carrier interference and inter-symbol interference. Motivated
by recent advances in deep learning, we propose to design a high-accuracy
low-complexity signal detector for banded linear systems based on convolutional
neural networks (CNNs). We develop a novel CNN-based detector by utilizing the
banded structure of the channel matrix. Specifically, the proposed CNN-based
detector consists of three modules: the input preprocessing module, the CNN
module, and the output postprocessing module. With such an architecture, the
proposed CNN-based detector is adaptive to different system sizes, and can
overcome the curse of dimensionality, which is a ubiquitous challenge in deep
learning. Through extensive numerical experiments, we demonstrate that the
proposed CNN-based detector outperforms conventional deep neural networks and
existing model-based detectors in both accuracy and computational time.
Moreover, we show that CNN is flexible for systems with large sizes or wide
bands. We also show that the proposed CNN-based detector can be easily extended
to near-banded systems such as doubly selective orthogonal frequency division
multiplexing (OFDM) systems and 2-D magnetic recording (TDMR) systems, in which
the channel matrices do not have a strictly banded structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03721</identifier>
 <datestamp>2019-05-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03721</id><created>2018-09-11</created><updated>2019-05-17</updated><authors><author><keyname>Jang</keyname><forenames>Jinhyeok</forenames></author><author><keyname>Cho</keyname><forenames>Hyunjoong</forenames></author><author><keyname>Kim</keyname><forenames>Jaehong</forenames></author><author><keyname>Lee</keyname><forenames>Jaeyeon</forenames></author><author><keyname>Yang</keyname><forenames>Seungjoon</forenames></author></authors><title>Deep Asymmetric Networks with a Set of Node-wise Variant Activation
  Functions</title><categories>cs.LG cs.CV cs.NE eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents deep asymmetric networks with a set of node-wise variant
activation functions. The nodes' sensitivities are affected by activation
function selections such that the nodes with smaller indices become
increasingly more sensitive. As a result, features learned by the nodes are
sorted by the node indices in the order of their importance. Asymmetric
networks not only learn input features but also the importance of those
features. Nodes of lesser importance in asymmetric networks can be pruned to
reduce the complexity of the networks, and the pruned networks can be retrained
without incurring performance losses. We validate the feature-sorting property
using both shallow and deep asymmetric networks as well as deep asymmetric
networks transferred from famous networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03733</identifier>
 <datestamp>2019-06-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03733</id><created>2018-09-11</created><updated>2019-06-04</updated><authors><author><keyname>Zavary</keyname><forenames>Elyar</forenames></author><author><keyname>Badri</keyname><forenames>Pouya</forenames></author><author><keyname>Sojoodi</keyname><forenames>Mahdi</forenames></author></authors><title>Consensus of a class of nonlinear fractional-order multi-agent systems
  via dynamic output feedback controller</title><categories>math.OC eess.SP math.NA</categories><comments>13 pages, 10 fidures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the consensus of a class of uncertain nonlinear
fractional-order multi-agent systems (FOMAS). First a fractional non-fragile
dynamic output feedback controller is put forward via the output measurements
of neighboring agents, then appropriate state transformation reduced the
consensus problem to a stability one. A sufficient condition based on direct
Lyapunov approach, for the robust asymptotic stability of the transformed
system and subsequently for the consensus of the main system is presented.
Additionally, utilizing S-procedure and Schur complement, the systematic
stabilization design algorithm is proposed for fractional-order system with and
without nonlinear term. The results are formulated as an optimization problem
with linear matrix inequality constraints. Simulation results are given to
verify the effectiveness of the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03749</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03749</id><created>2018-09-11</created><authors><author><keyname>Pedersen</keyname><forenames>Troels</forenames></author></authors><title>Stochastic Multipath Model for the In-Room Radio Channel based on Room
  Electromagnetics</title><categories>eess.SP</categories><comments>14 pages, Manuscript Submitted to IEEE Transaction on Antennas and
  Propagation</comments><doi>10.1109/TAP.2019.2891444</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a stochastic multipath model for the received signal for the case
where the transmitter and receiver, both with directive antennas, are situated
in the same rectangular room. This scenario is known to produce channel impulse
responses with a gradual specular-to-diffused transition in delay. Mirror
source theory predicts the arrival rate to be quadratic in delay, inversely
proportional to room volume and proportional to the product of the antenna beam
coverage fractions. We approximate the mirror source positions by a homogeneous
spatial Poisson point process and their gain as complex random variables with
the same second moment. The multipath delays in the resulting model form an
inhomogeneous Poisson point process which enables derivation of the
characteristic functional, power/kurtosis delay spectra, and the distribution
of order statistics of the arrival delays in closed form. We find that the
proposed model matches the mirror source model well in terms of power delay
spectrum, kurtosis delay spectrum, order statistics, and prediction of mean
delay and rms delay spread. The constant rate model, assumed in e.g. the
Saleh-Valenzuela model, is unable to reproduce the same effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03868</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03868</id><created>2018-09-08</created><authors><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Zahorian</keyname><forenames>Stephen</forenames></author><author><keyname>Chen</keyname><forenames>Xiao</forenames></author><author><keyname>Guzewich</keyname><forenames>Peter</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoyu</forenames></author></authors><title>Dual-label Deep LSTM Dereverberation For Speaker Verification</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>4 pages, 3 figures, submitted to Interspeech 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a reverberation removal approach for speaker
verification, utilizing dual-label deep neural networks (DNNs). The networks
perform feature mapping between the spectral features of reverberant and clean
speech. Long short term memory recurrent neural networks (LSTMs) are trained to
map corrupted Mel filterbank (MFB) features to two sets of labels: i) the clean
MFB features, and ii) either estimated pitch tracks or the fast Fourier
transform (FFT) spectrogram of clean speech. The performance of reverberation
removal is evaluated by equal error rates (EERs) of speaker verification
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03875</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03875</id><created>2018-09-08</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Guoqing</forenames></author><author><keyname>Wang</keyname><forenames>Zhenhao</forenames></author><author><keyname>Han</keyname><forenames>Zijiao</forenames></author><author><keyname>Bai</keyname><forenames>Xue</forenames></author></authors><title>A multifeature fusion approach for power system transient stability
  assessment using PMU data</title><categories>eess.SP</categories><comments>Accepted by Mathematical Problems in Engineering</comments><journal-ref>Mathematical Problems in Engineering 2015 (2015) 1-11</journal-ref><doi>10.1155/2015/786396</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking full advantage of synchrophasors provided by GPS-based wide-area
measurement system (WAMS), a novel VBpMKL-based transient stability assessment
(TSA) method through multifeature fusion is proposed in this paper. First, a
group of classification features reflecting the transient stability
characteristics of power systems are extracted from synchrophasors, and
according to the different stages of the disturbance process they are broken
into three nonoverlapped subsets; then a VBpMKL-based TSA model is built using
multifeature fusion through combining feature spaces corresponding to each
feature subset; and finally application of the proposed model to the IEEE
39-bus system and a real-world power system is demonstrated. The novelty of the
proposed approach is that it improves the classification accuracy and
reliability of TSA using multifeature fusion with synchrophasors. The
application results on the test systems verify the effectiveness of the
proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03899</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03899</id><created>2018-09-11</created><authors><author><keyname>Garc&#xed;a</keyname><forenames>Mario H. Casta&#xf1;eda</forenames></author><author><keyname>Iwanow</keyname><forenames>Marcin</forenames></author><author><keyname>Stirling-Gallacher</keyname><forenames>Richard A.</forenames></author></authors><title>LOS MIMO Design based on Multiple Optimum Antenna Separations</title><categories>eess.SP cs.IT math.IT</categories><comments>Published at the IEEE Vehicular Technology Conference 2018 Fall,
  where it was selected as the IEEE VTC 2018-Fall Conference's Best Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of multiple antennas in a transmit and receive antenna array for MIMO
wireless communication allows the spatial degrees of freedom in rich scattering
environments to be exploited. However, for line-of-sight (LOS) MIMO channels
with uniform linear arrays (ULAs) at the transmitter and receiver, the antenna
separations at the transmit and receive array need to be optimized to maximize
the spatial degrees of freedom and the channel capacity. In this paper, we
first revisit the derivation of the optimum antenna separation at the transmit
and receive ULAs in a LOS MIMO system, and provide the general expression for
the optimum antenna separation product, which consists of multiple solutions.
Although only the solution corresponding to the smallest antenna separation
product is usually considered in the literature, we exploit the multiple
solutions for a LOS MIMO design over a range of distances between the
transmitter and receiver. In particular, we consider the LOS MIMO design in a
vehicle-to-vehicle (V2V) communication scenario, over a range of distances
between the transmit and receive vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03930</identifier>
 <datestamp>2018-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03930</id><created>2018-09-11</created><authors><author><keyname>Piotrowski</keyname><forenames>Tomasz</forenames></author><author><keyname>Nikadon</keyname><forenames>Jan</forenames></author></authors><title>Localization of Brain Activity from EEG/MEG Using MV-PURE Framework</title><categories>eess.SP q-bio.NC</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><msc-class>94A12, 60G35, 92C55, 15A29</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of localization of sources of brain electrical
activity from electroencephalographic (EEG) and magnetoencephalographic (MEG)
measurements using spatial filtering techniques. We propose novel reduced-rank
activity indices based on the minimum-variance pseudo-unbiased reduced-rank
estimation (MV-PURE) framework. The main results of this paper establish the
key unbiasedness property of the proposed indices and their higher spatial
resolution compared with full-rank indices in challenging task of localizing
closely positioned and possibly highly correlated sources, especially in low
signal-to-noise regime. A numerical example is provided to illustrate the
practical applicability of the proposed activity indices. Simulations presented
in this paper use open-source EEG/MEG spatial filtering framework freely
available at https://github.com/IS-UMK/supFunSim.git.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03953</identifier>
 <datestamp>2019-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03953</id><created>2018-09-11</created><updated>2019-10-29</updated><authors><author><keyname>Bonfante</keyname><forenames>Andrea</forenames></author><author><keyname>Giordano</keyname><forenames>Lorenzo Galati</forenames></author><author><keyname>L&#xf3;pez-P&#xe9;rez</keyname><forenames>David</forenames></author><author><keyname>Garcia-Rodriguez</keyname><forenames>Adrian</forenames></author><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Baracca</keyname><forenames>Paolo</forenames></author><author><keyname>Butt</keyname><forenames>M. Majid</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author></authors><title>5G Massive MIMO Architectures: Self-Backhauled Small Cells versus Direct
  Access</title><categories>cs.NI eess.SP</categories><comments>The paper is published at IEEE Transactions on Vehicular Technology
  (TVT 2019). arXiv admin note: text overlap with arXiv:1806.10969</comments><journal-ref>IEEE Transactions on Vehicular Technology ( Volume: 68 , Issue: 10
  , Oct. 2019 )</journal-ref><doi>10.1109/TVT.2019.2937652</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on one of the key technologies for the
fifth-generation wireless communication networks, massive
multiple-input-multiple-output (mMIMO), by investigating two of its most
relevant architectures: 1) to provide in-band backhaul for the ultra-dense
network (UDN) of self-backhauled small cells (SCs), and 2) to provide direct
access (DA) to user equipments (UEs). Through comprehensive 3GPP-based
system-level simulations and analytical formulations, we show the end-to-end UE
rates achievable with these two architectures. Differently from the existing
works, we provide results for two strategies of self-backhauled SC deployments,
namely random and ad-hoc, where in the latter SCs are purposely positioned
close to UEs to achieve line-of-sight (LoS) access links. We also evaluate the
optimal backhaul and access time resource partition due to the in-band
self-backhauling (s-BH) operations. Our results show that the ad-hoc deployment
of self-backhauled SCs closer to the UEs with optimal resource partition and
with directive antenna patterns, provides rate improvements for cell-edge UEs
that amount to 30% and tenfold gain, as compared to mMIMO DA architecture with
pilot reuse 3 and reuse 1, respectively. On the other hand, mMIMO s-BH
underperforms mMIMO DA above the median value of the UE rates when the effect
of pilot contamination is less severe, and the LoS probability of the DA links
improves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.03958</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.03958</id><created>2018-09-11</created><authors><author><keyname>Muntoni</keyname><forenames>Anna Paola</forenames></author><author><keyname>Rojas</keyname><forenames>Rafael D&#xed;az Hern&#xe1;ndez</forenames></author><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author><author><keyname>Pagnani</keyname><forenames>Andrea</forenames></author><author><keyname>Castillo</keyname><forenames>Isaac P&#xe9;rez</forenames></author></authors><title>Non-convex image reconstruction via Expectation Propagation</title><categories>eess.IV cond-mat.dis-nn</categories><comments>12 pages, 6 figures</comments><journal-ref>Phys. Rev. E 100, 032134 (2019)</journal-ref><doi>10.1103/PhysRevE.100.032134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tomographic image reconstruction can be mapped to a problem of finding
solutions to a large system of linear equations which maximize a function that
includes \textit{a priori} knowledge regarding features of typical images such
as smoothness or sharpness. This maximization can be performed with standard
local optimization tools when the function is concave, but it is generally
intractable for realistic priors, which are non-concave. We introduce a new
method to reconstruct images obtained from Radon projections by using
Expectation Propagation, which allows us to reframe the problem from an
Bayesian inference perspective. We show, by means of extensive simulations,
that, compared to state-of-the-art algorithms for this task, Expectation
Propagation paired with very simple but non log-concave priors, is often able
to reconstruct images up to a smaller error while using a lower amount of
information per pixel. We provide estimates for the critical rate of
information per pixel above which recovery is error-free by means of
simulations on ensembles of phantom and real images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04105</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04105</id><created>2018-09-11</created><authors><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Kim</keyname><forenames>Junghoon</forenames></author></authors><title>On the Beneficial Roles of Fading and Transmit Diversity in Wireless
  Power Transfer with Nonlinear Energy Harvesting</title><categories>cs.IT eess.SP math.IT</categories><comments>accepted for publication in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effect of channel fading in Wireless Power Transfer (WPT) and
show that fading enhances the RFto- DC conversion efficiency of nonlinear RF
energy harvesters. We then develop a new form of signal design for WPT, denoted
as Transmit Diversity, that relies on multiple dumb antennas at the transmitter
to induce fast fluctuations of the wireless channel. Those fluctuations boost
the RF-to-DC conversion efficiency thanks to the energy harvester nonlinearity.
In contrast with (energy) beamforming, Transmit Diversity does not rely on
Channel State Information at the Transmitter (CSIT) and does not increase the
average power at the energy harvester input, though it still enhances the
overall end-to-end power transfer efficiency. Transmit Diversity is also
combined with recently developed (energy) waveform and modulation to provide
further enhancements. The efficacy of the scheme is analyzed using
physics-based and curve fitting-based nonlinear models of the energy harvester
and demonstrated using circuit simulations, prototyping and experimentation.
Measurements with two transmit antennas reveal gains of 50%in harvested DC
power over a single transmit antenna setup. The work (again) highlights the
crucial role played by the harvester nonlinearity and demonstrates that
multiple transmit antennas can be beneficial to WPT even in the absence of
CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04115</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04115</id><created>2018-09-11</created><authors><author><keyname>V&#xe9;lez</keyname><forenames>Ivette</forenames><affiliation>Instituto de Investigaciones en Matem&#xe1;ticas Aplicadas y en Sistemas</affiliation></author><author><keyname>Rascon</keyname><forenames>Caleb</forenames><affiliation>Instituto de Investigaciones en Matem&#xe1;ticas Aplicadas y en Sistemas</affiliation></author><author><keyname>Fuentes-Pineda</keyname><forenames>Gibr&#xe1;n</forenames><affiliation>Instituto de Investigaciones en Matem&#xe1;ticas Aplicadas y en Sistemas</affiliation></author></authors><title>One-Shot Speaker Identification for a Service Robot using a CNN-based
  Generic Verifier</title><categories>eess.AS cs.SD</categories><comments>8 pages, 9 figures, 2 tables. This paper is under review as a
  Submission for RA-L and ICRA for the IEEE Robotics and Automation Letters
  (RA-L). A video demonstration of the full system, as well as all relevant
  downloads (corpora, source code, models, etc.) can be found at:
  http://calebrascon.info/oneshotid/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In service robotics, there is an interest to identify the user by voice
alone. However, in application scenarios where a service robot acts as a waiter
or a store clerk, new users are expected to enter the environment frequently.
Typically, speaker identification models need to be retrained when this occurs,
which can take an impractical amount of time. In this paper, a new approach for
speaker identification through verification has been developed using a Siamese
Convolutional Neural Network architecture (SCNN), where it learns to
generically verify if two audio signals are from the same speaker. By having an
external database of recorded audio of the users, identification is carried out
by verifying the speech input with each of its entries. If new users are
encountered, it is only required to add their recorded audio to the external
database to be able to be identified, without retraining. The system was
evaluated in four different aspects: the performance of the verifier, the
performance of the system as a classifier using clean audio, its speed, and its
accuracy in real-life settings. Its performance in conjunction with its
one-shot-learning capabilities, makes the proposed system a viable alternative
for speaker identification for service robots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04207</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04207</id><created>2018-09-11</created><authors><author><keyname>Cheong</keyname><forenames>Joon Wayn</forenames></author><author><keyname>Dempster</keyname><forenames>Andrew G</forenames></author></authors><title>Cross Correlation-based Direct Positioning for Wideband Sources using
  Phased Arrays</title><categories>eess.SP</categories><comments>9 pages, 4 figures, submitted to IEEE Transactions on Signal
  Processing</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent developments in Phased Array direct positioning methods have improved
accuracy for passively geo-locating multiple radio frequency-emitting signal
sources. However, the number of geo-localisable signal sources is still limited
by the number of antenna elements at each node. This is the limitation for
methods based on MUSIC, otherwise known as signal subspace identification. This
paper attempts to exploit properties of wideband signal sources to
compartmentalise signals into their respective Time Differences of Arrival. By
performing direct positioning after the compartmentalisation process, we will
show that geolocation of a large number of sources can be achieved by our
proposed method at accuracies that exceed all existing methods, especially
under low signal-to-noise ratio conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04214</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04214</id><created>2018-09-11</created><authors><author><keyname>Shih</keyname><forenames>Shun-Yao</forenames></author><author><keyname>Chi</keyname><forenames>Heng-Yu</forenames></author></authors><title>Automatic, Personalized, and Flexible Playlist Generation using
  Reinforcement Learning</title><categories>cs.CL cs.IR cs.LG cs.SD eess.AS</categories><comments>7 pages, 4 figures, ISMIR 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Songs can be well arranged by professional music curators to form a riveting
playlist that creates engaging listening experiences. However, it is
time-consuming for curators to timely rearrange these playlists for fitting
trends in future. By exploiting the techniques of deep learning and
reinforcement learning, in this paper, we consider music playlist generation as
a language modeling problem and solve it by the proposed attention language
model with policy gradient. We develop a systematic and interactive approach so
that the resulting playlists can be tuned flexibly according to user
preferences. Considering a playlist as a sequence of words, we first train our
attention RNN language model on baseline recommended playlists. By optimizing
suitable imposed reward functions, the model is thus refined for corresponding
preferences. The experimental results demonstrate that our approach not only
generates coherent playlists automatically but is also able to flexibly
recommend personalized playlists for diversity, novelty and freshness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04229</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04229</id><created>2018-09-11</created><authors><author><keyname>Jang</keyname><forenames>Soobeom</forenames></author><author><keyname>Moon</keyname><forenames>Seong-Eun</forenames></author><author><keyname>Lee</keyname><forenames>Jong-Seok</forenames></author></authors><title>EEG-based video identification using graph signal modeling and graph
  convolutional neural network</title><categories>eess.SP cs.LG</categories><comments>Accepted and presented at ICASSP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel graph signal-based deep learning method for
electroencephalography (EEG) and its application to EEG-based video
identification. We present new methods to effectively represent EEG data as
signals on graphs, and learn them using graph convolutional neural networks.
Experimental results for video identification using EEG responses obtained
while watching videos show the effectiveness of the proposed approach in
comparison to existing methods. Effective schemes for graph signal
representation of EEG are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04274</identifier>
 <datestamp>2018-09-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04274</id><created>2018-09-12</created><updated>2018-09-13</updated><authors><author><keyname>Fang</keyname><forenames>Fuming</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Echizen</keyname><forenames>Isao</forenames></author><author><keyname>Sahidullah</keyname><forenames>Md</forenames></author><author><keyname>Kinnunen</keyname><forenames>Tomi</forenames></author></authors><title>Transforming acoustic characteristics to deceive playback spoofing
  countermeasures of speaker verification systems</title><categories>cs.SD cs.CR eess.AS</categories><comments>Accepted at WIFS2018</comments><journal-ref>IEEE International Workshop on Information Forensics and Security
  (WIFS), 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic speaker verification (ASV) systems use a playback detector to
filter out playback attacks and ensure verification reliability. Since current
playback detection models are almost always trained using genuine and
played-back speech, it may be possible to degrade their performance by
transforming the acoustic characteristics of the played-back speech close to
that of the genuine speech. One way to do this is to enhance speech &quot;stolen&quot;
from the target speaker before playback. We tested the effectiveness of a
playback attack using this method by using the speech enhancement generative
adversarial network to transform acoustic characteristics. Experimental results
showed that use of this &quot;enhanced stolen speech&quot; method significantly increases
the equal error rates for the baseline used in the ASVspoof 2017 challenge and
for a light convolutional neural network-based method. The results also showed
that its use degrades the performance of a Gaussian mixture model-universal
background model-based ASV system. This type of attack is thus an urgent
problem needing to be solved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04281</identifier>
 <datestamp>2018-12-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04281</id><created>2018-09-12</created><updated>2018-12-12</updated><authors><author><keyname>Huang</keyname><forenames>Cheng-Zhi Anna</forenames></author><author><keyname>Vaswani</keyname><forenames>Ashish</forenames></author><author><keyname>Uszkoreit</keyname><forenames>Jakob</forenames></author><author><keyname>Shazeer</keyname><forenames>Noam</forenames></author><author><keyname>Simon</keyname><forenames>Ian</forenames></author><author><keyname>Hawthorne</keyname><forenames>Curtis</forenames></author><author><keyname>Dai</keyname><forenames>Andrew M.</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew D.</forenames></author><author><keyname>Dinculescu</keyname><forenames>Monica</forenames></author><author><keyname>Eck</keyname><forenames>Douglas</forenames></author></authors><title>Music Transformer</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Improved skewing section and accompanying figures. Previous titles
  are &quot;An Improved Relative Self-Attention Mechanism for Transformer with
  Application to Music Generation&quot; and &quot;Music Transformer&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Music relies heavily on repetition to build structure and meaning.
Self-reference occurs on multiple timescales, from motifs to phrases to reusing
of entire sections of music, such as in pieces with ABA structure. The
Transformer (Vaswani et al., 2017), a sequence model based on self-attention,
has achieved compelling results in many generation tasks that require
maintaining long-range coherence. This suggests that self-attention might also
be well-suited to modeling music. In musical composition and performance,
however, relative timing is critically important. Existing approaches for
representing relative positional information in the Transformer modulate
attention based on pairwise distance (Shaw et al., 2018). This is impractical
for long sequences such as musical compositions since their memory complexity
for intermediate relative information is quadratic in the sequence length. We
propose an algorithm that reduces their intermediate memory requirement to
linear in the sequence length. This enables us to demonstrate that a
Transformer with our modified relative attention mechanism can generate
minute-long compositions (thousands of steps, four times the length modeled in
Oore et al., 2018) with compelling structure, generate continuations that
coherently elaborate on a given motif, and in a seq2seq setup generate
accompaniments conditioned on melodies. We evaluate the Transformer with our
relative attention mechanism on two datasets, JSB Chorales and
Piano-e-Competition, and obtain state-of-the-art results on the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04374</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04374</id><created>2018-09-12</created><authors><author><keyname>Yao</keyname><forenames>Fuqiang</forenames></author><author><keyname>Jia</keyname><forenames>Luliang</forenames></author></authors><title>A Collaborative Multi-agent Reinforcement Learning Anti-jamming
  Algorithm in Wireless Networks</title><categories>cs.GT eess.SP</categories><comments>4 pages, 6 figures, Submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we investigate the anti-jamming defense problem in multi-user
scenarios, where the coordination among users is taken into consideration. The
Markov game framework is employed to model and analyze the anti-jamming defense
problem, and a collaborative multi-agent anti-jamming algorithm (CMAA) is
proposed to obtain the optimal anti-jamming strategy. In sweep jamming
scenarios, on the one hand, the proposed CMAA can tackle the external malicious
jamming. On the other hand, it can effectively cope with the mutual
interference among users. Simulation results show that the proposed CMAA is
superior to both sensing based method and independent Q-learning method, and
has the highest normalized rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04397</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04397</id><created>2018-09-11</created><authors><author><keyname>Rajaratnam</keyname><forenames>Krishan</forenames></author><author><keyname>Shah</keyname><forenames>Kunal</forenames></author><author><keyname>Kalita</keyname><forenames>Jugal</forenames></author></authors><title>Isolated and Ensemble Audio Preprocessing Methods for Detecting
  Adversarial Examples against Automatic Speech Recognition</title><categories>cs.SD cs.CL cs.CR cs.LG cs.NE eess.AS</categories><comments>Accepted for oral presentation at the 30th Conference on
  Computational Linguistics and Speech Processing (ROCLING 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adversarial attack is an exploitative process in which minute alterations
are made to natural inputs, causing the inputs to be misclassified by neural
models. In the field of speech recognition, this has become an issue of
increasing significance. Although adversarial attacks were originally
introduced in computer vision, they have since infiltrated the realm of speech
recognition. In 2017, a genetic attack was shown to be quite potent against the
Speech Commands Model. Limited-vocabulary speech classifiers, such as the
Speech Commands Model, are used in a variety of applications, particularly in
telephony; as such, adversarial examples produced by this attack pose as a
major security threat. This paper explores various methods of detecting these
adversarial examples with combinations of audio preprocessing. One particular
combined defense incorporating compressions, speech coding, filtering, and
audio panning was shown to be quite effective against the attack on the Speech
Commands Model, detecting audio adversarial examples with 93.5% precision and
91.2% recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04437</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04437</id><created>2018-09-12</created><authors><author><keyname>Shon</keyname><forenames>Suwon</forenames></author><author><keyname>Tang</keyname><forenames>Hao</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>Frame-level speaker embeddings for text-independent speaker recognition
  and analysis of end-to-end model</title><categories>eess.AS cs.LG</categories><comments>Accepted at SLT 2018; Supplement materials:
  https://people.csail.mit.edu/swshon/supplement/slt18.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a Convolutional Neural Network (CNN) based speaker
recognition model for extracting robust speaker embeddings. The embedding can
be extracted efficiently with linear activation in the embedding layer. To
understand how the speaker recognition model operates with text-independent
input, we modify the structure to extract frame-level speaker embeddings from
each hidden layer. We feed utterances from the TIMIT dataset to the trained
network and use several proxy tasks to study the networks ability to represent
speech input and differentiate voice identity. We found that the networks are
better at discriminating broad phonetic classes than individual phonemes. In
particular, frame-level embeddings that belong to the same phonetic classes are
similar (based on cosine distance) for the same speaker. The frame level
representation also allows us to analyze the networks at the frame level, and
has the potential for other analyses to improve speaker recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04454</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04454</id><created>2018-09-11</created><authors><author><keyname>Wang</keyname><forenames>Yucheng</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Hua</forenames></author><author><keyname>You</keyname><forenames>Xiaohu</forenames></author></authors><title>Wideband mmWave Channel Estimation for Hybrid Massive MIMO with
  Low-Precision ADCs</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we investigate channel estimation for wideband
millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) under
hybrid architecture with lowprecision analog-to-digital converters (ADCs). To
design channel estimation for the hybrid structure, both analog processing
components and frequency-selective digital combiners need to be optimized. The
proposed channel estimator follows the typical linear-minimum-mean-square-error
(LMMSE) structure and applies for an arbitrary channel model. Moreover, for
sparsity channels as in mmWave, the proposed estimator performs more
efficiently by incorporating orthogonal matching pursuit (OMP) to mitigate
quantization noise caused by low-precision ADCs. Consequently, the proposed
estimator outperforms conventional ones as demonstrated by computer simulation
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04458</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04458</id><created>2018-09-12</created><authors><author><keyname>Shon</keyname><forenames>Suwon</forenames></author><author><keyname>Hsu</keyname><forenames>Wei-Ning</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>Unsupervised Representation Learning of Speech for Dialect
  Identification</title><categories>eess.AS cs.CL cs.LG</categories><comments>Accepted at SLT 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the use of a factorized hierarchical variational
autoencoder (FHVAE) model to learn an unsupervised latent representation for
dialect identification (DID). An FHVAE can learn a latent space that separates
the more static attributes within an utterance from the more dynamic attributes
by encoding them into two different sets of latent variables. Useful factors
for dialect identification, such as phonetic or linguistic content, are encoded
by a segmental latent variable, while irrelevant factors that are relatively
constant within a sequence, such as a channel or a speaker information, are
encoded by a sequential latent variable. The disentanglement property makes the
segmental latent variable less susceptible to channel and speaker variation,
and thus reduces degradation from channel domain mismatch. We demonstrate that
on fully-supervised DID tasks, an end-to-end model trained on the features
extracted from the FHVAE model achieves the best performance, compared to the
same model trained on conventional acoustic features and an i-vector based
system. Moreover, we also show that the proposed approach can leverage a large
amount of unlabeled data for FHVAE training to learn domain-invariant features
for DID, and significantly improve the performance in a low-resource condition,
where the labels for the in-domain data are not available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04533</identifier>
 <datestamp>2018-09-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04533</id><created>2018-09-10</created><authors><author><keyname>Hellbourg</keyname><forenames>Gregory</forenames></author></authors><title>SETI Detection Strategies for Single Dish Radio Telescopes</title><categories>eess.SP astro-ph.IM</categories><doi>10.1109/SSP.2018.8450739</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Radio Searches for Extra Terrestrial Intelligence aim at detecting artificial
transmissions from extra terrestrial communicative civilizations. The lack of
prior knowledge concerning these potential transmissions increase the search
parameter space. Ground-based single dish radio telescopes offer high
sensitivity, but standard data products are limited to power spectral density
estimates. To overcome important classical energy detector limitations, two
detection strategies based on asynchronous ON and OFF astronomical target
observations are proposed. Statistical models are described to enable threshold
selection and detection performance assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04553</identifier>
 <datestamp>2018-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04553</id><created>2018-09-12</created><authors><author><keyname>Tao</keyname><forenames>Fei</forenames></author><author><keyname>Busso</keyname><forenames>Carlos</forenames></author></authors><title>End-to-end Audiovisual Speech Activity Detection with Bimodal Recurrent
  Neural Models</title><categories>cs.CL cs.CV eess.AS</categories><comments>Submitted to Speech Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech activity detection (SAD) plays an important role in current speech
processing systems, including automatic speech recognition (ASR). SAD is
particularly difficult in environments with acoustic noise. A practical
solution is to incorporate visual information, increasing the robustness of the
SAD approach. An audiovisual system has the advantage of being robust to
different speech modes (e.g., whisper speech) or background noise. Recent
advances in audiovisual speech processing using deep learning have opened
opportunities to capture in a principled way the temporal relationships between
acoustic and visual features. This study explores this idea proposing a
\emph{bimodal recurrent neural network} (BRNN) framework for SAD. The approach
models the temporal dynamic of the sequential audiovisual data, improving the
accuracy and robustness of the proposed SAD system. Instead of estimating
hand-crafted features, the study investigates an end-to-end training approach,
where acoustic and visual features are directly learned from the raw data
during training. The experimental evaluation considers a large audiovisual
corpus with over 60.8 hours of recordings, collected from 105 speakers. The
results demonstrate that the proposed framework leads to absolute improvements
up to 1.2% under practical scenarios over a VAD baseline using only audio
implemented with deep neural network (DNN). The proposed approach achieves
92.7% F1-score when it is evaluated using the sensors from a portable tablet
under noisy acoustic environment, which is only 1.0% lower than the performance
obtained under ideal conditions (e.g., clean speech obtained with a high
definition camera and a close-talking microphone).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04626</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04626</id><created>2018-09-12</created><authors><author><keyname>Latychevskaia</keyname><forenames>Tatiana</forenames></author></authors><title>Iterative phase retrieval in coherent diffractive imaging: practical
  issues</title><categories>physics.optics eess.IV physics.comp-ph physics.data-an</categories><journal-ref>Applied Optics, Vol. 57, Issue 25, pp. 7187-7197 (2018)</journal-ref><doi>10.1364/AO.57.007187</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, issues in phase retrieval in the coherent diffractive imaging
(CDI) technique, from discussion on parameters for setting up a CDI experiment
to evaluation of the goodness of the final reconstruction, are discussed. The
distribution of objects under study by CDI often cannot be cross-validated by
another imaging technique. It is therefore important to make sure that the
developed CDI procedure delivers an artifact-free object reconstruction.
Critical issues that can lead to artifacts are presented and recipes on how to
avoid them are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04693</identifier>
 <datestamp>2018-09-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04693</id><created>2018-09-12</created><authors><author><keyname>Sun</keyname><forenames>Yu</forenames></author><author><keyname>Wohlberg</keyname><forenames>Brendt</forenames></author><author><keyname>Kamilov</keyname><forenames>Ulugbek S.</forenames></author></authors><title>An Online Plug-and-Play Algorithm for Regularized Image Reconstruction</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plug-and-play priors (PnP) is a powerful framework for regularizing imaging
inverse problems by using advanced denoisers within an iterative algorithm.
Recent experimental evidence suggests that PnP algorithms achieve
state-of-the-art performance in a range of imaging applications. In this paper,
we introduce a new online PnP algorithm based on the iterative
shrinkage/thresholding algorithm (ISTA). The proposed algorithm uses only a
subset of measurements at every iteration, which makes it scalable to very
large datasets. We present a new theoretical convergence analysis, for both
batch and online variants of PnP-ISTA, for denoisers that do not necessarily
correspond to proximal operators. We also present simulations illustrating the
applicability of the algorithm to image reconstruction in diffraction
tomography. The results in this paper have the potential to expand the
applicability of the PnP framework to very large and redundant datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04745</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04745</id><created>2018-09-12</created><updated>2019-06-25</updated><authors><author><keyname>Amalladinne</keyname><forenames>Vamsi K.</forenames></author><author><keyname>Chamberland</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author></authors><title>A Coded Compressed Sensing Scheme for Uncoordinated Multiple Access</title><categories>eess.SP cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a novel communication scheme, termed coded compressed
sensing, for unsourced multiple-access communication. The proposed
divide-and-conquer approach leverages recent advances in compressed sensing and
forward error correction to produce a novel uncoordinated access paradigm,
along with a computationally efficient decoding algorithm. Within this
framework, every active device partitions its data into several sub-blocks and,
subsequently, adds redundancy using a systematic linear block code. Compressed
sensing techniques are then employed to recover sub-blocks up to a permutation
of their order, and the original messages are obtained by stitching fragments
together using a tree-based algorithm. The error probability and computational
complexity of this access paradigm are characterized. An optimization
framework, which exploits the tradeoff between performance and computational
complexity, is developed to assign parity-check bits to each sub-block. In
addition, two emblematic parity bit allocation strategies are examined and
their performances are analyzed in the limit as the number of active users and
their corresponding payloads tend to infinity. The number of channel uses
needed and the computational complexity associated with these allocation
strategies are established for various scaling regimes. Numerical results
demonstrate that coded compressed sensing outperforms other existing practical
access strategies over a range of operational scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04768</identifier>
 <datestamp>2019-06-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04768</id><created>2018-09-13</created><updated>2019-01-14</updated><authors><author><keyname>Yonel</keyname><forenames>Bariscan</forenames></author><author><keyname>Mason</keyname><forenames>Eric</forenames></author><author><keyname>Yazici</keyname><forenames>Birsen</forenames></author></authors><title>Deep Learning for Waveform Estimation and Imaging in Passive Radar</title><categories>eess.SP</categories><comments>Submitted to IET Journal of Radar, Sonar and Navigation Special Issue
  on Passive High Resolution and Imaging Radar</comments><doi>10.1049/iet-rsn.2018.5228</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a bistatic configuration with a stationary transmitter
transmitting unknown waveforms of opportunity and a moving receiver, and
present a Deep Learning (DL) framework for passive synthetic aperture radar
(SAR) imaging. Existing passive radar methods require two or more antennas
which are either spatially separated or colocated with sufficient directivity
to estimate the underlying waveform prior to imaging. Our approach to passive
radar only requires a single receiver, hence reduces cost and increases
versatility. We approach DL from an optimization perspective and formulate
image reconstruction as a machine learning task. By unfolding the iterations of
a proximal gradient descent algorithm, we construct a deep recurrent neural
network (RNN) that is parameterized by transmitted waveforms. We cascade the
RNN structure with a decoder stage to form a recurrent-auto encoder
architecture. We then utilize backpropagation to learn transmitted waveforms by
training the network in an unsupervised manner using SAR measurements. The
highly non-convex problem of backpropagation is guided to a feasible solution
over the parameter space by initializing the network with the known components
of the SAR forward model. Moreover, prior information regarding the waveform
structure is incorporated during initialization and backpropagation. We
demonstrate the effectiveness of the DL-based approach through extensive
numerical simulations that show focused, high contrast imagery using a single
receiver antenna at realistic SNR levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04845</identifier>
 <datestamp>2018-09-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04845</id><created>2018-09-13</created><authors><author><keyname>Gao</keyname><forenames>Shanghua</forenames></author><author><keyname>Cheng</keyname><forenames>Wenchi</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Hailin</forenames></author></authors><title>Bifocal-Lens Antenna Based OAM Communications System</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orbital angular momentum (OAM) based radio vortex wireless communications
have received much attention recently because it can significantly increase the
spectrum efficiency. The uniform circular array (UCA) is a simple antenna
structure for high spectrum efficiency radio vortex wireless communications.
However, the OAM based electromagnetic waves are vortically hollow and
divergent, which may result in the signal loss. Moreover, the divergence of
corresponding OAM based electromagnetic wave increases as the order of OAM-mode
and radius of UCA increases. Therefore, it is difficult to use high-order
OAM-mode, because the corresponding received signal-to-noise ratio (SNR) is
very small. To overcome the difficulty of high-order OAM modes transmission, in
this paper we propose a lens antenna based electromagnetic waves converging
scheme, which maintains the angular identification of multiple OAM-modes for
radio vortex wireless communications. We further develop a bifocal lens antenna
to not only converge the electromagnetic wave, but also compensate the SNR loss
on traditional electromagnetic waves. Simulation results show that the proposed
bifocal lens can converge the OAM waves into cylinder-like beams, providing an
efficient way to increase the spectrum efficiency of wireless communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.04891</identifier>
 <datestamp>2019-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.04891</id><created>2018-09-13</created><updated>2019-09-04</updated><authors><author><keyname>Verdoja</keyname><forenames>Francesco</forenames></author><author><keyname>Lundell</keyname><forenames>Jens</forenames></author><author><keyname>Kyrki</keyname><forenames>Ville</forenames></author></authors><title>Deep Network Uncertainty Maps for Indoor Navigation</title><categories>cs.RO cs.LG eess.SP</categories><comments>Accepted for publication in &quot;2019 IEEE-RAS International Conference
  on Humanoid Robots (Humanoids)&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most mobile robots for indoor use rely on 2D laser scanners for localization,
mapping and navigation. These sensors, however, cannot detect transparent
surfaces or measure the full occupancy of complex objects such as tables. Deep
Neural Networks have recently been proposed to overcome this limitation by
learning to estimate object occupancy. These estimates are nevertheless subject
to uncertainty, making the evaluation of their confidence an important issue
for these measures to be useful for autonomous navigation and mapping. In this
work we approach the problem from two sides. First we discuss uncertainty
estimation in deep models, proposing a solution based on a fully convolutional
neural network. The proposed architecture is not restricted by the assumption
that the uncertainty follows a Gaussian model, as in the case of many popular
solutions for deep model uncertainty estimation, such as Monte-Carlo Dropout.
We present results showing that uncertainty over obstacle distances is actually
better modeled with a Laplace distribution. Then, we propose a novel approach
to build maps based on Deep Neural Network uncertainty models. In particular,
we present an algorithm to build a map that includes information over obstacle
distance estimates while taking into account the level of uncertainty in each
estimate. We show how the constructed map can be used to increase global
navigation safety by planning trajectories which avoid areas of high
uncertainty, enabling higher autonomy for mobile robots in indoor settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05067</identifier>
 <datestamp>2018-09-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05067</id><created>2018-09-13</created><authors><author><keyname>Xue</keyname><forenames>Tianfan</forenames></author><author><keyname>Wu</keyname><forenames>Jiajun</forenames></author><author><keyname>Zhang</keyname><forenames>Zhoutong</forenames></author><author><keyname>Zhang</keyname><forenames>Chengkai</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author><author><keyname>Freeman</keyname><forenames>William T.</forenames></author></authors><title>Seeing Tree Structure from Vibration</title><categories>cs.CV eess.IV</categories><comments>ECCV 2018. The first two authors contributed equally to this work.
  Project page: http://tree.csail.mit.edu/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans recognize object structure from both their appearance and motion;
often, motion helps to resolve ambiguities in object structure that arise when
we observe object appearance only. There are particular scenarios, however,
where neither appearance nor spatial-temporal motion signals are informative:
occluding twigs may look connected and have almost identical movements, though
they belong to different, possibly disconnected branches. We propose to tackle
this problem through spectrum analysis of motion signals, because vibrations of
disconnected branches, though visually similar, often have distinctive natural
frequencies. We propose a novel formulation of tree structure based on a
physics-based link model, and validate its effectiveness by theoretical
analysis, numerical simulation, and empirical experiments. With this
formulation, we use nonparametric Bayesian inference to reconstruct tree
structure from both spectral vibration signals and appearance cues. Our model
performs well in recognizing hierarchical tree structure from real-world videos
of trees and vessels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05220</identifier>
 <datestamp>2018-09-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05220</id><created>2018-09-13</created><authors><author><keyname>Jang</keyname><forenames>Soobeom</forenames></author><author><keyname>Lee</keyname><forenames>Jong-Seok</forenames></author></authors><title>On Evaluating Perceptual Quality of Online User-Generated Videos</title><categories>cs.MM eess.IV</categories><comments>Published in IEEE Transactions on Multimedia</comments><journal-ref>S. Jang and J. S. Lee, &quot;On evaluating perceptual quality of online
  user-generated videos,&quot;IEEE Transactions on Multimedia, vol. 18, no. 9, pp.
  1808-1818, Sep. 2016</journal-ref><doi>10.1109/TMM.2016.2581582</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the issue of the perceptual quality evaluation of
user-generated videos shared online, which is an important step toward
designing video-sharing services that maximize users' satisfaction in terms of
quality. We first analyze viewers' quality perception patterns by applying
graph analysis techniques to subjective rating data. We then examine the
performance of existing state-of-the-art objective metrics for the quality
estimation of user-generated videos. In addition, we investigate the
feasibility of metadata accompanied with videos in online video-sharing
services for quality estimation. Finally, various issues in the quality
assessment of online user-generated videos are discussed, including
difficulties and opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05273</identifier>
 <datestamp>2018-09-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05273</id><created>2018-09-14</created><authors><author><keyname>Schoeffauer</keyname><forenames>Richard</forenames></author><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author></authors><title>A linear algorithm for reliable predictive network control</title><categories>eess.SP</categories><comments>6 Pages, GlobeCom 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel control approach for network scheduling and
routing that is predictive and reliable in its nature, yet builds upon a linear
program, making it fast in execution. First, we describe the canonical system
model and how we expand it to be able to predict the success of transmissions.
Furthermore, we define a notion of reliability and then explain the algorithm.
With extended simulations, we demonstrate the gains in performance over the
well known MaxWeight policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05277</identifier>
 <datestamp>2018-09-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05277</id><created>2018-09-14</created><authors><author><keyname>Hahn</keyname><forenames>Jannik</forenames></author><author><keyname>Schoeffauer</keyname><forenames>Richard</forenames></author><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Stursberg</keyname><forenames>Olaf</forenames></author></authors><title>Distributed MPC with Prediction of Time-Varying Communication Delay</title><categories>cs.SY eess.SP</categories><comments>6 Pages, NecSys 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The novel idea presented in this paper is to interweave distributed model
predictive control with a reliable scheduling of the information that is
interchanged between local controllers of the plant subsystems. To this end, a
dynamic model of the communication network and a predictive scheduling
algorithm are proposed, the latter providing predictions of the delay between
sending and receiving information. These predictions can be used by the local
subsystem controllers to improve their control performance, as exemplary shown
for a platooning example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05363</identifier>
 <datestamp>2019-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05363</id><created>2018-09-14</created><updated>2019-06-25</updated><authors><author><keyname>Matera</keyname><forenames>Andrea</forenames></author><author><keyname>Rampa</keyname><forenames>Vittorio</forenames></author><author><keyname>Donati</keyname><forenames>Marcello</forenames></author><author><keyname>Colamonico</keyname><forenames>Armando</forenames></author><author><keyname>Cattoni</keyname><forenames>Andrea Fabio</forenames></author><author><keyname>Spagnolini</keyname><forenames>Umberto</forenames></author></authors><title>Analog MIMO Radio-over-Copper: Prototype and Preliminary Experimental
  Results</title><categories>eess.SP cs.IT math.IT</categories><comments>Part of this work has been accepted as a conference publication to
  ISWCS 2019</comments><doi>10.1109/ISWCS.2019.8877154</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analog Multiple-Input Multiple-Output Radio-over-Copper (A-MIMO-RoC) is an
effective all-analog FrontHaul (FH) architecture that exploits any pre-existing
Local Area Network (LAN) cabling infrastructure of buildings to distribute
Radio-Frequency (RF) signals indoors. A-MIMO-RoC, by leveraging a fully analog
implementation, completely avoids any dedicated digital interface by using a
transparent end-to-end system, with consequent latency, bandwidth and cost
benefits. Usually, LAN cables are exploited mainly in the low-frequency
spectrum portion, mostly due to the moderate cable attenuation and crosstalk
among twisted-pairs. Unlike current systems based on LAN cables, the key
feature of the proposed platform is to exploit more efficiently the huge
bandwidth capability offered by LAN cables, that contain 4 twisted-pairs
reaching up to 500 MHz bandwidth/pair when the length is below 100 m. Several
works proposed numerical simulations that assert the feasibility of employing
LAN cables for indoor FH applications up to several hundreds of MHz, but an
A-MIMO-RoC experimental evaluation is still missing. Here, we present some
preliminary results obtained with an A-MIMO-RoC prototype made by low-cost
all-analog/all-passive devices along the signal path. This setup demonstrates
experimentally the feasibility of the proposed analog relaying of MIMO RF
signals over LAN cables up to 400 MHz, thus enabling an efficient exploitation
of the LAN cables transport capabilities for 5G indoor applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05483</identifier>
 <datestamp>2019-02-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05483</id><created>2018-09-14</created><updated>2019-02-12</updated><authors><author><keyname>Gabrielli</keyname><forenames>Leonardo</forenames></author><author><keyname>Tomassetti</keyname><forenames>Stefano</forenames></author><author><keyname>Squartini</keyname><forenames>Stefano</forenames></author><author><keyname>Zinato</keyname><forenames>Carlo</forenames></author><author><keyname>Guaiana</keyname><forenames>Stefano</forenames></author></authors><title>A Multi-Stage Algorithm for Acoustic Physical Model Parameters
  Estimation</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the challenges in computational acoustics is the identification of
models that can simulate and predict the physical behavior of a system
generating an acoustic signal. Whenever such models are used for commercial
applications an additional constraint is the time-to-market, making automation
of the sound design process desirable. In previous works, a computational sound
design approach has been proposed for the parameter estimation problem
involving timbre matching by deep learning, which was applied to the synthesis
of pipe organ tones. In this work we refine previous results by introducing the
former approach in a multi-stage algorithm that also adds heuristics and a
stochastic optimization method operating on objective cost functions based on
psychoacoustics. The optimization method shows to be able to refine the first
estimate given by the deep learning approach and substantially improve the
objective metrics, with the additional benefit of reducing the sound design
process time. Subjective listening tests are also conducted to gather
additional insights on the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05635</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05635</id><created>2018-09-14</created><authors><author><keyname>Ozdenizci</keyname><forenames>Ozan</forenames></author><author><keyname>Gunay</keyname><forenames>Sezen Yagmur</forenames></author><author><keyname>Quivira</keyname><forenames>Fernando</forenames></author><author><keyname>Erdogmus</keyname><forenames>Deniz</forenames></author></authors><title>Hierarchical Graphical Models for Context-Aware Hybrid Brain-Machine
  Interfaces</title><categories>cs.HC eess.SP</categories><comments>40th International Engineering in Medicine and Biology Conference
  (EMBC 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel hierarchical graphical model based context-aware hybrid
brain-machine interface (hBMI) using probabilistic fusion of
electroencephalographic (EEG) and electromyographic (EMG) activities. Based on
experimental data collected during stationary executions and subsequent
imageries of five different hand gestures with both limbs, we demonstrate
feasibility of the proposed hBMI system through within session and online
across sessions classification analyses. Furthermore, we investigate the
context-aware extent of the model by a simulated probabilistic approach and
highlight potential implications of our work in the field of
neurophysiologically-driven robotic hand prosthetics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05689</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05689</id><created>2018-09-15</created><authors><author><keyname>Dorfer</keyname><forenames>Matthias</forenames></author><author><keyname>Haji&#x10d;</keyname><forenames>Jan</forenames><suffix>Jr.</suffix></author><author><keyname>Widmer</keyname><forenames>Gerhard</forenames></author></authors><title>Attention as a Perspective for Learning Tempo-invariant Audio Queries</title><categories>cs.SD cs.LG eess.AS</categories><comments>The 2018 Joint Workshop on Machine Learning for Music</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current models for audio--sheet music retrieval via multimodal embedding
space learning use convolutional neural networks with a fixed-size window for
the input audio. Depending on the tempo of a query performance, this window
captures more or less musical content, while notehead density in the score is
largely tempo-independent. In this work we address this disparity with a soft
attention mechanism, which allows the model to encode only those parts of an
audio excerpt that are most relevant with respect to efficient query codes.
Empirical results on classical piano music indicate that attention is
beneficial for retrieval performance, and exhibits intuitively appealing
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05734</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05734</id><created>2018-09-15</created><authors><author><keyname>M</keyname><forenames>Shree Prasad</forenames></author><author><keyname>Panigrahi</keyname><forenames>Trilochan</forenames></author><author><keyname>Hassan</keyname><forenames>Mahbub</forenames></author></authors><title>Classifying the Order of Higher Derivative Gaussian Pulses in Terahertz
  Wireless Communications</title><categories>eess.SP</categories><comments>6 pages, 17 figure, accepted for publication in 2018 IEEE Global
  Communications Conference: Workshops: International Workshop on Emerging
  Technologies for 5G and Beyond Wireless and Mobile Networks (GC'18 WS -
  ET5GB). arXiv admin note: text overlap with arXiv:1808.06764</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The terahertz band is considered the last frontier for wireless
communications and expected to play a significant role in beyond 5G networks.
Besides supporting extremely high data rates for existing devices, the
terahertz band is also expected to connect future nanoscale devices using
graphene-based nano-antenna, which happens to radiate in 0.1-10 THz band. In
this band, higher order derivatives of Gaussian pulses can provide
energy-efficient communication for nanodevices. In this paper, we propose a
metric, called root mean square (RMS) frequency spread, to detect the
derivative order of the pulse at the receiving base station using uniform
linear array antennas. Simulation experiments demonstrate that RMS frequency
spread can be used to detect the derivative order of Gaussian pulses with 99%
accuracy for distances up to 50 cm. This finding opens up a new design space
for nanoscale terahertz communication, which can encode information in time
derivative order of the transmitted pulse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05766</identifier>
 <datestamp>2019-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05766</id><created>2018-09-15</created><updated>2019-01-06</updated><authors><author><keyname>Davarikia</keyname><forenames>Hamzeh</forenames></author><author><keyname>Barati</keyname><forenames>Masoud</forenames></author><author><keyname>Chan</keyname><forenames>Yupo</forenames></author><author><keyname>Iqbal</keyname><forenames>Kamran</forenames></author></authors><title>Budget Allocation for Power Networks Reliability Improvement:
  Game-Theoretic Approach</title><categories>eess.SP</categories><comments>Accepted in IEEE TPEC 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Budget allocation for power system reliability improvement is considered
among the sophisticated problems because of its nonlinear nature. This
nonlinearity makes the problem intractable for large-scale power systems. This
paper compares two approaches for budget allocation for power system
reliability improvement. The first method is the traditional one, which suffers
from the non-convexity and nonlinearity. In the second approach, a linear
zero-sum mixed-strategy game is proposed where a limited budget is allocated
among the network elements based on the game variables along with an iterative
algorithm to find the solution. Both models are applied to the modified RTBS
system. The results show that while each strategy adopts a different tactic for
reliability improvement, both strategies improve the system reliability to the
same level for a given budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05767</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05767</id><created>2018-09-15</created><authors><author><keyname>Liu</keyname><forenames>Yuanwei</forenames></author><author><keyname>Qin</keyname><forenames>Zhijin</forenames></author><author><keyname>Cai</keyname><forenames>Yunlong</forenames></author><author><keyname>Gao</keyname><forenames>Yue</forenames></author><author><keyname>Li</keyname><forenames>Geoffrey Ye</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author></authors><title>UAV Communications Based on Non-Orthogonal Multiple Access</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a novel framework for unmaned aerial vehicle (UAV)
networks with massive access capability supported by non-orthogonal multiple
access (NOMA). In order to better understand NOMA enabled UAV networks, three
case studies are carried out. We first provide performance evaluation of NOMA
enabled UAV networks by adopting stochastic geometry to model the positions of
UAVs and ground users. Then we investigate the joint trajectory design and
power allocation for static NOMA users based on a simplified two-dimensional
(2D) model that UAV is flying around at fixed height. As a further advance, we
demonstrate the UAV placement issue with the aid of machine learning techniques
when the ground users are roaming and the UAVs are capable of adjusting their
positions in three-dimensions (3D) accordingly. With these case studies, we can
comprehensively understand the UAV systems from fundamental theory to practical
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05826</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05826</id><created>2018-09-16</created><authors><author><keyname>Joshi</keyname><forenames>Himani</forenames></author><author><keyname>Darak</keyname><forenames>Sumit J</forenames></author><author><keyname>Kumar</keyname><forenames>A Anil</forenames></author><author><keyname>Kumar</keyname><forenames>Rohit</forenames></author></authors><title>Throughput Optimized Non-Contiguous Wideband Spectrum Sensing via Online
  Learning and Sub-Nyquist Sampling</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider non-contiguous wideband spectrum sensing (WSS) for
spectrum characterization and allocation in next generation heterogeneous
networks. The proposed WSS consists of sub-Nyquist sampling and digital
reconstruction to sense multiple non-contiguous frequency bands. Since the
throughput (i.e. the number of vacant bands) increases while the probability of
successful reconstruction decreases with increase in the number of sensed
bands, we develop an online learning algorithm to characterize and select
frequency bands based on their spectrum statistics. We guarantee that the
proposed algorithm allows sensing of maximum possible number of frequency bands
and hence, it is referred to as throughput optimized WSS. We also provide a
lower bound on the number of time slots required to characterize spectrum
statistics. Simulation and experimental results in the real radio environment
show that the performance of the proposed approach converges to that of Myopic
approach which has prior knowledge of spectrum statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.05862</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.05862</id><created>2018-09-16</created><authors><author><keyname>Liu</keyname><forenames>Yu-Jeh</forenames></author><author><keyname>Casebeer</keyname><forenames>Jonah</forenames></author><author><keyname>Dokmani&#x107;</keyname><forenames>Ivan</forenames></author></authors><title>Cocktails, but no party: multipath-enabled private audio</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a private audio messaging system that uses echoes to unscramble
messages at a few predetermined locations in a room. The system works by
splitting the audio into short chunks and emitting them from different
loudspeakers. The chunks are filtered so that as they echo around the room,
they sum to noise everywhere except at a few chosen focusing spots where they
exactly reproduce the intended messages. Unlike in the case of standard
personal audio zones, the proposed method renders sound outside the focusing
spots unintelligible. Our method essentially depends on echoes: the room acts
as a mixing system such that at given points we get the desired output.
Finally, we only require a modest number of loudspeakers and only a few impulse
response measurements at points where the messages should be delivered. We
demonstrate the effectiveness of the proposed method via objective quantitative
metrics as well as informal listening experiments in a real room.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06005</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06005</id><created>2018-09-16</created><authors><author><keyname>Ghasemi</keyname><forenames>M. Amin</forenames></author><author><keyname>Parniani</keyname><forenames>Mostafa</forenames></author><author><keyname>Zarei</keyname><forenames>S. Fariborz</forenames></author><author><keyname>Foroushani</keyname><forenames>Hossein Mohammadian</forenames></author></authors><title>Fast Maximum Power Point Tracking for PV Arrays under Partial shaded
  Conditions</title><categories>eess.SP</categories><comments>12 pages, 11 figures, 2016 18th European Conference on Power
  Electronics and Applications (EPE'16 ECCE Europe)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  P-V characteristic of photovoltaic (PV) arrays under partially shaded
conditions (PSC) has multiple peaks, and conventional maximum power point
tracking (MPPT) algorithms may fail to track the global maximum power point
(GMPP) because of their insufficient intelligence in discriminating the local
and global peaks. This paper proposes a novel fast MPPT method to achieve GMPP
of PV array under all PSCs. The proposed method reduces the number of required
samples and increases the speed of GMPP tracking based on comprehensive study
of I-V and P-V characteristics of PV array. Performance of the proposed method
has been evaluated in simulations of different PSCs. Also, its performance has
been compared with two selected methods in the literature through simulations.
Comparisons highlight the superiority of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06013</identifier>
 <datestamp>2020-02-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06013</id><created>2018-09-17</created><updated>2020-01-31</updated><authors><author><keyname>Niu</keyname><forenames>Chuang</forenames></author><author><keyname>Ren</keyname><forenames>Shenghan</forenames></author><author><keyname>Liang</keyname><forenames>Jimin</forenames></author></authors><title>DASNet: Reducing Pixel-level Annotations for Instance and Semantic
  Segmentation</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pixel-level annotation demands expensive human efforts and limits the
performance of deep networks that usually benefits from more such training
data. In this work we aim to achieve high quality instance and semantic
segmentation results over a small set of pixel-level mask annotations and a
large set of box annotations. The basic idea is exploring detection models to
simplify the pixel-level supervised learning task and thus reduce the required
amount of mask annotations. Our architecture, named DASNet, consists of three
modules: detection, attention, and segmentation. The detection module detects
all classes of objects, the attention module generates multi-scale
class-specific features, and the segmentation module recovers the binary masks.
Our method demonstrates substantially improved performance compared to existing
semi-supervised approaches on PASCAL VOC 2012 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06023</identifier>
 <datestamp>2020-02-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06023</id><created>2018-09-17</created><updated>2020-02-15</updated><authors><author><keyname>Khojasteh</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Khina</keyname><forenames>Anatoly</forenames></author><author><keyname>Franceschetti</keyname><forenames>Massimo</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author></authors><title>Learning-based attacks in cyber-physical systems</title><categories>eess.SY cs.CR cs.IT cs.LG cs.SY math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the problem of learning-based attacks in a simple abstraction of
cyber-physical systems, the case of a discrete-time, linear, time-invariant
plant that may be subject to an attack that overrides the sensor readings and
the controller actions. The attacker attempts to learn the dynamics of the
plant and subsequently override the controller's actuation signal, to destroy
the plant without being detected. The attacker can feed fictitious sensor
readings to the controller using its estimate of the plant dynamics and mimic
the legitimate plant operation. The controller, on the other hand, is
constantly on the lookout for an attack; once the controller detects an attack,
it immediately shuts the plant off. In the case of scalar plants, we derive an
upper bound on the attacker's deception probability for any measurable control
policy when the attacker uses an arbitrary learning algorithm to estimate the
system dynamics. We then derive lower bounds for the attacker's deception
probability for both scalar and vector plants by assuming a specific
authentication test that inspects the empirical variance of the system
disturbance. We also show how the controller can improve the security of the
system by superimposing a carefully crafted privacy-enhancing signal on top of
the &quot;nominal control policy.&quot; Finally, for nonlinear scalar dynamics, that
belong to the Reproducing Kernel Hilbert Space (RKHS), we investigate the
performance of the nonlinear Gaussian processes based attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06033</identifier>
 <datestamp>2018-12-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06033</id><created>2018-09-17</created><updated>2018-12-13</updated><authors><author><keyname>Agrawal</keyname><forenames>Niharika</forenames></author></authors><title>New Reconfigurable L-Band Digital Aeronautical Communication System</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet an ever-increasing demand of the spectrum for communication between
aircraft and ground terminals, orthogonal frequency division multiplexing
(OFDM) based L-band (960-1164MHz) Digital Aeronautical Communication System
(LDACS) has been recently proposed as an alternative to existing narrowband
systems. However, OFDM based LDACS needs additional control signaling for time
and frequency alignment and the use of cyclic prefix and high out-of-band
emission limits the spectrum utilization efficiency to less than 50%. In this
paper, a new waveform has been proposed which offers better spectrum
utilization than OFDM without compromising on computational complexity and
interference to legacy users in L-band. It also allows transceivers to
dynamically adapt the transmission bandwidth to meet the desired quality of
service. Since the proposed waveform employs a reconfigurable multiband linear
phase filter, it is referred to as reconfigurable filtered OFDM (Ref-OFDM).
Simulation results and extensive analysis show that the proposed Ref-OFDM
offers around 40 dB better out-of-band emission than OFDM which in turn leads
to significant increase in the transmission bandwidth for a given BER and
interference constraints. The computational complexity of Ref-OFDM is slightly
higher than that of OFDM but it is significantly less than other waveforms
making Ref-OFDM an attractive waveform for next generation air-to-ground
communications. An end to end hardware prototyping of LDACS-DME coexistence is
also presented in this report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06117</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06117</id><created>2018-09-17</created><authors><author><keyname>Majidian</keyname><forenames>Sina</forenames></author><author><keyname>Mohades</keyname><forenames>M.</forenames></author><author><keyname>Kahaei</keyname><forenames>M. H.</forenames></author></authors><title>Matrix Completion with Weighted Constraint for Haplotype Estimation</title><categories>eess.SP</categories><comments>6 Pages, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new optimization design is proposed for matrix completion by weighting the
measurements and deriving the corresponding error bound. Accordingly, the
Haplotype reconstruction using nuclear norm minimization with Weighted
Constraint (HapWeC) is devised for haplotype estimation. Computer simulations
show the outperformance of the HapWeC compared to some recent algorithms in
terms of the normalized reconstruction error and reconstruction rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06127</identifier>
 <datestamp>2019-01-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06127</id><created>2018-09-17</created><updated>2019-01-21</updated><authors><author><keyname>Makris</keyname><forenames>Dimos</forenames></author><author><keyname>Kaliakatsos-Papakostas</keyname><forenames>Maximos</forenames></author><author><keyname>Kermanidis</keyname><forenames>Katia Lida</forenames></author></authors><title>DeepDrum: An Adaptive Conditional Neural Network</title><categories>cs.SD cs.IR eess.AS stat.ML</categories><comments>2018 Joint Workshop on Machine Learning for Music. The Federated
  Artificial Intelligence Meeting (FAIM), a joint workshop program of ICML,
  IJCAI/ECAI, and AAMAS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering music as a sequence of events with multiple complex dependencies,
the Long Short-Term Memory (LSTM) architecture has proven very efficient in
learning and reproducing musical styles. However, the generation of rhythms
requires additional information regarding musical structure and accompanying
instruments. In this paper we present DeepDrum, an adaptive Neural Network
capable of generating drum rhythms under constraints imposed by Feed-Forward
(Conditional) Layers which contain musical parameters along with given
instrumentation information (e.g. bass and guitar notes). Results on generated
drum sequences are presented indicating that DeepDrum is effective in producing
rhythms that resemble the learned style, while at the same time conforming to
given constraints that were unknown during the training process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06139</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06139</id><created>2018-09-17</created><authors><author><keyname>Fleury</keyname><forenames>Mathis</forenames><affiliation>VisAGeS</affiliation></author><author><keyname>Maurel</keyname><forenames>Pierre</forenames><affiliation>VisAGeS</affiliation></author><author><keyname>Mano</keyname><forenames>Marsel</forenames><affiliation>VisAGeS</affiliation></author><author><keyname>Bannier</keyname><forenames>Elise</forenames><affiliation>VisAGeS</affiliation></author><author><keyname>Barillot</keyname><forenames>Christian</forenames><affiliation>VisAGeS</affiliation></author></authors><title>Automatic Electrodes Detection during simultaneous EEG/fMRI acquisition</title><categories>eess.SP cs.CV q-bio.NC</categories><comments>ISMRM, Jun 2018, Paris, France. 2018, https://www.ismrm.org/</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous EEG/fMRI acquisition allows to measure brain activity at high
spatial-temporal resolution. The localisation of EEG sources depends on several
parameters including the position of the electrodes on the scalp. The position
of the MR electrodes during its acquisitions is obtained with the use of the
UTE sequence allowing their visualisation. The retrieval of the electrodes
consists in obtaining the volume where the electrodes are located by applying a
sphere detection algorithm. We detect around 90% of electrodes for each
subject, and our UTE-based electrode detection showed an average position error
of 3.7mm for all subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06176</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06176</id><created>2018-09-17</created><updated>2018-09-19</updated><authors><author><keyname>de Vrieze</keyname><forenames>Colin</forenames></author><author><keyname>Simi&#x107;</keyname><forenames>Ljiljana</forenames></author><author><keyname>M&#xe4;h&#xf6;nen</keyname><forenames>Petri</forenames></author></authors><title>The Importance of Being Earnest: Performance of Modulation
  Classification for Real RF Signals</title><categories>eess.SP cs.LG cs.NI</categories><comments>published in DySPAN 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital modulation classification (DMC) can be highly valuable for equipping
radios with increased spectrum awareness in complex emerging wireless networks.
However, as the existing literature is overwhelmingly based on theoretical or
simulation results, it is unclear how well DMC performs in practice. In this
paper we study the performance of DMC in real-world wireless networks, using an
extensive RF signal dataset of 250,000 over-the-air transmissions with
heterogeneous transceiver hardware and co-channel interference. Our results
show that DMC can achieve a high classification accuracy even under the
challenging real-world conditions of modulated co-channel interference and
low-grade hardware. However, this only holds if the training dataset fully
captures the variety of interference and hardware types in the real radio
environment; otherwise, the DMC performance deteriorates significantly. Our
work has two important engineering implications. First, it shows that it is not
straightforward to exchange learned classifier models among dissimilar radio
environments and devices in practice. Second, our analysis suggests that the
key missing link for real-world deployment of DMC is designing signal features
that generalize well to diverse wireless network scenarios. We are making our
RF signal dataset publicly available as a step towards a unified framework for
realistic DMC evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06199</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06199</id><created>2018-08-29</created><authors><author><keyname>Hashemi</keyname><forenames>Soheil</forenames></author><author><keyname>Hajiaghajani</keyname><forenames>Amirhossein</forenames></author><author><keyname>Abdolali</keyname><forenames>Ali</forenames></author></authors><title>Noninvasive Blockade of Action Potential by Electromagnetic Induction</title><categories>q-bio.NC eess.SP q-bio.TO</categories><comments>6 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional anesthesia methods such as injective anesthetic agents may cause
various side effects such as injuries, allergies, and infections. We aim to
investigate a noninvasive scheme of an electromagnetic radiator system to block
action potential (AP) in neuron fibers. We achieved a high-gradient and
unipolar tangential electric field by designing circular geometric coils on an
electric rectifier filter layer. An asymmetric sawtooth pulse shape supplied
the coils in order to create an effective blockage. The entire setup was placed
5 cm above 50 motor and sensory neurons of the spinal cord. A validated
time-domain full-wave analysis code Based on cable model of the neurons and the
electric and magnetic potentials is used to simulate and investigate the
proposed scheme. We observed action potential blockage on both motor and
sensory neurons. In addition, the introduced approach shows promising potential
for AP manipulation in the spinal cord.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06258</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06258</id><created>2018-09-17</created><authors><author><keyname>Butola</keyname><forenames>Mansi</forenames></author><author><keyname>Sunaina</keyname></author><author><keyname>Khare</keyname><forenames>Kedar</forenames></author></authors><title>Phase retrieval with complexity guidance</title><categories>eess.IV physics.optics</categories><doi>10.1364/JOSAA.36.000202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative phase retrieval methods based on the Gerchberg-Saxton (GS) or
Fienup algorithm require a large number of iterations to converge to a
meaningful solution. For complex-valued or phase objects, these approaches also
suffer from stagnation problems where the solution does not change much from
iteration to iteration but the resultant solution shows artifacts such as
presence of a twin. We introduce a complexity parameter $\zeta$ that can be
computed directly from the Fourier magnitude data and provides a measure of
fluctuations in the desired phase retrieval solution. It is observed that when
initiated with a uniformly random phase map, the complexity of the Fienup
solution containing stagnation artifacts stabilizes at a numerical value that
is much higher than $\zeta$. We propose a modified Fienup algorithm that uses a
controlled sparsity enhancing step such that in every iteration the complexity
of the resulting solution is explicitly made close to $\zeta$. This approach
which we refer to as complexity guided phase retrieval (CGPR) is seen to
significantly reduce the number of phase retrieval iterations required for
convergence to a meaningful solution and automatically addresses the stagnation
problems. The CGPR methodology can enable new applications of iterative phase
retrieval that are considered practically difficult due to large number of
iterations required for a reliable phase recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06314</identifier>
 <datestamp>2018-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06314</id><created>2018-09-17</created><authors><author><keyname>Shen</keyname><forenames>Yifei</forenames></author><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Scalable network adaptation for Cloud-RANs: An imitation learning
  approach</title><categories>eess.SP</categories><comments>GLOBALSIP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network adaptation is essential for the efficient operation of Cloud-RANs.
Unfortunately, it leads to highly intractable mixed-integer nonlinear
programming problems. Existing solutions typically rely on convex relaxation,
which yield performance gaps that are difficult to quantify. Meanwhile, global
optimization algorithms such as branch-and-bound can find optimal solutions but
with prohibitive computational complexity. In this paper, to obtain
near-optimal solutions at affordable complexity, we propose to approximate the
branch-and-bound algorithm via machine learning. Specifically, the pruning
procedure in branch-and-bound is formulated as a sequential decision problem,
followed by learning the oracle's action via imitation learning. A unique
advantage of this framework is that the training process only requires a small
dataset, and it is scalable to problem instances with larger dimensions than
the training setting. This is achieved by identifying and leveraging the
problem-size independent features. Numerical simulations demonstrate that the
learning based framework significantly outperforms competing methods, with
computational complexity much lower than the traditional branch-and-bound
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06365</identifier>
 <datestamp>2019-06-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06365</id><created>2018-09-15</created><updated>2019-06-04</updated><authors><author><keyname>Zavary</keyname><forenames>Elyar</forenames></author><author><keyname>Sojoodi</keyname><forenames>Mahdi</forenames></author></authors><title>A class of non-linear fractional-order system stabilisation via
  fixed-order dynamic output feedback controller</title><categories>math.OC cs.SY eess.SP math.DS math.NA</categories><comments>arXiv admin note: text overlap with arXiv:1809.03733</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the robust stabilisation of a class of
fractional-order non-linear systems via fixed-order dynamic output feedback
controller in terms of linear matrix inequalities (LMIs). The systematic
stabilisation algorithm design for low-order controller based on direct
Lyapunov approach is proposed. In the presented algorithm the conditions
containing the bilinear variables are decoupled into separate conditions
without imposing equality constraints or considering an iterative search of the
controller parameters. There is no any limiting constraint on the state space
matrices and also we assumed the most complete output feedback controller.
Simulations results are given to approve the effectiveness and the
straightforwardness of the proposed design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06431</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06431</id><created>2018-09-17</created><authors><author><keyname>Shahsavari</keyname><forenames>Shahram</forenames></author><author><keyname>Shirani</keyname><forenames>Farhad</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>A General Framework for Temporal Fair User Scheduling in NOMA Systems</title><categories>eess.SP</categories><doi>10.1109/JSTSP.2019.2903745</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-orthogonal multiple access (NOMA) is one of the promising radio access
techniques for next generation wireless networks. Opportunistic multi-user
scheduling is necessary to fully exploit multiplexing gains in NOMA systems,
but compared with traditional scheduling, interrelations between users'
throughputs induced by multi-user interference poses new challenges in the
design of NOMA schedulers. A successful NOMA scheduler has to carefully balance
the following three objectives: maximizing average system utility, satisfying
desired fairness constraints among the users and enabling real-time, and low
computational cost implementations. In this paper, scheduling for NOMA systems
under temporal fairness constraints is considered. Temporal fair scheduling
leads to communication systems with predictable latency as opposed to
utilitarian fair schedulers for which latency can be highly variable. It is
shown that optimal system utility is achieved using a class of opportunistic
scheduling schemes called threshold based strategies (TBS). One of the
challenges in temporal fair scheduling for heterogeneous NOMA scenarios - where
only specific users may be activated simultaneously - is to determine the set
of feasible temporal shares. In this work, a variable elimination algorithm is
proposed to accomplish this task. Furthermore, an (online) iterative algorithm
based on the Robbins-Monro method is proposed to construct a TBS by finding the
optimal thresholds for a given system utility metric. Various numerical
simulations of practical scenarios are provided to illustrate the effectiveness
of the proposed NOMA scheduling in static and mobile scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06534</identifier>
 <datestamp>2019-05-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06534</id><created>2018-09-18</created><authors><author><keyname>Cao</keyname><forenames>Zehong</forenames></author><author><keyname>Chuang</keyname><forenames>Chun-Hsiang</forenames></author><author><keyname>King</keyname><forenames>Jung-Kai</forenames></author><author><keyname>Lin</keyname><forenames>Chin-Teng</forenames></author></authors><title>Multi-channel EEG recordings during a sustained-attention driving task</title><categories>eess.SP q-bio.NC</categories><comments>This manuscript is submitting to Nature: Scientific Data</comments><journal-ref>Scientific Data (volume 6, Article number: 19) (2019)</journal-ref><doi>10.1038/s41597-019-0027-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We described driver behaviour and brain dynamics acquired from a 90-minute
sustained-attention task in an immersive driving simulator. The data include 62
copies of 32 channel electroencephalography (EEG) data for 27 subjects that
drove on a four lane highway and were asked to keep the car cruising in the
centre of the lane. Lane departure events were randomly induced to make the car
drift from the original cruising lane towards the left or right lane. A
complete trial includes events with deviation onset, response onset, and
response offset. The next trial, in which the subject has to drive back to the
original cruising lane, occurs from 5 to 10 seconds after finishing the current
trial. We hope that this dataset will lead to the development of novel neural
processing assays that can be used to index brain cortical dynamics and detect
driving fatigue and drowsiness. This publicly available dataset is beneficial
to the neuroscientific and brain computer interface communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06557</identifier>
 <datestamp>2018-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06557</id><created>2018-09-18</created><authors><author><keyname>Ge</keyname><forenames>Weifeng</forenames></author><author><keyname>Gong</keyname><forenames>Bingchen</forenames></author><author><keyname>Yu</keyname><forenames>Yizhou</forenames></author></authors><title>Image Super-Resolution via Deterministic-Stochastic Synthesis and Local
  Statistical Rectification</title><categories>cs.CV eess.IV</categories><comments>to appear in SIGGRAPH Asia 2018</comments><doi>10.1145/3272127.3275060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single image superresolution has been a popular research topic in the last
two decades and has recently received a new wave of interest due to deep neural
networks. In this paper, we approach this problem from a different perspective.
With respect to a downsampled low resolution image, we model a high resolution
image as a combination of two components, a deterministic component and a
stochastic component. The deterministic component can be recovered from the
low-frequency signals in the downsampled image. The stochastic component, on
the other hand, contains the signals that have little correlation with the low
resolution image. We adopt two complementary methods for generating these two
components. While generative adversarial networks are used for the stochastic
component, deterministic component reconstruction is formulated as a regression
problem solved using deep neural networks. Since the deterministic component
exhibits clearer local orientations, we design novel loss functions tailored
for such properties for training the deep regression network. These two methods
are first applied to the entire input image to produce two distinct
high-resolution images. Afterwards, these two images are fused together using
another deep neural network that also performs local statistical rectification,
which tries to make the local statistics of the fused image match the same
local statistics of the groundtruth image. Quantitative results and a user
study indicate that the proposed method outperforms existing state-of-the-art
algorithms with a clear margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06653</identifier>
 <datestamp>2019-02-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06653</id><created>2018-09-18</created><updated>2019-02-04</updated><authors><author><keyname>Seifert</keyname><forenames>Ann-Kathrin</forenames></author><author><keyname>Amin</keyname><forenames>Moeness G.</forenames></author><author><keyname>Zoubir</keyname><forenames>Abdelhak M.</forenames></author></authors><title>Toward Unobtrusive In-home Gait Analysis Based on Radar Micro-Doppler
  Signatures</title><categories>eess.SP</categories><comments>11 pages, 6 figures</comments><doi>10.1109/TBME.2019.2893528</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: In this paper, we demonstrate the applicability of radar for gait
classification with application to home security, medical diagnosis,
rehabilitation and assisted living. Aiming at identifying changes in gait
patterns based on radar micro-Doppler signatures, this work is concerned with
solving the intra motion category classification problem of gait recognition.
Methods: New gait classification approaches utilizing physical features,
subspace features and sum-of-harmonics modeling are presented and their
performances are evaluated using experimental K-band radar data of four test
subjects. Five different gait classes are considered for each person, including
normal, pathological and assisted walks. Results: The proposed approaches are
shown to outperform existing methods for radar-based gait recognition which
utilize physical features from the cadence-velocity data representation domain
as in this paper. The analyzed gait classes are correctly identified with an
average accuracy of 93.8%, where a classification rate of 98.5% is achieved for
a single gait class. When applied to new data of another individual a
classification accuracy on the order of 80% can be expected. Conclusion: Radar
micro-Doppler signatures and their Fourier transforms are well suited to
capture changes in gait. Five different walking styles are recognized with high
accuracy. Significance: Radar-based sensing of human gait is an emerging
technology with multi-faceted applications in security and health care
industries. We show that radar, as a contact-less sensing technology, can
supplement existing gait diagnostic tools with respect to long-term monitoring
and reproducibility of the examinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06671</identifier>
 <datestamp>2019-05-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06671</id><created>2018-09-18</created><updated>2018-09-29</updated><authors><author><keyname>Cao</keyname><forenames>Zehong</forenames></author><author><keyname>Ding</keyname><forenames>Weiping</forenames></author><author><keyname>Wang</keyname><forenames>Yu-Kai</forenames></author><author><keyname>Hussain</keyname><forenames>Farookh Khadeer</forenames></author><author><keyname>Al-Jumaily</keyname><forenames>Adel</forenames></author><author><keyname>Lin</keyname><forenames>Chin-Teng</forenames></author></authors><title>Effects of Repetitive SSVEPs on EEG Complexity using Multiscale Inherent
  Fuzzy Entropy</title><categories>eess.SP</categories><comments>The manuscript is pending publication in Neurocomputing</comments><journal-ref>Neurocomputing (Available online 8 May 2019)</journal-ref><doi>10.1016/j.neucom.2018.08.091</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiscale inherent fuzzy entropy is an objective measurement of
electroencephalography (EEG) complexity, reflecting the habituation of brain
systems. Entropy dynamics are generally believed to reflect the ability of the
brain to adapt to a visual stimulus environment. In this study, we explored
repetitive steady-state visual evoked potential (SSVEP)-based EEG complexity by
assessing multiscale inherent fuzzy entropy with relative measurements. We used
a wearable EEG device with Oz and Fpz electrodes to collect EEG signals from 40
participants under the following three conditions: a resting state (closed-eyes
(CE) and open- eyes (OE) stimulation with five 15-Hz CE SSVEPs and stimulation
with five 20-Hz OE SSVEPs. We noted monotonic enhancement of occipital EEG
relative complexity with increasing stimulus times in CE and OE conditions. The
occipital EEG relative complexity was significantly higher for the fifth SSVEP
than for the first SSEVP (FDR-adjusted p &lt; 0.05). Similarly, the prefrontal EEG
relative complexity tended to be significantly higher in the OE condition
compared to that in the CE condition (FDR-adjusted p &lt; 0.05). The results also
indicate that multiscale inherent fuzzy entropy is superior to other competing
multiscale-based entropy methods. In conclusion, EEG relative complexity
increases with stimulus times, a finding that reflects the strong habituation
of brain systems. These results suggest that multiscale inherent fuzzy entropy
is an EEG pattern with which brain complexity can be assessed using repetitive
SSVEP stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06676</identifier>
 <datestamp>2020-01-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06676</id><created>2018-09-18</created><authors><author><keyname>Li</keyname><forenames>Fali</forenames></author><author><keyname>Yi</keyname><forenames>Chanlin</forenames></author><author><keyname>Liao</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Jiang</keyname><forenames>Yuanling</forenames></author><author><keyname>Si</keyname><forenames>Yajing</forenames></author><author><keyname>Song</keyname><forenames>Limeng</forenames></author><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Yao</keyname><forenames>Dezhong</forenames></author><author><keyname>Zhang</keyname><forenames>Yangsong</forenames></author><author><keyname>Cao</keyname><forenames>Zehong</forenames></author><author><keyname>Xu</keyname><forenames>Peng</forenames></author></authors><title>Reconfiguration of Brain Network between Resting-state and Oddball
  Paradigm</title><categories>eess.SP q-bio.NC</categories><comments>This manuscript is submitting to IEEE Transactions on Cognitive and
  Developmental Systems</comments><doi>10.1109/TCDS.2020.2965135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The oddball paradigm is widely applied to the investigation of multiple
cognitive functions. Prior studies have explored the cortical oscillation and
power spectral differing from the resting-state conduction to oddball paradigm,
but whether brain networks existing the significant difference is still
unclear. Our study addressed how the brain reconfigures its architecture from a
resting-state condition (i.e., baseline) to P300 stimulus task in the visual
oddball paradigm. In this study, electroencephalogram (EEG) datasets were
collected from 24 postgraduate students, who were required to only mentally
count the number of target stimulus; afterwards the functional EEG networks
constructed in different frequency bands were compared between baseline and
oddball task conditions to evaluate the reconfiguration of functional network
in the brain. Compared to the baseline, our results showed the significantly (p
&lt; 0.05) enhanced delta/theta EEG connectivity and decreased alpha default mode
network in the progress of brain reconfiguration to the P300 task. Furthermore,
the reconfigured coupling strengths were demonstrated to relate to P300
amplitudes, which were then regarded as input features to train a classifier to
differentiate the high and low P300 amplitudes groups with an accuracy of
77.78%. The findings of our study help us to understand the changes of
functional brain connectivity from resting-state to oddball stimulus task, and
the reconfigured network pattern has the potential for the selection of good
subjects for P300-based brain- computer interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06687</identifier>
 <datestamp>2018-09-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06687</id><created>2018-09-06</created><authors><author><keyname>Gu</keyname><forenames>Jinjin</forenames></author><author><keyname>Liu</keyname><forenames>Guolong</forenames></author><author><keyname>Liang</keyname><forenames>Gaoqi</forenames></author><author><keyname>Zhao</keyname><forenames>Junhua</forenames></author></authors><title>Super-Resolution Perception for Industrial Sensor Data</title><categories>eess.SP cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the problem formulation and methodology framework
of Super-Resolution Perception (SRP) on industrial sensor data. Industrial
intelligence relies on high-quality industrial sensor data for system control,
diagnosis, fault detection, identification and monitoring. However, the
provision of high-quality data may be expensive in some cases. In this paper,
we propose a novel machine learning problem - the SRP problem as reconstructing
high-quality data from unsatisfactory sensor data in industrial systems.
Advanced generative models are then proposed to solve the SRP problem. This
technology makes it possible for empowering existing industrial facilities
without upgrading existing sensors or deploying additional sensors. We first
mathematically formulate the SRP problem under the Maximum a Posteriori (MAP)
estimation framework. A case study is then presented, which performs SRP on
smart meter data. A network namely SRPNet is proposed to generate
high-frequency load data from low-frequency data. Experiments demonstrate that
our SRP model can reconstruct high-frequency data effectively. Moreover, the
reconstructed high-frequency data can lead to better appliance monitoring
results without changing the monitoring appliances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06697</identifier>
 <datestamp>2018-09-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06697</id><created>2018-09-13</created><authors><author><keyname>Moctezuma</keyname><forenames>Luis Alfredo</forenames></author><author><keyname>Molinas</keyname><forenames>Marta</forenames></author></authors><title>EEG-based Subjects Identification based on Biometrics of Imagined Speech
  using EMD</title><categories>q-bio.NC cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When brain activity is translated into commands for real applications, the
potential for human capacities augmentation is promising. In this paper, EMD is
used to decompose EEG signals during Imagined Speech in order to use it as a
biometric marker for creating a Biometric Recognition System. For each EEG
channel, the most relevant Intrinsic Mode Functions (IMFs) are decided based on
the Minkowski distance, and for each IMF 4 features are computed: Instantaneous
and Teager energy distribution and Higuchi and Petrosian Fractal Dimension. To
test the proposed method, a dataset with 20 subjects who imagined 30
repetitions of 5 words in Spanish, is used. Four classifiers are used for this
task - random forest, SVM, naive Bayes, and k-NN - and their performances are
compared. The accuracy obtained (up to 0.92 using Linear SVM) after 10-folds
cross-validation suggest that the proposed method based on EMD can be valuable
for creating EEG-based biometrics of imagined speech for Subjects
identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06708</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06708</id><created>2018-09-15</created><authors><author><keyname>Park</keyname><forenames>Junhyeong</forenames></author><author><keyname>Park</keyname><forenames>Seungwoon</forenames></author><author><keyname>Kim</keyname><forenames>Do-Hoon</forenames></author><author><keyname>Park</keyname><forenames>Seong-Ook</forenames></author></authors><title>Leakage Mitigation in Heterodyne FMCW Radar For Small Drone Detection
  with Stationary Point Concentration Technique</title><categories>eess.SP</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. arXiv admin note: text overlap with arXiv:1807.06324</comments><journal-ref>IEEE Transactions on Microwave Theory and Techniques ( Volume: 67
  , Issue: 3 , March 2019 )</journal-ref><doi>10.1109/TMTT.2018.2889045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, as the drones have become smaller and smarter, they are emerging as
new threats to the military and the public. To prevent the threats, the
development of the radar to detect the small drones is needed. The frequency
modulated continuous wave (FMCW) radar is one of the radar types for the drone
detection. The heterodyne architecture for the FMCW radar is often applied to
resolve the dc offset problem. However, the leakage from a transmitter into a
receiver, the notorious problem of the FMCW radar, is still a challenging
problem. Especially, the phase noise of the leakage increases the noise floor
and deteriorates the signal to noise ratio. In order to mitigate this problem,
in this paper, the stationary point concentration technique is proposed.
Without additional hardware, the proposed technique can be implemented through
frequency planning and digital signal processing. The results show that the
proposed technique significantly lowers the noise floor over the desired range
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06741</identifier>
 <datestamp>2018-12-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06741</id><created>2018-09-17</created><authors><author><keyname>Smolyaninov</keyname><forenames>Igor I.</forenames></author><author><keyname>Balzano</keyname><forenames>Quirino</forenames></author><author><keyname>Davis</keyname><forenames>Christopher C.</forenames></author><author><keyname>Young</keyname><forenames>Dendy</forenames></author></authors><title>Surface Wave-Based Underwater Radio Communication</title><categories>eess.SP</categories><comments>4 pages, 10 figures</comments><journal-ref>IEEE Antennas and Wireless Propagation Letters 17, 2503-2507
  (2018)</journal-ref><doi>10.1109/LAWP.2018.2880008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An underwater portable radio antenna operating in the 50 MHz band and
efficient for launching surface electromagnetic waves at the seawater/air
interface is presented. The antenna operation is based on the field enhancement
at the antenna tip and on an impedance matching antenna enclosure, which is
filled with de-ionized water. This enclosure allows us to reduce antenna
dimensions and improve the coupling of electromagnetic energy to the
surrounding salt water medium. Since surface wave propagation length far
exceeds the skin depth of conventional radio waves at the same frequency, this
technique is useful for broadband underwater wireless communication over
several meters distances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06798</identifier>
 <datestamp>2018-09-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06798</id><created>2018-09-17</created><authors><author><keyname>Xu</keyname><forenames>Longting</forenames></author><author><keyname>Das</keyname><forenames>Rohan Kumar</forenames></author><author><keyname>Y&#x131;lmaz</keyname><forenames>Emre</forenames></author><author><keyname>Yang</keyname><forenames>Jichen</forenames></author><author><keyname>Li</keyname><forenames>Haizhou</forenames></author></authors><title>Generative x-vectors for text-independent speaker verification</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>Accepted for publication at SLT 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speaker verification (SV) systems using deep neural network embeddings,
so-called the x-vector systems, are becoming popular due to its good
performance superior to the i-vector systems. The fusion of these systems
provides improved performance benefiting both from the discriminatively trained
x-vectors and generative i-vectors capturing distinct speaker characteristics.
In this paper, we propose a novel method to include the complementary
information of i-vector and x-vector, that is called generative x-vector. The
generative x-vector utilizes a transformation model learned from the i-vector
and x-vector representations of the background data. Canonical correlation
analysis is applied to derive this transformation model, which is later used to
transform the standard x-vectors of the enrollment and test segments to the
corresponding generative x-vectors. The SV experiments performed on the NIST
SRE 2010 dataset demonstrate that the system using generative x-vectors
provides considerably better performance than the baseline i-vector and
x-vector systems. Furthermore, the generative x-vectors outperform the fusion
of i-vector and x-vector systems for long-duration utterances, while yielding
comparable results for short-duration utterances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06800</identifier>
 <datestamp>2018-09-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06800</id><created>2018-09-14</created><authors><author><keyname>Bear</keyname><forenames>Helen L</forenames></author></authors><title>Visual Speech Language Models</title><categories>eess.AS cs.SD</categories><comments>Extended abstract based on Decoding Visemes: improving machine
  lipreading, Bear &amp; Harvey, ICASSP 2016</comments><doi>10.13140/RG.2.2.15396.94081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language models (LM) are very powerful in lipreading systems. Language models
built upon the ground truth utterances of datasets learn grammar and structure
rules of words and sentences (the latter in the case of continuous speech).
However, visual co-articulation effects in visual speech signals damage the
performance of visual speech LM's as visually, people do not utter what the
language model expects. These models are commonplace but while higher-order
N-gram LM's may improve classification rates, the cost of this model is
disproportionate to the common goal of developing more accurate classifiers. So
we compare which unit would best optimize a lipreading (visual speech) LM to
observe their limitations. We compare three units; visemes (visual speech
units) \cite{lan2010improving}, phonemes (audible speech units), and words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06833</identifier>
 <datestamp>2019-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06833</id><created>2018-09-18</created><updated>2019-10-01</updated><authors><author><keyname>Ghorbani</keyname><forenames>Shahram</forenames></author><author><keyname>Bulut</keyname><forenames>Ahmet E.</forenames></author><author><keyname>Hansen</keyname><forenames>John H. L.</forenames></author></authors><title>Advancing Multi-Accented LSTM-CTC Speech Recognition using a Domain
  Specific Student-Teacher Learning Paradigm</title><categories>eess.AS</categories><comments>Accepted at SLT 2018</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Non-native speech causes automatic speech recognition systems to degrade in
performance. Past strategies to address this challenge have considered model
adaptation, accent classification with a model selection, alternate
pronunciation lexicon, etc. In this study, we consider a recurrent neural
network (RNN) with connectionist temporal classification (CTC) cost function
trained on multi-accent English data including US (Native), Indian and Hispanic
accents. We exploit dark knowledge from a model trained with the multi-accent
data to train student models under the guidance of both a teacher model and CTC
cost of target transcription. We show that transferring knowledge from a single
RNN-CTC trained model toward a student model, yields better performance than
the stand-alone teacher model. Since the outputs of different trained CTC
models are not necessarily aligned, it is not possible to simply use an
ensemble of CTC teacher models. To address this problem, we train accent
specific models under the guidance of a single multi-accent teacher, which
results in having multiple aligned and trained CTC models. Furthermore, we
train a student model under the supervision of the accent-specific teachers,
resulting in an even further complementary model, which achieves +20.1%
relative Character Error Rate (CER) reduction compared to the baseline trained
without any teacher. Having this effective multi-accent model, we can achieve
further improvement for each accent by adapting the model to each accent. Using
the accent specific model's outputs to regularize the adapting process (i.e., a
knowledge distillation version of Kullback-Leibler (KL) divergence) results in
even superior performance compared to the conventional approach using general
teacher models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06853</identifier>
 <datestamp>2018-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06853</id><created>2018-09-18</created><authors><author><keyname>Wang</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Bo</keyname><forenames>Zunwang</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author><author><keyname>Gong</keyname><forenames>Wenlin</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author><author><keyname>Han</keyname><forenames>Shensheng</forenames></author></authors><title>Error-Control-Coding Assisted Imaging</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error-control-coding (ECC) techniques are widely used in modern digital
communication systems to minimize the effect of noisy channels on the quality
of received signals. Motivated by the fact that both communication and imaging
can be considered as an information transfer process, in this paper we
demonstrate that ECC could yield significant improvements in the image quality
by reducing the effect of noise in the physical process of image acquisition.
In the demonstrated approach, the object is encoded by ECC structured light
fields generated by a digital-micromirror-device (DMD) while its image is
obtained by decoding the received signal collected by a bucket-detector (BD).
By applying a Luby Transform (LT) code as an example, our proof-of-concept
experiment validates that the object image can be effectively reconstructed
while errors induced by noisy reception can be significantly reduced. The
demonstrated approach informs a new imaging technique: ECC assisted imaging,
which can be scaled into applications such as remote sensing, spectroscopy and
biomedical imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06854</identifier>
 <datestamp>2018-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06854</id><created>2018-09-18</created><authors><author><keyname>Wu</keyname><forenames>Tengfei</forenames></author><author><keyname>Guo</keyname><forenames>Chengfei</forenames></author><author><keyname>Shao</keyname><forenames>Xiaopeng</forenames></author></authors><title>Non-invasive imaging through thin scattering layers with broadband
  illumination</title><categories>eess.IV physics.optics</categories><comments>4 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory-effect-based methods have been demonstrated to be feasible to observe
hidden objects through thin scattering layers, even from a single-shot speckle
pattern. However, most of the existing methods are performed with narrowband
illumination or require point light-sources adjacent to the hidden objects as
the references, to make an invasive pre-calibration of the imaging system.
Here, inspired by the shift-and-add algorithm, we propose that by randomly
selecting and averaging different sub-regions of the speckle patterns, an image
pattern resembling the autocorrelation (we call it R-autocorrelation) of the
hidden object can be extracted. By performing numerical simulations and
experiments, we demonstrate that comparing with true autocorrelation, the
pattern of R-autocorrelation has a significantly lower background and higher
contrast, which enables better reconstructions of hidden objects, especially in
the case of broadband illumination, or even with white-light.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06855</identifier>
 <datestamp>2018-12-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06855</id><created>2018-09-18</created><updated>2018-10-25</updated><authors><author><keyname>Beltran</keyname><forenames>Mario A.</forenames></author><author><keyname>Paganin</keyname><forenames>David M.</forenames></author></authors><title>Aberrated dark-field imaging systems</title><categories>eess.IV physics.optics</categories><journal-ref>Phys. Rev. A 98, 053849 (2018)</journal-ref><doi>10.1103/PhysRevA.98.053849</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study generalized dark-field imaging systems. These are a subset of linear
shift-invariant optical imaging systems, that exhibit arbitrary aberrations,
and for which normally-incident plane-wave input yields zero output. We write
down the theory for the forward problem of imaging coherent scalar optical
fields using such arbitrarily-aberrated dark-field systems, and give numerical
examples. The associated images may be viewed as a form of dark-field Gabor
holography, utilizing arbitrary outgoing Green functions as generalized
Huygens-type wavelets, and with the Young-type boundary wave forming the
holographic reference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.06959</identifier>
 <datestamp>2018-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.06959</id><created>2018-09-18</created><authors><author><keyname>Tygert</keyname><forenames>Mark</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author><author><keyname>Zbontar</keyname><forenames>Jure</forenames></author></authors><title>Compressed sensing with a jackknife and a bootstrap</title><categories>eess.IV eess.SP stat.ME</categories><comments>67 pages, 83 figures: the images in the appendix are low-quality;
  high-quality images are available at http://tygert.com/comps.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing proposes to reconstruct more degrees of freedom in a
signal than the number of values actually measured. Compressed sensing
therefore risks introducing errors -- inserting spurious artifacts or masking
the abnormalities that medical imaging seeks to discover. The present case
study of estimating errors using the standard statistical tools of a jackknife
and a bootstrap yields error &quot;bars&quot; in the form of full images that are
remarkably representative of the actual errors (at least when evaluated and
validated on data sets for which the ground truth and hence the actual error is
available). These images show the structure of possible errors -- without
recourse to measuring the entire ground truth directly -- and build confidence
in regions of the images where the estimated errors are small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07032</identifier>
 <datestamp>2018-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07032</id><created>2018-09-19</created><authors><author><keyname>Li</keyname><forenames>Xiaohui</forenames></author><author><keyname>Xing</keyname><forenames>Li</forenames></author></authors><title>Optimal Deployment of Drone Base Stations for Cellular Communication by
  Network-based Localization</title><categories>cs.SY eess.SP</categories><comments>6 pages, 7 figures,37th Chinese Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drone base stations can assist cellular networks in a variety of scenarios.
To serve the maximum number of users in an area without apriori user
distribution information, we proposed a two-stage algorithm to find the optimal
deployment of drone base stations. The algorithm involves UTDOA positioning,
coverage control and collision avoidance. To the best of our knowledge, the
concept that uses network-based localization to optimize the deployment of
drone-BSs has not been analyzed in the previous literature. Simulations are
presented showing that the proposed algorithm outperforms random search
algorithm in terms of the maximum number of severed users under the deployment
of drone-BSs they found, with limited user densities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07033</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07033</id><created>2018-09-19</created><authors><author><keyname>Li</keyname><forenames>Xiaohui</forenames></author></authors><title>Deployment of Drone Base Stations for Cellular Communication Without
  Apriori User Distribution Information</title><categories>cs.SY eess.SP</categories><comments>8 pages, 11 figures, 37th Chinese Control Conference. arXiv admin
  note: text overlap with arXiv:1809.07032</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drone base stations can provide cellular networks in areas that have lost
coverage due to disasters. To serve the maximum number of users in the disaster
area without apriori user distribution information, we proposed a 'sweep and
search' algorithm to find the optimal deployment of drone base stations. The
algorithm involves polygon area decomposition, coverage control and collision
avoidance. To the best of our knowledge, this paper is the first in the
literature that studied the deployment of drone base station without apriori
user distribution information. Simulations are presented showing that the
proposed algorithm outperforms the random search algorithm and the attractive
search algorithm regarding the maximum number of severed users under the
deployment of drone-BSs they found with a time limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07188</identifier>
 <datestamp>2018-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07188</id><created>2018-09-19</created><authors><author><keyname>Jin</keyname><forenames>Kyong Hwan</forenames></author><author><keyname>Kim</keyname><forenames>Gain</forenames></author><author><keyname>Leblebici</keyname><forenames>Yusuf</forenames></author><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author></authors><title>Direct Reconstruction of Saturated Samples in Band-Limited OFDM Signals</title><categories>eess.SP cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of samples, a few of them being possibly saturated, we propose an
efficient algorithm in order to cancel saturation while reconstructing
band-limited signals. Our method satisfies a minimum-loss constraint and relies
on sinc-related bases. It involves matrix inversion and is a direct,
non-iterative approach. It consists of two main steps: (i) regression, to
estimate the expansion coefficients of the signal model; (ii) interpolation, to
restore an estimated value for those samples that are saturated. Because the
proposed method is free from tuning parameters, it is hardware-friendly and we
expect that it will be particularly useful in the context of orthogonal
frequency-division multiplexing. There, the high peak-to-average power ratio of
the transmitted signal results in a challenging decoding stage in the presence
of saturation, which causes significant decoding errors due to the nonlinearity
of amplifiers and receivers, ultimately resulting in band distortion and
information loss. Our experiments on realistic simulations confirm that our
proposed reconstruction of the saturated samples can significantly reduce
transmission errors in modern high-throughput digital-communication receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07203</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07203</id><created>2018-09-19</created><updated>2019-02-09</updated><authors><author><keyname>Liu</keyname><forenames>Junyan</forenames></author><author><keyname>Kumar</keyname><forenames>Sandeep</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Parameter Estimation of Heavy-Tailed AR Model with Missing Data via
  Stochastic EM</title><categories>stat.AP eess.SP math.OC q-fin.ST</categories><comments>This is a companion document to a paper that is accepted to IEEE
  Transaction on Signal Processing 2019, complemented with the supplementary
  material</comments><doi>10.1109/TSP.2019.2899816</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The autoregressive (AR) model is a widely used model to understand time
series data. Traditionally, the innovation noise of the AR is modeled as
Gaussian. However, many time series applications, for example, financial time
series data, are non-Gaussian, therefore, the AR model with more general
heavy-tailed innovations is preferred. Another issue that frequently occurs in
time series is missing values, due to system data record failure or unexpected
data loss. Although there are numerous works about Gaussian AR time series with
missing values, as far as we know, there does not exist any work addressing the
issue of missing data for the heavy-tailed AR model. In this paper, we consider
this issue for the first time, and propose an efficient framework for parameter
estimation from incomplete heavy-tailed time series based on a stochastic
approximation expectation maximization (SAEM) coupled with a Markov Chain Monte
Carlo (MCMC) procedure. The proposed algorithm is computationally cheap and
easy to implement. The convergence of the proposed algorithm to a stationary
point of the observed data likelihood is rigorously proved. Extensive
simulations and real datasets analyses demonstrate the efficacy of the proposed
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07347</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07347</id><created>2018-09-19</created><authors><author><keyname>Diwale</keyname><forenames>Sanket</forenames></author><author><keyname>Jones</keyname><forenames>Colin</forenames></author></authors><title>A Generalized Representer Theorem for Hilbert Space - Valued Functions</title><categories>cs.LG cs.AI eess.SP math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The necessary and sufficient conditions for existence of a generalized
representer theorem are presented for learning Hilbert space-valued functions.
Representer theorems involving explicit basis functions and Reproducing Kernels
are a common occurrence in various machine learning algorithms like generalized
least squares, support vector machines, Gaussian process regression and kernel
based deep neural networks to name a few. Due to the more general structure of
the underlying variational problems, the theory is also relevant to other
application areas like optimal control, signal processing and decision making.
We present the generalized representer as a unified view for supervised and
semi-supervised learning methods, using the theory of linear operators and
subspace valued maps. The implications of the theorem are presented with
examples of multi input-multi output regression, kernel based deep neural
networks, stochastic regression and sparsity learning problems as being special
cases in this unified view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07356</identifier>
 <datestamp>2019-12-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07356</id><created>2018-09-19</created><updated>2019-12-04</updated><authors><author><keyname>Autthasan</keyname><forenames>Phairot</forenames></author><author><keyname>Du</keyname><forenames>Xiangqian</forenames></author><author><keyname>Arnin</keyname><forenames>Jetsada</forenames></author><author><keyname>Lamyai</keyname><forenames>Sirakorn</forenames></author><author><keyname>Perera</keyname><forenames>Maneesha</forenames></author><author><keyname>Itthipuripat</keyname><forenames>Sirawaj</forenames></author><author><keyname>Yagi</keyname><forenames>Tohru</forenames></author><author><keyname>Manoonpong</keyname><forenames>Poramate</forenames></author><author><keyname>Wilaiprasitporn</keyname><forenames>Theerawit</forenames></author></authors><title>A Single-Channel Consumer-Grade EEG Device for Brain-Computer Interface:
  Enhancing Detection of SSVEP and Its Amplitude Modulation</title><categories>eess.SP</categories><comments>IEEE Sensors (Accepted)</comments><journal-ref>IEEE Sensor Journal, 2019</journal-ref><doi>10.1109/JSEN.2019.2958210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain-Computer interfaces (BCIs) play a significant role in easing
neuromuscular patients on controlling computers and prosthetics. Due to their
high signal-to-noise ratio, steady-state visually evoked potentials (SSVEPs)
has been widely used to build BCIs. However, currently developed algorithms do
not predict the modulation of SSVEP amplitude, which is known to change as a
function of stimulus luminance contrast. In this study, we aim to develop an
integrated approach to simultaneously estimate the frequency and
contrast-related amplitude modulations of the SSVEP signal. To achieve that, we
developed a behavioral task in which human participants focused on a visual
flicking target which the luminance contrast can change through time in several
ways. SSVEP signals from 16 subjects were then recorded from electrodes placed
at the central occipital site using a low-cost, consumer-grade EEG. Our results
demonstrate that the filter bank canonical correlation analysis (FBCCA)
performed well in SSVEP frequency recognition, while the support vector
regression (SVR) outperformed the other supervised machine learning algorithms
in predicting the contrast-dependent amplitude modulations of the SSVEPs. These
findings indicate the applicability and strong performance of our integrated
method at simultaneously predicting both frequency and amplitude of visually
evoked signals, and have proven to be useful for advancing SSVEP-based
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07375</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07375</id><created>2018-09-19</created><authors><author><keyname>Ibarrola</keyname><forenames>Francisco</forenames></author><author><keyname>Di Persia</keyname><forenames>Leandro</forenames></author><author><keyname>Spies</keyname><forenames>Ruben</forenames></author></authors><title>Switching divergences for spectral learning in blind speech
  dereverberation</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When recorded in an enclosed room, a sound signal will most certainly get
affected by reverberation. This not only undermines audio quality, but also
poses a problem for many human-machine interaction technologies that use speech
as their input. In this work, a new blind, two-stage dereverberation approach
based in a generalized \beta-divergence as a fidelity term over a non-negative
representation is proposed. The first stage consists of learning the spectral
structure of the signal solely from the observed spectrogram, while the second
stage is devoted to model reverberation. Both steps are taken by minimizing a
cost function in which the aim is put either in constructing a dictionary or a
good representation by changing the divergence involved. In addition, an
approach for finding an optimal fidelity parameter for dictionary learning is
proposed. An algorithm for implementing the proposed method is described and
tested against state-of-the-art methods. Results show improvements for both
artificial reverberation and real recordings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07384</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07384</id><created>2018-09-19</created><authors><author><keyname>Chiea</keyname><forenames>Rafael Attili</forenames></author><author><keyname>Costa</keyname><forenames>M&#xe1;rcio Holsbach</forenames></author><author><keyname>Barrault</keyname><forenames>Guillaume</forenames></author></authors><title>New insights on the optimality of parameterized wiener filters for
  speech enhancement applications</title><categories>eess.AS cs.SD eess.SP</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a unified framework for defining a family of noise
reduction techniques for speech enhancement applications. The proposed approach
provides a unique theoretical foundation for some widely-applied soft and hard
time-frequency masks, which encompasses the well-known Wiener filter and the
heuristically-designed Binary mask. These techniques can now be considered as
optimal solutions of the same minimization problem. The proposed cost function
is defined by two design parameters that not only establish a desired trade-off
between noise reduction and speech distortion, but also provide an insightful
relationship with the mask morphology. Such characteristic may be useful for
applications that require online adaptation of the suppression function
according to variations of the acoustic scenario. Simulation examples indicate
that the derived conformable suppression mask has approximately the same
quality and intelligibility performance capability of the classical
heuristically-defined parametric Wiener filter. The proposed approach may be of
special interest for real-time embedded speech enhancement applications such as
hearing aids and cochlear implants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07435</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07435</id><created>2018-09-19</created><authors><author><keyname>De Asis</keyname><forenames>Kristopher</forenames></author><author><keyname>Bennett</keyname><forenames>Brendan</forenames></author><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>Predicting Periodicity with Temporal Difference Learning</title><categories>cs.LG cs.AI eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal difference (TD) learning is an important approach in reinforcement
learning, as it combines ideas from dynamic programming and Monte Carlo methods
in a way that allows for online and incremental model-free learning. A key idea
of TD learning is that it is learning predictive knowledge about the
environment in the form of value functions, from which it can derive its
behavior to address long-term sequential decision making problems. The agent's
horizon of interest, that is, how immediate or long-term a TD learning agent
predicts into the future, is adjusted through a discount rate parameter. In
this paper, we introduce an alternative view on the discount rate, with insight
from digital signal processing, to include complex-valued discounting. Our
results show that setting the discount rate to appropriately chosen complex
numbers allows for online and incremental estimation of the Discrete Fourier
Transform (DFT) of a signal of interest with TD learning. We thereby extend the
types of knowledge representable by value functions, which we show are
particularly useful for identifying periodic effects in the reward sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07454</identifier>
 <datestamp>2019-05-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07454</id><created>2018-09-19</created><updated>2019-05-15</updated><authors><author><keyname>Luo</keyname><forenames>Yi</forenames></author><author><keyname>Mesgarani</keyname><forenames>Nima</forenames></author></authors><title>Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for
  Speech Separation</title><categories>cs.SD cs.LG eess.AS</categories><comments>Accepted by IEEE/ACM Transactions on Audio, Speech and Language
  Processing. This version is the authors' version and may vary from the final
  publication in details</comments><doi>10.1109/TASLP.2019.2915167</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Single-channel, speaker-independent speech separation methods have recently
seen great progress. However, the accuracy, latency, and computational cost of
such methods remain insufficient. The majority of the previous methods have
formulated the separation problem through the time-frequency representation of
the mixed signal, which has several drawbacks, including the decoupling of the
phase and magnitude of the signal, the suboptimality of time-frequency
representation for speech separation, and the long latency in calculating the
spectrograms. To address these shortcomings, we propose a fully-convolutional
time-domain audio separation network (Conv-TasNet), a deep learning framework
for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder
to generate a representation of the speech waveform optimized for separating
individual speakers. Speaker separation is achieved by applying a set of
weighting functions (masks) to the encoder output. The modified encoder
representations are then inverted back to the waveforms using a linear decoder.
The masks are found using a temporal convolutional network (TCN) consisting of
stacked 1-D dilated convolutional blocks, which allows the network to model the
long-term dependencies of the speech signal while maintaining a small model
size. The proposed Conv-TasNet system significantly outperforms previous
time-frequency masking methods in separating two- and three-speaker mixtures.
Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude
masks in two-speaker speech separation as evaluated by both objective
distortion measures and subjective quality assessment by human listeners.
Finally, Conv-TasNet has a significantly smaller model size and a shorter
minimum latency, making it a suitable solution for both offline and real-time
speech separation applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07470</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07470</id><created>2018-09-20</created><authors><author><keyname>Rasekh</keyname><forenames>Maryam Eslami</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Madhow</keyname><forenames>Upamanyu</forenames></author></authors><title>Joint Routing and Resource Allocation for Millimeter Wave Picocellular
  Backhaul</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Picocellular architectures are essential for providing the spatial reuse
required to satisfy the ever-increasing demand for mobile data. A key
deployment challenge is to provide backhaul connections with sufficiently high
data rate. Providing wired support (e.g., using optical fiber) to pico base
stations deployed opportunistically on lampposts and rooftops is impractical,
hence wireless backhaul becomes an attractive approach. A multihop mesh network
comprised of directional millimeter wave links is considered here for this
purpose. The backhaul design problem is formulated as one of joint routing and
resource allocation, accounting for mutual interference across simultaneously
active links. A computationally tractable formulation is developed by
leveraging the localized nature of interference and the provable existence of a
sparse optimal allocation. Numerical results are provided for millimeter (mm)
wave mesh networks, which are well suited for scaling backhaul data rates due
to abundance of spectrum, and the ability to form highly directional,
electronically steerable beams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07475</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07475</id><created>2018-09-20</created><authors><author><keyname>AlShehhi</keyname><forenames>Hamad</forenames></author><author><keyname>Alzarouni</keyname><forenames>Mariam</forenames></author><author><keyname>AlYammahi</keyname><forenames>Noura</forenames></author><author><keyname>Shubair</keyname><forenames>Raed</forenames></author><author><keyname>Ali</keyname><forenames>Nazar</forenames></author></authors><title>Compact Low-Profile Wearable Antennas For Breast Cancer Detection</title><categories>eess.SP</categories><comments>78 pages, 44 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many lives can be saved if tumors are detected in early stages, which can
result in a bigger chance for recovery. Many patients find it irritating to get
regular checkups due to the fact that the majority of the monitoring systems
are complicated, not available everywhere and not mobile. Furthermore, for
medical field applications, micro-strip antennas are efficient and have
flexible properties that are utilized in imaging, diagnosis and treatment. It
is known that breast cancer is the most common type of cancer in the world, and
the earlier its been detected the better. In the early stages of breast cancer,
getting rid of from the tumors is much easier and more guaranteed. Nowadays,
the main method that is used in the hospitals for breast cancer detection is
the Ultra-Wideband method (UWB). However, Many patients find it irritating to
get regular check ups due to the fact that the majority of the monitoring
systems are complicated and not mobile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07524</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07524</id><created>2018-09-20</created><authors><author><keyname>An</keyname><forenames>Inkyu</forenames></author><author><keyname>Lee</keyname><forenames>Doheon</forenames></author><author><keyname>Choi</keyname><forenames>Jung-woo</forenames></author><author><keyname>Manocha</keyname><forenames>Dinesh</forenames></author><author><keyname>Yoon</keyname><forenames>Sung-eui</forenames></author></authors><title>Diffraction-Aware Sound Localization for a Non-Line-of-Sight Source</title><categories>cs.RO cs.SD eess.AS</categories><comments>Submitted to ICRA 2019. The working video is available at
  (https://www.youtube.com/watch?v=qqf7jM45bz4)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a novel sound localization algorithm for a non-line-of-sight
(NLOS) sound source in indoor environments. Our approach exploits the
diffraction properties of sound waves as they bend around a barrier or an
obstacle in the scene. We combine a ray tracing based sound propagation
algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate
bending effects by placing a virtual sound source on a wedge in the
environment. We precompute the wedges of a reconstructed mesh of an indoor
scene and use them to generate diffraction acoustic rays to localize the 3D
position of the source. Our method identifies the convergence region of those
generated acoustic rays as the estimated source position based on a particle
filter. We have evaluated our algorithm in multiple scenarios consisting of a
static and dynamic NLOS sound source. In our tested cases, our approach can
localize a source position with an average accuracy error, 0.7m, measured by
the L2 distance between estimated and actual source locations in a 7m*7m*3m
room. Furthermore, we observe 37% to 130% improvement in accuracy over a
state-of-the-art localization method that does not model diffraction effects,
especially when a sound source is not visible to the robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07549</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07549</id><created>2018-09-20</created><authors><author><keyname>Lin</keyname><forenames>Shoufeng</forenames></author></authors><title>Evaluating MCC-PHAT for the LOCATA Challenge - Task 1 and Task 3</title><categories>eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents test results for the \mbox{LOCATA} challenge
\cite{lollmann2018locata} using the recently developed MCC-PHAT (multichannel
cross correlation - phase transform) sound source localization method. The
specific tasks addressed are respectively the localization of a single static
and a single moving speakers using sound recordings of a variety of static
microphone arrays. The test results are compared with those of the MUSIC
(multiple signal classification) method. The optimal subpattern assignment
(OSPA) metric is used for quantitative performance evaluation. In most cases,
the MCC-PHAT method demonstrates more reliable and accurate location estimates,
in comparison with those of the MUSIC method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07575</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07575</id><created>2018-09-20</created><authors><author><keyname>Brunner</keyname><forenames>Gino</forenames></author><author><keyname>Wang</keyname><forenames>Yuyi</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author><author><keyname>Zhao</keyname><forenames>Sumu</forenames></author></authors><title>Symbolic Music Genre Transfer with CycleGAN</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Paper accepted at the 30th International Conference on Tools with
  Artificial Intelligence, ICTAI 2018, Volos, Greece</comments><acm-class>I.2.1; I.2.4; I.2.6; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep generative models such as Variational Autoencoders (VAEs) and Generative
Adversarial Networks (GANs) have recently been applied to style and domain
transfer for images, and in the case of VAEs, music. GAN-based models employing
several generators and some form of cycle consistency loss have been among the
most successful for image domain transfer. In this paper we apply such a model
to symbolic music and show the feasibility of our approach for music genre
transfer. Evaluations using separate genre classifiers show that the style
transfer works well. In order to improve the fidelity of the transformed music,
we add additional discriminators that cause the generators to keep the
structure of the original music mostly intact, while still achieving strong
genre transfer. Visual and audible results further show the potential of our
approach. To the best of our knowledge, this paper represents the first
application of GANs to symbolic music domain transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07600</identifier>
 <datestamp>2018-09-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07600</id><created>2018-09-20</created><authors><author><keyname>Brunner</keyname><forenames>Gino</forenames></author><author><keyname>Konrad</keyname><forenames>Andres</forenames></author><author><keyname>Wang</keyname><forenames>Yuyi</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>MIDI-VAE: Modeling Dynamics and Instrumentation of Music with
  Applications to Style Transfer</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Paper accepted at the 19th International Society for Music
  Information Retrieval Conference, ISMIR 2018, Paris, France</comments><acm-class>I.2.1; I.2.4; I.2.6; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce MIDI-VAE, a neural network model based on Variational
Autoencoders that is capable of handling polyphonic music with multiple
instrument tracks, as well as modeling the dynamics of music by incorporating
note durations and velocities. We show that MIDI-VAE can perform style transfer
on symbolic music by automatically changing pitches, dynamics and instruments
of a music piece from, e.g., a Classical to a Jazz style. We evaluate the
efficacy of the style transfer by training separate style validation
classifiers. Our model can also interpolate between short pieces of music,
produce medleys and create mixtures of entire songs. The interpolations
smoothly change pitches, dynamics and instrumentation to create a harmonic
bridge between two music pieces. To the best of our knowledge, this work
represents the first successful attempt at applying neural style transfer to
complete musical compositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07706</identifier>
 <datestamp>2019-02-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07706</id><created>2018-09-19</created><authors><author><keyname>Yang</keyname><forenames>Mu</forenames></author><author><keyname>Liu</keyname><forenames>Zheng-Hao</forenames></author><author><keyname>Cheng</keyname><forenames>Ze-Di</forenames></author><author><keyname>Xu</keyname><forenames>Jin-Shi</forenames></author><author><keyname>Li</keyname><forenames>Chuan-Feng</forenames></author><author><keyname>Guo</keyname><forenames>Guang-Can</forenames></author></authors><title>Deep Hybrid Scattering Image Learning</title><categories>eess.IV cs.LG physics.optics</categories><comments>8 pages, 6 figures</comments><journal-ref>J. Phys. D: Appl. Phys. 52 115105 (2019)</journal-ref><doi>10.1088/1361-6463/aafa3c</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-trained deep neural network is shown to gain capability of
simultaneously restoring two kinds of images, which are completely destroyed by
two distinct scattering medias respectively. The network, based on the U-net
architecture, can be trained by blended dataset of speckles-reference images
pairs. We experimentally demonstrate the power of the network in reconstructing
images which are strongly diffused by glass diffuser or multi-mode fiber. The
learning model further shows good generalization ability to reconstruct images
that are distinguished from the training dataset. Our work facilitates the
study of optical transmission and expands machine learning's application in
optics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07786</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07786</id><created>2018-09-20</created><authors><author><keyname>Sobhaninia</keyname><forenames>Zahra</forenames></author><author><keyname>Rezaei</keyname><forenames>Safiyeh</forenames></author><author><keyname>Noroozi</keyname><forenames>Alireza</forenames></author><author><keyname>Ahmadi</keyname><forenames>Mehdi</forenames></author><author><keyname>Zarrabi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Karimi</keyname><forenames>Nader</forenames></author><author><keyname>Emami</keyname><forenames>Ali</forenames></author><author><keyname>Samavi</keyname><forenames>Shadrokh</forenames></author></authors><title>Brain Tumor Segmentation Using Deep Learning by Type Specific Sorting of
  Images</title><categories>cs.CV eess.IV</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently deep learning has been playing a major role in the field of computer
vision. One of its applications is the reduction of human judgment in the
diagnosis of diseases. Especially, brain tumor diagnosis requires high
accuracy, where minute errors in judgment may lead to disaster. For this
reason, brain tumor segmentation is an important challenge for medical
purposes. Currently several methods exist for tumor segmentation but they all
lack high accuracy. Here we present a solution for brain tumor segmenting by
using deep learning. In this work, we studied different angles of brain MR
images and applied different networks for segmentation. The effect of using
separate networks for segmentation of MR images is evaluated by comparing the
results with a single network. Experimental evaluations of the networks show
that Dice score of 0.73 is achieved for a single network and 0.79 in obtained
for multiple networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07805</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07805</id><created>2018-09-19</created><authors><author><keyname>Loh</keyname><forenames>Tian Hong</forenames></author><author><keyname>Cheadle</keyname><forenames>David</forenames></author><author><keyname>Miller</keyname><forenames>Philip</forenames></author></authors><title>A Millimeter Wave MIMO Testbed for 5G Communications</title><categories>eess.SP</categories><comments>89th ARFTG Microwave Measurement Conference (ARFTG 2017)</comments><doi>10.1109/ARFTG.2017.8000845</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a 2 x 2 millimeter wave (mm-wave)
multiple-input-multiple-output (MIMO) testbed that operates at around 30 GHz.
The link assessment of the system operating at 26.25 GHz was carried out on a
test bench, with a short communication distance between the transmitting and
receiving antennas. A user-programmable, reconfigurable and real-time signal
processing field-programmable gate arrays (FPGAs)-based software defined radio
(SDR) system was employed as part of the testbed to validate the system-level
performance for a downlink time division long-term evolution (TD-LTE) duplex
scheme. Constellation diagram for quadrature phase shift keying (QPSK) digital
modulation were acquired while the testbed was operating at 30 GHz. The testbed
could be employed for the development of signal test, communication algorithm
and measurement metrology for 5G communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07811</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07811</id><created>2018-09-19</created><authors><author><keyname>Brown</keyname><forenames>Tim</forenames></author><author><keyname>Humphreys</keyname><forenames>David</forenames></author><author><keyname>Hudlicka</keyname><forenames>Martin</forenames></author><author><keyname>Loh</keyname><forenames>Tian Hong</forenames></author></authors><title>Prediction of SINR using BER and EVM for Massive MIMO Applications</title><categories>eess.SP</categories><comments>12th European Conference on Antennas and Propagation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future communication systems employing massive multiple input multiple output
will not have the ability to use channel state information at the mobile user
terminals. Instead, it will be necessary for such devices to evaluate the
downlink signal to interference and noise ratio (SINR) with interference both
from the base station serving other users within the same cell and other base
stations from adjacent cells. The SINR will act as an indicator of how well the
precoders have been applied at the base station. The results presented in this
paper from a 32 x 3 massive MIMO channel sounder measurement campaign at 2.4
GHz show how the received bit error rate and error vector magnitudes can be
used to obtain a prediction of both the average and dynamically changing SINR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07814</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07814</id><created>2018-09-19</created><authors><author><keyname>Wang</keyname><forenames>Min</forenames></author><author><keyname>Zhao</keyname><forenames>Yongjiu</forenames></author><author><keyname>Loh</keyname><forenames>Tian Hong</forenames></author><author><keyname>Xu</keyname><forenames>Qian</forenames></author><author><keyname>Zhou</keyname><forenames>Yonggang</forenames></author></authors><title>Efficient Uncertainty Evaluation of Vector Network Analyser Measurements
  Using Two-Tier Bayesian Analysis and Monte Carlo Method</title><categories>eess.SP</categories><comments>12th European Conference on Antennas and Propagation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Antennas are a key element in any communication system and vector network
analyser (VNA) is popular tool for charactering antenna impedance bandwidth. In
this paper, an efficient uncertainty evaluation method is proposed for VNA
measurement based on its uncertainty propagation mechanism using Bayesian
analysis and Monte Carlo method. The proposed method is generic and can be
applied to VNA with arbitrary number of ports. In order to obtain the complete
information of measurement uncertainty distribution, a two-tier Bayesian
analytic process is carried out. The proposed method contains three steps. In
the first step, the posterior distribution of each uncertainty source of VNA
calibrations is deduced by the use of prior and current sample information
through the first-tier Bayesian analysis. In the second step, the obtained
posterior distributions of uncertainty sources are taken into the Monte Carlo
simulation of one-port VNA measurement uncertainties. In the last step, the
results obtained in the second step are used as the prior distribution of the
secondary Bayesian evaluation, then the evaluation results of the measurement
uncertainty can be obtained with the means, variances and skewness of the
probabilistic distribution. The numerical analysis using an antenna measurement
results demonstrate the high-efficiency and reliability of this proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07817</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07817</id><created>2018-09-19</created><authors><author><keyname>Khan</keyname><forenames>Zia Ullah</forenames></author><author><keyname>Jilani</keyname><forenames>Syeda Fizzah</forenames></author><author><keyname>Belenguer</keyname><forenames>Angel</forenames></author><author><keyname>Loh</keyname><forenames>Tian Hong</forenames></author><author><keyname>Alomainy</keyname><forenames>Akram</forenames></author></authors><title>Empty Substrate Integrated Waveguide-Fed MMW Aperture-Coupled Patch
  Antenna for 5G Applications</title><categories>eess.SP</categories><comments>12th European Conference on Antennas and Propagation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a microstrip square patch antenna with a novel low-cost
and efficient feeding mechanism for millimetre-wave frequencies targeting at 28
GHz. The radiating patch is excited by the technique of empty substrate
integrated waveguide, and two different slot configurations have been examined
in this regard. It has been observed that the antenna gain profile and
radiation efficiency is significantly improved in both configurations as
compared to the microstrip square patch antenna provided with conventional
feeding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07818</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07818</id><created>2018-09-19</created><authors><author><keyname>Xu</keyname><forenames>Qian</forenames></author><author><keyname>Huang</keyname><forenames>Yi</forenames></author><author><keyname>Loh</keyname><forenames>Tian-Hong</forenames></author><author><keyname>Stanley</keyname><forenames>Manoj</forenames></author><author><keyname>Xing</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Min</forenames></author><author><keyname>Gan</keyname><forenames>Hui</forenames></author></authors><title>Single Layer PCB Broadband Circular Polarisation Millimetre Wave Massive
  MIMO Array</title><categories>eess.SP</categories><comments>12th European Conference on Antennas and Propagation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A single layer broadband millimetre wave (mm-wave) array with circular
polarisation is proposed for the fifth generation (5G) mm-wave communication
applications. The antenna element is designed to integrate the matching circuit
into a single layer. Hence no additional matching and feeding networks are
required which makes the antenna element very easy to synthesise a massive
array. The array has a 10 dB return loss bandwidth between 26 GHz and 32 GHz.
The polar angle axial ratio is less than 3 within the beamwidth of -60 degree
to 60 degree. The proposed antenna structure is simple, low cost and robust,
which we envisage suitable for the massive multiple-input and multiple-output
(MIMO) applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07824</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07824</id><created>2018-09-20</created><authors><author><keyname>Lakretz</keyname><forenames>Yair</forenames></author><author><keyname>Chechik</keyname><forenames>Gal</forenames></author><author><keyname>Cohen</keyname><forenames>Evan-Gary</forenames></author><author><keyname>Treves</keyname><forenames>Alessandro</forenames></author><author><keyname>Friedmann</keyname><forenames>Naama</forenames></author></authors><title>Metric Learning for Phoneme Perception</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric functions for phoneme perception capture the similarity structure
among phonemes in a given language and therefore play a central role in
phonology and psycho-linguistics. Various phenomena depend on phoneme
similarity, such as spoken word recognition or serial recall from verbal
working memory. This study presents a new framework for learning a metric
function for perceptual distances among pairs of phonemes. Previous studies
have proposed various metric functions, from simple measures counting the
number of phonetic dimensions that two phonemes share (place-,
manner-of-articulation and voicing), to more sophisticated ones such as
deriving perceptual distances based on the number of natural classes that both
phonemes belong to. However, previous studies have manually constructed the
metric function, which may lead to unsatisfactory account of the empirical
data. This study presents a framework to derive the metric function from
behavioral data on phoneme perception using learning algorithms. We first show
that this approach outperforms previous metrics suggested in the literature in
predicting perceptual distances among phoneme pairs. We then study several
metric functions derived by the learning algorithms and show how perceptual
saliencies of phonological features can be derived from them. For English, we
show that the derived perceptual saliencies are in accordance with a previously
described order among phonological features and show how the framework extends
the results to more features. Finally, we explore how the metric function and
perceptual saliencies of phonological features may vary across languages. To
this end, we compare results based on two English datasets and a new dataset
that we have collected for Hebrew.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07825</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07825</id><created>2018-09-19</created><authors><author><keyname>Loh</keyname><forenames>Tian Hong</forenames></author><author><keyname>Humphreys</keyname><forenames>David</forenames></author><author><keyname>Cheadle</keyname><forenames>David</forenames></author><author><keyname>Buisman</keyname><forenames>Koen</forenames></author></authors><title>An Evaluation of Distortion and Interference Sources originating Within
  a Millimeter-wave MIMO Testbed for 5G Communications</title><categories>eess.SP</categories><comments>2nd URSI Atlantic Radio Science Meeting (URSI AT-RASC 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an evaluation of distortion and interference sources,
namely, the harmonic distortion and antenna crosstalk, originating within a 2 x
2 millimeter-wave (mm-wave) multiple-input-multiple-output (MIMO) testbed. The
experience gained through the insight into the built testbed could be fed into
the design of future mm-wave massive MIMO testbeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07826</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07826</id><created>2018-09-19</created><authors><author><keyname>Wang</keyname><forenames>Min</forenames></author><author><keyname>Loh</keyname><forenames>Tian-Hong</forenames></author><author><keyname>Cheadle</keyname><forenames>David</forenames></author><author><keyname>Zhao</keyname><forenames>Yongjiu</forenames></author><author><keyname>Zhou</keyname><forenames>Yonggang</forenames></author></authors><title>A rigorous link performance and measurement uncertainty assessment for
  MIMO OTA characterisation</title><categories>eess.SP</categories><comments>The Loughborough Antennas &amp; Propagation Conference 2018 (LAPC 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a rigorous link performance test for MIMO OTA
(multiple-input-multiple-output over-the-air) characterisation with traceable
signal-to-interference-plus-noise ratio (SINR) is presented. Measurements were
made with three different testbeds which represent the 4G (fourth-generation)
LTE (long term evolution) SISO (single-input-single-output), 4G LTE MIMO and 5G
(fifth-generation) millimetre-wave (mm-wave) MIMO communication systems,
respectively, in the small antenna radiated test (SMART) screened fully
anechoic chamber, screened control room and reverberation chamber at the UK
National Physical Laboratory (NPL). The measurement campaign comprised of
automated data acquisition of channel power, downlink (DL) &amp; uplink (UL) error
vector magnitude (EVM) and throughput. The measurement repeatability has been
assessed with standard deviation plotted as error bars and the uncertainty
sources in the OTA test are analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.07827</identifier>
 <datestamp>2018-09-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.07827</id><created>2018-09-19</created><authors><author><keyname>Loh</keyname><forenames>Tian-Hong</forenames></author><author><keyname>Qi</keyname><forenames>Wanquan</forenames></author></authors><title>A comparison of MIMO antenna efficiency measurements performed in
  Anechoic Chamber and Reverberation Chamber</title><categories>eess.SP</categories><comments>85th Microwave Measurement Conference (ARFTG 2015)</comments><doi>10.1109/ARFTG.2015.7162900</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input-multiple-output (MIMO) antenna will play a key role in the
development of fifth generation (5G) wireless mobile communication systems due
to their performance-enhancement capability in multipath environment. Antenna
radiation efficiency is an important parameter for MIMO antenna system. In this
paper, we present a comparison of MIMO antenna efficiency measurements
performed in Anechoic Chamber (AC) and Reverberation Chamber (RC) at the UK
National Physical Laboratory. Two commercial available directional dual
polarized full LTE band MIMO antennas were measured both in AC and RC between 1
GHz and 3 GHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08001</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08001</id><created>2018-09-21</created><updated>2018-11-02</updated><authors><author><keyname>Chung</keyname><forenames>Soo-Whan</forenames></author><author><keyname>Chung</keyname><forenames>Joon Son</forenames></author><author><keyname>Kang</keyname><forenames>Hong-Goo</forenames></author></authors><title>Perfect match: Improved cross-modal embeddings for audio-visual
  synchronisation</title><categories>cs.CV cs.SD eess.AS</categories><comments>Preprint. Work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new strategy for learning powerful cross-modal
embeddings for audio-to-video synchronization. Here, we set up the problem as
one of cross-modal retrieval, where the objective is to find the most relevant
audio segment given a short video clip. The method builds on the recent
advances in learning representations from cross-modal self-supervision.
  The main contributions of this paper are as follows: (1) we propose a new
learning strategy where the embeddings are learnt via a multi-way matching
problem, as opposed to a binary classification (matching or non-matching)
problem as proposed by recent papers; (2) we demonstrate that performance of
this method far exceeds the existing baselines on the synchronization task; (3)
we use the learnt embeddings for visual speech recognition in self-supervision,
and show that the performance matches the representations learnt end-to-end in
a fully-supervised manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08088</identifier>
 <datestamp>2020-01-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08088</id><created>2018-09-17</created><authors><author><keyname>Matavalam</keyname><forenames>Amarsagar Reddy Ramapuram</forenames></author><author><keyname>Ajjarapu</keyname><forenames>Venkataramana</forenames></author></authors><title>PMU based Monitoring and Mitigation of Delayed Voltage Recovery using
  Admittances</title><categories>eess.SP</categories><comments>Submitted to IEEE Transactions on Power Systems</comments><doi>10.1109/TPWRS.2019.2913742</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the delayed voltage recovery phenomenon by simplifying
the Western Electricity Coordinating Council (WECC) composite load model using
the load admittance and the thermal relay dynamics. From this analysis, a
closed form expression approximating the recovery time is derived and the key
load parameters impacting the behavior of voltage recovery are identified. A
monitoring scheme based on the measured load admittance is then proposed to
detect the onset of stalling, even in the presence of voltage oscillations, and
estimate the duration of the delayed voltage recovery. A mitigation scheme
utilizing smart thermostats and offline learning is also derived to ensure that
the voltage recovers to the pre-contingency voltage within a specified time.
Both the monitoring and mitigation schemes only need local measurements at a
substation making them promising for online applications. Results for the
monitoring and mitigation schemes are described in detail for the IEEE 162 bus
system validating the various assumptions used for the analysis and
establishing the connection between the delayed voltage recovery phenomenon and
load admittance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08274</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08274</id><created>2018-08-30</created><authors><author><keyname>Rojas-Cessa</keyname><forenames>Roberto</forenames></author><author><keyname>Wong</keyname><forenames>Chuan-Kuo</forenames></author><author><keyname>Jiang</keyname><forenames>Zhengqi</forenames></author><author><keyname>Shah</keyname><forenames>Haard</forenames></author><author><keyname>Grebel</keyname><forenames>Haim</forenames></author><author><keyname>Mohamed</keyname><forenames>Ahmed</forenames></author></authors><title>An Energy Packet Switch for Digital Power Grids</title><categories>eess.SP cs.SY</categories><comments>This work was presented in IEEE iThing 2018, Jul. 30-Aug. 3, 2018.
  Keywords: Digital grid, energy packet switch, power switch, power grid
  Internet, boolean grid, quantum grid, energy transfer, discrete energy,
  digital energy</comments><report-no>NRL20180401EPS</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the design and electrical description of an energy packet switch
for forwarding and delivery of energy in digital power grids in this paper. The
proposed switch may receive energy from one or multiple power sources in the
form of energy packets, store them and aggregate the contained energy, and
forward the accumulated energy to requesting loads connected to one or multiple
output ports of the switch. Energy packets are discrete amounts of energy that
are associated in- or out-of-band with an address and other metadata. Loads
receive these discrete amounts of finely-controlled energy rather than
discretionary amounts after. The control and management of the proposed switch
are based on a request-grant protocol. Using energy packets helps to manage the
delivery of power in a reliable, robust, and function form that may enable
features not yet available in the present power grid. The switch, as any
element of a digital grid, uses a data network for the transmission of these
requests and grants. The energy packet switch may be the centerpiece for
creating infrastructure in the realization of the digital power grid. The
design of the energy packet switch is based on shared supercapacitors to shape
and manage discretization of energy. We introduce the design and analysis of
the electrical properties of the proposed switch and describe the procedure
used in the switch to determine the amount of energy transmitted to requesting
loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08353</identifier>
 <datestamp>2019-05-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08353</id><created>2018-09-21</created><updated>2019-05-30</updated><authors><author><keyname>Ioannidis</keyname><forenames>Vassilis N.</forenames></author><author><keyname>Zamzam</keyname><forenames>Ahmed S.</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Nicholas D.</forenames></author></authors><title>Coupled Graphs and Tensor Factorization for Recommender Systems and
  Community Detection</title><categories>stat.ML cs.LG cs.SI eess.SP</categories><comments>This paper is submitted to the IEEE Transactions on Knowledge and
  Data Engineering. A preliminary version of this work was accepted for
  presentation in the special track of GlobalSIP on Tensor Methods for Signal
  Processing and Machine Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint analysis of data from multiple information repositories facilitates
uncovering the underlying structure in heterogeneous datasets. Single and
coupled matrix-tensor factorization (CMTF) has been widely used in this context
for imputation-based recommendation from ratings, social network, and other
user-item data. When this side information is in the form of item-item
correlation matrices or graphs, existing CMTF algorithms may fall short.
Alleviating current limitations, we introduce a novel model coined coupled
graph-tensor factorization (CGTF) that judiciously accounts for graph-related
side information. The CGTF model has the potential to overcome practical
challenges, such as missing slabs from the tensor and/or missing rows/columns
from the correlation matrices. A novel alternating direction method of
multipliers (ADMM) is also developed that recovers the nonnegative factors of
CGTF. Our algorithm enjoys closed-form updates that result in reduced
computational complexity and allow for convergence claims. A novel direction is
further explored by employing the interpretable factors to detect graph
communities having the tensor as side information. The resulting community
detection approach is successful even when some links in the graphs are
missing. Results with real data sets corroborate the merits of the proposed
methods relative to state-of-the-art competing factorization techniques in
providing recommendations and detecting communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08410</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08410</id><created>2018-09-22</created><authors><author><keyname>Tung</keyname><forenames>Kuan</forenames></author><author><keyname>Liu</keyname><forenames>Po-Kang</forenames></author><author><keyname>Chuang</keyname><forenames>Yu-Chuan</forenames></author><author><keyname>Wang</keyname><forenames>Sheng-Hui</forenames></author><author><keyname>Wu</keyname><forenames>An-Yeu</forenames></author></authors><title>Entropy-Assisted Multi-Modal Emotion Recognition Framework Based on
  Physiological Signals</title><categories>q-bio.QM cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the result of the growing importance of the Human Computer Interface
system, understanding human's emotion states has become a consequential ability
for the computer. This paper aims to improve the performance of emotion
recognition by conducting the complexity analysis of physiological signals.
Based on AMIGOS dataset, we extracted several entropy-domain features such as
Refined Composite Multi-Scale Entropy (RCMSE), Refined Composite Multi-Scale
Permutation Entropy (RCMPE) from ECG and GSR signals, and Multivariate
Multi-Scale Entropy (MMSE), Multivariate Multi-Scale Permutation Entropy (MMPE)
from EEG, respectively. The statistical results show that RCMSE in GSR has a
dominating performance in arousal, while RCMPE in GSR would be the excellent
feature in valence. Furthermore, we selected XGBoost model to predict emotion
and get 68% accuracy in arousal and 84% in valence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08459</identifier>
 <datestamp>2018-10-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08459</id><created>2018-09-22</created><updated>2018-10-04</updated><authors><author><keyname>Brown</keyname><forenames>Daniel C.</forenames></author><author><keyname>Johnson</keyname><forenames>Shawn F.</forenames></author><author><keyname>Brownstead</keyname><forenames>Cale F.</forenames></author></authors><title>Simulation and Testing Results for a Sub-Bottom Imaging Sonar</title><categories>eess.SP</categories><comments>10 pages, 7 figures Revision: Fixed typo in author name</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting buried unexploded ordnance (UXO) is addressed with a
sensor deployed from a shallow-draft surface vessel. This sonar system produces
three dimensional synthetic aperture sonar (SAS) imagery of both surficial and
buried UXO across a range of environments. The sensor's hardware design was
based in part upon data created using a hybrid modeling approach that combined
results from separate environmental scattering and target scattering models.
This hybrid model produced synthetic sensor data where the
sensor/environment/target space could be modified to explore the expected
operating conditions. The simulated data were also used to adapt a set of
existing signal processing algorithms for formation of three-dimensional
acoustic imagery.
  Recently, the sonar system has been integrated to a test platform, and
experiments have been conducted at a trial site in the Foster Joseph Sayers
Reservoir near Howard, PA. This test site has been prepared with several buried
man-made objects. Initial results show that fully buried targets can be
detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08514</identifier>
 <datestamp>2019-03-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08514</id><created>2018-09-22</created><updated>2019-03-27</updated><authors><author><keyname>Soltani</keyname><forenames>Ramin</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author></authors><title>Fundamental Limits of Invisible Flow Fingerprinting</title><categories>cs.NI cs.CR cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network flow fingerprinting can be used to de-anonymize communications on
anonymity systems such as Tor by linking the ingress and egress segments of
anonymized connections. Assume Alice and Bob have access to the input and the
output links of an anonymous network, respectively, and they wish to
collaboratively reveal the connections between the input and the output links
without being detected by Willie who protects the network. Alice generates a
codebook of fingerprints, where each fingerprint corresponds to a unique
sequence of inter-packet delays and shares it only with Bob. For each input
flow, she selects a fingerprint from the codebook and embeds it in the flow,
i.e., changes the packet timings of the flow to follow the packet timings
suggested by the fingerprint, and Bob extracts the fingerprints from the output
flows. We model the network as parallel $M/M/1$ queues where each queue is
shared by a flow from Alice to Bob and other flows independent of the flow from
Alice to Bob. The timings of the flows are governed by independent Poisson
point processes. Assuming all input flows have equal rates and that Bob
observes only flows with fingerprints, we first present two scenarios: 1) Alice
fingerprints all the flows; 2) Alice fingerprints a subset of the flows,
unknown to Willie. Then, we extend the construction and analysis to the case
where flow rates are arbitrary as well as the case where not all the flows that
Bob observes have a fingerprint. For each scenario, we derive the number of
flows that Alice can fingerprint and Bob can trace by fingerprinting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08579</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08579</id><created>2018-09-23</created><authors><author><keyname>Lippuner</keyname><forenames>Stefan</forenames></author><author><keyname>Weber</keyname><forenames>Benjamin</forenames></author><author><keyname>Salomon</keyname><forenames>Mauro</forenames></author><author><keyname>Korb</keyname><forenames>Matthias</forenames></author><author><keyname>Huang</keyname><forenames>Qiuting</forenames></author></authors><title>EC-GSM-IoT Network Synchronization with Support for Large Frequency
  Offsets</title><categories>eess.SP</categories><comments>Wireless Communications and Networking Conference (WCNC), 2018</comments><doi>10.1109/WCNC.2018.8377168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EDGE-based EC-GSM-IoT is a promising candidate for the billion-device
cellular IoT (cIoT), providing similar coverage and battery life as NB-IoT. The
goal of 20 dB coverage extension compared to EDGE poses significant challenges
for the initial network synchronization, which has to be performed well below
the thermal noise floor, down to an SNR of -8.5 dB. We present a low-complexity
synchronization algorithm supporting up to 50 kHz initial frequency offset,
thus enabling the use of a low-cost +/-25 ppm oscillator. The proposed
algorithm does not only fulfill the 3GPP requirements, but surpasses them by 3
dB, enabling communication with an SNR of -11.5 dB or a maximum coupling loss
of up to 170.5 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08662</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08662</id><created>2018-09-23</created><authors><author><keyname>Ostrometzky</keyname><forenames>Jonatan</forenames></author><author><keyname>Messer</keyname><forenames>Hagit</forenames></author></authors><title>On the Information in Extreme Measurements for Parameter Estimation</title><categories>eess.SP cs.IT math.IT</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with parameter estimation from extreme measurements. While
being a special case of parameter estimation from partial data, in scenarios
where only one sample from a given set of K measurements can be extracted,
choosing only the minimum or the maximum (i.e., extreme) value from that set is
of special interest because of the ultra-low energy, storage, and processing
power required to extract extreme values from a given data set. We present a
new methodology to analyze the performance of parameter estimation from extreme
measurements. In particular, we present a general close-form approximation for
the Cramer-Rao Lower Bound on the parameter estimation error, based on extreme
values. We demonstrate our methodology on the case where the original
measurements are exponential distributed, which is related to many practical
applications. The analysis shows that the maximum values carry most of the
information about the parameter of interest and that the additional information
in the minimum is negligible. Moreover, it shows that for small sets of iid
measurements (e.g. K=15) the use of the maximum can provide data compression
with factor 15 while keeping about 50% of the information stored in the
complete set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08783</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08783</id><created>2018-09-24</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Bodhibrata</forenames></author><author><keyname>Anchal</keyname><forenames>Sahil</forenames></author><author><keyname>Kar</keyname><forenames>Subrat</forenames></author></authors><title>Person Identification using Seismic Signals generated from Footfalls</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Footfall based biometric system is perhaps the only person identification
technique which does not hinder the natural movement of an individual. This is
a clear edge over all other biometric systems which require a formidable amount
of human intervention and encroach upon an individual's privacy to some extent
or the other. This paper presents a Fog computing architecture for implementing
footfall based biometric system using widespread geographically distributed
geophones (vibration sensor). Results were stored in an Internet of Things
(IoT) cloud. We have tested our biometric system on an indigenous database
(created by us) containing 46000 footfall events from 8 individuals and
achieved an accuracy of 73%, 90% and 95% in case of 1, 5 and 10 footsteps per
sample. We also proposed a basis pursuit based data compression technique DS8BP
for wireless transmission of footfall events to the Fog. DS8BP compresses the
original footfall events (sampled at 8 kHz) by a factor of 108 and also acts as
a smoothing filter. These experimental results depict the high viability of our
technique in the realm of person identification and access control systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08910</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08910</id><created>2018-09-18</created><authors><author><keyname>Lu</keyname><forenames>Mengqi</forenames></author><author><keyname>Gao</keyname><forenames>Jinfeng</forenames></author><author><keyname>Li</keyname><forenames>Zuyi</forenames></author></authors><title>Functional Intrusive Load Monitor (FILM): A Model-based Platform for
  Non-Intrusive Load Monitoring System Development</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Intrusive Load Monitoring (NILM) is an important application to monitor
household appliance activities and provide related information to house owner
or/and utility company via a single sensor installed at the electrical entry of
the house. It can be used for different purposes in residential and industrial
sectors. Thus, an increasing number of new algorithms have been developed in
recent years. In these algorithms, researchers either use existing public
datasets or collect their own data which causes such problems as insufficiency
of electrical parameters, missing of ground-truth data, absence of many
appliances, and lack of appliance information. To solve these problems, this
paper presents a model-based platform for NILM system development, namely
Functional Intrusive Load Monitor (FILM). By using this platform, the state
transitions and activities of all the involved appliances can be preset by
researchers, and multiple electrical parameters such as harmonics and power
factor can be monitored or calculated. This platform will help researchers save
the time of collecting experimental data, utilize precise control of individual
appliance activities, and develop load signatures of devices. This paper
describes the steps, structure, and requirements of building this platform.
Case study is presented to help understand this platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08911</identifier>
 <datestamp>2019-01-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08911</id><created>2018-09-21</created><updated>2018-10-02</updated><authors><author><keyname>Chen</keyname><forenames>Xiao</forenames></author><author><keyname>Kairouz</keyname><forenames>Peter</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>Understanding Compressive Adversarial Privacy</title><categories>cs.LG cs.CY cs.SY eess.SP stat.ML</categories><journal-ref>2018 IEEE Conference on Decision and Control (CDC)</journal-ref><doi>10.1109/CDC.2018.8619455</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Designing a data sharing mechanism without sacrificing too much privacy can
be considered as a game between data holders and malicious attackers. This
paper describes a compressive adversarial privacy framework that captures the
trade-off between the data privacy and utility. We characterize the optimal
data releasing mechanism through convex optimization when assuming that both
the data holder and attacker can only modify the data using linear
transformations. We then build a more realistic data releasing mechanism that
can rely on a nonlinear compression model while the attacker uses a neural
network. We demonstrate in a series of empirical applications that this
framework, consisting of compressive adversarial privacy, can preserve
sensitive information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08912</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08912</id><created>2018-09-19</created><authors><author><keyname>Buisman</keyname><forenames>Koen</forenames></author><author><keyname>Cheadle</keyname><forenames>David</forenames></author><author><keyname>Loh</keyname><forenames>Tian Hong</forenames></author><author><keyname>Humphreys</keyname><forenames>David</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Millimeter-Wave Over-the-Air Signal-to-Interference-plus-Noise-Ratio
  Measurements Using a MIMO Testbed</title><categories>eess.SP</categories><comments>2nd URSI Atlantic Radio Science Meeting (URSI AT-RASC 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, over-the-air experiments with external and internal
interferences were performed using Chalmers millimeter-wave
multiple-input-multiple-output testbed MATE. The resulting SINR for both
interference experiments are compared and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08914</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08914</id><created>2018-09-20</created><authors><author><keyname>Almazrouei</keyname><forenames>Ebtesam</forenames></author><author><keyname>Shubair</keyname><forenames>Raed M.</forenames></author><author><keyname>Saffre</keyname><forenames>Fabrice</forenames></author></authors><title>Internet of NanoThings: Concepts and Applications</title><categories>cs.ET eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter focuses on Internet of Things from the nanoscale point of view.
The chapter starts with section 1 which provides an introduction of nanothings
and nanotechnologies. The nanoscale communication paradigms and the different
approaches are discussed for nanodevices development. Nanodevice
characteristics are discussed and the architecture of wireless nanodevices are
outlined. Section 2 describes Internet of NanoThing(IoNT), its network
architecture, and the challenges of nanoscale communication which is essential
for enabling IoNT. Section 3 gives some practical applications of IoNT. The
internet of Bio-NanoThing (IoBNT) and relevant biomedical applications are
discussed. Other Applications such as military, industrial, and environmental
applications are also outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.08934</identifier>
 <datestamp>2018-09-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.08934</id><created>2018-09-21</created><authors><author><keyname>Humphreys</keyname><forenames>David A.</forenames></author><author><keyname>Fatadin</keyname><forenames>Irshaad</forenames></author><author><keyname>Bieler</keyname><forenames>Mark</forenames></author><author><keyname>Struszewski</keyname><forenames>Paul</forenames></author><author><keyname>Hudlicka</keyname><forenames>Martin</forenames></author></authors><title>Optical and RF Metrology for 5G</title><categories>eess.SP</categories><comments>2017 IEEE Photonics Society Summer Topical Meeting Series (SUM)</comments><doi>10.1109/PHOSST.2017.8012717</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Specification standards will soon be available for 5G mobile RF
communications. What optical and electrical metrology is needed or available to
support the development of the supporting optical communication systems? Device
measurement, digital oscilloscope impairments and improving system resolution
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09166</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09166</id><created>2018-09-24</created><updated>2018-09-29</updated><authors><author><keyname>Roheda</keyname><forenames>Siddharth</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author><author><keyname>Wu</keyname><forenames>Tianfu</forenames></author></authors><title>Decision Level Fusion: An Event Driven Approach</title><categories>eess.SP</categories><comments>To appear in IEEE EUropean SIgnal Processing COnference (EUSIPCO)
  2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a technique that combines the occurrence of certain
events, as observed by different sensors, in order to detect and classify
objects. This technique explores the extent of dependence between features
being observed by the sensors, and generates more informed probability
distributions over the events. Provided some additional information about the
features of the object, this fusion technique can outperform other existing
decision level fusion approaches that may not take into account the
relationship between different features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09190</identifier>
 <datestamp>2018-09-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09190</id><created>2018-09-24</created><authors><author><keyname>Haghani</keyname><forenames>Parisa</forenames></author><author><keyname>Narayanan</keyname><forenames>Arun</forenames></author><author><keyname>Bacchiani</keyname><forenames>Michiel</forenames></author><author><keyname>Chuang</keyname><forenames>Galen</forenames></author><author><keyname>Gaur</keyname><forenames>Neeraj</forenames></author><author><keyname>Moreno</keyname><forenames>Pedro</forenames></author><author><keyname>Prabhavalkar</keyname><forenames>Rohit</forenames></author><author><keyname>Qu</keyname><forenames>Zhongdi</forenames></author><author><keyname>Waters</keyname><forenames>Austin</forenames></author></authors><title>From Audio to Semantics: Approaches to end-to-end spoken language
  understanding</title><categories>eess.AS cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional spoken language understanding systems consist of two main
components: an automatic speech recognition module that converts audio to a
transcript, and a natural language understanding module that transforms the
resulting text (or top N hypotheses) into a set of domains, intents, and
arguments. These modules are typically optimized independently. In this paper,
we formulate audio to semantic understanding as a sequence-to-sequence problem
[1]. We propose and compare various encoder-decoder based approaches that
optimize both modules jointly, in an end-to-end manner. Evaluations on a
real-world task show that 1) having an intermediate text representation is
crucial for the quality of the predicted semantics, especially the intent
arguments and 2) jointly optimizing the full system improves overall accuracy
of prediction. Compared to independently trained models, our best jointly
trained model achieves similar domain and intent prediction F1 scores, but
improves argument word error rate by 18% relative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09311</identifier>
 <datestamp>2018-09-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09311</id><created>2018-09-25</created><authors><author><keyname>Wang</keyname><forenames>Qiongqiong</forenames></author><author><keyname>Okabe</keyname><forenames>Koji</forenames></author><author><keyname>Lee</keyname><forenames>Kong Aik</forenames></author><author><keyname>Yamamoto</keyname><forenames>Hitoshi</forenames></author><author><keyname>Koshinaka</keyname><forenames>Takafumi</forenames></author></authors><title>Attention Mechanism in Speaker Recognition: What Does It Learn in Deep
  Speaker Embedding?</title><categories>cs.SD eess.AS</categories><comments>SLT 2018 (Workshop on Spoken Language Technology)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an experimental study on deep speaker embedding with an
attention mechanism that has been found to be a powerful representation
learning technique in speaker recognition. In this framework, an attention
model works as a frame selector that computes an attention weight for each
frame-level feature vector, in accord with which an utterancelevel
representation is produced at the pooling layer in a speaker embedding network.
In general, an attention model is trained together with the speaker embedding
network on a single objective function, and thus those two components are
tightly bound to one another. In this paper, we consider the possibility that
the attention model might be decoupled from its parent network and assist other
speaker embedding networks and even conventional i-vector extractors. This
possibility is demonstrated through a series of experiments on a NIST Speaker
Recognition Evaluation (SRE) task, with 9.0% EER reduction and 3.8%
min_Cprimary reduction when the attention weights are applied to i-vector
extraction. Another experiment shows that DNN-based soft voice activity
detection (VAD) can be effectively combined with the attention mechanism to
yield further reduction of minCprimary by 6.6% and 1.6% in deep speaker
embedding and i-vector systems, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09326</identifier>
 <datestamp>2019-01-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09326</id><created>2018-09-25</created><updated>2019-01-28</updated><authors><author><keyname>Michelini</keyname><forenames>Pablo Navarrete</forenames></author><author><keyname>Liu</keyname><forenames>Hanwen</forenames></author><author><keyname>Zhu</keyname><forenames>Dan</forenames></author></authors><title>Multigrid Backprojection Super-Resolution and Deep Filter Visualization</title><categories>eess.IV cs.CV cs.LG eess.SP</categories><comments>Spotlight paper in the Thirty-Third AAAI Conference on Artificial
  Intelligence (AAAI-19)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a novel deep-learning architecture for image upscaling by large
factors (e.g. 4x, 8x) based on examples of pristine high-resolution images. Our
target is to reconstruct high-resolution images from their downscale versions.
The proposed system performs a multi-level progressive upscaling, starting from
small factors (2x) and updating for higher factors (4x and 8x). The system is
recursive as it repeats the same procedure at each level. It is also residual
since we use the network to update the outputs of a classic upscaler. The
network residuals are improved by Iterative Back-Projections (IBP) computed in
the features of a convolutional network. To work in multiple levels we extend
the standard back-projection algorithm using a recursion analogous to
Multi-Grid algorithms commonly used as solvers of large systems of linear
equations. We finally show how the network can be interpreted as a standard
upsampling-and-filter upscaler with a space-variant filter that adapts to the
geometry. This approach allows us to visualize how the network learns to
upscale. Finally, our system reaches state of the art quality for models with
relatively few number of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09342</identifier>
 <datestamp>2018-09-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09342</id><created>2018-09-25</created><authors><author><keyname>Serrano</keyname><forenames>Alexander</forenames></author><author><keyname>Girault</keyname><forenames>Benjamin</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Graph Variogram: A novel tool to measure spatial stationarity</title><categories>eess.SP</categories><comments>Submitted to IEEE Global Conference on Signal and Information
  Processing 2018 (IEEE GlobalSIP 2018), Nov 2018, Anaheim, CA, United States.
  (https://2018.ieeeglobalsip.org/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Irregularly sampling a spatially stationary random field does not yield a
graph stationary signal in general. Based on this observation, we build a
definition of graph stationarity based on intrinsic stationarity, a less
restrictive definition of classical stationarity. We introduce the concept of
graph variogram, a novel tool for measuring spatial intrinsic stationarity at
local and global scales for irregularly sampled signals by selecting subgraphs
of local neighborhoods. Graph variograms are extensions of variograms used for
signals defined on continuous Euclidean space. Our experiments with
intrinsically stationary signals sampled on a graph, demonstrate that graph
variograms yield estimates with small bias of true theoretical models, while
being robust to sampling variation of the space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09573</identifier>
 <datestamp>2019-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09573</id><created>2018-09-25</created><updated>2019-09-19</updated><authors><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author><author><keyname>Lu</keyname><forenames>Yue M.</forenames></author><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author></authors><title>Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview</title><categories>cs.LG cs.IT eess.SP math.IT math.OC math.ST stat.ML stat.TH</categories><comments>Invited overview article</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 67, no. 20, pp.
  5239-5269, October 2019</journal-ref><doi>10.1109/TSP.2019.2937282</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Substantial progress has been made recently on developing provably accurate
and efficient algorithms for low-rank matrix factorization via nonconvex
optimization. While conventional wisdom often takes a dim view of nonconvex
optimization algorithms due to their susceptibility to spurious local minima,
simple iterative methods such as gradient descent have been remarkably
successful in practice. The theoretical footings, however, had been largely
lacking until recently.
  In this tutorial-style overview, we highlight the important role of
statistical models in enabling efficient nonconvex optimization with
performance guarantees. We review two contrasting approaches: (1) two-stage
algorithms, which consist of a tailored initialization step followed by
successive refinement; and (2) global landscape analysis and
initialization-free algorithms. Several canonical matrix factorization problems
are discussed, including but not limited to matrix sensing, phase retrieval,
matrix completion, blind deconvolution, robust principal component analysis,
phase synchronization, and joint alignment. Special care is taken to illustrate
the key technical insights underlying their analyses. This article serves as a
testament that the integrated consideration of optimization and statistics
leads to fruitful research findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09639</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09639</id><created>2018-09-25</created><updated>2019-09-10</updated><authors><author><keyname>Rencker</keyname><forenames>Lucas</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Wang</keyname><forenames>Wenwu</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Sparse Recovery and Dictionary Learning from Nonlinear Compressive
  Measurements</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2941070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding and dictionary learning are popular techniques for linear
inverse problems such as denoising or inpainting. However in many cases, the
measurement process is nonlinear, for example for clipped, quantized or 1-bit
measurements. These problems have often been addressed by solving constrained
sparse coding problems, which can be difficult to solve, and assuming that the
sparsifying dictionary is known and fixed. Here we propose a simple and unified
framework to deal with nonlinear measurements. We propose a cost function that
minimizes the distance to a convex feasibility set, which models our knowledge
about the nonlinear measurement. This provides an unconstrained, convex, and
differentiable cost function that is simple to optimize, and generalizes the
linear least squares cost commonly used in sparse coding. We then propose
proximal based sparse coding and dictionary learning algorithms, that are able
to learn directly from nonlinearly corrupted signals. We show how the proposed
framework and algorithms can be applied to clipped, quantized and 1-bit data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09671</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09671</id><created>2018-09-25</created><authors><author><keyname>Kundu</keyname><forenames>Lopamudra</forenames></author><author><keyname>Xiong</keyname><forenames>Gang</forenames></author><author><keyname>Cho</keyname><forenames>Joonyoung</forenames></author></authors><title>Physical Uplink Control Channel Design for 5G New Radio</title><categories>eess.SP</categories><comments>6 pages, 11 figures, accepted in IEEE 5G World Forum 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The next generation wireless communication system, 5G, or New Radio (NR) will
provide access to information and sharing of data anywhere, anytime by various
users and applications with diverse multi-dimensional requirements. Physical
Uplink Control Channel (PUCCH), which is mainly utilized to convey Uplink
Control Information (UCI), is a fundamental building component to enable NR
system. Compared to Long Term Evolution (LTE), more flexible PUCCH structure is
specified in NR, aiming to support diverse applications and use cases. This
paper describes the design principles of various NR PUCCH formats and the
underlying physical structures. Further, extensive simulation results are
presented to explain the considerations behind the NR PUCCH design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09755</identifier>
 <datestamp>2019-09-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09755</id><created>2018-09-25</created><updated>2019-09-05</updated><authors><author><keyname>Murphy</keyname><forenames>James</forenames></author><author><keyname>Pao</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Yuen</keyname><forenames>Albert</forenames></author></authors><title>Map matching when the map is wrong: Efficient vehicle tracking on- and
  off-road for map learning</title><categories>math.OC cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a sequence of possibly sparse and noisy GPS traces and a map of the
road network, map matching algorithms can infer the most accurate trajectory on
the road network. However, if the road network is wrong (for example due to
missing or incorrectly mapped roads, missing parking lots, misdirected turn
restrictions or misdirected one-way streets) standard map matching algorithms
fail to reconstruct the correct trajectory.
  In this paper, an algorithm to tracking vehicles able to move both on and off
the known road network is formulated. It efficiently unifies existing hidden
Markov model (HMM) approaches for map matching and standard free-space tracking
methods (e.g. Kalman smoothing) in a principled way. The algorithm is a form of
interacting multiple model (IMM) filter subject to an additional assumption on
the type of model interaction permitted, termed here as semi-interacting
multiple model (sIMM) filter. A forward filter (suitable for real-time
tracking) and backward MAP sampling step (suitable for MAP trajectory inference
and map matching) are described. The framework set out here is agnostic to the
specific tracking models used, and makes clear how to replace these components
with others of a similar type. In addition to avoiding generating misleading
map matching trajectories, this algorithm can be applied to learn map features
by detecting unmapped or incorrectly mapped roads and parking lots, incorrectly
mapped turn restrictions and road directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09756</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09756</id><created>2018-09-25</created><authors><author><keyname>Plantinga</keyname><forenames>Peter</forenames></author><author><keyname>Bagchi</keyname><forenames>Deblin</forenames></author><author><keyname>Fosler-Lussier</keyname><forenames>Eric</forenames></author></authors><title>An Exploration of Mimic Architectures for Residual Network Based
  Spectral Mapping</title><categories>cs.SD eess.AS</categories><comments>Published in the IEEE 2018 Workshop on Spoken Language Technology
  (SLT 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral mapping uses a deep neural network (DNN) to map directly from noisy
speech to clean speech. Our previous study found that the performance of
spectral mapping improves greatly when using helpful cues from an acoustic
model trained on clean speech. The mapper network learns to mimic the input
favored by the spectral classifier and cleans the features accordingly. In this
study, we explore two new innovations: we replace a DNN-based spectral mapper
with a residual network that is more attuned to the goal of predicting clean
speech. We also examine how integrating long term context in the mimic
criterion (via wide-residual biLSTM networks) affects the performance of
spectral mapping compared to DNNs. Our goal is to derive a model that can be
used as a preprocessor for any recognition system; the features derived from
our model are passed through the standard Kaldi ASR pipeline and achieve a WER
of 9.3%, which is the lowest recorded word error rate for CHiME-2 dataset using
only feature adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09841</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09841</id><created>2018-09-26</created><authors><author><keyname>Zhang</keyname><forenames>Mingyang</forenames></author><author><keyname>Sisman</keyname><forenames>Berrak</forenames></author><author><keyname>Rallabandi</keyname><forenames>Sai Sirisha</forenames></author><author><keyname>Li</keyname><forenames>Haizhou</forenames></author><author><keyname>Zhao</keyname><forenames>Li</forenames></author></authors><title>Error Reduction Network for DBLSTM-based Voice Conversion</title><categories>eess.AS</categories><comments>Accepted by APSIPA 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So far, many of the deep learning approaches for voice conversion produce
good quality speech by using a large amount of training data. This paper
presents a Deep Bidirectional Long Short-Term Memory (DBLSTM) based voice
conversion framework that can work with a limited amount of training data. We
propose to implement a DBLSTM based average model that is trained with data
from many speakers. Then, we propose to perform adaptation with a limited
amount of target data. Last but not least, we propose an error reduction
network that can improve the voice conversion quality even further. The
proposed framework is motivated by three observations. Firstly, DBLSTM can
achieve a remarkable voice conversion by considering the long-term dependencies
of the speech utterance. Secondly, DBLSTM based average model can be easily
adapted with a small amount of data, to achieve a speech that sounds closer to
the target. Thirdly, an error reduction network can be trained with a small
amount of training data, and can improve the conversion quality effectively.
The experiments show that the proposed voice conversion framework is flexible
to work with limited training data and outperforms the traditional frameworks
in both objective and subjective evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09847</identifier>
 <datestamp>2020-01-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09847</id><created>2018-09-26</created><updated>2020-01-16</updated><authors><author><keyname>Z&#xe1;vi&#x161;ka</keyname><forenames>Pavel</forenames></author><author><keyname>Mokr&#xfd;</keyname><forenames>Ond&#x159;ej</forenames></author><author><keyname>Rajmic</keyname><forenames>Pavel</forenames></author></authors><title>S-SPADE Done Right: Detailed Study of the Sparse Audio Declipper
  Algorithms</title><categories>math.OC eess.AS eess.SP</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This technical report shows and discusses in detail how Sparse Audio
Declipper (SPADE) algorithms are derived from the signal model using the ADMM
approach. The analysis version (A-SPADE) of Kiti\'c et. al. (LVA/ICA 2015) is
derived and justified. The synthesis version (S-SPADE) of the same research
team is shown to solve a different optimization task than intended. This issue
is corrected in this report, leading to the new S-SPADE algorithm which is in
line to A-SPADE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09948</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09948</id><created>2018-09-14</created><authors><author><keyname>Ozdenizci</keyname><forenames>Ozan</forenames></author><author><keyname>Cumpanasoiu</keyname><forenames>Catalina</forenames></author><author><keyname>Mazefsky</keyname><forenames>Carla</forenames></author><author><keyname>Siegel</keyname><forenames>Matthew</forenames></author><author><keyname>Erdogmus</keyname><forenames>Deniz</forenames></author><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames></author><author><keyname>Goodwin</keyname><forenames>Matthew S.</forenames></author></authors><title>Time-Series Prediction of Proximal Aggression Onset in Minimally-Verbal
  Youth with Autism Spectrum Disorder Using Physiological Biosignals</title><categories>cs.HC eess.SP</categories><comments>40th International Engineering in Medicine and Biology Conference
  (EMBC 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been suggested that changes in physiological arousal precede
potentially dangerous aggressive behavior in youth with autism spectrum
disorder (ASD) who are minimally verbal (MV-ASD). The current work tests this
hypothesis through time-series analyses on biosignals acquired prior to
proximal aggression onset. We implement ridge-regularized logistic regression
models on physiological biosensor data wirelessly recorded from 15 MV-ASD youth
over 64 independent naturalistic observations in a hospital inpatient unit. Our
results demonstrate proof-of-concept, feasibility, and incipient validity
predicting aggression onset 1 minute before it occurs using global,
person-dependent, and hybrid classifier models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09955</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09955</id><created>2018-09-11</created><authors><author><keyname>Morozov</keyname><forenames>Dmitry</forenames></author><author><keyname>Lezoche</keyname><forenames>Mario</forenames></author><author><keyname>Panetto</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>Knowledge extraction, modeling and formalization: EEG case study</title><categories>cs.IR eess.SP</categories><comments>arXiv admin note: text overlap with arXiv:1506.05018 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal Concept Analysis (FCA) is a well-established method for data analysis
which finds many applications in data mining. Its extension on complex data
representation formats brought a wave of new applications to the problems such
as gene expression mining, prediction of toxicity of chemical compounds or
clustering of sequences in process event logs. Insipired from this work our
research inherits their model and designs an experiment for mining
electroencephalographic recordings for patterns of sleep spindles. The
contribution of this paper lies in the specification of desritizition procedure
and the architecture of FCA experiment. We also provide some reflection on the
related research papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.09983</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.09983</id><created>2018-09-25</created><authors><author><keyname>Dokuchaev</keyname><forenames>Nikolai</forenames></author></authors><title>On recovery of signals with single point spectrum degeneracy</title><categories>eess.SP cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1604.04967</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper study recovery problem for discrete time signals with a finite
number of missing values. The paper establishes recoverability of these missing
values for signals with Z-transform vanishing with a certain rate at a single
point.
  The transfer functions for the corresponding recovering kernels are presented
explicitly.
  Some robustness of the recovery with respect to data truncation or noise
contamination is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10047</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10047</id><created>2018-09-26</created><authors><author><keyname>Bear</keyname><forenames>Helen L</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author></authors><title>An extensible cluster-graph taxonomy for open set sound scene analysis</title><categories>cs.SD eess.AS</categories><comments>To be presented at Detection and Classification of Audio Scenes and
  Events (DCASE) workshop, November 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new extensible and divisible taxonomy for open set sound scene
analysis. This new model allows complex scene analysis with tangible
descriptors and perception labels. Its novel structure is a cluster graph such
that each cluster (or subset) can stand alone for targeted analyses such as
office sound event detection, whilst maintaining integrity over the whole graph
(superset) of labels. The key design benefit is its extensibility as new labels
are needed during new data capture. Furthermore, datasets which use the same
taxonomy are easily augmented, saving future data collection effort. We balance
the details needed for complex scene analysis with avoiding 'the taxonomy of
everything' with our framework to ensure no duplicity in the superset of labels
and demonstrate this with DCASE challenge classifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10060</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10060</id><created>2018-09-26</created><updated>2018-10-17</updated><authors><author><keyname>Boccadoro</keyname><forenames>Pietro</forenames></author></authors><title>Smart Grids empowerment with Edge Computing: An Overview</title><categories>eess.SP</categories><comments>5 pages, 2 figures, 10 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electric grids represent the angular stone of distribution networks. Since
their introduction, a huge evolutionary process turned them from conventional
electrical power network to advanced, real-time monitoring systems. In this
process, the Internet of Things (IoT) proved itself as a fast forwarding
paradigm: smart devices, networks and communication protocol stacks are more
and more integrated in a number of general purpose, industrial grade systems.
In this framework, the upcoming Internet of Services (IoS) is going to
orchestrate the many sensors and components in Smart Grids, simultaneously
enabling complex information management for energy suppliers, operators and
consumers. The opportunity to continuously monitor and send important data
(i.e., energy production, distribution, usage and storage) issues and
facilitates the implementation of trailblazing functionalities. Of course, the
more information are sent throughout the network, the higher the overload will
be, thus resulting in a potentially worsening of the Quality of Service (QoS)
(i.e., communication latencies). This work overviews the concept of Smart Grids
while investigating de-centralized computing and elaboration possibilities,
leveraging the IoT paradigm towards the IoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10085</identifier>
 <datestamp>2018-12-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10085</id><created>2018-09-24</created><updated>2018-12-11</updated><authors><author><keyname>Grimaldi</keyname><forenames>Simone</forenames></author><author><keyname>Mahmood</keyname><forenames>Aamir</forenames></author><author><keyname>Gidlund</keyname><forenames>Mikael</forenames></author></authors><title>Real-time Interference Identification via Supervised Learning: Embedding
  Coexistence Awareness in IoT Devices</title><categories>eess.SP</categories><comments>14 pages, Updated title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy sampling-based interference detection and identification (IDI) methods
collide with the limitations of commercial off-the-shelf (COTS) IoT hardware.
Moreover, long sensing times, complexity and inability to track concurrent
interference strongly inhibit their applicability in most IoT deployments.
Motivated by the increasing need for on-device IDI for wireless coexistence, we
develop a lightweight and efficient method targeting interference
identification already at the level of single interference bursts. Our method
exploits real-time extraction of envelope and model-aided spectral features,
specifically designed considering the physical properties of signals captured
with COTS hardware. We adopt manifold supervised-learning (SL) classifiers
ensuring suitable performance and complexity trade-off for IoT platforms with
different computational capabilities. The proposed IDI method is capable of
real-time identification of IEEE 802.11b/g/n, 802.15.4, 802.15.1 and Bluetooth
Low Energy wireless standards, enabling isolation and extraction of
standard-specific traffic statistics even in the case of heavy concurrent
interference. We perform an experimental study in real environments with
heterogeneous interference scenarios, showing 90%-97% burst identification
accuracy. Meanwhile, the lightweight SL methods, running online on wireless
sensor networks-COTS hardware, ensure sub-ms identification time and limited
performance gap from machine-learning approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10086</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10086</id><created>2018-09-24</created><authors><author><keyname>Dion</keyname><forenames>Arnaud</forenames><affiliation>ISAE-SUPAERO</affiliation></author><author><keyname>Calmettes</keyname><forenames>Vincent</forenames><affiliation>ISAE-SUPAERO</affiliation></author><author><keyname>Bousquet</keyname><forenames>Michel</forenames><affiliation>ISAE-SUPAERO</affiliation></author><author><keyname>Boutillon</keyname><forenames>Emmanuel</forenames><affiliation>Lab-STICC</affiliation></author></authors><title>Performances of a GNSS receiver for space-based applications</title><categories>eess.SP astro-ph.IM</categories><proxy>ccsd</proxy><journal-ref>Toulouse Space Show, Jun 2010, Toulouse, France</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space Vehicle (SV) life span depends on its station keeping capability.
Station keeping is the ability of the vehicle to maintain position and
orientation. Due to external perturbations, the trajectory of the SV derives
from the ideal orbit. Actual positioning systems for satellites are mainly
based on ground equipment, which means heavy infrastructures. Autonomous
positioning and navigation systems using Global Navigation Satellite Systems
(GNSS) can then represent a great reduction in platform design and operating
costs. Studies have been carried out and the first operational systems, based
on GPS receivers, become available. But better availability of service could be
obtained considering a receiver able to process GPS and Galileo signals. Indeed
Galileo system will be compatible with the current and the modernized GPS
system in terms of signals representation and navigation data. The greater
availability obtained with such a receiver would allow significant increase of
the number of point solutions and performance enhancement. For a mid-term
perspective Thales Alenia Space finances a PhD to develop the concept of a
reconfigurable receiver able to deal with both the GPS system and the future
Galileo system. In this context, the aim of this paper is to assess the
performances of a receiver designed for Geosynchronous Earth Orbit (GEO)
applications. It is shown that high improvements are obtained with a receiver
designed to track both GPS and Galileo satellites. The performance assessments
have been used to define the specifications of the future satellite GNSS
receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10087</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10087</id><created>2018-09-24</created><authors><author><keyname>Xiong</keyname><forenames>Mingliang</forenames></author><author><keyname>Liu</keyname><forenames>Mingqing</forenames></author><author><keyname>Zhang</keyname><forenames>Qingqing</forenames></author><author><keyname>Liu</keyname><forenames>Qingwen</forenames></author><author><keyname>Wu</keyname><forenames>Jun</forenames></author><author><keyname>Xia</keyname><forenames>Pengfei</forenames></author></authors><title>TDMA in Adaptive Resonant Beam Charging for IoT Devices</title><categories>eess.SP</categories><journal-ref>M. Xiong, M. Liu, Q. Zhang, Q. Liu, J. Wu and P. Xia, &quot;TDMA in
  Adaptive Resonant Beam Charging for IoT Devices,&quot; in IEEE Internet of Things
  Journal, Aug. 2018</journal-ref><doi>10.1109/JIOT.2018.2863232</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resonant beam charging (RBC) can realize wireless power transfer (WPT) from a
transmitter to multiple receivers via resonant beams. The adaptive RBC (ARBC)
can effectively improve its energy utilization. In order to support multi-user
WPT in the ARBC system, we propose the time-division multiple access (TDMA)
method and design the TDMA-based WPT scheduling algorithm. Our TDMA WPT method
has the features of concurrently charging, continuous charging current,
individual user power control, constant driving power and flexible driving
power control. The simulation shows that the TDMA scheduling algorithm has high
efficiency, as the total charging time is roughly half (46.9% when charging 50
receivers) of that of the alternative scheduling algorithm. Furthermore, the
TDMA for WPT inspires the ideas of enhancing the ARBC system, such as flow
control and quality of service (QoS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10108</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10108</id><created>2018-08-16</created><authors><author><keyname>Li</keyname><forenames>Tiantian</forenames></author><author><keyname>Wang</keyname><forenames>Bo</forenames></author><author><keyname>Zhou</keyname><forenames>Min</forenames></author><author><keyname>Watada</keyname><forenames>Junzo</forenames></author></authors><title>Short-term load forecasting using optimized LSTM networks based on EMD</title><categories>eess.SP cs.AI</categories><comments>16 pages,11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short-term load forecasting is one of the crucial sections in smart grid.
Precise forecasting enables system operators to make reliable unit commitment
and power dispatching decisions. With the advent of big data, a number of
artificial intelligence techniques such as back propagation, support vector
machine have been used to predict the load of the next day. Nevertheless, due
to the noise of raw data and the randomness of power load, forecasting errors
of existing approaches are relatively large. In this study, a short-term load
forecasting method is proposed on the basis of empirical mode decomposition and
long short-term memory networks, the parameters of which are optimized by a
particle swarm optimization algorithm. Essentially, empirical mode
decomposition can decompose the original time series of historical data into
relatively stationary components and long short-term memory network is able to
emphasize as well as model the timing of data, the joint use of which is
expected to effectively apply the characteristics of data itself, so as to
improve the predictive accuracy. The effectiveness of this research is
exemplified on a realistic data set, the experimental results of which show
that the proposed method has higher forecasting accuracy and applicability, as
compared with existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10117</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10117</id><created>2018-09-26</created><authors><author><keyname>Giannopoulos</keyname><forenames>Michalis</forenames></author><author><keyname>Tsagkatakis</keyname><forenames>Grigorios</forenames></author><author><keyname>Blasi</keyname><forenames>Saverio</forenames></author><author><keyname>Toutounchi</keyname><forenames>Farzad</forenames></author><author><keyname>Mouchtaris</keyname><forenames>Athanasios</forenames></author><author><keyname>Tsakalides</keyname><forenames>Panagiotis</forenames></author><author><keyname>Mrak</keyname><forenames>Marta</forenames></author><author><keyname>Izquierdo</keyname><forenames>Ebroul</forenames></author></authors><title>Convolutional Neural Networks for Video Quality Assessment</title><categories>eess.IV cs.CV</categories><comments>Number of Pages: 12, Number of Figures: 17, Submitted to: Signal
  Processing: Image Communication (Elsevier)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video Quality Assessment (VQA) is a very challenging task due to its highly
subjective nature. Moreover, many factors influence VQA. Compression of video
content, while necessary for minimising transmission and storage requirements,
introduces distortions which can have detrimental effects on the perceived
quality. Especially when dealing with modern video coding standards, it is
extremely difficult to model the effects of compression due to the
unpredictability of encoding on different content types. Moreover, transmission
also introduces delays and other distortion types which affect the perceived
quality. Therefore, it would be highly beneficial to accurately predict the
perceived quality of video to be distributed over modern content distribution
platforms, so that specific actions could be undertaken to maximise the Quality
of Experience (QoE) of the users. Traditional VQA techniques based on feature
extraction and modelling may not be sufficiently accurate. In this paper, a
novel Deep Learning (DL) framework is introduced for effectively predicting VQA
of video content delivery mechanisms based on end-to-end feature learning. The
proposed framework is based on Convolutional Neural Networks, taking into
account compression distortion as well as transmission delays. Training and
evaluation of the proposed framework are performed on a user annotated VQA
dataset specifically created to undertake this work. The experiments show that
the proposed methods can lead to high accuracy of the quality estimation,
showcasing the potential of using DL in complex VQA scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10133</identifier>
 <datestamp>2018-09-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10133</id><created>2018-09-26</created><authors><author><keyname>Hosseini</keyname><forenames>Z. S.</forenames></author><author><keyname>Khodaei</keyname><forenames>A.</forenames></author><author><keyname>Paaso</keyname><forenames>E. A.</forenames></author><author><keyname>Hossan</keyname><forenames>M. S.</forenames></author><author><keyname>Lelic</keyname><forenames>D.</forenames></author></authors><title>Dynamic Solar Hosting Capacity Calculations in Microgrids</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microgrids, as small-scale islandable power systems, are considered as viable
promotors of renewable energy resources. In particular, microgrids can
efficiently integrate solar photovoltaic (PV) as the deployment of this
customer-deployed technology is growing in distribution grids. However, there
is a limit on how much PV can be hosted by a microgrid. High penetration of PVs
can potentially put the microgrid at operational risks including but not
limited to over and under voltages, excessive line losses, overloading of
transformers and feeders, and protection failure. To avoid such potential
negative impacts, the concept of hosting capacity is introduced and used. The
hosting capacity is defined as the total capacity of DERs that can be
integrated into a given feeder/microgrid without exceeding operational
restrictions and/or requiring system upgrades. Hosting capacity studies are
primarily done based on steadystate analyses. However, in case of microgrids
and when transitioning between grid-connected and islanded modes, dynamic
operation becomes more restrictive than steady-state operation and thus is
worthy of detailed investigation to provide a better understanding of the
amount of DER that the microgrid can host. This paper examines this problem and
provides extensive simulations on a practical test system to show its
importance and merits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10206</identifier>
 <datestamp>2018-10-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10206</id><created>2018-09-26</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Yang</keyname><forenames>Zhen</forenames></author><author><keyname>Li</keyname><forenames>Guoqing</forenames></author><author><keyname>Mu</keyname><forenames>Yunfei</forenames></author><author><keyname>Zhao</keyname><forenames>Dongbo</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Shen</keyname><forenames>Bo</forenames></author></authors><title>Optimal scheduling of isolated microgrid with an electric vehicle
  battery swapping station in multi-stakeholder scenarios: a bi-level
  programming approach via real-time pricing</title><categories>eess.SP math.OC</categories><comments>Accepted by Applied Energy</comments><journal-ref>Applied Energy 232 (2018) 54-68</journal-ref><doi>10.1016/j.apenergy.2018.09.211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to coordinate the scheduling problem between an isolated microgrid
(IMG) and electric vehicle battery swapping stations (BSSs) in
multi-stakeholder scenarios, a new bi-level optimal scheduling model is
proposed for promoting the participation of BSSs in regulating the IMG economic
operation. In this model, the upper-level sub-problem is formulated to minimize
the IMG net costs, while the lower-level aims to maximize the profits of the
BSS under real-time pricing environments determined by demand responses in the
upper-level decision. To solve the model, a hybrid algorithm, called JAYA-BBA,
is put forward by combining a real/integer-coded JAYA algorithm and the branch
and bound algorithm (BBA), in which the JAYA and BBA are respectively employed
to address the upper- and lower- level sub-problems, and the bi-level model is
eventually solved through alternate iterations between the two levels. The
simulation results on a microgrid test system verify the effectiveness and
superiority of the presented approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10244</identifier>
 <datestamp>2018-09-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10244</id><created>2018-09-24</created><authors><author><keyname>Lu</keyname><forenames>Yantao</forenames></author><author><keyname>Kakillioglu</keyname><forenames>Burak</forenames></author><author><keyname>Velipasalar</keyname><forenames>Senem</forenames></author></authors><title>Autonomously and Simultaneously Refining Deep Neural Network Parameters
  by a Bi-Generative Adversarial Network Aided Genetic Algorithm</title><categories>cs.CV eess.IV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1805.09712</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The choice of parameters, and the design of the network architecture are
important factors affecting the performance of deep neural networks. Genetic
Algorithms (GA) have been used before to determine parameters of a network.
Yet, GAs perform a finite search over a discrete set of pre-defined candidates,
and cannot, in general, generate unseen configurations. In this paper, to move
from exploration to exploitation, we propose a novel and systematic method that
autonomously and simultaneously optimizes multiple parameters of any deep
neural network by using a GA aided by a bi-generative adversarial network
(Bi-GAN). The proposed Bi-GAN allows the autonomous exploitation and choice of
the number of neurons, for fully-connected layers, and number of filters, for
convolutional layers, from a large range of values. Our proposed Bi-GAN
involves two generators, and two different models compete and improve each
other progressively with a GAN-based strategy to optimize the networks during
GA evolution. Our proposed approach can be used to autonomously refine the
number of convolutional layers and dense layers, number and size of kernels,
and the number of neurons for the dense layers; choose the type of the
activation function; and decide whether to use dropout and batch normalization
or not, to improve the accuracy of different deep neural network architectures.
Without loss of generality, the proposed method has been tested with the
ModelNet database, and compared with the 3D Shapenets and two GA-only methods.
The results show that the presented approach can simultaneously and
successfully optimize multiple neural network parameters, and achieve higher
accuracy even with shallower networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10262</identifier>
 <datestamp>2019-06-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10262</id><created>2018-09-26</created><updated>2019-06-06</updated><authors><author><keyname>Zhang</keyname><forenames>Zhongping</forenames></author><author><keyname>Lin</keyname><forenames>Youzuo</forenames></author></authors><title>Data-driven Seismic Waveform Inversion: A Study on the Robustness and
  Generalization</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustic- and elastic-waveform inversion is an important and widely used
method to reconstruct subsurface velocity image. Waveform inversion is a
typical non-linear and ill-posed inverse problem. Existing physics-driven
computational methods for solving waveform inversion suffer from the cycle
skipping and local minima issues, and not to mention solving waveform inversion
is computationally expensive. In recent years, data-driven methods become a
promising way to solve the waveform inversion problem. However, most deep
learning frameworks suffer from generalization and over-fitting issue. In this
paper, we developed a real-time data-driven technique and we call it
VelocityGAN, to accurately reconstruct subsurface velocities. Our VelocityGAN
is built on a generative adversarial network (GAN) and trained end-to-end to
learn a mapping function from the raw seismic waveform data to the velocity
image. Different from other encoder-decoder based data-driven seismic waveform
inversion approaches, our VelocityGAN learns regularization from data and
further impose the regularization to the generator so that inversion accuracy
is improved. We further develop a transfer learning strategy based on
VelocityGAN to alleviate the generalization issue. A series of experiments are
conducted on the synthetic seismic reflection data to evaluate the
effectiveness, efficiency, and generalization of VelocityGAN. We not only
compare it with existing physics-driven approaches and data-driven frameworks
but also conduct several transfer learning experiments. The experiment results
show that VelocityGAN achieves state-of-the-art performance among the baselines
and can improve the generalization results to some extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10288</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10288</id><created>2018-09-25</created><updated>2018-09-28</updated><authors><author><keyname>Tanaka</keyname><forenames>Kou</forenames></author><author><keyname>Kaneko</keyname><forenames>Takuhiro</forenames></author><author><keyname>Hojo</keyname><forenames>Nobukatsu</forenames></author><author><keyname>Kameoka</keyname><forenames>Hirokazu</forenames></author></authors><title>WaveCycleGAN: Synthetic-to-natural speech waveform conversion using
  cycle-consistent adversarial networks</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>SLT2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a learning-based filter that allows us to directly modify a
synthetic speech waveform into a natural speech waveform. Speech-processing
systems using a vocoder framework such as statistical parametric speech
synthesis and voice conversion are convenient especially for a limited number
of data because it is possible to represent and process interpretable acoustic
features over a compact space, such as the fundamental frequency (F0) and
mel-cepstrum. However, a well-known problem that leads to the quality
degradation of generated speech is an over-smoothing effect that eliminates
some detailed structure of generated/converted acoustic features. To address
this issue, we propose a synthetic-to-natural speech waveform conversion
technique that uses cycle-consistent adversarial networks and which does not
require any explicit assumption about speech waveform in adversarial learning.
In contrast to current techniques, since our modification is performed at the
waveform level, we expect that the proposed method will also make it possible
to generate `vocoder-less' sounding speech even if the input speech is
synthesized using a vocoder framework. The experimental results demonstrate
that our proposed method can 1) alleviate the over-smoothing effect of the
acoustic features despite the direct modification method used for the waveform
and 2) greatly improve the naturalness of the generated speech sounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10296</identifier>
 <datestamp>2018-09-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10296</id><created>2018-09-26</created><authors><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Zhong</keyname><forenames>Chen</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author><author><keyname>Velipasalar</keyname><forenames>Senem</forenames></author></authors><title>Learning-Based Delay-Aware Caching in Wireless D2D Caching Networks</title><categories>cs.IT eess.SP math.IT</categories><comments>14 pages, 11 figures. arXiv admin note: text overlap with
  arXiv:1704.01984</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, wireless caching techniques have been studied to satisfy lower
delay requirements and offload traffic from peak periods. By storing parts of
the popular files at the mobile users, users can locate some of their requested
files in their own caches or the caches at their neighbors. In the latter case,
when a user receives files from its neighbors, device-to-device(D2D)
communication is performed. D2D communication underlaid with cellular networks
is also a new paradigm for the upcoming wireless systems. By allowing a pair of
adjacent D2D users to communicate directly, D2D communication can achieve
higher throughput, better energy efficiency and lower traffic delay. In this
work, we propose an efficient learning-based caching algorithm operating
together with a non-parametric estimator to minimize the average transmission
delay in D2D-enabled cellular networks. It is assumed that the system does not
have any prior information regarding the popularity of the files, and the
non-parametric estimator is aimed at learning the intensity function of the
file requests. An algorithm is devised to determine the best &lt;file,user&gt; pairs
that provide the best delay improvement in each loop to form a caching policy
with very low transmission delay and high throughput. This algorithm is also
extended to address a more general scenario, in which the distributions of
fading coefficients and values of system parameters potentially change over
time. Via numerical results, the superiority of the proposed algorithm is
verified by comparing it with a naive algorithm, in which all users simply
cache their favorite files, and by comparing with a probabilistic algorithm, in
which the users cache a file with a probability that is proportional to its
popularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10413</identifier>
 <datestamp>2018-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10413</id><created>2018-09-27</created><updated>2018-10-02</updated><authors><author><keyname>Peng</keyname><forenames>Fei</forenames></author><author><keyname>Zhang</keyname><forenames>Shunqing</forenames></author><author><keyname>Cao</keyname><forenames>Shan</forenames></author><author><keyname>Xu</keyname><forenames>Shugong</forenames></author></authors><title>A Prototype Performance Analysis for V2V Communications using USRP-based
  Software Defined Radio Platform</title><categories>eess.SP</categories><comments>5 pages, 6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous driving is usually recognized as a promising technology to replace
human drivers in the near future. To guarantee the safety performance in the
daily life scenario, multiple-car intelligence with high quality inter-vehicle
communication capability is necessary in general. In this paper, to figure out
the potential practical issues in the vehicle-to-vehicle transmission, we
present a software defined radio platform for V2V communication using universal
software radio peripheral (USRP). Based on the LTE framework, we modify the
frame structure, the signal processing mechanisms and the resource allocation
schemes to emulate the updated LTE-V standard and generate the corresponding
numerical results based on the real measured signals. As shown through some
empirical studies, one to four dB back-off is in general required to guarantee
the reliability performance for V2V communication environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10450</identifier>
 <datestamp>2018-10-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10450</id><created>2018-09-27</created><updated>2018-09-28</updated><authors><author><keyname>Alam</keyname><forenames>Mehmood</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author></authors><title>Sequence Block based Compressed Sensing Multiuser Detection for 5G</title><categories>eess.SP</categories><comments>Accepted in IEEE GLOBECOM, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing based multiuser detection (CSMUD) is a promising candidate
to cope with the massive connectivity requirements of the massive machine type
communication (mMTC) in the fifth generation (5G) wireless communication
system. It facilitates grant-free non-orthogonal code division multiple access
(CDMA) to accommodate massive number of IoT devices. In non-orthogonal CDMA,
the users are assigned with non-orthogonal sequences which serve as their
signatures. However, the activity detection which is based on the correlation
between the spreading sequences, degrades with increase in the number of users,
especially in the lower SNR region. In this paper, to improve the performance
of the CSMUD, we propose a sequence block based CSMUD, in which block of
sequences is used as signature of the user instead of single sequence. A
sequence block based group orthogonal matching pursuit algorithm is proposed to
jointly detect the activity and data. The proposed scheme reduces the detection
error rate (DER) by a magnitude of two at SNR = 10 dB in a system where the
number of users are three times more than the number of available resources
with each user having activity probability of 0.1. The DER of the proposed
scheme is below 10^-2 even at activity probability of 0.16 for sequence length
of 20 and overloading factor of 300%. Furthermore, at SNR = 10 dB, the DER of
the proposed scheme outperforms the conventional scheme by a magnitude of one
for a system with overloading factor of 500%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10452</identifier>
 <datestamp>2019-05-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10452</id><created>2018-09-27</created><updated>2019-05-06</updated><authors><author><keyname>Lee</keyname><forenames>Jooyoung</forenames></author><author><keyname>Cho</keyname><forenames>Seunghyun</forenames></author><author><keyname>Beack</keyname><forenames>Seung-Kwon</forenames></author></authors><title>Context-adaptive Entropy Model for End-to-end Optimized Image
  Compression</title><categories>eess.IV</categories><comments>Published as a conference paper at ICLR 2019. The test code,
  evaluation results and reconstructed images are publicly available at
  https://github.com/JooyoungLeeETRI/CA_Entropy_Model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a context-adaptive entropy model for use in end-to-end optimized
image compression. Our model exploits two types of contexts, bit-consuming
contexts and bit-free contexts, distinguished based upon whether additional bit
allocation is required. Based on these contexts, we allow the model to more
accurately estimate the distribution of each latent representation with a more
generalized form of the approximation models, which accordingly leads to an
enhanced compression performance. Based on the experimental results, the
proposed method outperforms the traditional image codecs, such as BPG and
JPEG2000, as well as other previous artificial-neural-network (ANN) based
approaches, in terms of the peak signal-to-noise ratio (PSNR) and multi-scale
structural similarity (MS-SSIM) index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10454</identifier>
 <datestamp>2018-10-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10454</id><created>2018-09-27</created><updated>2018-09-28</updated><authors><author><keyname>Alam</keyname><forenames>Mehmood</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author></authors><title>Novel Codebook-based MC-CDMA with Compressive Sensing Multiuser
  Detection for Sporadic mMTC</title><categories>eess.SP</categories><comments>Accepted in IEEE GLOBECOM Non-Orthogonal Multiple Access Techniques
  for 5G Workshop, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive machine type communication (mMTC) is one of the basic components of
the future fifth generation (5G) wireless communication system. In mMTC, the
information processing at the sensor nodes is required to be simple, low power
consuming and spectral efficient. In order to increase the spectral efficiency
at the cost of minimum performance loss and to simplify the information
processing at the sensor node, in this paper a codebook based spreading
technique is proposed. In the proposed scheme, first, the modulation and
spreading are combined into a direct symbol-to-sequence spreader, which
directly maps the input bits into a codeword from the user specific codebook.
Secondly, utilizing the sporadic node activity of the mMTC a compressive
sensing based algorithm is designed for multiuser activity and data detection.
Exploiting the multidimensional structure of the codebook, the required signal
to noise ratio (SNR) to increase the modulation order, is reduced as compared
to the conventional scheme. Besides reducing the power consumption, it is shown
that for modulation order of eight, the proposed scheme achieves a gain of 1 dB
over the conventional scheme at bit error rate (BER) of 10^-5. However, more
sophisticated decoding algorithm is required to achieve the performance gain at
a lower modulation order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10579</identifier>
 <datestamp>2018-09-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10579</id><created>2018-09-27</created><authors><author><keyname>Abbasi</keyname><forenames>Muhammad Ali Babar</forenames></author><author><keyname>Tataria</keyname><forenames>Harsh</forenames></author><author><keyname>Fusco</keyname><forenames>Vincent F.</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author></authors><title>On the Impact of Spillover Losses in 28 GHz Rotman Lens Arrays for 5G
  Applications</title><categories>eess.SP</categories><comments>3 pages, 5 figures, presented in Proc. IEEE IMWS-5G, Jul. 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work demonstrates the sensitivity of lens antenna arrays operating at
millimeter-wave (mmWave) frequencies. Considering a Rotman lens array in
receive mode, our investigation focuses on its most imperative defect:
aberration of electromagnetic (EM) energy. Aberration leads to spillover of
electric fields to neighboring ports, reducing the lens' ability to focus the
EM energy to a desired port. With full EM simulations, we design a 28 GHz, 13
beam and 13 array port Rotman lens array to characterize its performance with
the aforementioned impairment. Our findings show that the impact of aberration
is more pronounced when the beam angles are close to the array end-fire. More
critically, the corresponding impact of aberration on the desired signal and
interference powers is also investigated for an uplink multiuser cellular
system operating at 28 GHz. The presented results can be used as a reference to
re-calibrate our expectations for Rotman lens arrays at mmWave frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10581</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10581</id><created>2018-09-27</created><updated>2019-04-30</updated><authors><author><keyname>Kataoka</keyname><forenames>Hidetomo</forenames><affiliation>Graduate School of Information Science and Engineering, Ritsumeikan University, Kusatsu, Shiga, Japan</affiliation></author><author><keyname>Ijiri</keyname><forenames>Takashi</forenames><affiliation>College of Engineering, Shibaura Institute of Technology, Toyosu, Tokyo, Japan</affiliation></author><author><keyname>Matsumura</keyname><forenames>Kohei</forenames><affiliation>Graduate School of Information Science and Engineering, Ritsumeikan University, Kusatsu, Shiga, Japan</affiliation></author><author><keyname>White</keyname><forenames>Jeremy</forenames><affiliation>Graduate School of Information Science and Engineering, Ritsumeikan University, Kusatsu, Shiga, Japan</affiliation></author><author><keyname>Hirabayashi</keyname><forenames>Akira</forenames><affiliation>Graduate School of Information Science and Engineering, Ritsumeikan University, Kusatsu, Shiga, Japan</affiliation></author></authors><title>Acoustic Probing for Estimating the Storage Time and Firmness of
  Tomatoes and Mandarin Oranges</title><categories>cs.SD eess.AS</categories><comments>8 pages, 9 figures. After submitting the first version, we have
  continued measurements and found some results indicating a possibility that
  the conditions of our measurement devices had an influence to the estimation
  results. We are still continuing experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces an acoustic probing technique to estimate the storage
time and firmness of fruits; we emit an acoustic signal to fruit from a small
speaker and capture the reflected signal with a tiny microphone. We collect
reflected signals for fruits with various storage times and firmness
conditions, using them to train regressors for estimation. To evaluate the
feasibility of our acoustic probing, we performed experiments; we prepared 162
tomatoes and 153 mandarin oranges, collected their reflected signals using our
developed device and measured their firmness with a fruit firmness tester, for
a period of 35 days for tomatoes and 60 days for mandarin oranges. We performed
cross validation by using this data set. The average estimation errors of
storage time and firmness for tomatoes were 0.89 days and 9.47 g/mm2. Those for
mandarin oranges were 1.67 days and 15.67 g/mm2. The estimation of storage time
was sufficiently accurate for casual users to select fruits in their favorite
condition at home. In the experiments, we tested four different acoustic probes
and found that sweep signals provide highly accurate estimation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10711</identifier>
 <datestamp>2019-01-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10711</id><created>2018-09-27</created><updated>2019-01-28</updated><authors><author><keyname>Michelini</keyname><forenames>Pablo Navarrete</forenames></author><author><keyname>Zhu</keyname><forenames>Dan</forenames></author><author><keyname>Liu</keyname><forenames>Hanwen</forenames></author></authors><title>Multi-Scale Recursive and Perception-Distortion Controllable Image
  Super-Resolution</title><categories>eess.IV cs.CV cs.LG eess.SP</categories><comments>In ECCV 2018 Workshops. Won 2nd place in Region 3 of PIRM-SR
  Challenge 2018. Code and models are available at
  https://github.com/pnavarre/pirm-sr-2018</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We describe our solution for the PIRM Super-Resolution Challenge 2018 where
we achieved the 2nd best perceptual quality for average RMSE&lt;=16, 5th best for
RMSE&lt;=12.5, and 7th best for RMSE&lt;=11.5. We modify a recently proposed
Multi-Grid Back-Projection (MGBP) architecture to work as a generative system
with an input parameter that can control the amount of artificial details in
the output. We propose a discriminator for adversarial training with the
following novel properties: it is multi-scale that resembles a progressive-GAN;
it is recursive that balances the architecture of the generator; and it
includes a new layer to capture significant statistics of natural images.
Finally, we propose a training strategy that avoids conflicts between
reconstruction and perceptual losses. Our configuration uses only 281k
parameters and upscales each image of the competition in 0.2s in average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10795</identifier>
 <datestamp>2018-10-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10795</id><created>2018-09-27</created><authors><author><keyname>Zhang</keyname><forenames>Zhe</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Tian</keyname><forenames>Zhi</forenames></author></authors><title>A Hybrid Neural Network Framework and Application to Radar Automatic
  Target Recognition</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks (DNNs) have found applications in diverse signal
processing (SP) problems. Most efforts either directly adopt the DNN as a
black-box approach to perform certain SP tasks without taking into account of
any known properties of the signal models, or insert a pre-defined SP operator
into a DNN as an add-on data processing stage. This paper presents a novel
hybrid-NN framework in which one or more SP layers are inserted into the DNN
architecture in a coherent manner to enhance the network capability and
efficiency in feature extraction. These SP layers are properly designed to make
good use of the available models and properties of the data. The network
training algorithm of hybrid-NN is designed to actively involve the SP layers
in the learning goal, by simultaneously optimizing both the weights of the DNN
and the unknown tuning parameters of the SP operators. The proposed hybrid-NN
is tested on a radar automatic target recognition (ATR) problem. It achieves
high validation accuracy of 96\% with 5,000 training images in radar ATR.
Compared with ordinary DNN, hybrid-NN can markedly reduce the required amount
of training data and improve the learning performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10871</identifier>
 <datestamp>2018-10-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10871</id><created>2018-09-28</created><authors><author><keyname>Zhang</keyname><forenames>Qilong</forenames></author><author><keyname>Zhang</keyname><forenames>Qiwei</forenames></author><author><keyname>Zhang</keyname><forenames>Wuxiong</forenames></author><author><keyname>Shen</keyname><forenames>Fei</forenames></author><author><keyname>Loh</keyname><forenames>Tian Hong</forenames></author><author><keyname>Qin</keyname><forenames>Fei</forenames></author></authors><title>Understanding the Temporal Fading in Wireless Industrial Networks:
  Measurements and Analyses</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide deployment of wireless industrial networks still faces the challenge
of unreliable service due to severe multipath fading in industrial
environments. Such fading effects are not only caused by the massive metal
surfaces existing within the industrial environment but also, more
significantly, the moving objects including operators and logistical vehicles.
As a result, the mature analytical framework of mobile fading channel may not
be appropriate for the wireless industrial networks especially the majority
fixed wireless links. In this paper, we propose a qualitative analysis
framework to characterize the temporal fading effects of the fixed wireless
links in industrial environments, which reveals the essential reason of
correlated temporal variation of both the specular and scattered power.
Extensive measurements with both the envelop distribution and impulse response
from field experiments validate the proposed qualitative framework, which will
be applicable to simulate the industrial multipath fading characteristics and
to derive accurate link quality metrics to support reliable wireless network
service in various industrial applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10875</identifier>
 <datestamp>2019-06-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10875</id><created>2018-09-28</created><updated>2019-06-05</updated><authors><author><keyname>Yang</keyname><forenames>Zhuolin</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Song</keyname><forenames>Dawn</forenames></author></authors><title>Characterizing Audio Adversarial Examples Using Temporal Dependency</title><categories>cs.LG cs.AI cs.CR cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have highlighted adversarial examples as a ubiquitous threat
to different neural network models and many downstream applications.
Nonetheless, as unique data properties have inspired distinct and powerful
learning principles, this paper aims to explore their potentials towards
mitigating adversarial inputs. In particular, our results reveal the importance
of using the temporal dependency in audio data to gain discriminate power
against adversarial examples. Tested on the automatic speech recognition (ASR)
tasks and three recent audio adversarial attacks, we find that (i) input
transformation developed from image adversarial defense provides limited
robustness improvement and is subtle to advanced attacks; (ii) temporal
dependency can be exploited to gain discriminative power against audio
adversarial examples and is resistant to adaptive attacks considered in our
experiments. Our results not only show promising means of improving the
robustness of ASR systems, but also offer novel insights in exploiting
domain-specific data properties to mitigate negative effects of adversarial
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10932</identifier>
 <datestamp>2019-02-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10932</id><created>2018-09-28</created><updated>2019-02-01</updated><authors><author><keyname>Phan</keyname><forenames>Huy</forenames></author><author><keyname>Andreotti</keyname><forenames>Fernando</forenames></author><author><keyname>Cooray</keyname><forenames>Navin</forenames></author><author><keyname>Ch&#xe9;n</keyname><forenames>Oliver Y.</forenames></author><author><keyname>De Vos</keyname><forenames>Maarten</forenames></author></authors><title>SeqSleepNet: End-to-End Hierarchical Recurrent Neural Network for
  Sequence-to-Sequence Automatic Sleep Staging</title><categories>cs.LG eess.SP stat.ML</categories><comments>This article has been published in IEEE Transactions on Neural
  Systems and Rehabilitation Engineering</comments><doi>10.1109/TNSRE.2019.2896659</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic sleep staging has been often treated as a simple classification
problem that aims at determining the label of individual target polysomnography
(PSG) epochs one at a time. In this work, we tackle the task as a
sequence-to-sequence classification problem that receives a sequence of
multiple epochs as input and classifies all of their labels at once. For this
purpose, we propose a hierarchical recurrent neural network named SeqSleepNet.
At the epoch processing level, the network consists of a filterbank layer
tailored to learn frequency-domain filters for preprocessing and an
attention-based recurrent layer designed for short-term sequential modelling.
At the sequence processing level, a recurrent layer placed on top of the
learned epoch-wise features for long-term modelling of sequential epochs. The
classification is then carried out on the output vectors at every time step of
the top recurrent layer to produce the sequence of output labels. Despite being
hierarchical, we present a strategy to train the network in an end-to-end
fashion. We show that the proposed network outperforms state-of-the-art
approaches, achieving an overall accuracy, macro F1-score, and Cohen's kappa of
87.1%, 83.3%, and 0.815 on a publicly available dataset with 200 subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.10936</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.10936</id><created>2018-09-28</created><updated>2019-02-26</updated><authors><author><keyname>Li</keyname><forenames>Xiaofei</forenames></author><author><keyname>Ban</keyname><forenames>Yutong</forenames></author><author><keyname>Girin</keyname><forenames>Laurent</forenames></author><author><keyname>Alameda-Pineda</keyname><forenames>Xavier</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Online Localization and Tracking of Multiple Moving Speakers in
  Reverberant Environments</title><categories>cs.SD eess.AS</categories><comments>IEEE Journal of Selected Topics in Signal Processing, 2019</comments><doi>10.1109/JSTSP.2019.2903472</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of online localization and tracking of multiple moving
speakers in reverberant environments. The paper has the following
contributions. We use the direct-path relative transfer function (DP-RTF), an
inter-channel feature that encodes acoustic information robust against
reverberation, and we propose an online algorithm well suited for estimating
DP-RTFs associated with moving audio sources. Another crucial ingredient of the
proposed method is its ability to properly assign DP-RTFs to audio-source
directions. Towards this goal, we adopt a maximum-likelihood formulation and we
propose to use an exponentiated gradient (EG) to efficiently update
source-direction estimates starting from their currently available values. The
problem of multiple speaker tracking is computationally intractable because the
number of possible associations between observed source directions and physical
speakers grows exponentially with time. We adopt a Bayesian framework and we
propose a variational approximation of the posterior filtering distribution
associated with multiple speaker tracking, as well as an efficient variational
expectation-maximization (VEM) solver. The proposed online localization and
tracking method is thoroughly evaluated using two datasets that contain
recordings performed in real environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.11009</identifier>
 <datestamp>2018-10-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.11009</id><created>2018-09-28</created><authors><author><keyname>Hague</keyname><forenames>David A.</forenames></author></authors><title>The Generalized Sinusoidal Frequency Modulated Waveform for Active Sonar
  Systems</title><categories>eess.SP</categories><comments>This document is the author's PhD Dissertation. It is 138 pages long
  and contains 36 Figures and 4 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pulse Compression (PC) active sonar waveforms provide a significant
improvement in range resolution over single frequency sinusoidal waveforms also
known as Continuous Wave (CW) waveforms. Since their inception in the 1940's, a
wide variety of PC waveforms have been designed using either Frequency
Modulation (FM), phase coding, or frequency hopping to suite particular sonar
applications. The Sinusoidal FM (SFM) waveform modulates its Instantaneous
Frequency (IF) by a single frequency sinusoid to achieve high Doppler
sensitivity which also aids in suppressing reverberation. This allows the SFM
waveform to resolve target velocities. While the SFM's resolution in range is
inversely proportional to its bandwidth, the SFM's Auto-Correlation Function
(ACF) contains many large sidelobes. The periodicity of the SFM's IF creates
these sidelobes and impairs the SFM's ability to clearly distinguish multiple
targets in range. This dissertation describes a generalization of the SFM
waveform, referred to as the Generalized SFM (GSFM) waveform, that modifies the
IF to resemble the time/voltage characteristic of a FM chirp waveform. As a
result of this modification, the Doppler sensitivity of the SFM is preserved
while substantially reducing the high range sidelobes producing a waveform
whose Ambiguity Function (AF) approaches a thumbtack shape. This dissertation
describes the properties of the GSFM's thumbtack AF shape, compares it to other
well known waveforms with a similar AF shape, and additionally considers some
of the practical considerations of active sonar systems including transmitting
the GSFM on piezoelectric transducers and the GSFM's ability to suppress
reverberation. Lastly, this dissertation also describes designing a family of
in-band nearly orthogonal waveforms with potential applications to Continuous
Active Sonar (CAS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.11068</identifier>
 <datestamp>2018-10-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.11068</id><created>2018-09-28</created><authors><author><keyname>Zeinali</keyname><forenames>Hossein</forenames></author><author><keyname>Burget</keyname><forenames>Lukas</forenames></author><author><keyname>Sameti</keyname><forenames>Hossein</forenames></author><author><keyname>Cernocky</keyname><forenames>Jan</forenames></author></authors><title>Spoken Pass-Phrase Verification in the i-vector Space</title><categories>cs.SD cs.CL eess.AS</categories><journal-ref>Proc. Odyssey 2018 The Speaker and Language Recognition Workshop</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of spoken pass-phrase verification is to decide whether a test
utterance contains the same phrase as given enrollment utterances. Beside other
applications, pass-phrase verification can complement an independent speaker
verification subsystem in text-dependent speaker verification. It can also be
used for liveness detection by verifying that the user is able to correctly
respond to a randomly prompted phrase. In this paper, we build on our previous
work on i-vector based text-dependent speaker verification, where we have shown
that i-vectors extracted using phrase specific Hidden Markov Models (HMMs) or
using Deep Neural Network (DNN) based bottle-neck (BN) features help to reject
utterances with wrong pass-phrases. We apply the same i-vector extraction
techniques to the stand-alone task of speaker-independent spoken pass-phrase
classification and verification. The experiments on RSR2015 and RedDots
databases show that very simple scoring techniques (e.g. cosine distance
scoring) applied to such i-vectors can provide results superior to those
previously published on the same data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1809.11091</identifier>
 <datestamp>2019-05-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1809.11091</id><created>2018-09-28</created><updated>2019-05-19</updated><authors><author><keyname>Xiong</keyname><forenames>Mingliang</forenames></author><author><keyname>Liu</keyname><forenames>Qingwen</forenames></author><author><keyname>Liu</keyname><forenames>Mingqing</forenames></author><author><keyname>Xia</keyname><forenames>Pengfei</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author></authors><title>Resonant Beam Communications</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vision and requirements of the sixth generation (6G) mobile communication
systems are expected to adopt free-space optical communication (FSO) and
wireless power transfer (WPT). The laser-based WPT or wireless information
transfer (WIT) usually faces the challenges of mobility and safety. We present
here a mobile and safe resonant beam communication (RBCom) system, which can
realize high-rate simultaneous wireless information and power transfer (SWIPT).
We propose the analytical model to depict its SWIPT procedure. The numerical
results show that RBCom can achieve 7.5 Gbit/s with 200 mW received optical
power, which seems to connect the transmitter and the receiver with a mobile
&quot;wireless optical fiber&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00017</identifier>
 <datestamp>2018-12-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00017</id><created>2018-09-28</created><updated>2018-11-15</updated><authors><author><keyname>Raj</keyname><forenames>A. Govinda</forenames><affiliation>Georgia Institute of Technology</affiliation></author><author><keyname>McClellan</keyname><forenames>J. H.</forenames><affiliation>Georgia Institute of Technology</affiliation></author></authors><title>Single Snapshot Super-Resolution DOA Estimation for Arbitrary Array
  Geometries</title><categories>eess.SP cs.IT math.IT</categories><comments>Accepted for publication in IEEE Signal Processing Letters</comments><journal-ref>A. Govinda Raj and J. H. McClellan, &quot;Single Snapshot
  Super-Resolution DOA Estimation for Arbitrary Array Geometries&quot;, IEEE Signal
  Processing Letters, vol. 26, no. 1, pp. 119-123, 2019. (Date of Publication:
  16 Nov. 2018)</journal-ref><doi>10.1109/LSP.2018.2881927</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of search-free direction of arrival (DOA) estimation
for sensor arrays of arbitrary geometry under the challenging conditions of a
single snapshot and coherent sources. We extend a method of searchfree
super-resolution beamforming, originally applicable only for uniform linear
arrays, to arrays of arbitrary geometry. The infinite dimensional primal atomic
norm minimization problem in continuous angle domain is converted to a dual
problem. By exploiting periodicity, the dual function is then represented with
a trigonometric polynomial using a truncated Fourier series. A linear rule of
thumb is derived for selecting the minimum number of Fourier coefficients
required for accurate polynomial representation, based on the distance of the
farthest sensor from a reference point. The dual problem is then expressed as a
semidefinite program and solved efficiently. Finally, the searchfree DOA
estimates are obtained through polynomial rooting, and source amplitudes are
recovered through least squares. Simulations using circular and random planar
arrays show perfect DOA estimation in noise-free cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00059</identifier>
 <datestamp>2019-05-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00059</id><created>2018-09-28</created><updated>2019-05-08</updated><authors><author><keyname>Nikvand</keyname><forenames>Nima</forenames></author><author><keyname>Wang</keyname><forenames>Zhou</forenames></author><author><keyname>Fernando</keyname><forenames>Xavier</forenames></author><author><keyname>Farjow</keyname><forenames>Wisam</forenames></author></authors><title>Perceptually Inspired Normalized Conditional Compression Distance</title><categories>eess.IV</categories><comments>I have received major revisions from IEEE Access magazine and I have
  decided not to resubmit the paper to that magazine. I will not revise and
  resubmit in the foreseeable future.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image similarity measurement is a common issue in a broad range of
applications in image processing, recognition, classification and retrieval.
Conventional image similarity measures are often limited to specific
applications and cannot be applied in general scenarios. The theory of
Kolmogorov complexity provides a universal framework for a generic similarity
metric based on information distance between objects. Normalized Information
Distance (NID) has been shown to be a valid and universal distance metric
applicable in measurement of similarity of any two objects, and has been
successfully applied to a wide range of applications in the past. The
difficulty of NID lies in the non-computable nature of the Kolmogorov
complexity, and thus approximation has to be applied in practice. Here we
propose a perceptually-inspired Normalized Conditional Compression Distance
(NCCD) measure by using the Divisive Normalization Transform (DNT) as a means
to model the non-linear behavior of the Human Visual System (HVS) in reducing
statistical dependencies of visual signals for efficient representation, and
show that this perceptual extension of NID can be used in a wide range of image
processing applications, including texture classification and face recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00071</identifier>
 <datestamp>2018-12-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00071</id><created>2018-09-28</created><updated>2018-12-03</updated><authors><author><keyname>Kuznetsov</keyname><forenames>N. V.</forenames></author><author><keyname>Ladvanszky</keyname><forenames>J.</forenames></author><author><keyname>Yuldashev</keyname><forenames>M. V.</forenames></author><author><keyname>Yuldashev</keyname><forenames>R. V.</forenames></author></authors><title>PLL and Costas loop based carrier recovery circuits for 4QAM: non-linear
  analysis and simulation</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design of stable carrier recovery circuits are used in many applications:
wireless digital communication, optical communication, microwave devices and
other applications. Quadrature Phase-Shift Keying (QPSK, 4-QAM) is used as
modulation technique in many of these applications, since QPSK provides double
the data rate of classic Binary PSK (BPSK) modulation. Analysis of Costas loop
is a hard task because of its non-linearity. In this work we consider two
well-known modifications of 4QAM Costas loop circuits and discuss a new circuit
proposed by J. Ladvanszky. MATLAB Simulink models of the circuits are provided
and the noise analysis is performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00077</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00077</id><created>2018-09-28</created><authors><author><keyname>Jain</keyname><forenames>Ish Kumar</forenames></author></authors><title>Millimeter Wave Beam Training: A Survey</title><categories>eess.SP</categories><comments>A short 2 page survey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The initial access is required to establish a connection between millimeter
wave access point (AP) and users. The large isotropic pathloss at high
frequencies can be mitigated by the use of highly directional antennas.
However, it complicated the initial access procedures and increased the
latency, defying one of the major low latency objective of 5G systems. Beam
training algorithms are developed for the AP as well as for the users to find
the desired beam quickly and reduce the initial access delay. But, these beam
training protocols have to be run very frequently due to outage events, user's
mobility and period of sleep cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00128</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00128</id><created>2018-09-28</created><authors><author><keyname>Chow</keyname><forenames>Jacky C. K.</forenames></author><author><keyname>Detchev</keyname><forenames>Ivan</forenames></author><author><keyname>Ang</keyname><forenames>Kathleen</forenames></author><author><keyname>Morin</keyname><forenames>Kristian</forenames></author><author><keyname>Mahadevan</keyname><forenames>Karthik</forenames></author><author><keyname>Louie</keyname><forenames>Nicholas</forenames></author></authors><title>Robot Vision: Calibration of Wide-Angle Lens Cameras Using Collinearity
  Condition and K-Nearest Neighbour Regression</title><categories>cs.RO cs.CV cs.LG eess.IV</categories><comments>ISPRS TC I Mid-term Symposium &quot;Innovative Sensing - From Sensors to
  Methods and Applications&quot;, 10-12 October 2018. Karlsruhe, Germany</comments><journal-ref>The International Archives of the Photogrammetry, Remote Sensing
  and Spatial Information Sciences, Volume XLII-1, 2018, pp. 93-99</journal-ref><doi>10.5194/isprs-archives-XLII-1-93-2018</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Visual perception is regularly used by humans and robots for navigation. By
either implicitly or explicitly mapping the environment, ego-motion can be
determined and a path of actions can be planned. The process of mapping and
navigation are delicately intertwined; therefore, improving one can often lead
to an improvement of the other. Both processes are sensitive to the interior
orientation parameters of the camera system and mathematically modelling these
systematic errors can often improve the precision and accuracy of the overall
solution. This paper presents an automatic camera calibration method suitable
for any lens, without having prior knowledge about the sensor. Statistical
inference is performed to map the environment and localize the camera
simultaneously. K-nearest neighbour regression is used to model the geometric
distortions of the images. A normal-angle lens Nikon camera and wide-angle lens
GoPro camera were calibrated using the proposed method, as well as the
conventional bundle adjustment with self-calibration method (for comparison).
Results showed that the mapping error was reduced from an average of 14.9 mm to
1.2 mm (i.e. a 92% improvement) and 66.6 mm to 1.5 mm (i.e. a 98% improvement)
using the proposed method for the Nikon and GoPro cameras, respectively. In
contrast, the conventional approach achieved an average 3D error of 0.9 mm
(i.e. 94% improvement) and 3.3 mm (i.e. 95% improvement) for the Nikon and
GoPro cameras, respectively. Thus, the proposed method performs well
irrespective of the lens/sensor used: it yields results that are comparable to
the conventional approach for normal-angle lens cameras, and it has the
additional benefit of improving calibration results for wide-angle lens
cameras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00138</identifier>
 <datestamp>2018-10-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00138</id><created>2018-09-28</created><updated>2018-10-19</updated><authors><author><keyname>Chow</keyname><forenames>Jacky C. K.</forenames></author><author><keyname>Lichti</keyname><forenames>Derek</forenames></author><author><keyname>Ang</keyname><forenames>Kathleen</forenames></author><author><keyname>Kuntze</keyname><forenames>Gregor</forenames></author><author><keyname>Sharma</keyname><forenames>Gulshan</forenames></author><author><keyname>Ronsky</keyname><forenames>Janet</forenames></author></authors><title>Modelling Errors in X-ray Fluoroscopic Imaging Systems Using
  Photogrammetric Bundle Adjustment With a Data-Driven Self-Calibration
  Approach</title><categories>eess.IV cs.CV cs.LG</categories><comments>ISPRS TC I Mid-term Symposium &quot;Innovative Sensing - From Sensors to
  Methods and Applications&quot;, 10-12 October 2018. Karlsruhe, Germany</comments><journal-ref>The International Archives of the Photogrammetry, Remote Sensing
  and Spatial Information Sciences, Volume XLII-1, 2018</journal-ref><doi>10.5194/isprs-archives-XLII-1-101-2018</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  X-ray imaging is a fundamental tool of routine clinical diagnosis.
Fluoroscopic imaging can further acquire X-ray images at video frame rates,
thus enabling non-invasive in-vivo motion studies of joints, gastrointestinal
tract, etc. For both the qualitative and quantitative analysis of static and
dynamic X-ray images, the data should be free of systematic biases. Besides
precise fabrication of hardware, software-based calibration solutions are
commonly used for modelling the distortions. In this primary research study, a
robust photogrammetric bundle adjustment was used to model the projective
geometry of two fluoroscopic X-ray imaging systems. However, instead of relying
on an expert photogrammetrist's knowledge and judgement to decide on a
parametric model for describing the systematic errors, a self-tuning
data-driven approach is used to model the complex non-linear distortion profile
of the sensors. Quality control from the experiment showed that 0.06 mm to 0.09
mm 3D reconstruction accuracy was achievable post-calibration using merely 15
X-ray images. As part of the bundle adjustment, the location of the virtual
fluoroscopic system relative to the target field can also be spatially resected
with an RMSE between 3.10 mm and 3.31 mm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00142</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00142</id><created>2018-09-28</created><authors><author><keyname>Xu</keyname><forenames>Ding</forenames></author><author><keyname>Li</keyname><forenames>Qun</forenames></author></authors><title>Resource Allocation for Secure Communications in Cooperative Cognitive
  Wireless Powered Communication Networks</title><categories>eess.SP</categories><comments>Submitted to IEEE Systems Journal for possible publication</comments><doi>10.1109/JSYST.2018.2883491</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cognitive wireless powered communication network (CWPCN)
sharing the spectrum with a primary network who faces security threats from
eavesdroppers (EAVs). We propose a new cooperative protocol for the wireless
powered secondary users (SU) to cooperate with the primary user (PU). In the
protocol, the SUs first harvest energy from the power signals transmitted by
the cognitive hybrid access point during the wireless power transfer (WPT)
phase, and then use the harvested energy to interfere with the EAVs and gain
transmission opportunities at the same time during the wireless information
transfer (WIT) phase. Taking the maximization of the SU ergodic rate as the
design objective, resource allocation algorithms based on the dual optimization
method and the block coordinate descent method are proposed for the cases of
perfect channel state information (CSI) and collusive/non-collusive EAVs under
the PU secrecy constraint. More PU favorable greedy algorithms aimed at
minimizing the PU secrecy outage probability are also proposed. We furthermore
consider the unknown EAVs' CSI case and propose an efficient algorithm to
improve the PU security performance. Extensive simulations show that our
proposed protocol and corresponding resource allocation algorithms can not only
let the SU gain transmission opportunities but also improve the PU security
performance even with unknown EAVs' CSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00186</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00186</id><created>2018-09-29</created><authors><author><keyname>Youssef</keyname><forenames>Mona</forenames></author><author><keyname>Ghanim</keyname><forenames>Fatima</forenames></author><author><keyname>Imad</keyname><forenames>Noor</forenames></author><author><keyname>Alqasim</keyname><forenames>Ayesha</forenames></author><author><keyname>Shubair</keyname><forenames>Raed</forenames></author></authors><title>Design of Intra-body Nano-communication Network for Future Nano-medicine</title><categories>eess.SP physics.med-ph</categories><comments>107 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intra-body communication is a method that utilizes the human body as a
broadcast biological medium for electromagnetic signals to inter-connect
wireless body sensors. Study of the collaboration between electromagnetic waves
and human cells has gained importance in recent years leading towards
developing and establishing new novel concept which is the idea of
nano-communications using nano-networks to form in-vivo communication that are
aimed to offer wireless communication between interior nano-sensors. The
emergent of this advanced unprecedented prospective approach of deploying the
in vivo communication concept in the health sector is considered as a key
potential technology that enhances healthcare delivery and enables the progress
of future applications and services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00189</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00189</id><created>2018-09-29</created><authors><author><keyname>Alsuwaidi</keyname><forenames>Alyazyah</forenames></author><author><keyname>Alzarouni</keyname><forenames>Aisha</forenames></author><author><keyname>Bazazeh</keyname><forenames>Dana</forenames></author><author><keyname>Almoosa</keyname><forenames>Nawaf</forenames></author><author><keyname>Khalaf</keyname><forenames>Kinda</forenames></author><author><keyname>Shubair</keyname><forenames>Raed</forenames></author></authors><title>Wearable Posture Monitoring System with Vibration Feedback</title><categories>eess.SP</categories><comments>45 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Around 50 billion dollars is spent yearly on therapy for low back pain in the
United States alone. Low back pain is one of the most common reasons for doctor
visits. Having poor posture has been found to be a main cause of lower back
pain as it impacts the transverse abdominal muscle. Maintaining a good posture
and changing position from time to time is considered to significantly improve
and maintain personal health. The world has witnessed a vast amount of smart
monitoring devices that are used to enhance the quality of life by providing
different types of support. Smart wearable technology has been the main focus
of this century, specifically in the medical field, where the advances range
from heartbeat monitors to hearing aids. This report highlights the design,
development and validation process of a compact wearable device that uses
multiple sensors to measure the back posture of a user in real time and notify
them once poor posture is detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00222</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00222</id><created>2018-09-29</created><authors><author><keyname>Bitton</keyname><forenames>Adrien</forenames></author><author><keyname>Esling</keyname><forenames>Philippe</forenames></author><author><keyname>Chemla-Romeu-Santos</keyname><forenames>Axel</forenames></author></authors><title>Modulated Variational auto-Encoders for many-to-many musical timbre
  transfer</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have been successfully applied to image style transfer and
domain translation. However, there is still a wide gap in the quality of
results when learning such tasks on musical audio. Furthermore, most
translation models only enable one-to-one or one-to-many transfer by relying on
separate encoders or decoders and complex, computationally-heavy models. In
this paper, we introduce the Modulated Variational auto-Encoders (MoVE) to
perform musical timbre transfer. We define timbre transfer as applying parts of
the auditory properties of a musical instrument onto another. First, we show
that we can achieve this task by conditioning existing domain translation
techniques with Feature-wise Linear Modulation (FiLM). Then, we alleviate the
need for additional adversarial networks by replacing the usual translation
criterion by a Maximum Mean Discrepancy (MMD) objective. This allows a faster
and more stable training along with a controllable latent space encoder. By
further conditioning our system on several different instruments, we can
generalize to many-to-many transfer within a single variational architecture
able to perform multi-domain transfers. Our models map inputs to 3-dimensional
representations, successfully translating timbre from one instrument to another
and supporting sound synthesis from a reduced set of control parameters. We
evaluate our method in reconstruction and generation tasks while analyzing the
auditory descriptor distributions across transferred domains. We show that this
architecture allows for generative controls in multi-domain transfer, yet
remaining light, fast to train and effective on small datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00223</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00223</id><created>2018-09-29</created><authors><author><keyname>Seki</keyname><forenames>Shogo</forenames></author><author><keyname>Kameoka</keyname><forenames>Hirokazu</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Toda</keyname><forenames>Tomoki</forenames></author><author><keyname>Takeda</keyname><forenames>Kazuya</forenames></author></authors><title>Generalized Multichannel Variational Autoencoder for Underdetermined
  Source Separation</title><categories>stat.ML cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a multichannel audio source separation problem under
underdetermined conditions. Multichannel Non-negative Matrix Factorization
(MNMF) is one of powerful approaches, which adopts the NMF concept for source
power spectrogram modeling. This concept is also employed in Independent
Low-Rank Matrix Analysis (ILRMA), a special class of the MNMF framework
formulated under determined conditions. While these methods work reasonably
well for particular types of sound sources, one limitation is that they can
fail to work for sources with spectrograms that do not comply with the NMF
model. To address this limitation, an extension of ILRMA called the
Multichannel Variational Autoencoder (MVAE) method was recently proposed, where
a Conditional VAE (CVAE) is used instead of the NMF model for source power
spectrogram modeling. This approach has shown to perform impressively in
determined source separation tasks thanks to the representation power of DNNs.
While the original MVAE method was formulated under determined mixing
conditions, this paper generalizes it so that it can also deal with
underdetermined cases. We call the proposed framework the Generalized MVAE
(GMVAE). The proposed method was evaluated on a underdetermined source
separation task of separating out three sources from two microphone inputs.
Experimental results revealed that the GMVAE method achieved better performance
than the MNMF method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00237</identifier>
 <datestamp>2019-09-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00237</id><created>2018-09-29</created><updated>2019-09-06</updated><authors><author><keyname>Interdonato</keyname><forenames>Giovanni</forenames></author><author><keyname>Karlsson</keyname><forenames>Marcus</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Downlink Spectral Efficiency of Cell-Free Massive MIMO with Full-Pilot
  Zero-Forcing</title><categories>cs.IT eess.SP math.IT</categories><comments>Paper published in 2018 IEEE Global Conference on Signal and
  Information Processing (GlobalSIP). {\copyright} 2019 IEEE. Personal use of
  this material is permitted. Permission from IEEE must be obtained for all
  other uses</comments><doi>10.1109/GlobalSIP.2018.8646666.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell-free Massive multiple-input multiple-output (MIMO) ensures ubiquitous
communication at high spectral efficiency (SE) thanks to increased
macro-diversity as compared cellular communications. However, system
scalability and performance are limited by fronthauling traffic and
interference. Unlike conventional precoding schemes that only suppress
intra-cell interference, full-pilot zero-forcing (fpZF), introduced in [1],
actively suppresses also inter-cell interference, without sharing channel state
information (CSI) among the access points (APs). In this study, we derive a new
closed-form expression for the downlink (DL) SE of a cell-free Massive MIMO
system with multi-antenna APs and fpZF precoding, under imperfect CSI and pilot
contamination. The analysis also includes max-min fairness DL power
optimization. Numerical results show that fpZF significantly outperforms
maximum ratio transmission scheme, without increasing the fronthauling
overhead, as long as the system is sufficiently distributed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00321</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00321</id><created>2018-09-30</created><authors><author><keyname>Rishani</keyname><forenames>Nadeen</forenames></author><author><keyname>Elayan</keyname><forenames>Hadeel</forenames></author><author><keyname>Shubair</keyname><forenames>Raed</forenames></author><author><keyname>Kiourti</keyname><forenames>Asimina</forenames></author></authors><title>Wearable, Epidermal, and Implantable Sensors for Medical Applications</title><categories>eess.SP</categories><comments>48 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous health monitoring using wireless body area networks (WBANs) of
wearable, epidermal and implantable medical devices is envisioned as a
transformative approach to healthcare. Rapid advances in biomedical sensors,
low-power electronics, and wireless communications have brought this vision to
the verge of reality. However, key challenges still remain to be addressed.
This paper surveys the current state-of-the-art in the area of wireless sensors
for medical applications. Specifically, it focuses on presenting the recent
advancements in wearable, epidermal and implantable technologies, and discusses
reported ways of powering up such sensors. Furthermore, this paper addresses
the challenges that exist in the various Open Systems Interconnection (OSI)
layers and illustrates future research areas concerning the utilization of
wireless sensors in healthcare applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00322</identifier>
 <datestamp>2019-07-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00322</id><created>2018-09-30</created><updated>2019-07-30</updated><authors><author><keyname>Feigin</keyname><forenames>Micha</forenames></author><author><keyname>Freedman</keyname><forenames>Daniel</forenames></author><author><keyname>Anthony</keyname><forenames>Brian W.</forenames></author></authors><title>A Deep Learning Framework for Single-Sided Sound Speed Inversion in
  Medical Ultrasound</title><categories>cs.LG eess.SP q-bio.TO stat.ML</categories><journal-ref>IEEE Trans Biomed Eng. 2019 Jul 25</journal-ref><doi>10.1109/TBME.2019.2931195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: Ultrasound elastography is gaining traction as an accessible and
useful diagnostic tool for such things as cancer detection and differentiation
and thyroid disease diagnostics. Unfortunately, state of the art shear wave
imaging techniques, essential to promote this goal, are limited to high-end
ultrasound hardware due to high power requirements; are extremely sensitive to
patient and sonographer motion, and generally, suffer from low frame rates.
Motivated by research and theory showing that longitudinal wave sound speed
carries similar diagnostic abilities to shear wave imaging, we present an
alternative approach using single sided pressure-wave sound speed measurements
from channel data.
  Methods: In this paper, we present a single-sided sound speed inversion
solution using a fully convolutional deep neural network. We use simulations
for training, allowing the generation of limitless ground truth data.
  Results: We show that it is possible to invert for longitudinal sound speed
in soft tissue at high frame rates. We validate the method on simulated data.
We present highly encouraging results on limited real data.
  Conclusion: Sound speed inversion on channel data has significant potential,
made possible in real time with deep learning technologies.
  Significance: Specialized shear wave ultrasound systems remain inaccessible
in many locations. longitudinal sound speed and deep learning technologies
enable an alternative approach to diagnosis based on tissue elasticity. High
frame rates are possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00484</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00484</id><created>2018-09-30</created><authors><author><keyname>Krivoku&#x107;a</keyname><forenames>Maja</forenames></author><author><keyname>Koroteev</keyname><forenames>Maxim</forenames></author><author><keyname>Chou</keyname><forenames>Philip A.</forenames></author></authors><title>A Volumetric Approach to Point Cloud Compression</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compression of point clouds has so far been confined to coding the positions
of a discrete set of points in space and the attributes of those discrete
points. We introduce an alternative approach based on volumetric functions,
which are functions defined not just on a finite set of points, but throughout
space. As in regression analysis, volumetric functions are continuous functions
that are able to interpolate values on a finite set of points as linear
combinations of continuous basis functions. Using a B-spline wavelet basis, we
are able to code volumetric functions representing both geometry and
attributes. Geometry is represented implicitly as the level set of a volumetric
function (the signed distance function or similar). Attributes are represented
by a volumetric function whose coefficients can be regarded as a critically
sampled orthonormal transform that generalizes the recent successful
region-adaptive hierarchical (or Haar) transform to higher orders. Experimental
results show that both geometry and attribute compression using volumetric
functions improve over those used in the emerging MPEG Point Cloud Compression
standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00532</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00532</id><created>2018-10-01</created><authors><author><keyname>Nooraiepour</keyname><forenames>Alireza</forenames></author><author><keyname>Hamidouche</keyname><forenames>Kenza</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan</forenames></author></authors><title>How Secure are Multicarrier Communication Systems Against Signal
  Exploitation Attacks?</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, robustness of non-contiguous orthogonal frequency division
multiplexing (NC-OFDM) transmissions is investigated and contrasted to OFDM
transmissions for fending off signal exploitation attacks. In contrast to ODFM
transmissions, NC-OFDM transmissions take place over a subset of active
subcarriers to either avoid incumbent transmissions or for strategic
considerations. A point-to-point communication system is considered in this
paper in the presence of an adversary (exploiter) that aims to infer
transmission parameters (e.g., the subset of active subcarriers and duration of
the signal) using a deep neural network (DNN). This method has been proposed
since the existing methods for exploitation, which are based on cyclostationary
analysis, have been shown to have limited success in NC-OFDM systems. A good
estimation of the transmission parameters allows the adversary to transmit
spurious data and attack the legitimate receiver. Simulation results show that
the DNN can infer the transmit parameters of OFDM signals with very good
accuracy. However, NC-OFDM with fully random selection of active subcarriers
makes it difficult for the adversary to exploit the waveform and thus for the
receiver to be affected by the spurious data. Moreover, the more structured the
set of active subcarriers selected by the transmitter is, the easier it is for
the adversary to infer the transmission parameters and attack the receiver
using a DNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00551</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00551</id><created>2018-10-01</created><authors><author><keyname>Iqbal</keyname><forenames>Talha</forenames></author><author><keyname>Ali</keyname><forenames>Hazrat</forenames></author></authors><title>Generative Adversarial Network for Medical Images (MI-GAN)</title><categories>cs.LG cs.CV eess.IV stat.ML</categories><comments>Journal of Medical Systems</comments><journal-ref>Med Syst (2018) 42: 231</journal-ref><doi>10.1007/s10916-018-1072-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning algorithms produces state-of-the-art results for different
machine learning and computer vision tasks. To perform well on a given task,
these algorithms require large dataset for training. However, deep learning
algorithms lack generalization and suffer from over-fitting whenever trained on
small dataset, especially when one is dealing with medical images. For
supervised image analysis in medical imaging, having image data along with
their corresponding annotated ground-truths is costly as well as time consuming
since annotations of the data is done by medical experts manually. In this
paper, we propose a new Generative Adversarial Network for Medical Imaging
(MI-GAN). The MI-GAN generates synthetic medical images and their segmented
masks, which can then be used for the application of supervised analysis of
medical images. Particularly, we present MI-GAN for synthesis of retinal
images. The proposed method generates precise segmented images better than the
existing techniques. The proposed model achieves a dice coefficient of 0.837 on
STARE dataset and 0.832 on DRIVE dataset which is state-of-the-art performance
on both the datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00568</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00568</id><created>2018-10-01</created><authors><author><keyname>Yu</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Shunqing</forenames></author><author><keyname>Cao</keyname><forenames>Shan</forenames></author><author><keyname>Xu</keyname><forenames>Shugong</forenames></author></authors><title>Performance Evaluation for LTE-V based Vehicle-to-Vehicle Platooning
  Communication</title><categories>eess.SP</categories><comments>6 pages, APCC 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the raising demand for autonomous driving, vehicle-to-vehicle
communications becomes a key technology enabler for the future intelligent
transportation system. Based on our current knowledge field, there is limited
network simulator that can support end-to-end performance evaluation for LTE-V
based vehicle-to-vehicle platooning systems. To address this problem, we start
with an integrated platform that combines traffic generator and network
simulator together, and build the V2V transmission capability according to
LTE-V specification. On top of that, we simulate the end-to-end throughput and
delay profiles in different layers to compare different configurations of
platooning systems. Through numerical experiments, we show that the LTE-V
system is unable to support the highest degree of automation under shadowing
effects in the vehicle platooning scenarios, which requires ultra-reliable
low-latency communication enhancement in 5G networks. Meanwhile, the throughput
and delay performance for vehicle platooning changes dramatically in PDCP
layers, where we believe further improvements are necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00618</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00618</id><created>2018-10-01</created><authors><author><keyname>Amusan</keyname><forenames>Akinwumi</forenames></author><author><keyname>Amusan</keyname><forenames>Elizabeth</forenames></author></authors><title>Design and simulation of 1.28 Tbps dense wavelength division multiplex
  system suitable for long haul backbone</title><categories>eess.SP physics.app-ph</categories><comments>Accepted for publication in Journal of Optical Communications - De
  Gruyter</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Wavelength division multiplex (WDM) system with on / off keying (OOK)
modulation and direct detection (DD) is generally simple to implement, less
expensive and energy efficient. The determination of the possible design
capacity limit, in terms of the bit rate-distance product in WDM-OOK-DD systems
is therefore crucial, considering transmitter / receiver simplicity, as well as
energy and cost efficiency. A 32-channel wavelength division multiplex system
is designed and simulated over 1000 km fiber length using Optsim commercial
simulation software. The standard channel spacing of 0.4 nm was used in the
C-band range from 1.5436-1.556 nm. Each channel used the simple non return to
zero - on / off keying (NRZ-OOK) modulation format to modulate a continuous
wave (CW) laser source at 40 Gbps using an external modulator, while the
receiver uses a DD scheme. It is proposed that the design will be suitable for
long haul mobile backbone in a national network, since up to 1.28 Tbps data
rates can be transmitted over 1000 km. A bit rate-length product of 1.28
Pbps.km was obtained as the optimum capacity limit in 32 channel dispersion
managed WDM-OOK-DD system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00651</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00651</id><created>2018-08-20</created><authors><author><keyname>Sadat</keyname><forenames>Sayed Abdullah</forenames></author><author><keyname>Sahraei-Ardakani</keyname><forenames>Mostafa</forenames></author></authors><title>Reducing the Risk of Cascading Failures via Transmission Switching</title><categories>eess.SP cs.SY</categories><comments>8 Pages, 6 figures, 6 tables, Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After decades of research, cascading blackouts remain one of the unresolved
challenges in the bulk power systems. A new perspective for measuring the
susceptibility of the system to cascading failures is clearly needed. The newly
developed concept of system stress metrics may be able to provide new insight
into this problem. The method employs power engineering and graph theory to
analyze the network structure and electrical properties of the system, with
metrics that measure stress as the susceptibility to cascading failures. In
this paper, we investigate the effectiveness of transmission switching in
reducing the risk of cascading failures, measured in system stress metrics. A
case study, analyzing different metrics on IEEE-118 bus test system, is
presented. The results show that transmission switching can be used as a
preventive as well as a corrective mechanism to reduce the system's
susceptibility to cascading failures. Contrary to the conventional operation
wisdom that switching lines out of service jeopardizes reliability, our results
suggest the opposite; system operators can use transmission switching, when the
system is under stress, as a tool to reduce the risk of cascading failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00658</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00658</id><created>2018-09-08</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Guoqing</forenames></author><author><keyname>Wang</keyname><forenames>Zhenhao</forenames></author></authors><title>Rule extraction based on extreme learning machine and an improved
  ant-miner algorithm for transient stability assessment</title><categories>eess.SP</categories><comments>Accepted by PloS one</comments><journal-ref>PloS one 10 (2015) 1-18</journal-ref><doi>10.1371/journal.pone.0130814</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to overcome the problems of poor understandability of the pattern
recognition-based transient stability assessment (PRTSA) methods, a new rule
extraction method based on extreme learning machine (ELM) and an improved
Ant-miner (IAM) algorithm is presented in this paper. First, the basic
principles of ELM and Ant-miner algorithm are respectively introduced. Then,
based on the selected optimal feature subset, an example sample set is
generated by the trained ELM-based PRTSA model. And finally, a set of
classification rules are obtained by IAM algorithm to replace the original ELM
network. The novelty of this proposal is that transient stability rules are
extracted from an example sample set generated by the trained ELM-based
transient stability assessment model by using IAM algorithm. The effectiveness
of the proposed method is shown by the application results on the New England
39-bus power system and a practical power system - the southern power system of
Hebei province.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00659</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00659</id><created>2018-09-19</created><authors><author><keyname>Pan</keyname><forenames>Jiachun</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>Identifying Rumor Sources Using Dominant Eigenvalue of Nonbacktracking
  Matrix</title><categories>eess.SP cs.SI physics.soc-ph</categories><comments>To appear at GlobalSIP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of identifying rumor sources in a network, in which
rumor spreading obeys a time-slotted susceptible-infected model. Unlike
existing approaches, our proposed algorithm identifies as sources those nodes,
which when set as sources, result in the smallest dominant eigenvalue of the
corresponding reduced nonbacktracking matrix deduced from message passing
equations. We also propose a reduced-complexity algorithm derived from the
previous algorithm through a perturbation approximation. Numerical experiments
on synthesized and real-world networks suggest that these proposed algorithms
generally have higher accuracy compared with representative existing
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00661</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00661</id><created>2018-09-23</created><authors><author><keyname>Yang</keyname><forenames>Jingkang</forenames></author><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author></authors><title>Enhancing Geometric Deep Learning via Graph Filter Deconvolution</title><categories>eess.SP</categories><comments>5 pages, 8 figures, to appear in the proceedings of the 2018 6th IEEE
  Global Conference on Signal and Information Processing, November 26-29, 2018,
  Anaheim, California, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we incorporate a graph filter deconvolution step into the
classical geometric convolutional neural network pipeline. More precisely,
under the assumption that the graph domain plays a role in the generation of
the observed graph signals, we pre-process every signal by passing it through a
sparse deconvolution operation governed by a pre-specified filter bank. This
deconvolution operation is formulated as a group-sparse recovery problem, and
convex relaxations that can be solved efficiently are put forth. The
deconvolved signals are then fed into the geometric convolutional neural
network, yielding better classification performance than their unprocessed
counterparts. Numerical experiments showcase the effectiveness of the
deconvolution step on classification tasks on both synthetic and real-world
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00790</identifier>
 <datestamp>2018-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00790</id><created>2018-10-01</created><authors><author><keyname>Lostanlen</keyname><forenames>Vincent</forenames></author></authors><title>Eigentriads and Eigenprogressions on the Tonnetz</title><categories>cs.SD eess.AS</categories><comments>Proceedings of the Late-Breaking / Demo session (LBD) of the
  International Society of Music Information Retrieval (ISMIR). September 2018,
  Paris, France. Source code at github.com/lostanlen/ismir2018-lbd</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We introduce a new multidimensional representation, named eigenprogression
transform, that characterizes some essential patterns of Western tonal harmony
while being equivariant to time shifts and pitch transpositions. This
representation is deep, multiscale, and convolutional in the piano-roll domain,
yet incurs no prior training, and is thus suited to both supervised and
unsupervised MIR tasks. The eigenprogression transform combines ideas from the
spiral scattering transform, spectral graph theory, and wavelet shrinkage
denoising. We report state-of-the-art results on a task of supervised composer
recognition (Haydn vs. Mozart) from polyphonic music pieces in MIDI format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.00852</identifier>
 <datestamp>2018-12-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.00852</id><created>2018-10-01</created><updated>2018-12-01</updated><authors><author><keyname>Fannjiang</keyname><forenames>Albert</forenames></author></authors><title>Raster Grid Pathology and the Cure</title><categories>eess.IV cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind ptychography is a phase retrieval method using multiple coded
diffraction patterns from different, overlapping parts of the unknown extended
object illuminated with an unknown window function. The window function is also
known as the probe in the optics literature. As such blind ptychography is an
inverse problem of simultaneous recovery of the object and the window function
given the intensities of the windowed Fourier transform and has a multi-scale
set-up in which the probe has an intermediate scale between the pixel scale and
the macro-scale of the extended object. Uniqueness problem for blind
ptychography is analyzed rigorously for the raster scan (of a constant step
size {\tau}) and its variants, in which another scale comes into play: the
overlap between adjacent blocks (the shifted windows). The block phases are
shown to form an arithmetic progression and the complete characterization of
the raster scan ambiguities is given, including: First, the periodic raster
grid pathology of degrees of freedom proportional to {\tau}^2 and, second, a
non-periodic, arithmetically progressing phase shift from block to block.
Finally irregularly perturbed raster scans are shown to remove all ambiguities
other than the inherent ambiguities of the scaling factor and the affine phase
ambiguity under the minimum requirement of roughly 50% overlap ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01081</identifier>
 <datestamp>2018-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01081</id><created>2018-10-02</created><authors><author><keyname>Wang</keyname><forenames>Michael</forenames></author></authors><title>On the Interference from VDE-SAT Downlink to the Incumbent Land Mobile
  System</title><categories>eess.SP</categories><comments>submitted to IEEE transactions for potential publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although both frequency utilization plans for VDE-SAT downlink are within the
frequency range from 156.0125 to 162.0375 MHz which belongs to the VHF maritime
mobile band, on land, this band is mostly allocated for conventional and
trunked land mobile systems by safety agencies, utilities and transportation
companies. The challenge lies in the fact that there is no existing regulatory
rule directly established for protection of the land system against the
satellite system or the like, and hence the evaluation of the potential impact
on the incumbent land systems becomes difficult if not impossible. A method
adopted in the current analysis is to place general restrictions on the
emissions from the satellite stations inferred from the existing regulatory
rules for interference protection between land systems specified by ITU and
ECC. The restrictions are expressed in terms of values of maximum allowed power
flux density (PFD) emitted by any space stations to the surface of the Earth at
all possible incident angles in a reference bandwidth that serves as a
protection mask for the land system such that the actual interference that the
land system experiences is no worse than that from another land mobile system
permitted by these regulations. This article examines the regulatory
constraints specified by ITU and ECC in these bands and its implications on the
PFD mask.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01169</identifier>
 <datestamp>2018-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01169</id><created>2018-10-02</created><authors><author><keyname>Rey-Otero</keyname><forenames>Ives</forenames></author><author><keyname>Sulam</keyname><forenames>Jeremias</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Variations on the CSC model</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, the celebrated sparse representation model has achieved
impressive results in various signal and image processing tasks. A
convolutional version of this model, termed convolutional sparse coding (CSC),
has been recently reintroduced and extensively studied. CSC brings a natural
remedy to the limitation of typical sparse enforcing approaches of handling
global and high-dimensional signals by local, patch-based, processing. While
the classic field of sparse representations has been able to cater for the
diverse challenges of different signal processing tasks by considering a wide
range of problem formulations, almost all available algorithms that deploy the
CSC model consider the same $\ell_1 - \ell_2$ problem form. As we argue in this
paper, this CSC pursuit formulation is also too restrictive as it fails to
explicitly exploit some local characteristics of the signal. This work expands
the range of formulations for the CSC model by proposing two convex
alternatives that merge global norms with local penalties and constraints. The
main contribution of this work is the derivation of efficient and provably
converging algorithms to solve these new sparse coding formulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01248</identifier>
 <datestamp>2018-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01248</id><created>2018-09-27</created><authors><author><keyname>Peng</keyname><forenames>Xutan</forenames></author><author><keyname>Li</keyname><forenames>Chen</forenames></author><author><keyname>Cai</keyname><forenames>Zhi</forenames></author><author><keyname>Shi</keyname><forenames>Faqiang</forenames></author><author><keyname>Liu</keyname><forenames>Yidan</forenames></author><author><keyname>Li</keyname><forenames>Jianxin</forenames></author></authors><title>A Lightweight Music Texture Transfer System</title><categories>cs.SD cs.LG cs.MM eess.AS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning researches on the transformation problems for image and text
have raised great attention. However, present methods for music feature
transfer using neural networks are far from practical application. In this
paper, we initiate a novel system for transferring the texture of music, and
release it as an open source project. Its core algorithm is composed of a
converter which represents sounds as texture spectra, a corresponding
reconstructor and a feed-forward transfer network. We evaluate this system from
multiple perspectives, and experimental results reveal that it achieves
convincing results in both sound effects and computational performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01349</identifier>
 <datestamp>2018-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01349</id><created>2018-10-02</created><authors><author><keyname>Guerra</keyname><forenames>David William Marques</forenames></author><author><keyname>Fukuda</keyname><forenames>Rafael Masashi</forenames></author><author><keyname>Kobayashi</keyname><forenames>Ricardo Tadashi</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>Efficient Detectors for MIMO-OFDM Systems under Spatial Correlation
  Antenna Arrays</title><categories>eess.SP</categories><comments>26 pgs, 16 figures and 5 tables</comments><journal-ref>ETRI Journal, Vol. 40, Issue 5, October 2018, Pages 570-581</journal-ref><doi>10.4218/etrij.2018-0005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work analyzes the performance of the implementable detectors for
multiple-input-multiple-output (MIMO) orthogonal frequency division
multiplexing (OFDM) technique under specific and realistic operation system
condi- tions, including antenna correlation and array configuration.
Time-domain channel model has been used to evaluate the system performance
under realistic communication channel and system scenarios, including different
channel correlation, modulation order and antenna arrays configurations. A
bunch of MIMO-OFDM detectors were analyzed for the purpose of achieve high
performance combined with high capacity systems and manageable computational
complexity. Numerical Monte-Carlo simulations (MCS) demonstrate the channel
selectivity effect, while the impact of the number of antennas, adoption of
linear against heuristic-based detection schemes, and the spatial correlation
effect under linear and planar antenna arrays are analyzed in the MIMO-OFDM
context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01395</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01395</id><created>2018-10-02</created><updated>2019-03-07</updated><authors><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author><author><keyname>Wichern</keyname><forenames>Gordon</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Sarroff</keyname><forenames>Andy</forenames></author><author><keyname>Hershey</keyname><forenames>John R.</forenames></author></authors><title>Phasebook and Friends: Leveraging Discrete Representations for Source
  Separation</title><categories>cs.SD cs.CL cs.LG eess.AS stat.ML</categories><doi>10.1109/JSTSP.2019.2904183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning based speech enhancement and source separation systems have
recently reached unprecedented levels of quality, to the point that performance
is reaching a new ceiling. Most systems rely on estimating the magnitude of a
target source by estimating a real-valued mask to be applied to a
time-frequency representation of the mixture signal. A limiting factor in such
approaches is a lack of phase estimation: the phase of the mixture is most
often used when reconstructing the estimated time-domain signal. Here, we
propose &quot;magbook&quot;, &quot;phasebook&quot;, and &quot;combook&quot;, three new types of layers based
on discrete representations that can be used to estimate complex time-frequency
masks. Magbook layers extend classical sigmoidal units and a recently
introduced convex softmax activation for mask-based magnitude estimation.
Phasebook layers use a similar structure to give an estimate of the phase mask
without suffering from phase wrapping issues. Combook layers are an alternative
to the magbook-phasebook combination that directly estimate complex masks. We
present various training and inference schemes involving these representations,
and explain in particular how to include them in an end-to-end learning
framework. We also present an oracle study to assess upper bounds on
performance for various types of masks using discrete phase representations. We
evaluate the proposed methods on the wsj0-2mix dataset, a well-studied corpus
for single-channel speaker-independent speaker separation, matching the
performance of state-of-the-art mask-based approaches without requiring
additional phase reconstruction steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01442</identifier>
 <datestamp>2018-12-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01442</id><created>2018-10-02</created><updated>2018-12-16</updated><authors><author><keyname>Chen</keyname><forenames>Jianlin</forenames></author><author><keyname>Raye</keyname><forenames>Devin</forenames></author><author><keyname>Khawaja</keyname><forenames>Wahab</forenames></author><author><keyname>Sinha</keyname><forenames>Priyanka</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author></authors><title>Impact of 3D UWB Antenna Radiation Pattern on Air-to-Ground Drone
  Connectivity</title><categories>eess.SP</categories><comments>The paper was accepted and presented in VTC Fall 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three dimensional (3D) radiation pattern of an antenna mounted at a drone can
significantly influence the air-to-ground (A2G) link quality. Even when a drone
transmitter is very close to a ground receiver, if the antenna orientations are
not aligned properly, a significant degradation can be observed in the received
signal power at the receiver. To characterize such effects for a
doughnut-shaped antenna radiation pattern, using an ultra-wideband (UWB)
transmitter at the drone and a UWB receiver at the ground, we carry out A2G
channel measurements to capture the link quality at the ground receiver for
various link distances, drone heights, and antenna orientations. We develop a
simple analytical model to approximate the influence of 3D antenna patterns on
the received signal strength (RSS), which show reasonable agreement with
measurements despite the simplicity of the model and the complicated 3D
radiation from the UWB antennas. We also explore how the signal strength can be
improved when multiple antennas with different orientations are utilized at
transmitter/receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01469</identifier>
 <datestamp>2018-10-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01469</id><created>2018-10-02</created><authors><author><keyname>Rahimian</keyname><forenames>Ardavan</forenames></author></authors><title>Modeling and Performance of Microwave and Millimeter-Wave Layered
  Waveguide Filters</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents novel designs, analysis, and performance of 4-pole and
8-pole microwave and millimeter-wave (MMW) waveguide filters for operation at X
and Y frequency bands. The waveguide filters have been designed and analyzed
based on the RF mode matching and coupled resonators design techniques
employing layered technology. Thorough waveguide filters working at X-band and
Y-band have been designed, analyzed, fabricated, and also tested along with the
analysis of the output characteristics. Accurate designs of RF waveguides along
with their filters based on the E-plane filter concept have been carried out
with the ability of fitting into the layered technology in high frequency
production techniques. The filters demonstrate the appropriateness in order to
develop high-performance well-established designs for systems that are intended
for the multi-layer microwave, millimeter- and sub-millimeter-waves devices and
systems; with the potential employment in radar, satellite, and radio astronomy
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01487</identifier>
 <datestamp>2018-10-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01487</id><created>2018-10-01</created><authors><author><keyname>Costa</keyname><forenames>Bruno Felipe</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>Closed-Form Directivity Expression for Arbitrary Volumetric Antenna
  Arrays</title><categories>eess.SP</categories><comments>12 pages, 2 tables, paper accepted to IEEE-TAP, IEEE Transactions on
  Antennas and Propagation, 2018</comments><doi>10.1109/TAP.2018.2869243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proposed a closed-form expression of directivity for an arbitrary
volumetric antenna arrays using a general element pattern expression of type
$\sin^u{(\theta)}\cos^v{(\theta)}$, with $v &gt; -\frac{1}{2}$ and $u &gt; -1$, and
$u, v \in \mathbb{Z}$. Variations of this expression for different values of
$v$ and $u$ are analyzed from the analytical and numerical perspectives. The
parameters found in the closed-form expression are related to the order $v$ and
$u$ of the element patterns, the rectangular spatial coordinate of each antenna
element, the magnitude and phase excitation coefficients (complex excitation)
of all elements, and the desired angle in spherical coordinates $(\theta_0,
\phi_0)$. The expression found in this work has been validated by numerical
results, considering distinct configuration scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01488</identifier>
 <datestamp>2018-10-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01488</id><created>2018-10-01</created><authors><author><keyname>Yuan</keyname><forenames>B.</forenames></author><author><keyname>Tan</keyname><forenames>Y. J.</forenames></author><author><keyname>Mudunuru</keyname><forenames>M. K.</forenames></author><author><keyname>Marcillo</keyname><forenames>O. E.</forenames></author><author><keyname>Delorey</keyname><forenames>A. A.</forenames></author><author><keyname>Roberts</keyname><forenames>P. M.</forenames></author><author><keyname>Webster</keyname><forenames>J. D.</forenames></author><author><keyname>Gammans</keyname><forenames>C. N. L.</forenames></author><author><keyname>Karra</keyname><forenames>S.</forenames></author><author><keyname>Guthrie</keyname><forenames>G. D.</forenames></author><author><keyname>Johnson</keyname><forenames>P. A.</forenames></author></authors><title>Using Machine Learning to Discern Eruption in Noisy Environments: A Case
  Study using CO2-driven Cold-Water Geyser in Chimayo, New Mexico</title><categories>eess.SP cs.LG physics.data-an physics.geo-ph stat.ML</categories><comments>16 pages,7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach based on machine learning (ML) to distinguish eruption
and precursory signals of Chimay\'{o} geyser (New Mexico, USA) under noisy
environments. This geyser can be considered as a natural analog of
$\mathrm{CO}_2$ intrusion into shallow water aquifers. By studying this geyser,
we can understand upwelling of $\mathrm{CO}_2$-rich fluids from depth, which
has relevance to leak monitoring in a $\mathrm{CO}_2$ sequestration project. ML
methods such as Random Forests (RF) are known to be robust multi-class
classifiers and perform well under unfavorable noisy conditions. However, the
extent of the RF method's accuracy is poorly understood for this
$\mathrm{CO}_2$-driven geysering application. The current study aims to
quantify the performance of RF-classifiers to discern the geyser state. Towards
this goal, we first present the data collected from the seismometer that is
installed near the Chimay\'{o} geyser. The seismic signals collected at this
site contain different types of noises such as daily temperature variations,
seasonal trends, animal movement near the geyser, and human activity. First, we
filter the signals from these noises by combining the Butterworth-Highpass
filter and an Autoregressive method in a multi-level fashion. We show that by
combining these filtering techniques, in a hierarchical fashion, leads to
reduction in the noise in the seismic data without removing the precursors and
eruption event signals. We then use RF on the filtered data to classify the
state of geyser into three classes -- remnant noise, precursor, and eruption
states. We show that the classification accuracy using RF on the filtered data
is greater than 90\%.These aspects make the proposed ML framework attractive
for event discrimination and signal enhancement under noisy conditions, with
strong potential for application to monitoring leaks in $\mathrm{CO}_2$
sequestration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01523</identifier>
 <datestamp>2018-10-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01523</id><created>2018-10-02</created><authors><author><keyname>Yao</keyname><forenames>Miao</forenames></author><author><keyname>Carrick</keyname><forenames>Matt</forenames></author><author><keyname>Sohul</keyname><forenames>Munawwar M.</forenames></author><author><keyname>Marojevic</keyname><forenames>Vuk</forenames></author><author><keyname>Patterson</keyname><forenames>Cameron D.</forenames></author><author><keyname>Reed</keyname><forenames>Jeffrey H.</forenames></author></authors><title>Semidefinite Relaxation-Based PAPR-Aware Precoding for Massive MIMO-OFDM
  Systems</title><categories>eess.SP</categories><journal-ref>IEEE Transactions on Vehicular Technology 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO requires a large number of antennas and the same amount of power
amplifiers (PAs), one per antenna. As opposed to 4G base stations, which could
afford highly linear PAs, next-generation base stations will need to use
inexpensive PAs, which have a limited region of linear amplification. One of
the research challenges is effectively handling signals which have high
peak-to-average power ratios (PAPRs), such as orthogonal frequency division
multiplexing (OFDM). This paper introduces a PAPR-aware precoding scheme that
exploits the excessive spatial degrees-of-freedom of large scale multiple-input
multipleoutput (MIMO) antenna systems. This typically requires finding a
solution to a nonconvex optimization problem. Instead of relaxing the problem
to minimize the peak power, we introduce a practical semidefinite relaxation
(SDR) framework that enables accurately and efficiently approximating the
theoretical PAPR-aware precoding performance for OFDM-based massive MIMO
systems. The framework allows incorporating channel uncertainties and intercell
coordination. Numerical results show that several orders of magnitude
improvements can be achieved w.r.t. state of the art techniques, such as
instantaneous power consumption reduction and multiuser interference
cancellation. The proposed PAPRaware precoding can be effectively handled along
with the multicell signal processing by the centralized baseband processing
platforms of next-generation radio access networks. Performance can be traded
for the computing efficiency for other platforms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01534</identifier>
 <datestamp>2018-10-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01534</id><created>2018-10-02</created><authors><author><keyname>Burghal</keyname><forenames>Daoud</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Band Assignment in Dual Band Systems: A Learning-based Approach</title><categories>eess.SP cs.LG cs.NI</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the band assignment problem in dual band systems, where the
base-station (BS) chooses one of the two available frequency bands
(centimeter-wave and millimeter-wave bands) to communicate data to the mobile
station (MS). While the millimeter-wave band offers higher data rate when it is
available, there is a significant probability of outage during which the
communication should be carried on the centimeter-wave band.
  In this work, we use a machine learning framework to provide an efficient and
practical solution to the band assignment problem. In particular, the BS trains
a Neural Network (NN) to predict the right band assignment decision using
observed channel information. We study the performance of the NN in two
environments: (i) A stochastic channel model with correlated bands, and (ii)
microcellular outdoor channels obtained by simulations with a commercial
ray-tracer. For the former case, for sake of comparison we also develop a
threshold based band assignment that relies on the optimal mean square error
estimator of the best band. In addition, we study the performance of the
NN-based solution with different NN structures and different observed
parameters (position, field strength, etc.). We compare the achieved
performance to linear and logistic regression based solutions as well as the
threshold based solution. Under practical constraints, the learning based band
assignment shows competitive or superior performance in both environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01600</identifier>
 <datestamp>2018-10-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01600</id><created>2018-10-03</created><authors><author><keyname>Fukuda</keyname><forenames>Rafael Masashi</forenames></author><author><keyname>Guerra</keyname><forenames>David William Marques</forenames></author><author><keyname>Kobayashi</keyname><forenames>Ricardo Tadashi</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>DE/PSO-aided Hybrid Linear Detectors for MIMO-OFDM Systems under
  Correlated Arrays</title><categories>eess.SP</categories><comments>19 pages, 7 figures and 5 tables</comments><doi>10.1002/ett.3495</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the performance of evolutionary heuristic-aided
linear detectors deployed in Multiple-Input Multiple-Output (MIMO) Orthogonal
Frequency-Division Multiplexing (OFDM) systems, considering realistic operating
scenarios. Hybrid linear-heuristic detectors under different initial solutions
provided by linear detectors are considered, namely differential evolution (DE)
and particle swarm optimization (PSO). Numerical results demonstrated the
applicability of hybrid detection approach, which can improve considerably the
performance of minimum mean-square error (MMSE) and matched filter (MF)
detectors. Furthermore, we discuss how the complexity of the presented
algorithms scales with the number of antennas, besides of verifying the spatial
correlation effects on MIMO-OFDM performance assisted by linear, heuristic and
hybrid detection schemes. The influence of the initial point in the performance
improvement and complexity reduction is evaluated numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01739</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01739</id><created>2018-10-03</created><updated>2018-12-20</updated><authors><author><keyname>Hague</keyname><forenames>David A.</forenames></author><author><keyname>Buck</keyname><forenames>John R.</forenames></author></authors><title>An Experimental Evaluation of the Generalized Sinusoidal Frequency
  Modulated Waveform for Active Sonar Systems</title><categories>eess.SP</categories><comments>The following article has been submitted to the Journal of the
  Acoustical Society of America. After it is published, it will be found at
  http://asa.scitation.org/journal/jas</comments><doi>10.1121/1.5113581</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper experimentally evaluates the Generalized Sinusoidal Frequency
Modulated (GSFM) waveform, a generalization of the Sinusoidal Frequency
Modulated (SFM) waveform. The Instantaneous Frequency (IF) of the GSFM
resembles the time/voltage characteristic of a Linear FM (LFM) chirp waveform.
Consequently, the GSFM possesses an Ambiguity Function (AF) that resembles a
thumbtack shape. Practical sonar system design must consider two factors beyond
the AF. The spectral efficiency (SE), defined as the ratio of energy in an
operational frequency band to the total waveform energy, is another important
metric for waveform design. The Peak-to-Average-Power Ratio (PAPR) quantifies
how close the waveform is to constant amplitude. These measures predict a
waveform's energy efficiency and ability to be accurately replicated on
practical piezoelectric transducers, which have limits on both their bandwidth
and maximum transmit power. This paper explores these design considerations for
the GSFM waveform and evaluates its performance against a host of other well
established waveforms using simulated and experimental acoustic data. The GSFM
possesses superior SE, PAPR, and overall energy efficiency when compared to
thumbtack waveforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01760</identifier>
 <datestamp>2019-01-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01760</id><created>2018-10-03</created><updated>2018-12-29</updated><authors><author><keyname>Shi</keyname><forenames>Yue</forenames></author><author><keyname>Baran</keyname><forenames>Mesut</forenames></author></authors><title>A Coordinated Volt-Var Control Scheme for Distribution Systems with High
  DER Penetration</title><categories>eess.SP</categories><comments>This work was done in 2017, when Yue Shi was a PhD student at North
  Carolina State University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new Volt/Var Control (VVC) scheme is proposed to facilitate
the coordination between the conventional VVC devices and the new smart PV
inverters to provide an effective voltage control on a system with high PV
penetration. The proposed scheme decomposes the problem into two levels. The
first level uses Load Tap Changer (LTC) and Voltage Regulators (VRs) to adjust
the voltage level on the circuit to keep the voltages along the circuit within
the desired range. The second level determines Var support needed from smart
inverters to smooth the fast voltage variations while providing effective power
factor correction to keep the power losses at minimum. The case study shows
that the proposed VVC method is very effective in maintaining acceptable
voltages on the system under various operating conditions while meeting the
operational constrains. The results also show the computational efficiency of
the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01865</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01865</id><created>2018-10-03</created><updated>2018-11-07</updated><authors><author><keyname>Pittino</keyname><forenames>Federico</forenames></author><author><keyname>Diversi</keyname><forenames>Roberto</forenames></author><author><keyname>Benini</keyname><forenames>Luca</forenames></author><author><keyname>Bartolini</keyname><forenames>Andrea</forenames></author></authors><title>Robust identification of thermal models for in-production
  High-Performance-Computing clusters with machine learning-based data
  selection</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power and thermal management are critical components of
High-Performance-Computing (HPC) systems, due to their high power density and
large total power consumption. The assessment of thermal dissipation by means
of compact models directly from the thermal response of the final device
enables more robust and precise thermal control strategies as well as automated
diagnosis. However, when dealing with large scale systems &quot;in production&quot;, the
accuracy of learned thermal models depends on the dynamics of the power
excitation, which depends also on the executed workload, and measurement
nonidealities, such as quantization. In this paper we show that, using an
advanced system identification algorithm, we are able to generate very accurate
thermal models (average error lower than our sensors quantization step of
1{\deg}C) for a large scale HPC system on real workloads for very long time
periods. However, we also show that: 1) not all real workloads allow for the
identification of a good model; 2) starting from the theory of system
identification it is very difficult to evaluate if a trace of data leads to a
good estimated model. We then propose and validate a set of techniques based on
machine learning and deep learning algorithms for the choice of data traces to
be used for model identification. We also show that deep learning techniques
are absolutely necessary to correctly choose such traces up to 96% of the
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01930</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01930</id><created>2018-10-03</created><updated>2019-03-25</updated><authors><author><keyname>Noraky</keyname><forenames>James</forenames></author><author><keyname>Sze</keyname><forenames>Vivienne</forenames></author></authors><title>Low Power Depth Estimation of Rigid Objects for Time-of-Flight Imaging</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Depth sensing is useful in a variety of applications that range from
augmented reality to robotics. Time-of-flight (TOF) cameras are appealing
because they obtain dense depth measurements with minimal latency. However, for
many battery-powered devices, the illumination source of a TOF camera is power
hungry and can limit the battery life of the device. To address this issue, we
present an algorithm that lowers the power for depth sensing by reducing the
usage of the TOF camera and estimating depth maps using concurrently collected
images. Our technique also adaptively controls the TOF camera and enables it
when an accurate depth map cannot be estimated. To ensure that the overall
system power for depth sensing is reduced, we design our algorithm to run on a
low power embedded platform, where it outputs 640x480 depth maps at 30 frames
per second. We evaluate our approach on several RGB-D datasets, where it
produces depth maps with an overall mean relative error of 0.96% and reduces
the usage of the TOF camera by 85%. When used with commercial TOF cameras, we
estimate that our algorithm can lower the total power for depth sensing by up
to 73%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.01938</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.01938</id><created>2018-10-03</created><authors><author><keyname>Brifman</keyname><forenames>Alon</forenames></author><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Unified Single-Image and Video Super-Resolution via Denoising Algorithms</title><categories>eess.IV</categories><doi>10.1109/TIP.2019.2924173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single Image Super-Resolution (SISR) aims to recover a high-resolution image
from a given low-resolution version of it. Video Super Resolution (VSR) targets
series of given images, aiming to fuse them to create a higher resolution
outcome. Although SISR and VSR seem to have a lot in common, most SISR
algorithms do not have a simple and direct extension to VSR. VSR is considered
a more challenging inverse problem, mainly due to its reliance on a sub-pixel
accurate motion-estimation, which has no parallel in SISR. Another complication
is the dynamics of the video, often addressed by simply generating a single
frame instead of a complete output sequence.
  In this work we suggest a simple and robust super-resolution framework that
can be applied to single images and easily extended to video. Our work relies
on the observation that denoising of images and videos is well-managed and very
effectively treated by a variety of methods. We exploit the Plug-and-Play-Prior
framework and the Regularization-by-Denoising (RED) approach that extends it,
and show how to use such denoisers in order to handle the SISR and the VSR
problems using a unified formulation and framework. This way, we benefit from
the effectiveness and efficiency of existing image/video denoising algorithms,
while solving much more challenging problems. More specifically, harnessing the
VBM3D video denoiser, we obtain a strongly competitive motion-estimation free
VSR algorithm, showing tendency to a high-quality output and fast processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02026</identifier>
 <datestamp>2019-04-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02026</id><created>2018-10-03</created><authors><author><keyname>Ahn</keyname><forenames>Jinyoup</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author><author><keyname>Lee</keyname><forenames>Kwang Bok</forenames></author></authors><title>EP-based Joint Active User Detection and Channel Estimation for Massive
  Machine-Type Communications</title><categories>eess.SP cs.IT math.IT</categories><comments>submitted for a possible future publication in IEEE Transactions on
  Communications</comments><doi>10.1109/TCOMM.2019.2907853</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive machine-type communication (mMTC) is a newly introduced service
category in 5G wireless communication systems to support a variety of
Internet-of-Things (IoT) applications. In recovering sparsely represented
multi-user vectors, compressed sensing based multi-user detection (CS-MUD) can
be used. CS-MUD is a feasible solution to the grant-free uplink non-orthogonal
multiple access (NOMA) environments. In CS-MUD, active user detection (AUD) and
channel estimation (CE) should be performed before data detection. In this
paper, we propose the expectation propagation based joint AUD and CE
(EP-AUD/CE) technique for mMTC networks. The expectation propagation (EP)
algorithm is a Bayesian framework that approximates a computationally
intractable probability distribution to an easily tractable distribution. The
proposed technique finds the best approximation of the posterior distribution
of the sparse channel vector. Using the approximate distribution, AUD and CE
are jointly performed. We show by numerical simulations that the proposed
technique substantially enhances AUD and CE performances over competing
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02027</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02027</id><created>2018-10-03</created><updated>2018-10-07</updated><authors><author><keyname>Teng</keyname><forenames>Chieh-Fang</forenames></author><author><keyname>Liao</keyname><forenames>Ching-Chun</forenames></author><author><keyname>Chen</keyname><forenames>Chun-Hsiang</forenames></author><author><keyname>Wu</keyname><forenames>An-Yeu</forenames></author></authors><title>Polar Feature Based Deep Architectures for Automatic Modulation
  Classification Considering Channel Fading</title><categories>eess.SP cs.IT cs.LG math.IT</categories><comments>5 pages, accepted by the 2018 Sixth IEEE Global Conference on Signal
  and Information Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To develop intelligent receivers, automatic modulation classification (AMC)
plays an important role for better spectrum utilization. The emerging deep
learning (DL) technique has received much attention in AMC due to its superior
performance in classifying data with deep structure. In this work, a novel
polar-based deep learning architecture with channel compensation network (CCN)
is proposed. Our test results show that learning features from polar domain
(r-theta) can improve recognition accuracy by 5% and reduce training overhead
by 48%. Besides, the proposed CCN is also robust to channel fading, such as
amplitude and phase offsets, and can improve the recognition accuracy by 14%
under practical channel environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02085</identifier>
 <datestamp>2018-10-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02085</id><created>2018-10-04</created><authors><author><keyname>Van Nguyen</keyname><forenames>Binh</forenames></author><author><keyname>Nguyen</keyname><forenames>Minh Tuan</forenames></author><author><keyname>Jung</keyname><forenames>Hyoyoung</forenames></author><author><keyname>Kim</keyname><forenames>Kiseon</forenames></author></authors><title>Designing Anti-Jamming Receivers for NR-DCSK Systems Utilizing ICA, WPD,
  and VMD Methods</title><categories>eess.SP cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider an advanced noise reduction differential chaotic
shift keying (NR-DCSK) system in which a single antenna source communicates
with a single antenna destination under the attack of a single antenna jammer.
We devote our efforts to design a novel anti-jamming (AJ) receiver for the
considered system. Particularly, we propose a variational mode
decomposition-independent component analysis-wavelet packet decomposition-based
(VMD-ICA-WPD-based) structure, in which the VMD method is firstly exploited to
generate multiple signals from the single received one. Secondly, the ICA
method is applied to coarsely separate chaotic and jamming signals. After that,
the WPD method is used to finely estimate and mitigate jamming signals that
exist on all outputs of the ICA method. Finally, an inverse ICA procedure is
carried out, followed by a summation, and the outcome is passed through the
conventional correlation-based receiver for recovering the transmitted
information. Simulation results show that the proposed receiver provides
significant system performance enhancement compared to that given by the
conventional correlation-based receiver with WPD, i.e. 8 dB gain at BER =0.03
and Eb/N0 = 20 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02092</identifier>
 <datestamp>2018-10-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02092</id><created>2018-10-04</created><authors><author><keyname>Amiri</keyname><forenames>Abolfazl</forenames></author><author><keyname>Angjelichinoski</keyname><forenames>Marko</forenames></author><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Extremely Large Aperture Massive MIMO: Low Complexity Receiver
  Architectures</title><categories>eess.SP</categories><comments>Accepted in IEEE Global Communications Conference (Globecom) 2018
  Workshops- 5GNR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on new communication paradigms arising in massive
multiple-input-multiple-output systems where the antenna array at the base
station is of extremely large dimension (xMaMIMO). Due to the extreme dimension
of the array, xMaMIMO is characterized by spatial non-stationary field
properties along the array; this calls for a multi-antenna transceiver design
that is adapted to the array dimension but also its non-stationary properties.
We address implementation aspects of xMaMIMO, with computational efficiency as
our primary objective. To reduce the computational burden of centralized
schemes, we distribute the processing into smaller, disjoint subarrays. Then,
we consider several low-complexity data detection algorithms as candidates for
uplink communication in crowded xMaMIMO systems. Drawing inspiration from coded
random access, one of the main contributions of the paper is the design of low
complexity scheme that exploits the non-stationary nature of xMaMIMO systems
and where the data processing is decentralized. We evaluate the bit-error-rate
performance of the transceivers in crowded xMaMIMO scenarios. The results
confirm their practical potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02184</identifier>
 <datestamp>2019-02-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02184</id><created>2018-10-04</created><authors><author><keyname>da Silva</keyname><forenames>Edson P.</forenames></author><author><keyname>Yankov</keyname><forenames>Metodi P.</forenames></author><author><keyname>Da Ros</keyname><forenames>Francesco</forenames></author><author><keyname>Morioka</keyname><forenames>Toshio</forenames></author><author><keyname>Oxenl&#xf8;we</keyname><forenames>Leif K.</forenames></author></authors><title>Perturbation-based FEC-assisted Iterative Nonlinearity Compensation for
  WDM Systems</title><categories>eess.SP</categories><doi>10.1109/JLT.2018.2882638</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A perturbation-based nonlinear compensation scheme assisted by a feedback
from the forward error correction (FEC) decoder is numerically and
experimentally investigated. It is shown by numerical simulations and
transmission experiments that a feedback from the FEC decoder enables improved
compensation performance, allowing the receiver to operate very close to the
full data-aided performance bounds. The experimental analysis considers the
dispersion uncompensated transmission of a 5 x 32 GBd WDM system with DP-16QAM
and DP-64QAM after 4200 km and 1120 km, respectively. The experimental results
show that the proposed scheme outperforms single-channel digital
backpropagation. A perturbation-based nonlinear compensation scheme assisted by
a feedback from the forward error correction (FEC) decoder is numerically and
experimentally investigated. It is shown by numerical simulations and
transmission experiments that a feedback from the FEC decoder enables improved
compensation performance, allowing the receiver to operate very close to the
full data-aided performance bounds. The experimental analysis considers the
dispersion uncompensated transmission of a 5 x 32 GBd WDM system with DP-16QAM
and DP-64QAM after 4200 km and 1120 km, respectively. The experimental results
show that the proposed scheme outperforms single-channel digital
backpropagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02187</identifier>
 <datestamp>2018-10-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02187</id><created>2018-09-18</created><authors><author><keyname>Robinson</keyname><forenames>Martin</forenames></author><author><keyname>Simonov</keyname><forenames>Alexandr N</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Bond</keyname><forenames>Alan</forenames></author><author><keyname>Gavaghan</keyname><forenames>David</forenames></author></authors><title>Separating the effects of experimental noise from inherent system
  variability in voltammetry: the $[$Fe(CN)$_6]^{3-/ 4-}$ process</title><categories>eess.SP physics.data-an</categories><comments>30 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, we have introduced the use of techniques drawn from Bayesian
statistics to recover kinetic and thermodynamic parameters from voltammetric
data, and were able to show that the technique of large amplitude ac
voltammetry yielded significantly more accurate parameter values than the
equivalent dc approach. In this paper we build on this work to show that this
approach allows us, for the first time, to separate the effects of random
experimental noise and inherent system variability in voltammetric experiments.
We analyse ten repeated experimental data sets for the $[$Fe(CN)$_6]^{3-/ 4-}$
process, again using large-amplitude ac cyclic voltammetry. In each of the ten
cases we are able to obtain an extremely good fit to the experimental data and
obtain very narrow distributions of the recovered parameters governing both the
faradaic (the reversible formal faradaic potential, $E_0$, the standard
heterogeneous charge transfer rate constant $k_0$, and the charge transfer
coefficient $\alpha$) and non-faradaic terms (uncompensated resistance, $R_u$,
and double layer capacitance, $C_{dl}$). We then employ hierarchical Bayesian
methods to recover the underlying &quot;hyperdistribution&quot; of the faradaic and
non-faradaic parameters, showing that in general the variation between the
experimental data sets is significantly greater than suggested by individual
experiments, except for $\alpha$ where the inter-experiment variation was
relatively minor. Correlations between pairs of parameters are provided, and
for example, reveal a weak link between $k_0$ and $C_{dl}$ (surface activity of
a glassy carbon electrode surface). Finally, we discuss the implications of our
findings for voltammetric experiments more generally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02242</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02242</id><created>2018-09-10</created><updated>2018-10-11</updated><authors><author><keyname>Zhou</keyname><forenames>Man</forenames></author><author><keyname>Wang</keyname><forenames>Qian</forenames></author><author><keyname>Yang</keyname><forenames>Jingxiao</forenames></author><author><keyname>Li</keyname><forenames>Qi</forenames></author><author><keyname>Xiao</keyname><forenames>Feng</forenames></author><author><keyname>Wang</keyname><forenames>Zhibo</forenames></author><author><keyname>Chen</keyname><forenames>Xiaofeng</forenames></author></authors><title>PatternListener: Cracking Android Pattern Lock Using Acoustic Signals</title><categories>eess.SP cs.SY</categories><comments>13 pages, accepted by the 25th ACM Conference on Computer and
  Communications Security (CCS'18)</comments><doi>10.1145/3243734.3243777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern lock has been widely used for authentication to protect user privacy
on mobile devices (e.g., smartphones and tablets). Given its pervasive usage,
the compromise of pattern lock could lead to serious consequences. Several
attacks have been constructed to crack the lock. However, these approaches
require the attackers to either be physically close to the target device or be
able to manipulate the network facilities (e.g., WiFi hotspots) used by the
victims. Therefore, the effectiveness of the attacks is significantly impacted
by the environment of mobile devices. Also, these attacks are not scalable
since they cannot easily infer unlock patterns of a large number of devices.
  Motivated by an observation that fingertip motions on the screen of a mobile
device can be captured by analyzing surrounding acoustic signals on it, we
propose PatternListener, a novel acoustic attack that cracks pattern lock by
analyzing imperceptible acoustic signals reflected by the fingertip. It
leverages speakers and microphones of the victim's device to play imperceptible
audio and record the acoustic signals reflected by the fingertip. In
particular, it infers each unlock pattern by analyzing individual lines that
compose the pattern and are the trajectories of the fingertip. We propose
several algorithms to construct signal segments according to the captured
signals for each line and infer possible candidates of each individual line
according to the signal segments. Finally, we map all line candidates into grid
patterns and thereby obtain the candidates of the entire unlock pattern. We
implement a PatternListener prototype by using off-the-shelf smartphones and
thoroughly evaluate it using 130 unique patterns. The real experimental results
demonstrate that PatternListener can successfully exploit over 90% patterns
within five attempts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02364</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02364</id><created>2018-10-04</created><authors><author><keyname>Solovyev</keyname><forenames>Roman A.</forenames></author><author><keyname>Vakhrushev</keyname><forenames>Maxim</forenames></author><author><keyname>Radionov</keyname><forenames>Alexander</forenames></author><author><keyname>Aliev</keyname><forenames>Vladimir</forenames></author><author><keyname>Shvets</keyname><forenames>Alexey A.</forenames></author></authors><title>Deep Learning Approaches for Understanding Simple Speech Commands</title><categories>cs.SD cs.HC eess.AS</categories><comments>12 page, 4 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Automatic classification of sound commands is becoming increasingly
important, especially for mobile and embedded devices. Many of these devices
contain both cameras and microphones, and companies that develop them would
like to use the same technology for both of these classification tasks. One way
of achieving this is to represent sound commands as images, and use
convolutional neural networks when classifying images as well as sounds. In
this paper we consider several approaches to the problem of sound
classification that we applied in TensorFlow Speech Recognition Challenge
organized by Google Brain team on the Kaggle platform. Here we show different
representation of sounds (Wave frames, Spectrograms, Mel-Spectrograms, MFCCs)
and apply several 1D and 2D convolutional neural networks in order to get the
best performance. Our experiments show that we found appropriate sound
representation and corresponding convolutional neural networks. As a result we
achieved good classification accuracy that allowed us to finish the challenge
on 8-th place among 1315 teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02416</identifier>
 <datestamp>2018-10-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02416</id><created>2018-10-04</created><updated>2018-10-15</updated><authors><author><keyname>Bai</keyname><forenames>Hua</forenames></author></authors><title>Estimation of Parameters in Avian Movement Models</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The knowledge of the movement of animals is important and necessary for
ecologists to do further analysis such as exploring the animal migration route.
A novel method which is based on the state space modeling has been proposed to
track the bird, where the VHF transmitter is attached to the bird to emit the
signal and several towers with antenna arrays installed on its top are built to
receive the signal. The method consists of two parts, the first one is called
movement model which accounts for prediction of the dynamic movement of the
target, and the second part is the measurement model which links the target's
state variables to the available measurements data, the measurement includes
the time when the signal was detected, the ID of the antenna array which
detected the signal and integers between 0 and 255, the integers are
proportional to the strength of received signal. The extended Kalman filter is
then applied to estimate the location of the target with combing the movement
model and measurement model. In the movement model, several parameters with
positive values are deployed to define the change of the state variables with
time, these parameters reflect the relationship of the state variables at
current time and next time. In this paper, a method based on the maximum
likelihood estimation is proposed to estimate the appropriate values for these
unknown constant variables with given measurement data, and a kite is applied
to demonstrate the validity of the proposed method. Furthermore, the unscented
transformation is applied in Kalman filter to achieve more accurate estimation
of the target's states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02450</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02450</id><created>2018-10-04</created><authors><author><keyname>Roy</keyname><forenames>Sandip</forenames></author><author><keyname>Xue</keyname><forenames>Mengran</forenames></author></authors><title>Comment on `Detecting Topology Variations in Networks of Linear
  Dynamical Systems'</title><categories>cs.SY eess.SP math.OC</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditions for the detectability of topology variations in dynamical networks
are developed in a recent article in the IEEE Transactions on Control of
Network Systems [1]. Here, an example is presented which illustrates an error
in the network-theoretic conditions for detectability developed in [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02506</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02506</id><created>2018-10-04</created><authors><author><keyname>Ko</keyname><forenames>Yeongwoo</forenames></author><author><keyname>Kim</keyname><forenames>Sang-Hyo</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author></authors><title>Uplink Time Scheduling with Power Level Modulation in Wireless Powered
  Communication Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose downlink signal design and optimal uplink
scheduling for the wireless powered communication networks (WPCNs). Prior works
give attention to resource allocation in a static channel because users are
equipped with only energy receiver and users cannot update varying uplink
schedulling. For uplink scheduling, we propose a downlink signal design scheme,
called a power level modulation, which conveys uplink scheduling information to
users. First, we design a downlink energy signal using power level modulation.
Hybrid-access point (H-AP) allocates different power level in each subslot of
the downlink energy signal according to channel condition and users optimize
their uplink time subslots for signal transmission based on the power levels of
their received signals. Further, we formulate the sum throughput maximization
problem for the proposed scheme by determining the uplink and downlink time
allocation using convex optimization problem. Numerical results confirm that
the throughput of the proposed scheme outperforms that of the conventional
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02530</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02530</id><created>2018-10-05</created><authors><author><keyname>Shahgholian</keyname><forenames>M.</forenames></author><author><keyname>Gharavian</keyname><forenames>D.</forenames></author></authors><title>Advanced Traffic Management Systems: An Overview and A Development
  Strategy</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays the problem of traffic congestion is known as the main cause of air
pollution in the cities of the world. Urban traffic engineers and managers have
proposed three general approaches to dealing with this phenomenon. The first
approach expands the capacity of the urban traffic network (UTN). This approach
cannot be implemented in many urban areas due to urban density and traffic
volume. The second approach can be called the traffic assignment. In this
approach, through software applications or information boards, network managers
are informed about the users' status in the network and offer the best route to
them. Finally, the third approach involves optimizing the capacity of the UTN.
This approach tries to control the traffic actuators in order to create the
maximum capacity for network users. Therefore, it can be said that the advanced
traffic management systems (ATMS) are based on four main sections. These four
sections include traffic information, traffic assignment, traffic optimization,
and traffic prediction. This paper initially presents an overview of these four
sections, in the end, it proposes a development strategy for the ATMSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02566</identifier>
 <datestamp>2019-04-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02566</id><created>2018-10-05</created><updated>2019-04-16</updated><authors><author><keyname>Sarker</keyname><forenames>Md. Abdul Latif</forenames></author><author><keyname>Kader</keyname><forenames>Md. Fazlul</forenames></author><author><keyname>Han</keyname><forenames>Dong Seog</forenames></author></authors><title>Rate-Loss Mitigation for a Millimeter-Wave Beamspace MIMO Lens Antenna
  Array System Using a Hybrid Beam-Selection Scheme</title><categories>eess.SP</categories><comments>5 pages, 3 figures, Table 1, Submit to IEEE System Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rate loss is an important issue for multi-user beamspace multiple-input
multiple-output (MIMO) millimeter-wave systems. In such systems, multi-cluster
channel propagation leads as a function of the quantization error and, as a
result, each active user suffers a significant rate loss. Particularly, a
conventional single beam selection (SBS) scheme is used in an equivalent
beamspace MIMO channel to reduce the required number of radio frequency (RF)
chains, but this scheme does not properly mitigate the quantization error due
to the limited number of the channel propagation clusters per-user. In
addition, optimally selected beams or RF chains are operated at the same number
of the precoded data streams. Consequently, each active user inherently loses a
huge number of data streams utilizing the conventional SBS scheme. Therefore,
we propose a hybrid beam selection (HBS) scheme in an equivalent beamspace MIMO
channel to implement a multiple beam or RF chain group selection opportunity
and choose a reliable channel propagation cluster per-active-user, which
directly affects to mitigate the quantization error. The numerical result is
verified by computer simulations in terms of a millimeter-wave MIMO downlink
channel environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02568</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02568</id><created>2018-10-05</created><authors><author><keyname>Venkataramani</keyname><forenames>Shrikant</forenames></author><author><keyname>Smaragdis</keyname><forenames>Paris</forenames></author></authors><title>End-to-end Networks for Supervised Single-channel Speech Separation</title><categories>eess.AS cs.LG cs.SD eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of single channel source separation algorithms has improved
greatly in recent times with the development and deployment of neural networks.
However, many such networks continue to operate on the magnitude spectrogram of
a mixture, and produce an estimate of source magnitude spectrograms, to perform
source separation. In this paper, we interpret these steps as additional neural
network layers and propose an end-to-end source separation network that allows
us to estimate the separated speech waveform by operating directly on the raw
waveform of the mixture. Furthermore, we also propose the use of masking based
end-to-end separation networks that jointly optimize the mask and the latent
representations of the mixture waveforms. These networks show a significant
improvement in separation performance compared to existing architectures in our
experiments. To train these end-to-end models, we investigate the use of
composite cost functions that are derived from objective evaluation metrics as
measured on waveforms. We present subjective listening test results that
demonstrate the improvement attained by using masking based end-to-end networks
and also reveal insights into the performance of these cost functions for
end-to-end source separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02574</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02574</id><created>2018-10-05</created><authors><author><keyname>Peng-fei</keyname><forenames>Xu</forenames></author><author><keyname>Yin-jie</keyname><forenames>Jia</forenames></author><author><keyname>Zhi-jian</keyname><forenames>Wang</forenames></author></authors><title>Underdetermined Blind Source Separation for Sparse Signals based on the
  Law of Large Numbers and Minimum Intersection Angle Rule</title><categories>eess.SP</categories><comments>7 pages,9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Underdetermined Blind Source Separation(UBSS) is an important issue, for
sparse signals, a novel two-step approach for UBSS based on the law of large
numbers and minimum intersection angle rule (LM method) is presented. In the
first step, the estimation of the mixed matrix is obtained by using the law of
large numbers, and the number of source signals is displayed graphically. In
the second step, the method of estimating the source signal with the minimum
intersection angle rule is proposed. Finally, two simulation results that
illustrate the effectiveness of the theoretical results are presented. It has
simple principle and good transplantation capability and can be widely applied
in various fields of digital signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02584</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02584</id><created>2018-10-05</created><authors><author><keyname>Wang</keyname><forenames>Xi</forenames></author><author><keyname>Gkogkidis</keyname><forenames>C. Alexis</forenames></author><author><keyname>Schirrmeister</keyname><forenames>Robin T.</forenames></author><author><keyname>Heilmeyer</keyname><forenames>Felix A.</forenames></author><author><keyname>Gierthmuehlen</keyname><forenames>Mortimer</forenames></author><author><keyname>Kohler</keyname><forenames>Fabian</forenames></author><author><keyname>Schuettler</keyname><forenames>Martin</forenames></author><author><keyname>Stieglitz</keyname><forenames>Thomas</forenames></author><author><keyname>Ball</keyname><forenames>Tonio</forenames></author></authors><title>Deep Learning for micro-Electrocorticographic ({\mu}ECoG) Data</title><categories>eess.SP q-bio.NC</categories><comments>6 pages, 7 figures, 2018 IEEE EMBS conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning can extract information from neural recordings, e.g.,
surface EEG, ECoG and {\mu}ECoG, and therefore plays an important role in many
research and clinical applications. Deep learning with artificial neural
networks has recently seen increasing attention as a new approach in brain
signal decoding. Here, we apply a deep learning approach using convolutional
neural networks to {\mu}ECoG data obtained with a wireless, chronically
implanted system in an ovine animal model. Regularized linear discriminant
analysis (rLDA), a filter bank component spatial pattern (FBCSP) algorithm and
convolutional neural networks (ConvNets) were applied to auditory evoked
responses captured by {\mu}ECoG. We show that compared with rLDA and FBCSP,
significantly higher decoding accuracy can be obtained by ConvNets trained in
an end-to-end manner, i.e., without any predefined signal features. Deep
learning thus proves a promising technique for {\mu}ECoG-based brain-machine
interfacing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02609</identifier>
 <datestamp>2019-02-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02609</id><created>2018-10-05</created><updated>2019-02-23</updated><authors><author><keyname>Kuznetsov</keyname><forenames>N. V.</forenames></author><author><keyname>Yuldashev</keyname><forenames>M. V.</forenames></author><author><keyname>Yuldashev</keyname><forenames>R. V.</forenames></author><author><keyname>Blagov</keyname><forenames>M. V.</forenames></author><author><keyname>Kudryashova</keyname><forenames>E. V.</forenames></author><author><keyname>Kuznetsova</keyname><forenames>O. A.</forenames></author><author><keyname>Mokaev</keyname><forenames>T. N.</forenames></author></authors><title>Comment on &quot;Analysis of a Charge-Pump PLL: A New Model&quot; by M. van Paemel</title><categories>eess.SP</categories><comments>arXiv admin note: substantial text overlap with arXiv:1901.01468</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short communication we comment on the non-linear mathematical model
of CP-PLL introduced by V.Paemel. We reveal and obviate shortcomings in the
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02657</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02657</id><created>2018-10-05</created><updated>2018-11-12</updated><authors><author><keyname>Arjmandi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Zoofaghari</keyname><forenames>Mohammad</forenames></author><author><keyname>Noel</keyname><forenames>Adam</forenames></author></authors><title>Diffusive Molecular Communication in a Biological Spherical Environment
  with Partially Absorbing Boundary</title><categories>eess.SP</categories><comments>arXiv admin note: text overlap with arXiv:1807.02683</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusive molecular communication (DMC) is envisioned as a promising approach
to help realize healthcare applications within bounded biological environments.
In this paper, a DMC system within a biological spherical environment is
considered, inspired by bounded biological sphere-like structures throughout
the body. As a biological environment, it is assumed that the inner surface of
the sphere's boundary is fully covered by biological receptors that may
irreversibly react with hitting molecules. Moreover, information molecules
diffusing in the sphere may undergo a degradation reaction and be transformed
to another molecule type. Concentration Green's function (CGF) of diffusion
inside this environment is analytically obtained in terms of a convergent
infinite series. By employing the obtained CGF, the information channel between
transmitter and transparent receiver of DMC in this environment is
characterized. Interestingly, it is revealed that the information channel is
reciprocal, i.e., interchanging the position of receiver and transmitter does
not change the information channel. Our results indicate that the conventional
simplifying assumption that the environment is unbounded may lead to an
inaccurate characterization in such biological environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02719</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02719</id><created>2018-10-05</created><authors><author><keyname>Arvanitis</keyname><forenames>Gerasimos</forenames></author><author><keyname>Lalos</keyname><forenames>Aris S.</forenames></author><author><keyname>Moustakas</keyname><forenames>Konstantinos</forenames></author></authors><title>Block-Based Spectral Processing of Static and Dynamic 3D Meshes using
  Orthogonal Iterations</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral methods are widely used in geometry processing of 3D models. They
rely on the projection of the mesh geometry on the basis defined by the
eigenvectors of the graph Laplacian operator, becoming computationally
prohibitive as the density of the models increases. In this paper, we propose a
novel approach for supporting fast and efficient spectral processing of dense
3D meshes, ideally suited for real-time compression and denoising scenarios. To
achieve that, we apply the problem of tracking graph Laplacian eigenspaces via
orthogonal iterations, exploiting potential spectral coherence between adjacent
parts. To avoid perceptual distortions when a fixed number of eigenvectors is
used for all the individual parts, we propose a flexible solution that
automatically identifies the optimal subspace size for satisfying a given
reconstruction quality constraint. Extensive simulations carried out with
different 3D meshes in compression and denoising setups, showed that the
proposed schemes are very fast alternatives of SVD based spectral processing
while achieving at the same time similar or even better reconstruction quality.
More importantly, the proposed approach can be employed by several other
state-of-the-art denoising methods as a preprocessing step, optimizing both
their reconstruction quality and their computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02726</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02726</id><created>2018-10-05</created><authors><author><keyname>Parvaneh</keyname><forenames>Saman</forenames></author><author><keyname>Rubin</keyname><forenames>Jonathan</forenames></author><author><keyname>Samadani</keyname><forenames>Ali</forenames></author><author><keyname>Katuwal</keyname><forenames>Gajendra</forenames></author></authors><title>Automatic Detection of Arousals during Sleep using Multiple
  Physiological Signals</title><categories>cs.CV eess.SP</categories><comments>Computing in Cardiology 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The visual scoring of arousals during sleep routinely conducted by sleep
experts is a challenging task warranting an automatic approach. This paper
presents an algorithm for automatic detection of arousals during sleep. Using
the Physionet/CinC Challenge dataset, an 80-20% subject-level split was
performed to create in-house training and test sets, respectively. The data for
each subject in the training set was split to 30-second epochs with no overlap.
A total of 428 features from EEG, EMG, EOG, airflow, and SaO2 in each epoch
were extracted and used for creating subject-specific models based on an
ensemble of bagged classification trees, resulting in 943 models. For marking
arousal and non-arousal regions in the test set, the data in the test set was
split to 30-second epochs with 50% overlaps. The average of arousal
probabilities from different patient-specific models was assigned to each
30-second epoch and then a sample-wise probability vector with the same length
as test data was created for model evaluation. Using the PhysioNet/CinC
Challenge 2018 scoring criteria, AUPRCs of 0.25 and 0.21 were achieved for the
in-house test and blind test sets, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02744</identifier>
 <datestamp>2018-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02744</id><created>2018-10-05</created><authors><author><keyname>Hernandes</keyname><forenames>Aislan Gabriel</forenames></author><author><keyname>Junior</keyname><forenames>Mario Proenca Lemes</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>Improved Weighted Average Consensus in Distributed Cooperative Spectrum
  Sensing Networks</title><categories>eess.SP</categories><comments>30pages, 8 figures, 5 tables</comments><journal-ref>ETT-Wiley -- Trans Emerging Tel Tech. 2018; vol. 29, Issue 3,
  page&gt; 1-22</journal-ref><doi>10.1002/ett.3259</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a fully distributed improved weighted average consensus
(IWAC and WAC-AE) technique applied to cooperative spectrum sensing problem in
cognitive radio systems. This method allows the secondary users cooperate based
on only local information exchange without a fusion centre (FC). We have
compared four rules of average consensus (AC) algorithms. The first rule is the
simple AC without weights. The AC rule presents {performance comparable to the
traditional cooperative spectrum sensing} (CSS) techniques, such as the equal
gain combining (EGC) rule, which is a soft combining centralised method.
Another technique is the weighted average consensus (WAC) rule using the
weights based on the SUs channel condition. This technique results in a
performance similar to the maximum ratio combining (MRC) with soft combining
(centralised CSS). Two new AC rules are analysed, namely weighted average
consensus accuracy exchange (WAC-AE), and improved weighted average consensus
(IWAC); the former relates the weights to the channel conditions of the SUs
neighbours, while the latter combines the conditions of WAC and WAC-AE in the
same rule. All methods are compared each other and with the hard combining
centralised CSS. The WAC-AE results in a similar performance of WAC technique
but with fast convergence, while the IWAC can deliver suitable performance with
small complexity increment{. Moreover, IWAC method results in a similar
convergence rate than the WAC-AE method but slightly higher than the AC and WAC
methods}. Hence, the computational complexity of IWAC, WAC-AE, and WAC are
proven to be very similar. The analyses are based on the numerical Monte-Carlo
simulations (MCS), while algorithm's convergence is evaluated for both fixed
and dynamic-mobile communication scenarios, and under AWGN and Rayleigh
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02812</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02812</id><created>2018-10-04</created><authors><author><keyname>Vu</keyname><forenames>Tiep</forenames></author><author><keyname>Nguyen</keyname><forenames>Lam</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author></authors><title>Classifying Multi-channel UWB SAR Imagery via Tensor Sparsity Learning
  Techniques</title><categories>eess.SP cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using low-frequency (UHF to L-band) ultra-wideband (UWB) synthetic aperture
radar (SAR) technology for detecting buried and obscured targets, e.g. bomb or
mine, has been successfully demonstrated recently. Despite promising recent
progress, a significant open challenge is to distinguish obscured targets from
other (natural and manmade) clutter sources in the scene. The problem becomes
exacerbated in the presence of noisy responses from rough ground surfaces. In
this paper, we present three novel sparsity-driven techniques, which not only
exploit the subtle features of raw captured data but also take advantage of the
polarization diversity and the aspect angle dependence information from
multi-channel SAR data. First, the traditional sparse representation-based
classification (SRC) is generalized to exploit shared information of classes
and various sparsity structures of tensor coefficients for multi-channel data.
Corresponding tensor dictionary learning models are consequently proposed to
enhance classification accuracy. Lastly, a new tensor sparsity model is
proposed to model responses from multiple consecutive looks of objects, which
is a unique characteristic of the dataset we consider. Extensive experimental
results on a high-fidelity electromagnetic simulated dataset and radar data
collected from the U.S. Army Research Laboratory side-looking SAR demonstrate
the advantages of proposed tensor sparsity models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02845</identifier>
 <datestamp>2019-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02845</id><created>2018-10-05</created><updated>2019-11-01</updated><authors><author><keyname>Han</keyname><forenames>Jun</forenames></author><author><keyname>Lombardo</keyname><forenames>Salvator</forenames></author><author><keyname>Schroers</keyname><forenames>Christopher</forenames></author><author><keyname>Mandt</keyname><forenames>Stephan</forenames></author></authors><title>Deep Generative Video Compression</title><categories>cs.CV cs.LG eess.IV stat.ML</categories><comments>Accepted at NeurIPS 2019, 15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The usage of deep generative models for image compression has led to
impressive performance gains over classical codecs while neural video
compression is still in its infancy. Here, we propose an end-to-end, deep
generative modeling approach to compress temporal sequences with a focus on
video. Our approach builds upon variational autoencoder (VAE) models for
sequential data and combines them with recent work on neural image compression.
The approach jointly learns to transform the original sequence into a
lower-dimensional representation as well as to discretize and entropy code this
representation according to predictions of the sequential VAE. Rate-distortion
evaluations on small videos from public data sets with varying complexity and
diversity show that our model yields competitive results when trained on
generic video content. Extreme compression performance is achieved when
training the model on specialized content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02859</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02859</id><created>2018-10-05</created><authors><author><keyname>Claudino</keyname><forenames>Lucas</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>Efficient ZF-WF Strategy for Sum-Rate Maximization of MU-MISO Cognitive
  Radio Networks</title><categories>eess.SP</categories><comments>23 pages, 9 figures, 2 tables</comments><journal-ref>AEU - International Journal of Electronics and Communications,
  Vol. 84, February 2018, Pages 366-374</journal-ref><doi>10.1016/j.aeue.2017.12.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents an efficient quasi-optimal sum rate (SR) maximization
technique based on zero-forcing water-filling (ZFWF) algorithm directly applied
to cognitive radio networks (CRNs). We have defined the non-convexity nature of
the optimization problem in the context of CRNs while we have offered all
necessary conditions to solve the related SR maximization problem, which
considers power limit at cognitive transmitter and interference levels at
primary users (PUs) and secondary users (SUs). A general expression capable to
determine the optimal number of users as a function of the main system
parameters, namely the signal-to-interference-plus-noise ratio (SINR) and the
number of BS antennas is proposed. Our numerical results for the CRN
performance are analyzed in terms of both BER and sum-capacity for the proposed
ZF-WF precoding technique, and compared to the classical minimum mean square
error (MMSE), corroborating the effectiveness of the proposed technique
operating in multi user multiple input single output (MU-MISO) CRNs
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02866</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02866</id><created>2018-10-05</created><authors><author><keyname>Eskandarpour</keyname><forenames>Rozhin</forenames></author><author><keyname>Khodaei</keyname><forenames>Amin</forenames></author><author><keyname>Paaso</keyname><forenames>A.</forenames></author><author><keyname>Abdullah</keyname><forenames>N. M.</forenames></author></authors><title>Artificial Intelligence Assisted Power Grid Hardening in Response to
  Extreme Weather Events</title><categories>eess.SP cs.LG cs.SY</categories><journal-ref>2018 Grid of the Future Symposium</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an artificial intelligence based grid hardening model is
proposed with the objective of improving power grid resilience in response to
extreme weather events. At first, a machine learning model is proposed to
predict the component states (either operational or outage) in response to the
extreme event. Then, these predictions are fed into a hardening model, which
determines strategic locations for placement of distributed generation (DG)
units. In contrast to existing literature in hardening and resilience
enhancement, this paper co-optimizes grid economic and resilience objectives by
considering the intricate dependencies of the two. The numerical simulations on
the standard IEEE 118-bus test system illustrate the merits and applicability
of the proposed hardening model. The results indicate that the proposed
hardening model through decentralized and distributed local energy resources
can produce a more robust solution that can protect the system significantly
against multiple component outages due to an extreme event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02871</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02871</id><created>2018-10-05</created><authors><author><keyname>Marinello</keyname><forenames>Jose Carlos</forenames></author><author><keyname>Panazio</keyname><forenames>Cristiano Magalhaes</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>Massive MIMO Pilot Assignment Optimization based on Total Capacity</title><categories>eess.SP</categories><journal-ref>T. Telecommun Syst. (2018). pp 1-15</journal-ref><doi>10.1007/s11235-018-0452-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the effects of pilot assignment in multi-cell massive
multiple-input multiple-output systems. When deploying a large number of
antennas at base station (BS), and linear detection/precoding algorithms, the
system performance in both uplink (UL) and downlink (DL) is mainly limited by
pilot contamination. This interference is proper of each pilot, and thus system
performance can be improved by suitably assigning the pilot sequences to the
users within the cell, according to the desired metric. We show in this paper
that UL and DL performances constitute conflicting metrics, in such a way that
one cannot achieve the best performance in UL and DL with a single pilot
assignment configuration. Thus, we propose an alternative metric, namely total
capacity, aiming to simultaneously achieve a suitable performance in both
links. Since the PA problem is combinatorial, and the search space grows with
the number of pilots in a factorial fashion, we also propose a low complexity
suboptimal algorithm that achieves promising capacity performance avoiding the
exhaustive search. Besides, the combination of our proposed PA schemes with an
efficient power control algorithm unveils the great potential of the proposed
techniques in providing improved performance for a higher number of users. Our
numerical results demonstrate that with 64 BS antennas serving 10 users, our
proposed method can assure a 95%-likely rate of 4.2 Mbps for both DL and UL,
and a symmetric 95%-likely rate of 1.4 Mbps when serving 32 users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02968</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02968</id><created>2018-10-06</created><authors><author><keyname>Elnashar</keyname><forenames>Ayman</forenames></author><author><keyname>El-Saidny</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Yehia</keyname><forenames>Mohamed</forenames></author></authors><title>Performance Evaluation of VoLTE Based on Field Measurement Data</title><categories>cs.NI cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice over Long-Term Evolution (VoLTE) has been witnessing a rapid deployment
by network carriers worldwide. During the phases of VoLTE deployments, carriers
would typically face challenges in understanding the factors affecting the
VoLTE performance and then optimizing it to meet or exceed the performance of
the legacy circuit switched (CS) network (i.e., 2G/3G). The main challenge of
VoLTE service quality is the LTE network optimization and the performance
aspects of the service in different LTE deployment scenarios. In this paper, we
present a detailed practical performance analysis of VoLTE based on
commercially deployed 3GPP Release-10 LTE networks. The analysis evaluates
VoLTE performance in terms of real-time transport protocol (RTP) error rate,
RTP jitter and delays, block error rate (BLER) in different radio conditions
and VoLTE voice quality in terms of mean opinion score (MOS). In addition, the
paper evaluates key VoLTE features such as RObust Header Compression (ROHC) and
transmission time interval (TTI) bundling. This paper provides guidelines for
best practices of VoLTE deployment as well as practical performance evaluation
based on field measurement data from commercial LTE networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.02989</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.02989</id><created>2018-10-06</created><authors><author><keyname>Kumar</keyname><forenames>Rohit</forenames></author><author><keyname>Darak</keyname><forenames>Sumit J.</forenames></author><author><keyname>Hanawal</keyname><forenames>Manjesh K.</forenames></author><author><keyname>Yadav</keyname><forenames>Ankit</forenames></author></authors><title>Distributed Learning Algorithms for Opportunistic Spectrum Access in
  Infrastructure-less Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An opportunistic spectrum access (OSA) for the infrastructure-less (or
cognitive ad-hoc) network has received significant attention thanks to emerging
paradigms such as the Internet of Things (IoTs) and smart grids. Research in
this area has evolved from the \r{ho}rand algorithm requiring prior knowledge
of the number of active secondary users (SUs) to the musical chair (MC)
algorithm where the number of SUs are unknown and estimated independently at
each SU. These works ignore the number of collisions in the network leading to
wastage of power and bring down the effective life of battery operated SUs. In
this paper, we develop algorithms for OSA that learn faster and incurs fewer
number of collisions i.e. energy efficient. We consider two types of
infrastructure-less decentralized networks: 1) static network where the number
of SUs are fixed but unknown, and 2) dynamic network where SUs can
independently enter or leave the network. We set up the problem as a
multi-player mult-armed bandit and develop two distributed algorithms. The
analysis shows that when all the SUs independently implement the proposed
algorithms, the loss in throughput compared to the optimal throughput, i.e.
regret, is a constant with high probability and significantly outperforms
existing algorithms both in terms of regret and number of collisions. Fewer
collisions make them ideally suitable for battery operated SU terminals. We
validate our claims through exhaustive simulated experiments as well as through
a realistic USRP based experiments in a real radio environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03033</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03033</id><created>2018-10-06</created><authors><author><keyname>Negr&#xe3;o</keyname><forenames>Jo&#xe3;o Lucas</forenames></author><author><keyname>Abr&#xe3;o</keyname><forenames>Taufik</forenames></author></authors><title>Efficient Detection in Uniform Linear and Planar Arrays MIMO Systems
  under Spatial Correlated Channels</title><categories>eess.SP</categories><journal-ref>Int J Commun Syst.. Vol. 31, Issue 11, 25 July 2018</journal-ref><doi>10.1002/dac.3697</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the efficiency of various MIMO detectors was analyzed from the
perspective of highly correlated channels, where MIMO systems have a lack of
performance, besides in some cases an increasing complexity. Considering this
hard, but {a useful} scenario, various MIMO detection schemes {were accurately
evaluated concerning} complexity and bit error rate (BER) performance.
Specifically, successive interference cancellation (SIC), lattice reduction
(LR) and the combination of them were associated with conventional linear MIMO
detection techniques. {To demonstrate effectiveness}, a wide range of the
number of antennas and modulation formats have been considered aiming to verify
the potential of such MIMO detection techniques according to their
performance-complexity trade-off. We have also studied the correlation effect
when both transmit and receiver sides are equipped with uniform linear array
(ULA) and uniform planar array (UPA) antenna configurations. The performance of
different detectors is carefully compared when both antenna array
configurations are deployed {considering} a different number of antennas and
modulation order, especially {under} near-massive MIMO condition. We have also
discussed the relationship between the array factor (AF) and the BER
performance of both {antenna array} structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03041</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03041</id><created>2018-10-06</created><authors><author><keyname>Silva</keyname><forenames>Giovanni Maciel Ferreira</forenames></author><author><keyname>Filho</keyname><forenames>Jose Carlos Marinello</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>Sequential likelihood ascent search detector for massive MIMO systems</title><categories>eess.SP</categories><journal-ref>Int. J. Electron. Commun. (AEU) 96 (2018) 30-39</journal-ref><doi>10.1016/j.aeue.2018.09.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have analyzed the performance-complexity tradeoff of {a
selective} likelihood ascent search (LAS) algorithm initialized by a linear
detector, such as matched filtering (MF), zero forcing (ZF) and minimum mean
square error (MMSE), {and considering an optimization factor $\rho$ from the
bit flipping rule}. The scenario is the uplink of a massive MIMO (M-MIMO)
system, and the {analysis has }been developed by means of computer simulations.
With the increasing number of base station (BS) antennas, the classical
detectors become inefficient. Therefore, the LAS is employed for
performance-complexity tradeoff improvement. Using an adjustable optimized
threshold on the bit flip rule of LAS, much better solutions have been achieved
in terms of BER with no further complexity increment, indicating that there is
an optimal threshold for each scenario. Considering a $32\times32$ antennas
scenario, {the large-scale MIMO system equipped with the proposed LAS detector
with} factor $\rho$ = 0.8 {requires} 5 dB less {in terms of SNR than the
conventional LAS of the} literature ($\rho$ = 1.0) to achieve the same bit
error rate of $10^{-3}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03110</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03110</id><created>2018-10-07</created><authors><author><keyname>Saeed</keyname><forenames>Nasir</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Outlier Detection and Optimal Anchor Placement for 3D Underwater Optical
  Wireless Sensor Networks Localization</title><categories>eess.SP</categories><comments>14 pages, 11 figures, Accepted for Publication in IEEE Transactions
  on Communications</comments><doi>10.1109/TCOMM.2018.2875083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location is one of the basic information required for underwater optical
wireless sensor networks (UOWSNs) for different purposes such as relating the
sensing measurements with precise sensor positions, enabling efficient
geographic routing techniques, and sustaining link connectivity between the
nodes. Even though various two-dimensional UOWSNs localization methods have
been proposed in the past, the directive nature of optical wireless
communications and three-dimensional (3D) deployment of sensors require to
develop 3D underwater localization methods. Additionally, the localization
accuracy of the network strongly depends on the placement of the anchors.
Therefore, we propose a robust 3D localization method for partially connected
UOWSNs which can accommodate the outliers and optimize the placement of the
anchors to improve the localization accuracy. The proposed method formulates
the problem of missing pairwise distances and outliers as an optimization
problem which is solved through half quadratic minimization. Furthermore,
analysis is provided to optimally place the anchors in the network which
improves the localization accuracy. The problem of optimal anchor placement is
formulated as a combination of Fisher information matrices for the sensor nodes
where the condition of D-optimality is satisfied. The numerical results
indicate that the proposed method outperforms the literature substantially in
the presence of outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03159</identifier>
 <datestamp>2019-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03159</id><created>2018-10-07</created><updated>2019-08-17</updated><authors><author><keyname>Shao</keyname><forenames>Mingjie</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author><author><keyname>So</keyname><forenames>Anthony Man-Cho</forenames></author></authors><title>A Framework for One-Bit and Constant-Envelope Precoding over Multiuser
  Massive MISO Channels</title><categories>cs.IT eess.SP math.IT</categories><comments>To appear in IEEE Trans. Signal Processing</comments><doi>10.1109/TSP.2019.2937280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following problem: A multi-antenna base station (BS) sends
multiple symbol streams to multiple single-antenna users via precoding.
However, unlike conventional multiuser precoding, the transmitted signals are
subjected to binary, unit-modulus, or even discrete unit-modulus constraints.
Such constraints arise in the one-bit and constant-envelope (CE) massive MIMO
scenarios, wherein high-resolution digital-to-analog converters (DACs) are
replaced by one-bit DACs and phase shifters, respectively, for cutting down
hardware cost and energy consumption. Multiuser precoding under one-bit and CE
restrictions poses significant design difficulty. In this paper we establish a
framework for designing multiuser precoding under the one-bit, continuous CE
and discrete CE scenarios---all within one theme. We first formulate a
precoding design that focuses on minimizations of symbol-error probabilities
(SEPs), assuming quadrature amplitude modulation (QAM) constellations. We then
devise an algorithm for our SEP-based design. The algorithm combines i) a novel
penalty method for handling binary, unit-modulus and discrete unit-modulus
constraints; and ii) a first-order non-convex optimization recipe custom-built
for the design. Specifically, the latter is an inexact
majorization-minimization method via accelerated projected gradient, which, as
shown by simulations, runs very fast and can handle a large number of decision
variables. Simulation results indicate that the proposed design offers
significantly better bit-error rate performance than the existing designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03226</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03226</id><created>2018-10-07</created><authors><author><keyname>Koh</keyname><forenames>Eunjeong Stella</forenames></author><author><keyname>Dubnov</keyname><forenames>Shlomo</forenames></author><author><keyname>Wright</keyname><forenames>Dustin</forenames></author></authors><title>Rethinking Recurrent Latent Variable Model for Music Composition</title><categories>cs.SD cs.LG cs.MM eess.AS</categories><comments>Published as a conference paper at IEEE MMSP 2018</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a model for capturing musical features and creating novel
sequences of music, called the Convolutional Variational Recurrent Neural
Network. To generate sequential data, the model uses an encoder-decoder
architecture with latent probabilistic connections to capture the hidden
structure of music. Using the sequence-to-sequence model, our generative model
can exploit samples from a prior distribution and generate a longer sequence of
music. We compare the performance of our proposed model with other types of
Neural Networks using the criteria of Information Rate that is implemented by
Variable Markov Oracle, a method that allows statistical characterization of
musical information dynamics and detection of motifs in a song. Our results
suggest that the proposed model has a better statistical resemblance to the
musical structure of the training data, which improves the creation of new
sequences of music in the style of the originals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03255</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03255</id><created>2018-10-07</created><authors><author><keyname>Tunc</keyname><forenames>Sait</forenames></author><author><keyname>Altug</keyname><forenames>Yucel</forenames></author><author><keyname>Kozat</keyname><forenames>Serdar</forenames></author><author><keyname>Mihcak</keyname><forenames>Kivanc</forenames></author></authors><title>Information Theoretic Analysis of the Fundamental Limits of Content
  Identification</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the content identification problem from an information
theoretic perspective and derive its fundamental limits. Here, a rights-holder
company desires to keep track of illegal uses of its commercial content, by
utilizing resources of a security company, while securing the privacy of its
content. Due to privacy issues, the rights-holder company only reveals certain
hash values of the original content to the security company. We view the
commercial content of the rights-holder company as the codebook of an encoder
and the hash values of the content (made available to the security company) as
the codebook of a decoder, i.e., the corresponding codebooks of the encoder and
the decoder are not the same. Hence, the content identification is modelled as
a communication problem using asymmetric codebooks by an encoder and a decoder.
We further address &quot;the privacy issue&quot; in the content identification by adding
&quot;security&quot; constraints to the communication setup to prevent estimation of the
encoder codewords given the decoder codewords. By this modeling, the proposed
problem of reliable communication with asymmetric codebooks with security
constraints provides the fundamental limits of the content identification
problem. To this end, we introduce an information capacity and prove that this
capacity is equal to the operation capacity of the system under i.i.d. encoder
codewords providing the fundamental limits for content identification. As a
well known and widely studied framework, we evaluate the capacity for a binary
symmetric channel and provide closed form expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03305</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03305</id><created>2018-10-08</created><authors><author><keyname>Shao</keyname><forenames>Hao-Chiang</forenames></author></authors><title>A Coarse-to-Fine Multiscale Mesh Representation and its Applications</title><categories>eess.IV cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel coarse-to-fine framework that derives a semi-regular
multiscale mesh representation of an original input mesh via remeshing. Our
approach differs from the conventional mesh wavelet transform strategy in two
ways. First, based on a lazy wavelet framework, it can convert an input mesh
into a multiresolution representation through a single remeshing procedure. By
contrast, the conventional strategy requires two steps: remeshing and mesh
wavelet transform. Second, the proposed method can conditionally convert input
mesh models into ones sharing the same adjacency matrix, so it is able be
invariant against the triangular tilings of the inputs. Our experiment results
show that the proposed multiresolution representation method is efficient in
various applications, such as 3D shape property analysis, mesh scalable coding
and mesh morphing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03337</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03337</id><created>2018-10-08</created><updated>2018-10-09</updated><authors><author><keyname>Kotsonias</keyname><forenames>Andreas</forenames></author><author><keyname>Hadjidemetriou</keyname><forenames>Lenos</forenames></author><author><keyname>Asprou</keyname><forenames>Markos</forenames></author><author><keyname>Kyriakides</keyname><forenames>Elias</forenames></author></authors><title>Monitoring of Low Voltage Distribution Grid Considering the Neutral
  Conductor</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most widely used method for monitoring Low Voltage Distribution Grids
(LVDGs) is the three phase Weighted Least Squares (WLS) State Estimation (SE),
which was initially developed for Medium Voltage Distribution Grids (MVDGs).
This methodology is implicitly applied in LVDGs with an assumption that the
neutral conductor represents a zero potential across the whole system. However,
this assumption is often not valid for LVDGs as the consumers loads are highly
asymmetrical and the neutral conductor is usually grounded only at the MV-LV
transformer substation. Therefore, if this method is applied for the monitoring
of LVDGs it may deteriorate the performance of the WLS SE leading to inaccurate
results. In this paper, an investigation is initially conducted in order to
evaluate the performance of the conventional WLS SE methodology when applied
for the monitoring of LVDGs. The results of this study indicate that the
performance of this methodology is not consistent and is highly affected by the
operating conditions of the LVDG. In view of these results, a new methodology
is proposed and developed that takes into consideration the effect of the
neutral conductor and as a result an outstanding performance is achieved
compared to the conventional methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03401</identifier>
 <datestamp>2018-11-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03401</id><created>2018-10-08</created><authors><author><keyname>Jain</keyname><forenames>Neha</forenames></author><author><keyname>Gupta</keyname><forenames>Anubha</forenames></author><author><keyname>Bohara</keyname><forenames>Vivek Ashok</forenames></author></authors><title>PCI-MDR: Missing Data Recovery in Wireless Sensor Networks using Partial
  Canonical Identity Matrix</title><categories>eess.SP</categories><comments>4pages, 3 figures and under revision in IEEE wireless communication
  letter</comments><journal-ref>IEEE wireless communication letter, 2018</journal-ref><doi>10.1109/LWC.2018.2882403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data loss in wireless sensor networks (WSNs) is quite prevalent. Since sensor
nodes are employed for various critical applications, accurate recovery of
missing data is important. Researchers have exploited different characteristics
of WSN data, such as low rank, spatial and temporal correlation for missing
data recovery. However, the performance of existing methods is dependent on
various factors. For instance, correct rank estimation is required for
exploiting the low-rank behaviour of WSNs, whereas correlation information
among the nodes should be known for exploiting spatial correlation. Further,
the amount of missing data should not be massive for exploiting temporal
correlation. To overcome the above-mentioned drawbacks, a novel method PCI-MDR
has been proposed in this paper. It utilizes compressive sensing with partial
canonical identity matrix for the recovery of missing data in WSNs. To validate
the proposed method, the results have been obtained on the real data set of
temperature sensors from the Intel Lab. The proposed method is observed to
perform superior to the existing methods, yielding significant improvement
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03459</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03459</id><created>2018-10-04</created><authors><author><keyname>Cho</keyname><forenames>Jaejin</forenames></author><author><keyname>Baskar</keyname><forenames>Murali Karthick</forenames></author><author><keyname>Li</keyname><forenames>Ruizhi</forenames></author><author><keyname>Wiesner</keyname><forenames>Matthew</forenames></author><author><keyname>Mallidi</keyname><forenames>Sri Harish</forenames></author><author><keyname>Yalta</keyname><forenames>Nelson</forenames></author><author><keyname>Karafiat</keyname><forenames>Martin</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author></authors><title>Multilingual sequence-to-sequence speech recognition: architecture,
  transfer learning, and language modeling</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively
new direction in speech research. The approach benefits by performing model
training without using lexicon and alignments. However, this poses a new
problem of requiring more data compared to conventional DNN-HMM systems. In
this work, we attempt to use data from 10 BABEL languages to build a
multi-lingual seq2seq model as a prior model, and then port them towards 4
other BABEL languages using transfer learning approach. We also explore
different architectures for improving the prior multilingual seq2seq model. The
paper also discusses the effect of integrating a recurrent neural network
language model (RNNLM) with a seq2seq model during decoding. Experimental
results show that the transfer learning approach from the multilingual model
shows substantial gains over monolingual models across all 4 BABEL languages.
Incorporating an RNNLM also brings significant improvements in terms of %WER,
and achieves recognition performance comparable to the models trained with
twice more training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03481</identifier>
 <datestamp>2019-01-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03481</id><created>2018-10-05</created><authors><author><keyname>Cheng</keyname><forenames>Yi Fei</forenames></author><author><keyname>Strachan</keyname><forenames>Megan</forenames></author><author><keyname>Weiss</keyname><forenames>Zachary</forenames></author><author><keyname>Deb</keyname><forenames>Moniher</forenames></author><author><keyname>Carone</keyname><forenames>Dawn</forenames></author><author><keyname>Ganapati</keyname><forenames>Vidya</forenames></author></authors><title>Illumination Pattern Design with Deep Learning for Single-Shot Fourier
  Ptychographic Microscopy</title><categories>eess.IV</categories><doi>10.1364/OE.27.000644</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fourier ptychographic microscopy allows for the collection of images with a
high space-bandwidth product at the cost of temporal resolution. In Fourier
ptychographic microscopy, the light source of a conventional widefield
microscope is replaced with a light-emitting diode (LED) matrix, and multiple
images are collected with different LED illumination patterns. From these
images, a higher-resolution image can be computationally reconstructed without
sacrificing field-of-view. We use deep learning to achieve single-shot imaging
without sacrificing the space-bandwidth product, reducing the acquisition time
in Fourier ptychographic microscopy by a factor of 69. In our deep learning
approach, a training dataset of high-resolution images is used to jointly
optimize a single LED illumination pattern with the parameters of a
reconstruction algorithm. Our work paves the way for high-throughput imaging in
biological studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03586</identifier>
 <datestamp>2018-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03586</id><created>2018-10-08</created><authors><author><keyname>Liu</keyname><forenames>Fuchen</forenames></author><author><keyname>Cu&#xe9;nod</keyname><forenames>Charles-Andr&#xe9;</forenames></author><author><keyname>Thomassin-Naggara</keyname><forenames>Isabelle</forenames></author><author><keyname>Chemouny</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Rozenholc</keyname><forenames>Yves</forenames></author></authors><title>Hierarchical segmentation using equivalence test (HiSET): Application to
  DCE image sequences</title><categories>eess.IV stat.AP</categories><comments>58 pages</comments><msc-class>62G10, 62P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamical contrast enhanced (DCE) imaging allows non invasive access to
tissue micro-vascularization. It appears as a promising tool to build imaging
biomark-ers for diagnostic, prognosis or anti-angiogenesis treatment monitoring
of cancer. However, quantitative analysis of DCE image sequences suffers from
low signal to noise ratio (SNR). SNR may be improved by averaging functional
information in a large region of interest when it is functionally homogeneous.
  We propose a novel method for automatic segmentation of DCE image sequences
into functionally homogeneous regions, called DCE-HiSET. Using an observation
model which depends on one parameter a and is justified a posteri-ori,
DCE-HiSET is a hierarchical clustering algorithm. It uses the p-value of a
multiple equivalence test as dissimilarity measure and consists of two steps.
The first exploits the spatial neighborhood structure to reduce complexity and
takes advantage of the regularity of anatomical features, while the second
recovers (spatially) disconnected homogeneous structures at a larger (global)
scale. Given a minimal expected homogeneity discrepancy for the multiple
equivalence test, both steps stop automatically by controlling the Type I
error. This provides an adaptive choice for the number of clusters. Assuming
that the DCE image sequence is functionally piecewise constant with signals on
each piece sufficiently separated, we prove that DCE-HiSET will retrieve the
exact partition with high probability as soon as the number of images in the
sequence is large enough. The minimal expected homogeneity discrepancy appears
as the tuning parameter controlling the size of the segmentation.
  DCE-HiSET has been implemented in C++ for 2D and 3D image sequences with
competitive speed.
  Keywords : DCE imaging, automatic clustering, hierarchical segmentation,
equivalence test
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03655</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03655</id><created>2018-10-08</created><authors><author><keyname>Yoshioka</keyname><forenames>Takuya</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author><author><keyname>Chen</keyname><forenames>Zhuo</forenames></author><author><keyname>Xiao</keyname><forenames>Xiong</forenames></author><author><keyname>Alleva</keyname><forenames>Fil</forenames></author></authors><title>Recognizing Overlapped Speech in Meetings: A Multichannel Separation
  Approach Using Neural Networks</title><categories>eess.AS cs.CL cs.SD</categories><journal-ref>Proc. Interspeech 2018, 3038-3042</journal-ref><doi>10.21437/Interspeech.2018-2284</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to develop a meeting transcription system that can
recognize speech even when utterances of different speakers are overlapped.
While speech overlaps have been regarded as a major obstacle in accurately
transcribing meetings, a traditional beamformer with a single output has been
exclusively used because previously proposed speech separation techniques have
critical constraints for application to real meetings. This paper proposes a
new signal processing module, called an unmixing transducer, and describes its
implementation using a windowed BLSTM. The unmixing transducer has a fixed
number, say J, of output channels, where J may be different from the number of
meeting attendees, and transforms an input multi-channel acoustic signal into J
time-synchronous audio streams. Each utterance in the meeting is separated and
emitted from one of the output channels. Then, each output signal can be simply
fed to a speech recognition back-end for segmentation and transcription. Our
meeting transcription system using the unmixing transducer outperforms a system
based on a state-of-the-art neural mask-based beamformer by 10.8%. Significant
improvements are observed in overlapped segments. To the best of our knowledge,
this is the first report that applies overlapped speech recognition to
unconstrained real meeting audio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03743</identifier>
 <datestamp>2018-12-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03743</id><created>2018-10-08</created><updated>2018-12-10</updated><authors><author><keyname>Liu</keyname><forenames>Luoluo</forenames></author><author><keyname>Chin</keyname><forenames>Sang Peter</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>JOBS: Joint-Sparse Optimization from Bootstrap Samples</title><categories>stat.ML cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical signal recovery based on $\ell_1$ minimization solves the least
squares problem with all available measurements via sparsity-promoting
regularization. In practice, it is often the case that not all measurements are
available or required for recovery. Measurements might be corrupted/missing or
they arrive sequentially in streaming fashion. In this paper, we propose a
global sparse recovery strategy based on subsets of measurements, named JOBS,
in which multiple measurements vectors are generated from the original pool of
measurements via bootstrapping, and then a joint-sparse constraint is enforced
to ensure support consistency among multiple predictors. The final estimate is
obtained by averaging over the $K$ predictors. The performance limits
associated with different choices of number of bootstrap samples $L$ and number
of estimates $K$ is analyzed theoretically. Simulation results validate some of
the theoretical analysis, and show that the proposed method yields
state-of-the-art recovery performance, outperforming $\ell_1$ minimization and
a few other existing bootstrap-based techniques in the challenging case of low
levels of measurements and is preferable over other bagging-based methods in
the streaming setting since it performs better with small $K$ and $L$ for
data-sets with large sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03745</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03745</id><created>2018-10-08</created><authors><author><keyname>Olesen</keyname><forenames>Alexander Neergaard</forenames></author><author><keyname>Jennum</keyname><forenames>Poul</forenames></author><author><keyname>Peppard</keyname><forenames>Paul</forenames></author><author><keyname>Mignot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Sorensen</keyname><forenames>Helge Bjarup Dissing</forenames></author></authors><title>Deep residual networks for automatic sleep stage classification of raw
  polysomnographic waveforms</title><categories>cs.CV eess.SP stat.ML</categories><doi>10.1109/EMBC.2018.8513080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed an automatic sleep stage classification algorithm based on
deep residual neural networks and raw polysomnogram signals. Briefly, the raw
data is passed through 50 convolutional layers before subsequent classification
into one of five sleep stages. Three model configurations were trained on 1850
polysomnogram recordings and subsequently tested on 230 independent recordings.
Our best performing model yielded an accuracy of 84.1% and a Cohen's kappa of
0.746, improving on previous reported results by other groups also using only
raw polysomnogram data. Most errors were made on non-REM stage 1 and 3
decisions, errors likely resulting from the definition of these stages. Further
testing on independent cohorts is needed to verify performance for clinical
use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03900</identifier>
 <datestamp>2020-01-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03900</id><created>2018-10-09</created><updated>2019-08-22</updated><authors><author><keyname>&#x15e;ahin</keyname><forenames>Serdar</forenames></author><author><keyname>Cipriano</keyname><forenames>Antonio Maria</forenames></author><author><keyname>Poulliat</keyname><forenames>Charly</forenames></author><author><keyname>Boucheret</keyname><forenames>Marie-Laure</forenames></author></authors><title>Iterative Decision Feedback Equalization Using Online Prediction</title><categories>eess.SP</categories><comments>8 pages, 9 figures, paper submitted to IEEE</comments><doi>10.1109/ACCESS.2020.2970340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, a new category of soft-input soft-output (SISO) minimum-mean
square error (MMSE) finite-impulse response (FIR) decision feedback equalizers
(DFEs) with iteration-wise static filters (i.e. iteration variant) is
investigated.
  It has been recently shown that SISO MMSE DFE with dynamic filters (i.e.
time-varying) reaches very attractive operating points for high-data rate
applications, when compared to alternative turbo-equalizers of the same
category, thanks to sequential estimation of data symbols [1].
  However the dependence of filters on the feedback incurs high amount of
latency and computational costs, hence SISO MMSE DFEs with static filters
provide an attractive alternative for computational complexity-performance
trade-off.
  However, the latter category of receivers faces a fundamental design issue on
the estimation of the decision feedback reliability for filter computation.
  To address this issue, a novel approach to decision feedback reliability
estimation through online prediction is proposed and applied for SISO FIR DFE
with either a posteriori probability (APP) or expectation propagation (EP)
based soft feedback.
  This novel method for filter computation is shown to improve detection
performance compared to previously known alternative methods, and finite-length
and asymptotic analysis show that DFE with static filters still remains
well-suited for high-spectral efficiency applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03985</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03985</id><created>2018-10-09</created><authors><author><keyname>Shu</keyname><forenames>Feng</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Xia</keyname><forenames>Guiyang</forenames></author><author><keyname>Xu</keyname><forenames>Tingzhen</forenames></author><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Jiangzhou</forenames></author></authors><title>High-performance Power Allocation Strategies for Secure Spatial
  Modulation</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal power allocation (PA) strategies can make a significant rate
improvement in secure spatial modulation (SM). Due to the lack of secrecy rate
(SR) closed-form expression in secure SM networks, it is hard to optimize the
PA factor. In this paper, two PA strategies are proposed: gradient descent, and
maximum product of signal-to-interference-plus-noise ratio (SINR) and
artificial-noise-to-signal-plus-noise ratio (ANSNR)(Max-P-SINR-ANSNR). The
former is an iterative method and the latter is a closed-form solution.
Compared to the former, the latter is of low-complexity. Simulation results
show that the proposed two PA methods can approximately achieve the same SR
performance as exhaustive search method and perform far better than three fixed
PA ones. With extremely low complexity, the SR performance of the proposed
Max-P-SINR-ANSNR performs slightly better and worse than that of the proposed
GD in the low to medium, and high signal-to-noise ratio regions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.03986</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.03986</id><created>2018-10-03</created><updated>2018-11-14</updated><authors><author><keyname>Shen</keyname><forenames>Yu-Han</forenames></author><author><keyname>He</keyname><forenames>Ke-Xin</forenames></author><author><keyname>Zhang</keyname><forenames>Wei-Qiang</forenames></author></authors><title>SAM-GCNN: A Gated Convolutional Neural Network with Segment-Level
  Attention Mechanism for Home Activity Monitoring</title><categories>cs.SD cs.CV eess.AS</categories><comments>6 pages, accepted by ISSPIT 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method for home activity monitoring. We
demonstrate our model on dataset of Detection and Classification of Acoustic
Scenes and Events (DCASE) 2018 Challenge Task 5. This task aims to classify
multi-channel audios into one of the provided pre-defined classes. All of these
classes are daily activities performed in a home environment. To tackle this
task, we propose a gated convolutional neural network with segment-level
attention mechanism (SAM-GCNN). The proposed framework is a convolutional model
with two auxiliary modules: a gated convolutional neural network and a
segment-level attention mechanism. Furthermore, we adopted model ensemble to
enhance the capability of generalization of our model. We evaluated our work on
the development dataset of DCASE 2018 Task 5 and achieved competitive
performance, with a macro-averaged F-1 score increasing from 83.76% to 89.33%,
compared with the convolutional baseline system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04028</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04028</id><created>2018-10-07</created><authors><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Ma</keyname><forenames>Jianwei</forenames></author></authors><title>Hartley Spectral Pooling for Deep Learning</title><categories>cs.CV cs.LG eess.SP stat.ML</categories><comments>5 pages, 6 figures, letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most convolution neural networks (CNNs), downsampling hidden layers is
adopted for increasing computation efficiency and the receptive field size.
Such operation is commonly so-called pooling. Maximation and averaging over
sliding windows (max/average pooling), and plain downsampling in the form of
strided convolution are popular pooling methods. Since the pooling is a lossy
procedure, a motivation of our work is to design a new pooling approach for
less lossy in the dimensionality reduction. Inspired by the Fourier spectral
pooling(FSP) proposed by Rippel et. al. [1], we present the Hartley transform
based spectral pooling method in CNNs. Compared with FSP, the proposed spectral
pooling avoids the use of complex arithmetic for frequency representation and
reduces the computation. Spectral pooling preserves more structure features for
network's discriminability than max and average pooling. We empirically show
that Hartley spectral pooling gives rise to the convergence of training CNNs on
MNIST and CIFAR-10 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04080</identifier>
 <datestamp>2018-12-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04080</id><created>2018-10-09</created><updated>2018-12-04</updated><authors><author><keyname>Kiti&#x107;</keyname><forenames>Sr&#x111;an</forenames></author><author><keyname>Gu&#xe9;rin</keyname><forenames>Alexandre</forenames></author></authors><title>TRAMP: Tracking by a Real-time AMbisonic-based Particle filter</title><categories>cs.SD eess.AS</categories><comments>In Proceedings of the LOCATA ChallengeWorkshop - a satellite event of
  IWAENC 2018 (arXiv:1811.08482 )</comments><report-no>LOCATAchallenge/2018/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a multiple sound source localization and tracking
system, fed by the Eigenmike array. The First Order Ambisonics (FOA) format is
used to build a pseudointensity-based spherical histogram, from which the
source position estimates are deduced. These instantaneous estimates are
processed by a wellknown tracking system relying on a set of particle filters.
While the novelty within localization and tracking is incremental, the
fully-functional, complete and real-time running system based on these
algorithms is proposed for the first time. As such, it could serve as an
additional baseline method of the LOCATA challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04085</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04085</id><created>2018-10-08</created><authors><author><keyname>G&#xf3;mez-Casco</keyname><forenames>David</forenames></author><author><keyname>L&#xf3;pez-Salcedo</keyname><forenames>Jos&#xe9; A.</forenames></author><author><keyname>Seco-Granados</keyname><forenames>Gonzalo</forenames></author></authors><title>Optimal Post-Detection Integration Technique for the Reacquisition of
  Weak GNSS Signals</title><categories>eess.SP</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles the problem of finding the optimal non-coherent detector
for the reacquisition of weak Global Navigation Satellite System (GNSS) signals
in the presence of bits and phase uncertainty. Two solutions are derived based
on using two detection frameworks: the Bayesian approach and the generalized
likelihood ratio test (GLRT). We also derive approximate detectors of reduced
computation complexity and without noticeable performance degradation.
Simulation results reveal a clear improvement of the detection probability for
the proposed techniques with respect to the conventional detectors implemented
in high sensitivity GNSS (HS-GNSS) receivers to acquire weak GNSS signals.
Finally, we draw conclusions on which is the best technique to reacquire weak
GNSS signals in practice considering a trade-off between performance and
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04105</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04105</id><created>2018-10-06</created><authors><author><keyname>Zhang</keyname><forenames>J. Andrew</forenames></author><author><keyname>Huang</keyname><forenames>Xiaojing</forenames></author><author><keyname>Guo</keyname><forenames>Y. Jay</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Multibeam for Joint Communication and Sensing Using Steerable Analog
  Antenna Arrays</title><categories>eess.SP cs.NI</categories><comments>14 pages, 10 figures, Journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beamforming has great potential for joint communication and sensing (JCAS),
which is becoming a demanding feature on many emerging platforms such as
unmanned aerial vehicles and smart cars. Although beamforming has been
extensively studied for communication and radar sensing respectively, its
application in the joint system is not straightforward due to different
beamforming requirements by communication and sensing. In this paper, we
propose a novel multibeam framework using steerable analog antenna arrays,
which allows seamless integration of communication and sensing. Different to
conventional JCAS schemes that support JCAS using a single beam, our framework
is based on the key innovation of multibeam technology: providing fixed subbeam
for communication and packet-varying scanning subbeam for sensing,
simultaneously from a single transmitting array. We provide a system
architecture and protocols for the proposed framework, complying well with
modern packet communication systems with multicarrier modulation. We also
propose low-complexity and effective multibeam design and generation methods,
which offer great flexibility in meeting different communication and sensing
requirements. We further develop sensing parameter estimation algorithms using
conventional digital Fourier transform and 1D compressive sensing techniques,
matching well with the multibeam framework. Simulation results are provided and
validate the effectiveness of our proposed framework, beamforming design
methods and the sensing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04106</identifier>
 <datestamp>2019-07-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04106</id><created>2018-10-05</created><updated>2019-07-18</updated><authors><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Han</keyname><forenames>Jinsong</forenames></author><author><keyname>Lin</keyname><forenames>Feng</forenames></author><author><keyname>Ren</keyname><forenames>Kui</forenames></author></authors><title>WiPIN: Operation-free Passive Person Identification Using Wi-Fi Signals</title><categories>eess.SP cs.LG stat.ML</categories><comments>accepted by GLOBECOM 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wi-Fi signals-based person identification attracts increasing attention in
the booming Internet-of-Things era mainly due to its pervasiveness and
passiveness. Most previous work applies gaits extracted from WiFi distortions
caused by the person walking to achieve the identification. However, to extract
useful gait, a person must walk along a pre-defined path for several meters,
which requires user high collaboration and increases identification time
overhead, thus limiting use scenarios. Moreover, gait based work has severe
shortcoming in identification performance, especially when the user volume is
large. In order to eliminate the above limitations, in this paper, we present
an operation-free person identification system, namely WiPIN, that requires
least user collaboration and achieves good performance. WiPIN is based on an
entirely new insight that Wi-Fi signals would carry person body information
when propagating through the body, which is potentially discriminated for
person identification. Then we demonstrate the feasibility on commodity
off-the-shelf Wi-Fi devices by well-designed signal pre-processing, feature
extraction, and identity matching algorithms. Results show that WiPIN achieves
92% identification accuracy over 30 users, high robustness to various
experimental settings, and low identifying time overhead, i.e., less than
300ms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04121</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04121</id><created>2018-09-27</created><authors><author><keyname>Guo</keyname><forenames>Li</forenames></author><author><keyname>Sim</keyname><forenames>Gavin</forenames></author><author><keyname>Matuszewski</keyname><forenames>Bogdan</forenames></author></authors><title>Inter-Patient ECG Classification with Convolutional and Recurrent Neural
  Networks</title><categories>eess.SP q-bio.QM</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent advances in ECG sensor devices provide opportunities for user
self-managed auto-diagnosis and monitoring services over the internet. This
imposes the requirements for generic ECG classification methods that are
inter-patient and device independent. In this paper, we present our work on
using the densely connected convolutional neural network (DenseNet) and gated
recurrent unit network (GRU) for addressing the inter-patient ECG
classification problem. A deep learning model architecture is proposed and is
evaluated using the MIT-BIH Arrhythmia and Supraventricular Databases. The
results obtained show that without applying any complicated data pre-processing
or feature engineering methods, both of our models have considerably
outperformed the state-of-the-art performance for supraventricular (SVEB) and
ventricular (VEB) arrhythmia classifications on the unseen testing dataset
(with the F1 score improved from 51.08 to 61.25 for SVEB detection and from
88.59 to 89.75 for VEB detection respectively). As no patient-specific or
device-specific information is used at the training stage in this work, it can
be considered as a more generic approach for dealing with scenarios in which
varieties of ECG signals are collected from different patients using different
types of sensor devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04122</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04122</id><created>2018-10-04</created><authors><author><keyname>John</keyname><forenames>Jennifer N.</forenames></author><author><keyname>Galloway</keyname><forenames>Conner</forenames></author><author><keyname>Valys</keyname><forenames>Alexander</forenames></author></authors><title>Deep Convolutional Neural Networks for Noise Detection in ECGs</title><categories>eess.SP cs.LG cs.NE stat.ML</categories><comments>8 pages</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile electrocardiogram (ECG) recording technologies represent a promising
tool to fight the ongoing epidemic of cardiovascular diseases, which are
responsible for more deaths globally than any other cause. While the ability to
monitor one's heart activity at any time in any place is a crucial advantage of
such technologies, it is also the cause of a drawback: signal noise due to
environmental factors can render the ECGs illegible. In this work, we develop
convolutional neural networks (CNNs) to automatically label ECGs for noise,
training them on a novel noise-annotated dataset. By reducing distraction from
noisy intervals of signals, such networks have the potential to increase the
accuracy of models for the detection of atrial fibrillation, long QT syndrome,
and other cardiovascular conditions. Comparing several architectures, we find
that a 16-layer CNN adapted from the VGG16 network which generates one
prediction per second on a 10-second input performs exceptionally well on this
task, with an AUC of 0.977.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04123</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04123</id><created>2018-09-27</created><authors><author><keyname>Dinakarrao</keyname><forenames>Sai Manoj Pudukotai</forenames></author><author><keyname>Wess</keyname><forenames>Matthias</forenames></author></authors><title>Computer-Aided Arrhythmia Diagnosis by Learning ECG Signal</title><categories>eess.SP q-bio.QM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electrocardiogram (ECG) is one of the non-invasive and low-risk methods to
monitor the condition of the human heart. Any abnormal pattern(s) in the ECG
signal is an indicative measure of malfunctioning of the heart, termed as
arrhythmia. Due to the lack of human expertise and high probability to
misdiagnose, computer-aided diagnosis and analysis are preferred. In this work,
we perform arrhythmia detection with an optimized neural network having
piecewise linear approximation based activation function to alleviate the
complex computations in the traditional activation functions. Further, we
propose a self-learning method for arrhythmia detection by learning and
analyzing the characteristics (period) of the ECG signal. Self-learning based
approach achieves 97.28% of arrhythmia detection accuracy, and neural network
with optimized activation functions achieve an arrhythmia detection accuracy of
99.56%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04126</identifier>
 <datestamp>2019-02-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04126</id><created>2018-10-08</created><updated>2019-02-08</updated><authors><author><keyname>Arnold</keyname><forenames>Maximilian</forenames></author><author><keyname>Hoydis</keyname><forenames>Jakob</forenames></author><author><keyname>Brink</keyname><forenames>Stephan ten</forenames></author></authors><title>Novel Massive MIMO Channel Sounding Data Applied to Deep Learning-based
  Indoor Positioning</title><categories>eess.SP</categories><comments>Accepted SCC2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With a significant increase in area throughput, Massive MIMO has become an
enabling technology for fifth generation (5G) wireless mobile communication
systems. Although prototypes were built, an openly available dataset for
channel impulse responses to verify assumptions, e.g. regarding channel
sparsity, is not yet available. In this paper, we introduce a novel channel
sounder architecture, capable of measuring multiantenna and multi-subcarrier
channel state information (CSI) at different frequency bands, antenna
geometries and propagation environments. The channel sounder has been verified
by evaluation of channel data from first measurements. Such datasets can be
used to study various deep-learning (DL) techniques in different applications,
e.g., for indoor user positioning in three dimensions, as is done in this
paper. Not only we do achieve an accuracy better than 75 cm for line of sight
(LoS), as is comparable to state-of-the-art conventional positioning
techniques, but also obtain similar precision for the more challenging case of
non-line of sight (NLoS). Further extensive indoor/outdoor measurement
campaigns will provide a more comprehensive open CSI dataset, tagged with
positions, for the scientific community to further test various algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04127</identifier>
 <datestamp>2018-10-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04127</id><created>2018-10-05</created><authors><author><keyname>Hossan</keyname><forenames>Md. Tanvir</forenames></author><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Islam</keyname><forenames>Amirul</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>A Novel Indoor Mobile Localization System Based on Optical Camera
  Communication</title><categories>eess.SP cs.NI</categories><journal-ref>Wireless Communications and Mobile Computing, vol. 2018, Jan. 2018</journal-ref><doi>10.1155/2018/9353428</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localizing smartphones in indoor environments offers excellent opportunities
for e-commerce. In this paper, we propose a localization technique for
smartphones in indoor environments. This technique can calculate the
coordinates of a smartphone using existing illumination infrastructure with
light-emitting diodes (LEDs). The system can locate smartphones without further
modification of the existing LED light infrastructure. Smartphones do not have
fixed position and they may move frequently anywhere in an environment. Our
algorithm uses multiple (i.e., more than two) LED lights simultaneously. The
smartphone gets the LED-IDs from the LED lights that are within the field of
view (FOV) of the smartphone's camera. These LED-IDs contain the coordinate
information of the LED lights. Concurrently, the pixel area on the image sensor
(IS) of projected image changes with the relative motion between the smartphone
and each LED light which allows the algorithm to calculate the distance from
the smartphone to that LED.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04273</identifier>
 <datestamp>2018-10-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04273</id><created>2018-10-01</created><authors><author><keyname>Zeinali</keyname><forenames>Hossein</forenames></author><author><keyname>Burget</keyname><forenames>Lukas</forenames></author><author><keyname>Cernocky</keyname><forenames>Jan</forenames></author></authors><title>Convolutional Neural Networks and x-vector Embedding for DCASE2018
  Acoustic Scene Classification Challenge</title><categories>eess.AS cs.SD</categories><journal-ref>Proceedings of the Detection and Classification of Acoustic Scenes
  and Events 2018 Workshop (DCASE2018)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Brno University of Technology (BUT) team submissions for
Task 1 (Acoustic Scene Classification, ASC) of the DCASE-2018 challenge are
described. Also, the analysis of different methods on the leaderboard set is
provided. The proposed approach is a fusion of two different Convolutional
Neural Network (CNN) topologies. The first one is the common two-dimensional
CNNs which is mainly used in image classification. The second one is a
one-dimensional CNN for extracting fixed-length audio segment embeddings, so
called x-vectors, which has also been used in speech processing, especially for
speaker recognition. In addition to the different topologies, two types of
features were tested: log mel-spectrogram and CQT features. Finally, the
outputs of different systems are fused using a simple output averaging in the
best performing system. Our submissions ranked third among 24 teams in the ASC
sub-task A (task1a).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04276</identifier>
 <datestamp>2018-10-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04276</id><created>2018-10-04</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author></authors><title>Current Trends and Future Research Directions for Interactive Music</title><categories>cs.SD cs.AI eess.AS</categories><journal-ref>Journal of Theoretical &amp; Applied Information Technologies 96(16),
  2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this review, it is explained and compared different software and
formalisms used in music interaction: sequencers, computer-assisted
improvisation, meta- instruments, score-following, asynchronous dataflow
languages, synchronous dataflow languages, process calculi, temporal
constraints and interactive scores. Formal approaches have the advantage of
providing rigorous semantics of the behavior of the model and proving
correctness during execution. The main disadvantage of formal approaches is
lack of commercial tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04325</identifier>
 <datestamp>2018-10-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04325</id><created>2018-10-09</created><authors><author><keyname>Yoon</keyname><forenames>Jong-Yoon</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author></authors><title>Analysis of Maximal Topologies Achieving Optimal DoF and DoF
  $\frac{1}{n}$ in Topological Interference Management</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological interference management (TIM) can obtain degrees of freedom (DoF)
gains with no channel state information at the transmitters (CSIT) except
topological information of network in the interference channel. It was shown
that TIM achieves the optimal symmetric DoF when internal conflict does not
exist among messages. However, it is difficult to assure whether a specific
topology can achieve the optimal DoF without scrutinizing internal conflict,
which requires lots of works. Also, it is hard to design a specific optimal
topology directly from the conventional condition for the optimal DoF. With
these problems in mind, we propose a method to derive maximal topology directly
in TIM, named as alliance construction in K-user interference channel. That is,
it is proved that a topology is maximal if and only if it is derived from
alliance construction. We translate a topology design by alliance construction
in message graph into topology matrix and propose conditions for maximal
topology matrix (MTM). Moreover, we propose a generalized alliance construction
that derives a topology achieving DoF 1/n for n&gt;=3 by generalizing
sub-alliances. A topology matrix can also be used to analyze maximality of
topology with DoF 1/n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04376</identifier>
 <datestamp>2018-10-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04376</id><created>2018-10-10</created><authors><author><keyname>Marojevic</keyname><forenames>Vuk</forenames></author><author><keyname>Padaki</keyname><forenames>Aditya V.</forenames></author><author><keyname>Rao</keyname><forenames>Raghunandan M.</forenames></author><author><keyname>Reed</keyname><forenames>Jeffrey H.</forenames></author></authors><title>Measuring Hardware Impairments with Software-Defined Radios</title><categories>eess.SP</categories><comments>IEEE Frontiers in Education 2018 (FIE 2018)</comments><msc-class>94A12, 97U50, 97M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Innovative Practice Full Paper introduces a novel tool for educating
electrical engineering students about hardware impairments in wireless
communications. A radio frequency (RF) front end is an essential part of a
wireless transmitter or receiver. It features analog processing components and
data converters which are driven by today's digital communication systems.
Advancements in computing and software-defined radio (SDR) technology have
enabled shaping waveforms in software and using experimental and easily
accessible plug-and-play RF front ends for education, research and development.
We use this same technology to teach nonlinear effects of RF front ends and
their implications. It uses widely available RF instruments and components and
SDR technology--well-established affordable hardware and free open source
software--to teach students how to characterize the nonlinearity of RF
receivers while providing hands-on experience with SDR tools. We present the
hardware, software and procedures of our laboratory session that enable easy
reproducibility in other classrooms. We discuss different forms of evaluating
the suitability of the new class modules and conclude that it provides a
valuable learning experience that bolsters the theory that is typically
provided in lectures only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04425</identifier>
 <datestamp>2018-10-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04425</id><created>2018-10-10</created><authors><author><keyname>Qiao</keyname><forenames>Mengyun</forenames></author><author><keyname>Wang</keyname><forenames>Yuanyuan</forenames></author><author><keyname>van der Geest</keyname><forenames>Rob J.</forenames></author><author><keyname>Tao</keyname><forenames>Qian</forenames></author></authors><title>Fully Automated Left Atrium Cavity Segmentation from 3D GE-MRI by
  Multi-Atlas Selection and Registration</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a fully automated method to segment the complex left
atrial (LA) cavity, from 3D Gadolinium-enhanced magnetic resonance imaging
(GE-MRI) scans. The proposed method consists of four steps: (1) preprocessing
to convert the original GE-MRI to a probability map, (2) atlas selection to
match the atlases to the target image, (3) multi-atlas registration and fusion,
and (4) level-set refinement. The method was evaluated on the datasets provided
by the MICCAI 2018 STACOM Challenge with 100 dataset for training. Compared to
manual annotation, the proposed method achieved an average Dice overlap index
of 0.88.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04490</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04490</id><created>2018-10-10</created><updated>2019-07-23</updated><authors><author><keyname>Hegde</keyname><forenames>Ganapati</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames></author></authors><title>Interference Exploitation-based Hybrid Precoding with Robustness Against
  Phase Errors</title><categories>eess.SP</categories><journal-ref>IEEE Transactions on Wireless Communications, vol. 18, no. 7, pp.
  3683-3696, July 2019</journal-ref><doi>10.1109/TWC.2019.2917064</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid analog-digital precoding significantly reduces the hardware costs in
massive MIMO transceivers when compared to fully-digital precoding at the
expense of increased transmit power. In order to mitigate the above shortfall,
we use the concept of constructive interference-based precoding, which has been
shown to offer significant transmit power savings when compared with the
conventional interference suppression-based precoding in fully-digital
multiuser MIMO systems. Moreover, in order to circumvent the potential
quality-of-service degradation at the users due to the hardware impairments in
the transmitters, we judiciously incorporate robustness against such
vulnerabilities in the precoder design. Since the undertaken constructive
interference-based robust hybrid precoding problem is nonconvex with infinite
constraints and thus difficult to solve optimally, we decompose the problem
into two subtasks, namely, analog precoding and digital precoding. In this
paper, we propose an algorithm to compute the optimal constructive
interference-based robust digital precoders. Furthermore, we devise a scheme to
facilitate the implementation of the proposed algorithm in a low-complexity and
distributed manner. We also discuss block-level analog precoding techniques.
Simulation results demonstrate the superiority of the proposed algorithm and
its implementation scheme over the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04506</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04506</id><created>2018-10-10</created><updated>2019-05-20</updated><authors><author><keyname>Lostanlen</keyname><forenames>Vincent</forenames></author></authors><title>On Time-frequency Scattering and Computer Music</title><categories>cs.SD eess.AS</categories><comments>5 pages. Published as a chapter in the book: &quot;Florian Hecker:
  Halluzination, Perspektive, Synthese&quot;, pp. 97--102. Nicolaus Schafhausen,
  Vanessa Joan M\&quot;uller, editors. Sternberg Press, Berlin, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-frequency scattering is a mathematical transformation of sound waves.
Its core purpose is to mimick the way the human auditory system extracts
information from its environment. In the context of improving the artificial
intelligence of sounds, it has found succesful applications in automatic speech
transcription as well as the recognition of urban sounds and musical sounds. In
this article, we show that time-frequency scattering can also be useful for
applications in contemporary music creations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04520</identifier>
 <datestamp>2018-10-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04520</id><created>2018-10-03</created><authors><author><keyname>Anh</keyname><forenames>Tran The</forenames></author><author><keyname>Luong</keyname><forenames>Nguyen Cong</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Liang</keyname><forenames>Ying-Chang</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author></authors><title>Deep Reinforcement Learning for Time Scheduling in RF-Powered
  Backscatter Cognitive Radio Networks</title><categories>cs.LG cs.GT cs.NI eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an RF-powered backscatter cognitive radio network, multiple secondary
users communicate with a secondary gateway by backscattering or harvesting
energy and actively transmitting their data depending on the primary channel
state. To coordinate the transmission of multiple secondary transmitters, the
secondary gateway needs to schedule the backscattering time, energy harvesting
time, and transmission time among them. However, under the dynamics of the
primary channel and the uncertainty of the energy state of the secondary
transmitters, it is challenging for the gateway to find a time scheduling
mechanism which maximizes the total throughput. In this paper, we propose to
use the deep reinforcement learning algorithm to derive an optimal time
scheduling policy for the gateway. Specifically, to deal with the problem with
large state and action spaces, we adopt a Double Deep-Q Network (DDQN) that
enables the gateway to learn the optimal policy. The simulation results clearly
show that the proposed deep reinforcement learning algorithm outperforms
non-learning schemes in terms of network throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04529</identifier>
 <datestamp>2018-10-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04529</id><created>2018-10-10</created><authors><author><keyname>Francis</keyname><forenames>Jobin</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Power Allocation for Massive MIMO-based, Fronthaul-constrained Cloud RAN
  Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud radio access network (C-RAN) and massive multiple-input-multiple-output
(MIMO) are two key enabling technologies to meet the diverse and stringent
requirements of the 5G use cases. In a C-RAN system with massive MIMO,
fronthaul is often the bottleneck due to its finite capacity and transmit
precoding is moved to the remote radio head to reduce the capacity requirements
on fronthaul. For such a system, we optimize the power allocated to the users
to maximize first the weighted sum rate and then the energy efficiency (EE)
while explicitly incorporating the capacity constraints on fronthaul. We
consider two different fronthaul constraints, which model capacity constraints
on different parts of the fronthaul network. We develop successive convex
approximation algorithms that achieve a stationary point of these non-convex
problems. To this end, we first present novel, locally tight bounds for the
user rate expression. They are used to obtain convex approximations of the
original non-convex problems, which are then solved by solving their dual
problems. In EE maximization, we also employ the Dinkelbach algorithm to handle
the fractional form of the objective function. Numerical results show that the
proposed algorithms significantly improve the network performance compared to a
case with no power control and achieves a better performance than an existing
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04582</identifier>
 <datestamp>2019-09-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04582</id><created>2018-10-10</created><updated>2019-08-09</updated><authors><author><keyname>Lakhan</keyname><forenames>Payongkit</forenames></author><author><keyname>Banluesombatkul</keyname><forenames>Nannapas</forenames></author><author><keyname>Changniam</keyname><forenames>Vongsagon</forenames></author><author><keyname>Dhithijaiyratn</keyname><forenames>Ratwade</forenames></author><author><keyname>Leelaarporn</keyname><forenames>Pitshaporn</forenames></author><author><keyname>Boonchieng</keyname><forenames>Ekkarat</forenames></author><author><keyname>Hompoonsup</keyname><forenames>Supanida</forenames></author><author><keyname>Wilaiprasitporn</keyname><forenames>Theerawit</forenames></author></authors><title>Consumer Grade Brain Sensing for Emotion Recognition</title><categories>eess.SP cs.HC</categories><journal-ref>IEEE Sensor Journal, 2019</journal-ref><doi>10.1109/JSEN.2019.2928781</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several decades, electroencephalography (EEG) has featured as one of the
most commonly used tools in emotional state recognition via monitoring of
distinctive brain activities. An array of datasets have been generated with the
use of diverse emotion-eliciting stimuli and the resulting brainwave responses
conventionally captured with high-end EEG devices. However, the applicability
of these devices is to some extent limited by practical constraints and may
prove difficult to be deployed in highly mobile context omnipresent in everyday
happenings. In this study, we evaluate the potential of OpenBCI to bridge this
gap by first comparing its performance to research grade EEG system, employing
the same algorithms that were applied on benchmark datasets. Moreover, for the
purpose of emotion classification, we propose a novel method to facilitate the
selection of audio-visual stimuli of high/low valence and arousal. Our setup
entailed recruiting 200 healthy volunteers of varying years of age to identify
the top 60 affective video clips from a total of 120 candidates through
standardized self assessment, genre tags, and unsupervised machine learning.
Additional 43 participants were enrolled to watch the pre-selected clips during
which emotional EEG brainwaves and peripheral physiological signals were
collected. These recordings were analyzed and extracted features fed into a
classification model to predict whether the elicited signals were associated
with a high or low level of valence and arousal. As it turned out, our
prediction accuracies were decidedly comparable to those of previous studies
that utilized more costly EEG amplifiers for data acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04719</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04719</id><created>2018-10-10</created><updated>2019-02-19</updated><authors><author><keyname>Zhang</keyname><forenames>Aonan</forenames></author><author><keyname>Wang</keyname><forenames>Quan</forenames></author><author><keyname>Zhu</keyname><forenames>Zhenyao</forenames></author><author><keyname>Paisley</keyname><forenames>John</forenames></author><author><keyname>Wang</keyname><forenames>Chong</forenames></author></authors><title>Fully Supervised Speaker Diarization</title><categories>eess.AS cs.LG stat.ML</categories><comments>Accepted by ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a fully supervised speaker diarization approach,
named unbounded interleaved-state recurrent neural networks (UIS-RNN). Given
extracted speaker-discriminative embeddings (a.k.a. d-vectors) from input
utterances, each individual speaker is modeled by a parameter-sharing RNN,
while the RNN states for different speakers interleave in the time domain. This
RNN is naturally integrated with a distance-dependent Chinese restaurant
process (ddCRP) to accommodate an unknown number of speakers. Our system is
fully supervised and is able to learn from examples where time-stamped speaker
labels are annotated. We achieved a 7.6% diarization error rate on NIST SRE
2000 CALLHOME, which is better than the state-of-the-art method using spectral
clustering. Moreover, our method decodes in an online fashion while most
state-of-the-art systems rely on offline clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04803</identifier>
 <datestamp>2018-10-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04803</id><created>2018-10-09</created><authors><author><keyname>Gheth</keyname><forenames>Waled</forenames></author><author><keyname>Rabie</keyname><forenames>Khaled M.</forenames></author><author><keyname>Adebisi</keyname><forenames>Bamidele</forenames></author><author><keyname>Ijaz</keyname><forenames>Muhammad</forenames></author><author><keyname>Harris</keyname><forenames>Georgina</forenames></author></authors><title>On the Performance of DF-based Power-Line/Visible-Light Communication
  Systems</title><categories>eess.SP</categories><comments>arXiv admin note: text overlap with arXiv:1807.10169</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comprehensive performance analysis of an integrated
indoor power line communication (PLC)/visible light communication (VLC) system
with the presence of a decode-and-forward (DF) relay. The existing indoor power
line networks are used as the backbone for VLCs. The performance of the
proposed system is evaluated in terms of the average capacity and the outage
probability. A new unified mathematical method is developed for the PLC/VLC
system and analytical expressions for the aforementioned performance metrics
are derived. Monte Carlo simulations are provided throughout the paper to
verify the correctness of the analysis. The results reveal that the performance
of the proposed system deteriorates with increasing the end-to-end distance and
improves with increasing the relay transmit power. It is also shown that the
outage probability of the system under consideration is negatively affected by
the vertical distance to user plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04826</identifier>
 <datestamp>2019-06-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04826</id><created>2018-10-10</created><updated>2019-06-19</updated><authors><author><keyname>Wang</keyname><forenames>Quan</forenames></author><author><keyname>Muckenhirn</keyname><forenames>Hannah</forenames></author><author><keyname>Wilson</keyname><forenames>Kevin</forenames></author><author><keyname>Sridhar</keyname><forenames>Prashant</forenames></author><author><keyname>Wu</keyname><forenames>Zelin</forenames></author><author><keyname>Hershey</keyname><forenames>John</forenames></author><author><keyname>Saurous</keyname><forenames>Rif A.</forenames></author><author><keyname>Weiss</keyname><forenames>Ron J.</forenames></author><author><keyname>Jia</keyname><forenames>Ye</forenames></author><author><keyname>Moreno</keyname><forenames>Ignacio Lopez</forenames></author></authors><title>VoiceFilter: Targeted Voice Separation by Speaker-Conditioned
  Spectrogram Masking</title><categories>eess.AS cs.LG eess.SP stat.ML</categories><comments>To appear in Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel system that separates the voice of a target
speaker from multi-speaker signals, by making use of a reference signal from
the target speaker. We achieve this by training two separate neural networks:
(1) A speaker recognition network that produces speaker-discriminative
embeddings; (2) A spectrogram masking network that takes both noisy spectrogram
and speaker embedding as input, and produces a mask. Our system significantly
reduces the speech recognition WER on multi-speaker signals, with minimal WER
degradation on single-speaker signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04840</identifier>
 <datestamp>2018-10-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04840</id><created>2018-10-11</created><authors><author><keyname>Kongara</keyname><forenames>Gayathri</forenames></author><author><keyname>Yang</keyname><forenames>Lei</forenames></author><author><keyname>He</keyname><forenames>Cuiwei</forenames></author><author><keyname>Armstrong</keyname><forenames>Jean</forenames></author></authors><title>A Comparison of CP-OFDM, PCC-OFDM and UFMC for 5G Uplink Communications</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial-cancellation-coded orthogonal frequency division multiplexing
(PCC-OFDM) is a form of OFDM that has waveforms which are very well localized
in both the time and frequency domains and so it is ideally suited for use in
the 5G network. This paper analyzes the performance of PCC-OFDM in the uplink
of a multiuser system using orthogonal frequency division multiple access
(OFDMA) and compares it with conventional cyclic prefix OFDM (CP-OFDM), and
universal filtered multicarrier (UFMC). PCC-OFDM is shown to be much less
sensitive than either CP-OFDM or UFMC to time and frequency offsets. For a
given constellation size, PCC-OFDM in additive white Gaussian noise (AWGN)
requires 3dB lower signal-to-noise ratio (SNR) for a given bit-error-rate, and
the SNR advantage of PCC-OFDM increases rapidly when there are timing and/or
frequency offsets. For PCC-OFDM no frequency guard band is required between
different OFDMA users. PCC-OFDM is completely compatible with CP-OFDM and adds
negligible complexity and latency, as it uses a simple mapping of data onto
pairs of subcarriers at the transmitter, and a simple weighting-and-adding of
pairs of subcarriers at the receiver. The weighting and adding step, which has
been omitted in some of the literature, is shown to contribute substantially to
the SNR advantage of PCC-OFDM. A disadvantage of PCC-OFDM (without overlapping)
is the potential reduction in spectral efficiency because subcarriers are
modulated in pairs, but this reduction is more than regained because no guard
band or cyclic prefix is required and because, for a given channel, larger
constellations can be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04846</identifier>
 <datestamp>2018-10-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04846</id><created>2018-10-11</created><authors><author><keyname>Choi</keyname><forenames>Junsung</forenames></author><author><keyname>Marojevic</keyname><forenames>Vuk</forenames></author><author><keyname>Labib</keyname><forenames>Mina</forenames></author><author><keyname>Kabra</keyname><forenames>Siddharth</forenames></author><author><keyname>Rao</keyname><forenames>Jayanthi</forenames></author><author><keyname>Das</keyname><forenames>Sushanta</forenames></author><author><keyname>Reed</keyname><forenames>Jeffery H.</forenames></author><author><keyname>Dietrich</keyname><forenames>Carl B.</forenames></author></authors><title>Regulatory Options and Technical Challenges for the 5.9 GHz Spectrum:
  Survey and Analysis</title><categories>eess.SP cs.NI</categories><msc-class>94-02, 94A99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1999, the Federal Communications Commission (FCC) allocated 75 MHz in the
5.9 GHz ITS Band (5850-5925 MHz) for use by Dedicated Short Range
Communications (DSRC) to facilitate information transfer between equipped
vehicles and roadside systems. This allocation for DSRC in the ITS band has
been a co-primary allocation while the band is shared with the Fixed Satellite
Service (FSS), fixed microwave service, amateur radio services and other
Federal users authorized by the National Telecommunications and Information
Administration (NTIA). In recent time, Cellular V2X (C-V2X), introduced in 3GPP
Release 14 LTE standard, has received significant attention due to its
perceived ability to deliver superior performance with respect to vehicular
safety applications. There is a strong momentum in the industry for C-V2X to be
considered as a viable alternative to DSRC and accordingly, to operate in the
ITS spectrum. In another recent notice, the FCC is soliciting input for a
proposed rulemaking to open up more bandwidth for unlicensed Wi-Fi devices,
mainly based on the 802.11ac standard. The FCC plans to work with the
Department of Transportation (DoT), and the automotive and communications
industries to evaluate potential sharing techniques in the ITS band between
DSRC and Wi-Fi devices. This paper analyzes the expected scenarios that might
emerge from FCC and the National Highway Traffic Safety Administration (NHTSA)
regulation options and identifies the technical challenge associated with each
scenario. We also provide a literature survey and find that many of resulting
technical challenges remain open research problems that need to be addressed.
We conclude that the most challenging issue is related to the interoperability
between DSRC and C-V2X in the 5.9 GHz band and the detection and avoidance of
harmful adjacent and co-channel interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04894</identifier>
 <datestamp>2019-08-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04894</id><created>2018-10-11</created><updated>2019-08-23</updated><authors><author><keyname>Drayer</keyname><forenames>Elisabeth</forenames></author><author><keyname>Routtenberg</keyname><forenames>Tirza</forenames></author></authors><title>Detection of false data injection attacks in smart grids based on graph
  signal processing</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smart grid combines the classical power system with information
technology, leading to a cyber-physical system. In such an environment the
malicious injection of data has the potential to cause severe consequences.
Classical residual-based methods for bad data detection are unable to detect
well designed false data injection (FDI) attacks. Moreover, most work on FDI
attack detection is based on the linearized DC model of the power system and
fails to detect attacks based on the AC model. The aim of this paper is to
address these problems by using the graph structure of the grid and the AC
power flow model. We derive an attack detection method that is able to detect
previously undetectable FDI attacks. This method is based on concepts
originating from graph signal processing (GSP). The proposed detection scheme
calculates the graph Fourier transform of an estimated grid state and filters
the graph's high-frequency components. By comparing the maximum norm of this
outcome with a threshold we can detect the presence of FDI attacks. Case
studies on the IEEE 14-bus system demonstrate that the proposed method is able
to detect a wide range of previously undetectable attacks, both on angles and
on magnitudes of the voltages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.04906</identifier>
 <datestamp>2018-10-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.04906</id><created>2018-10-11</created><authors><author><keyname>Ghatak</keyname><forenames>Gourab</forenames></author><author><keyname>De Domenico</keyname><forenames>Antonio</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author></authors><title>Accurate Characterization of Dynamic Cell Load in Noise-Limited Random
  Cellular Networks</title><categories>cs.IT eess.SP math.IT</categories><comments>Presented in IEEE VTC-Fall 2018, Chicago, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analyses of cellular network performance based on stochastic geometry
generally ignore the traffic dynamics in the network. This restricts the proper
evaluation and dimensioning of the network from the perspective of a mobile
operator. To address the effect of dynamic traffic, recently, the mean cell
approach has been introduced, which approximates the average network load by
the zero cell load. However, this is not a realistic characterization of the
network load, since a zero cell is statistically larger than a random cell
drawn from the population of cells, i.e., a typical cell. In this paper, we
analyze the load of a noise-limited network characterized by high signal to
noise ratio (SNR). The noise-limited assumption can be applied to a variety of
scenarios, e.g., millimeter wave networks with efficient interference
management mechanisms. First, we provide an analytical framework to obtain the
cumulative density function of the load of the typical cell. Then, we obtain
two approximations of the average load of the typical cell. We show that our
study provides a more realistic characterization of the average load of the
network as compared to the mean cell approach. Moreover, the prescribed
closed-form approximation is more tractable than the mean cell approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05246</identifier>
 <datestamp>2019-03-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05246</id><created>2018-10-11</created><updated>2019-03-22</updated><authors><author><keyname>Donahue</keyname><forenames>Chris</forenames></author><author><keyname>Simon</keyname><forenames>Ian</forenames></author><author><keyname>Dieleman</keyname><forenames>Sander</forenames></author></authors><title>Piano Genie</title><categories>cs.LG cs.HC cs.SD eess.AS stat.ML</categories><comments>Published as a conference paper at ACM IUI 2019</comments><doi>10.1145/3301275.3302288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Piano Genie, an intelligent controller which allows non-musicians
to improvise on the piano. With Piano Genie, a user performs on a simple
interface with eight buttons, and their performance is decoded into the space
of plausible piano music in real time. To learn a suitable mapping procedure
for this problem, we train recurrent neural network autoencoders with discrete
bottlenecks: an encoder learns an appropriate sequence of buttons corresponding
to a piano piece, and a decoder learns to map this sequence back to the
original piece. During performance, we substitute a user's input for the
encoder output, and play the decoder's prediction each time the user presses a
button. To improve the intuitiveness of Piano Genie's performance behavior, we
impose musically meaningful constraints over the encoder's outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05248</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05248</id><created>2018-10-11</created><authors><author><keyname>Alkishriwo</keyname><forenames>Osama A. S.</forenames></author><author><keyname>Elghariani</keyname><forenames>Ali A.</forenames></author><author><keyname>Akan</keyname><forenames>Aydin</forenames></author></authors><title>Iterative Time-Varying Filter Algorithm Based on Discrete Linear Chirp
  Transform</title><categories>eess.SP</categories><comments>6 pages, conference paper</comments><journal-ref>First Conference for Engineering Sciences and Technology
  (CEST-2018)</journal-ref><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Denoising of broadband non--stationary signals is a challenging problem in
communication systems. In this paper, we introduce a time-varying filter
algorithm based on the discrete linear chirp transform (DLCT), which provides
local signal decomposition in terms of linear chirps. The method relies on the
ability of the DLCT for providing a sparse representation to a wide class of
broadband signals. The performance of the proposed algorithm is compared with
the discrete fractional Fourier transform (DFrFT) filtering algorithm.
Simulation results show that the DLCT algorithm provides better performance
than the DFrFT algorithm and consequently achieves high quality filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05260</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05260</id><created>2018-10-11</created><authors><author><keyname>Alkishriwo</keyname><forenames>Osama A. S.</forenames></author></authors><title>A Novel Chaotic Uniform Quantizer for Speech Coding</title><categories>eess.AS cs.SD eess.SP</categories><comments>6 pages</comments><journal-ref>First Conference for Engineering Sciences and Technology
  (CEST-2018)</journal-ref><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Quantization is an essential step in the analog-to-digital conversion process
and it is very important in all modern telecommunication systems. In this
paper, a novel chaotic uniform quantizer is proposed and its application for
speech coding is presented. The proposed system consists of three stages: two
PCM coders separated by an XOR operation with a chaotic sequence, where the
first step is used for continuous signal sampling and second stage performs
data encryption, while the third stage provides additional data compression.
The performance of the presented quantizer for Laplacian distributed signals
and real speech signals is investigated and compared with that of the
well-known uniform and non-uniform quantizers. Simulation results show that the
proposed quantizer provides secured data with higher levels of SQNR compared to
others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05263</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05263</id><created>2018-10-11</created><authors><author><keyname>Abdelgader</keyname><forenames>Aya H. S.</forenames></author><author><keyname>Aboughalia</keyname><forenames>Raneem A.</forenames></author><author><keyname>Alkishriwo</keyname><forenames>Osama A. S.</forenames></author></authors><title>Combined Image Encryption and Steganography Algorithm in the Spatial
  Domain</title><categories>eess.IV cs.MM</categories><comments>6 pages</comments><journal-ref>First Conference for Engineering Sciences and Technology
  (CEST-2018)</journal-ref><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In recent years, steganography has emerged as one of the main research areas
in information security. Least significant bit (LSB) steganography is one of
the fundamental and conventional spatial domain methods, which is capable of
hiding larger secret information in a cover image without noticeable visual
distortions. In this paper, a combined algorithm based on LSB steganography and
chaotic encryption is proposed. Experimental results show the feasibility of
the proposed method. In comparison with existing steganographic spatial domain
based algorithms, the suggested algorithm is shown to have some advantages over
existing ones, namely, larger key space and a higher level of security against
some existing attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05269</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05269</id><created>2018-10-11</created><authors><author><keyname>Alkishriwo</keyname><forenames>Osama A.</forenames></author><author><keyname>Chaparro</keyname><forenames>Luis F.</forenames></author></authors><title>Instantaneous frequency estimation using the discrete linear chirp
  transform and the Wigner distribution</title><categories>eess.SP</categories><comments>4 pages</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this paper, we propose a new method to estimate instantaneous frequency
using a combined approach based on the discrete linear chirp transform (DLCT)
and the Wigner distribution (WD). The DLCT locally represents a signal as a
superposition of linear chirps while the WD provides maximum energy
concentration along the instantaneous frequency in the time-frequency domain
for each of the chirps. The developed approach takes advantage of the
separation of the linear chirps given by the DLCT, and that for each of them,
the WD provides an ideal representation. Combining the WD of the linear chirp
components, we obtain a time-frequency representation free of cross-terms that
clearly displays the instantaneous frequency. Applying this procedure locally,
we obtain an instantaneous frequency estimate of a non-stationary
multicomponent signal. The proposed method is illustrated by simulation. The
results indicate the method is efficient for the instantaneous frequency
estimation of multicomponent signals embedded in noise, even in cases of low
signal to noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05275</identifier>
 <datestamp>2019-05-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05275</id><created>2018-10-11</created><authors><author><keyname>Zarabie</keyname><forenames>Ahmad Khaled</forenames></author><author><keyname>Das</keyname><forenames>Sanjoy</forenames></author><author><keyname>Faqiry</keyname><forenames>Mohammed Nazif</forenames></author></authors><title>Fairness-Regularized DLMP-Based Bilevel Transactive Energy Mechanism in
  Distribution Systems</title><categories>eess.SP</categories><doi>10.1109/TSG.2019.2895527</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Distribution locational marginal pricing (DLMP) can adversely affect users in
a grid-constrained transactive distribution system market (DSM) that are at a
distance away from the substation, requiring longer paths to connect to the
substation. When the grid operates closer to its physical limits in terms of
line capacities and voltage deviations, these users are more likely to cause
grid violations than others in the vicinity of the substation. Conversely,
increased energy consumption by users near the substation can choke off the
supply to those at the grid s extremities. This research describes a novel
mechanism to charge users in a more equitable manner, by regularizing the
distribution system operator (DSO)s social welfare based objective function
with a fairness criterion. The Jains index of fairness is used in this context
and corresponding fairness DLMP component is derived. The overall problem
entails the maximization of the regularized objective within a set of linear
constraints. The constraints that ensure that the grids voltage and line power
limits are not violated. Constrained optimization is carried out in an
iterative manner through dual decomposition where the dual variables and unit
costs are incrementally updated by the DSO using the augmented Lagrangian
method (ALM). Simulations reported in this study confirm the effectiveness of
the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05316</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05316</id><created>2018-10-11</created><updated>2019-04-10</updated><authors><author><keyname>Kong</keyname><forenames>Linglin</forenames></author><author><keyname>Ling</keyname><forenames>Li</forenames></author><author><keyname>Zhang</keyname><forenames>Xu</forenames></author></authors><title>SCMA based resource management of D2D communications for maximum
  sum-revenue</title><categories>eess.SP</categories><comments>This paper was accepted for the WOCC 2019 in Beijing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The device-to-device (D2D) communication is one of the promising technologies
of the future Internet of Things (IoT), but its security-related issues remain
challenging. The block-chain is considered to be a secure and reliable
distributed ledger, so we can treat the device user equipment (D-UE) request
for the reusing resources of cellular user equipment (C-UE) as a transaction
and put it into a transaction pool, then package the record into the
block-chain. In this paper, we study the D2D communication resource allocation
scheme based on sparse code multiple access (SCMA). Firstly, the system's
interference model and block-chain-based transaction flow are analyzed. Then we
propose the optimization problem so that C-UE can get the maximum revenue by
sharing its resources to D-UE. This problem is NP-hard, so we propose a
heuristic algorithm based on semi-definite relaxation (SDR) programming to
solve it. Finally, the performance of the proposed algorithm is verified by
simulation of different system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05319</identifier>
 <datestamp>2019-07-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05319</id><created>2018-10-11</created><updated>2019-07-01</updated><authors><author><keyname>Rabiee</keyname><forenames>Azam</forenames></author><author><keyname>Kim</keyname><forenames>Geonmin</forenames></author><author><keyname>Kim</keyname><forenames>Tae-Ho</forenames></author><author><keyname>Lee</keyname><forenames>Soo-Young</forenames></author></authors><title>A Fully Time-domain Neural Model for Subband-based Speech Synthesizer</title><categories>eess.AS cs.SD</categories><comments>5 pages, 3 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a deep neural network model for subband-based speech
synthesizer. The model benefits from the short bandwidth of the subband signals
to reduce the complexity of the time-domain speech generator. We employed the
multi-level wavelet analysis/synthesis to decompose/reconstruct the signal into
subbands in time domain. Inspired from the WaveNet, a convolutional neural
network (CNN) model predicts subband speech signals fully in time domain. Due
to the short bandwidth of the subbands, a simple network architecture is enough
to train the simple patterns of the subbands accurately. In the ground truth
experiments with teacher-forcing, the subband synthesizer outperforms the
fullband model significantly in terms of both subjective and objective
measures. In addition, by conditioning the model on the phoneme sequence using
a pronunciation dictionary, we have achieved the fully time-domain neural model
for subband-based text-to-speech (TTS) synthesizer, which is nearly end-to-end.
The generated speech of the subband TTS shows comparable quality as the
fullband one with a slighter network architecture for each subband.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05333</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05333</id><created>2018-10-11</created><updated>2019-03-22</updated><authors><author><keyname>Ji</keyname><forenames>Feng</forenames></author><author><keyname>Tang</keyname><forenames>Wenchang</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author></authors><title>On the Properties of Gromov Matrices and their Applications in Network
  Inference</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2908133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spanning tree heuristic is a commonly adopted procedure in network
inference and estimation. It allows one to generalize an inference method
developed for trees, which is usually based on a statistically rigorous
approach, to a heuristic procedure for general graphs by (usually randomly)
choosing a spanning tree in the graph to apply the approach developed for
trees. However, there are an intractable number of spanning trees in a dense
graph. In this paper, we represent a weighted tree with a matrix, which we call
a Gromov matrix. We propose a method that constructs a family of Gromov
matrices using convex combinations, which can be used for inference and
estimation instead of a randomly selected spanning tree. This procedure
increases the size of the candidate set and hence enhances the performance of
the classical spanning tree heuristic. On the other hand, our new scheme is
based on simple algebraic constructions using matrices, and hence is still
computationally tractable. We discuss some applications on network inference
and estimation to demonstrate the usefulness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05367</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05367</id><created>2018-10-12</created><updated>2018-10-14</updated><authors><author><keyname>Song</keyname><forenames>Ke</forenames></author><author><keyname>Yuan</keyname><forenames>Chun</forenames></author><author><keyname>Gao</keyname><forenames>Peng</forenames></author><author><keyname>Sun</keyname><forenames>Yunxu</forenames></author></authors><title>FPGA-based Acceleration System for Visual Tracking</title><categories>cs.CV eess.IV</categories><comments>Accepted by IEEE 14th International Conference on Solid-State and
  Integrated Circuit Technology (ICSICT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual tracking is one of the most important application areas of computer
vision. At present, most algorithms are mainly implemented on PCs, and it is
difficult to ensure real-time performance when applied in the real scenario. In
order to improve the tracking speed and reduce the overall power consumption of
visual tracking, this paper proposes a real-time visual tracking algorithm
based on DSST(Discriminative Scale Space Tracking) approach. We implement a
hardware system on Xilinx XC7K325T FPGA platform based on our proposed visual
tracking algorithm. Our hardware system can run at more than 153 frames per
second. In order to reduce the resource occupation, our system adopts the batch
processing method in the feature extraction module. In the filter processing
module, the FFT IP core is time-division multiplexed. Therefore, our hardware
system utilizes LUTs and storage blocks of 33% and 40%, respectively. Test
results show that the proposed visual tracking hardware system has excellent
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05389</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05389</id><created>2018-10-12</created><authors><author><keyname>Ge</keyname><forenames>Yinghao</forenames></author><author><keyname>Zhang</keyname><forenames>Weile</forenames></author><author><keyname>Gao</keyname><forenames>Feifei</forenames></author><author><keyname>Li</keyname><forenames>Geoffrey Ye</forenames></author></authors><title>Frequency Synchronization for Uplink Massive MIMO with Adaptive MUI
  Suppression in Angle-domain</title><categories>eess.SP</categories><comments>14 pages (double columns), 7 figures</comments><doi>10.1109/TSP.2019.2901344</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a novel angle-domain adaptive filtering
(ADAF)-based frequency synchronization method for the uplink of a massive
multiple-input multiple-output (MIMO) multiuser network, which is applicable
for users with either separate or overlapped angle-of-arrival (AoA) regions.
For each user, we first introduce the angle-constraining matrix (ACM), which
consists of a set of selected match-filter (MF) beamformers pointing to the
AoAs of the interested user. Then, the adaptive beamformer can be acquired by
appropriately designing the ADAF vectors. Such beamformer can achieve two-stage
adaptive multiuser interference (MUI) suppression, i.e., inherently suppressing
the MUI from non-overlapping users by ACM in the first stage and substantially
mitigating the MUI from adjacent overlapping users by ADAF vector in the second
stage. For both separate and mutually overlapping users, the carrier frequency
offset (CFO) estimation and subsequent data detection can be performed
independently for each user, which significantly reduces computational
complexity. Moreover, ADAF is rather robust to imperfect AoA knowledge and side
cluster of scatterers, making itself promising for multiuser uplink
transmission. We also analyze the performance of CFO estimation and obtain an
approximated closed-form expression of mean-squared error (MSE). The
effectiveness of the proposed method and its superiority over the existing
methods are demonstrated by numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05422</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05422</id><created>2018-10-12</created><authors><author><keyname>Alam</keyname><forenames>Mehmood</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author></authors><title>A Survey: Non-Orthogonal Multiple Access with Compressed Sensing
  Multiuser Detection for mMTC</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One objective of the 5G communication system and beyond is to support massive
machine type of communication (mMTC) to propel the fast growth of diverse
Internet of Things use cases. The mMTC aims to provide connectivity to tens of
billions sensor nodes. The dramatic increase of sensor devices and massive
connectivity impose critical challenges for the network to handle the enormous
control signaling overhead with limited radio resource. Non-Orthogonal Multiple
Access (NOMA) is a new paradigm shift in the design of multiple user detection
and multiple access. NOMA with compressive sensing based multiuser detection is
one of the promising candidates to address the challenges of mMTC. The survey
article aims at providing an overview of the current state-of-art research work
in various compressive sensing based techniques that enable NOMA. We present
characteristics of different algorithms and compare their pros and cons,
thereby provide useful insights for researchers to make further contributions
in NOMA using compressive sensing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05460</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05460</id><created>2018-10-12</created><updated>2018-11-10</updated><authors><author><keyname>Xu</keyname><forenames>Tingzhen</forenames></author><author><keyname>Xu</keyname><forenames>Ling</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Lu</keyname><forenames>Zaoyu</forenames></author></authors><title>Covert Communication with A Full-Duplex Receiver Based on Channel
  Distribution Information</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a system of covert communication with the aid of a
full-duplex (FD) receiver to enhance the performance in a more realistic
scenario, i.e., only the channel distribution information (CDI) rather than
channel state information (CSI) is known to a warden. Our work shows that
transmitting random AN can improve the covert communication with the infinite
blocklength. Specifically, we jointly design the optimal transmit power and AN
power by minimizing the outage probability at Bob, and we find that the outage
probability decreases and then increases as the maximum allowable AN power
increases. Intuitively, once AN exceeds an optimal value, the performance will
become worse because of the self-interference. The simulation results also show
that the performance behaviors of CDI and CSI are different. When Willie only
knows CDI, there is an optimal AN power that minimizes Bob's outage
probability. However, when Willie knows CSI, the outage probability
monotonically decreases with AN power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05502</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05502</id><created>2018-10-06</created><authors><author><keyname>K</keyname><forenames>Devipriya T</forenames></author><author><keyname>A</keyname><forenames>Jovita Franci</forenames></author><author><keyname>R</keyname><forenames>Deepa</forenames></author><author><keyname>Josh</keyname><forenames>Godwin Sam</forenames></author></authors><title>Asynchronous Wi-Fi Control Interface (AWCI) Using Socket IO Technology</title><categories>eess.SP cs.DC</categories><comments>5 pages, 5 figures, published with Global Research and Development
  Journal for Engineering</comments><journal-ref>Global Research and Development Journal for Engineering, 1(3),
  pp.66-70, 2017</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things (IoT) is a system of interrelated computing devices to
the Internet that are provided with unique identifiers which has the ability to
transfer data over a network without requiring human-to- human or human-to-
computer interaction. Raspberry pi-3 a popular, cheap, small and powerful
computer with built in Wi-Fi can be used to make any devices smart by
connecting to that particular device and embedding the required software to
Raspberry pi-3 and connect it to Internet. It is difficult to install a full
Linux OS inside a small devices like light switch so in that case to connect to
a Wi-Fi connection a model was proposed known as Asynchronous Wi-Fi Control
Interface (AWCI) which is a simple Wi-Fi connectivity software for a Debian
compatible Linux OS). The objective of this paper is to make the interactive
user interface for Wi-Fi connection in Raspberry Pi touch display by providing
live updates using Socket IO technology. The Socket IO technology enables
real-time bidirectional communication between client and server. Asynchronous
Wi-Fi Control Interface (AWCI) is compatible with every platform, browser or
device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05510</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05510</id><created>2018-09-22</created><authors><author><keyname>Amer</keyname><forenames>Ramy</forenames></author><author><keyname>Elsawy</keyname><forenames>Hesham</forenames></author><author><keyname>Butt</keyname><forenames>M. Majid</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author></authors><title>Optimizing Joint Probabilistic Caching and Communication for Clustered
  D2D Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching at mobile devices and leveraging device-to-device (D2D) communication
are two promising approaches to support massive content delivery over wireless
networks. The analysis of such D2D caching networks based on a physical
interference model is usually carried out by assuming that devices are
uniformly distributed. However, this approach does not fully consider and
characterize the fact that devices are usually grouped into clusters. Motivated
by this fact, this paper presents a comprehensive performance analysis and
joint communication and caching optimization for a clustered D2D network.
Devices are distributed according to a Thomas cluster process (TCP) and are
assumed to have a surplus memory which is exploited to proactively cache files
from a known library, following a random probabilistic caching scheme. Devices
can retrieve the requested files from their caches, from neighbouring devices
in their proximity (cluster), or from the base station as a last resort. Three
key performance metrics are optimized in this paper, namely, the offloading
gain, energy consumption, and latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05512</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05512</id><created>2018-10-09</created><updated>2019-02-18</updated><authors><author><keyname>Leroy</keyname><forenames>David</forenames></author><author><keyname>Coucke</keyname><forenames>Alice</forenames></author><author><keyname>Lavril</keyname><forenames>Thibaut</forenames></author><author><keyname>Gisselbrecht</keyname><forenames>Thibault</forenames></author><author><keyname>Dureau</keyname><forenames>Joseph</forenames></author></authors><title>Federated Learning for Keyword Spotting</title><categories>eess.AS cs.CL cs.LG cs.SD stat.ML</categories><comments>Accepted for publication to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a practical approach based on federated learning to solve
out-of-domain issues with continuously running embedded speech-based models
such as wake word detectors. We conduct an extensive empirical study of the
federated averaging algorithm for the &quot;Hey Snips&quot; wake word based on a
crowdsourced dataset that mimics a federation of wake word users. We
empirically demonstrate that using an adaptive averaging strategy inspired from
Adam in place of standard weighted model averaging highly reduces the number of
communication rounds required to reach our target performance. The associated
upstream communication costs per user are estimated at 8 MB, which is a
reasonable in the context of smart home voice assistants. Additionally, the
dataset used for these experiments is being open sourced with the aim of
fostering further transparent research in the application of federated learning
to speech data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05528</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05528</id><created>2018-10-10</created><authors><author><keyname>Gonz&#xe1;lez-Rodr&#xed;guez</keyname><forenames>Marta</forenames></author><author><keyname>Gelonch-Bosch</keyname><forenames>Antoni</forenames></author><author><keyname>Marojevic</keyname><forenames>Vuk</forenames></author></authors><title>A Software Radio Challenge Accelerating Education and Innovation in
  Wireless Communications</title><categories>eess.SP</categories><comments>Frontiers in Education 2018 (FIE 2018)</comments><msc-class>97U50, 97U40, 94A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Innovative Practice Full Paper presents our methodology and tools for
introducing competition in the electrical engineering curriculum to accelerate
education and innovation in wireless communications. Software radio or
software-defined radio (SDR) enables wireless technology, systems and standards
education where the student acts as the radio developer or engineer. This is
still a huge endeavor because of the complexity of current wireless systems and
the diverse student backgrounds. We suggest creating a competition among
student teams to potentiate creativity while leveraging the SDR development
methodology and open-source tools to facilitate cooperation. The proposed
student challenge follows the European UEFA Champions League format, which
includes a qualification phase followed by the elimination round or playoffs.
The students are tasked to build an SDR transmitter and receiver following the
guidelines of the long-term evolution standard. The metric is system
performance. After completing this course, the students will be able to (1)
analyze alternative radio design options and argue about their benefits and
drawbacks and (2) contribute to the evolution of wireless standards. We discuss
our experiences and lessons learned with particular focus on the suitability of
the proposed teaching and evaluation methodology and conclude that competition
in the electrical engineering classroom can spur innovation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05541</identifier>
 <datestamp>2018-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05541</id><created>2018-10-11</created><authors><author><keyname>Nguyen</keyname><forenames>Van Hoa</forenames></author><author><keyname>Besanger</keyname><forenames>Yvon</forenames></author><author><keyname>Tran</keyname><forenames>Quoc Tuan</forenames></author><author><keyname>Le</keyname><forenames>Minh Tri</forenames></author></authors><title>On the applicability of distributed ledger architectures to peer-to-peer
  energy trading framework</title><categories>cs.CY eess.SP</categories><comments>IEEE EEEIC 2018, Palermo, Italy, June 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As more and more distributed renewable energy resources are integrated to the
grid, the traditional consumers have become the prosumers who can sell back
their surplus energy to the others who are in energy shortage. This
peer-to-peer (P2P) energy transaction framework benefits the end users,
financially and in term of energy security; and the network operators, in term
of flexibility in DRES management, peak load shifting and regulation of
voltage/frequency. Environmentally, P2P energy transaction also helps to reduce
carbon footprint, reduces DRES payback period and incentivizes the installation
of DRES. The current centralized market model is no longer suitable and it is
therefore necessary to develop an adapted decentralized architecture for the
advanced P2P energy transaction framework intra/inter-microgrid. In this paper,
we discuss several distributed ledger approaches for such framework:
Blockchain, Block Lattice and Directed Acyclic Graph (the Tangle). The
technical advantages of these architectures as well as the persistent
challenges are then considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05670</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05670</id><created>2018-10-10</created><authors><author><keyname>Hoshi</keyname><forenames>Ikuo</forenames></author><author><keyname>Shimobaba</keyname><forenames>Tomoyoshi</forenames></author><author><keyname>Kakue</keyname><forenames>Takashi</forenames></author><author><keyname>Ito</keyname><forenames>Tomoyoshi</forenames></author></authors><title>Computational ghost imaging using a field-programmable gate array</title><categories>eess.IV cs.AR cs.CV eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational ghost imaging is a promising technique for single-pixel imaging
because it is robust to disturbance and can be operated over broad wavelength
bands, unlike common cameras. However, one disadvantage of this method is that
it has a long calculation time for image reconstruction. In this paper, we have
designed a dedicated calculation circuit that accelerated the process of
computational ghost imaging. We implemented this circuit by using a
field-programmable gate array, which reduced the calculation time for the
circuit compared to a CPU. The dedicated circuit reconstructs images at a frame
rate of 300 Hz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05677</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05677</id><created>2018-10-12</created><authors><author><keyname>Koutrouvelis</keyname><forenames>Andreas I.</forenames></author><author><keyname>Hendriks</keyname><forenames>Richard C.</forenames></author><author><keyname>Heusdens</keyname><forenames>Richard</forenames></author><author><keyname>Jensen</keyname><forenames>Jesper</forenames></author></authors><title>Robust Joint Estimation of Multi-Microphone Signal Model Parameters</title><categories>eess.AS cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest challenges in multi-microphone applications is the
estimation of the parameters of the signal model such as the power spectral
densities (PSDs) of the sources, the early (relative) acoustic transfer
functions of the sources with respect to the microphones, the PSD of late
reverberation, and the PSDs of microphone-self noise. Typically, the existing
methods estimate subsets of the aforementioned parameters and assume some of
the other parameters to be known a priori. This may result in inconsistencies
and inaccurately estimated parameters and potential performance degradation in
the applications using these estimated parameters. So far, there is no method
to jointly estimate all the aforementioned parameters. In this paper, we
propose a robust method for jointly estimating all the aforementioned
parameters using confirmatory factor analysis. The estimation accuracy of the
signal-model parameters thus obtained outperforms existing methods in most
cases. We experimentally show significant performance gains in several
multi-microphone applications over state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05835</identifier>
 <datestamp>2019-02-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05835</id><created>2018-10-13</created><authors><author><keyname>Atienza</keyname><forenames>N.</forenames></author><author><keyname>Escudero</keyname><forenames>L. M.</forenames></author><author><keyname>Jimenez</keyname><forenames>M. J.</forenames></author><author><keyname>Soriano-Trigueros</keyname><forenames>M.</forenames></author></authors><title>Characterising epithelial tissues using persistent entropy</title><categories>eess.IV cs.CV q-bio.QM</categories><comments>12 pages, 7 figures, 4 tables</comments><msc-class>68T10, 92B99, 65D18 94A17, 55N99, 5504</msc-class><doi>10.1007/978-3-030-10828-1_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply persistent entropy, a novel topological statistic,
for characterization of images of epithelial tissues. We have found out that
persistent entropy is able to summarize topological and geometric information
encoded by \alpha-complexes and persistent homology. After using some
statistical tests, we can guarantee the existence of significant differences in
the studied tissues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05837</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05837</id><created>2018-10-13</created><authors><author><keyname>Abdul-Rashid</keyname><forenames>Ramadan</forenames></author><author><keyname>Zerguine</keyname><forenames>Azzedine</forenames></author></authors><title>Time Synchronization in Wireless Sensor Networks based on Newtons
  Adaptive Algorithm</title><categories>eess.SP</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel time synchronization protocol inspired by the
adaptive Newton search algorithm. The clock model of nodes are modeled as an
adaptive filter and a pairwise steady state and convergence analyses are
presented. A protocol is derived from the derived algorithm and compared
experimentally to Gradient Descent Synchronization(GraDeS) protocol and the
Average Proportional Integral Synchronization (AvgPISync) protocol.
Experimental results showed similar error performance as AvgPISync but far
outperformed both protocols in terms of convergence time. The protocol is
lightweight, robust and simple to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05893</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05893</id><created>2018-10-13</created><updated>2019-02-19</updated><authors><author><keyname>Soltani</keyname><forenames>Mehran</forenames></author><author><keyname>Pourahmadi</keyname><forenames>Vahid</forenames></author><author><keyname>Mirzaei</keyname><forenames>Ali</forenames></author><author><keyname>Sheikhzadeh</keyname><forenames>Hamid</forenames></author></authors><title>Deep Learning-Based Channel Estimation</title><categories>cs.IT cs.LG eess.SP math.IT stat.ML</categories><comments>4 pages , 5 figures , Accepted for publication in the IEEE
  Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a deep learning (DL) algorithm for channel
estimation in communication systems. We consider the time-frequency response of
a fast fading communication channel as a two-dimensional image. The aim is to
find the unknown values of the channel response using some known values at the
pilot locations. To this end, a general pipeline using deep image processing
techniques, image super-resolution (SR) and image restoration (IR) is proposed.
This scheme considers the pilot values, altogether, as a low-resolution image
and uses an SR network cascaded with a denoising IR network to estimate the
channel. Moreover, an implementation of the proposed pipeline is presented. The
estimation error shows that the presented algorithm is comparable to the
minimum mean square error (MMSE) with full knowledge of the channel statistics
and it is better than ALMMSE (an approximation to linear MMSE). The results
confirm that this pipeline can be used efficiently in channel estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05919</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05919</id><created>2018-10-13</created><authors><author><keyname>Lu</keyname><forenames>Si</forenames></author></authors><title>No-reference Image Denoising Quality Assessment</title><categories>eess.IV cs.CV</categories><comments>17 pages, 41 figures, accepted by Computer Vision Conference (CVC)
  2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide variety of image denoising methods are available now. However, the
performance of a denoising algorithm often depends on individual input noisy
images as well as its parameter setting. In this paper, we present a
no-reference image denoising quality assessment method that can be used to
select for an input noisy image the right denoising algorithm with the optimal
parameter setting. This is a challenging task as no ground truth is available.
This paper presents a data-driven approach to learn to predict image denoising
quality. Our method is based on the observation that while individual existing
quality metrics and denoising models alone cannot robustly rank denoising
results, they often complement each other. We accordingly design denoising
quality features based on these existing metrics and models and then use Random
Forests Regression to aggregate them into a more powerful unified metric. Our
experiments on images with various types and levels of noise show that our
no-reference denoising quality assessment method significantly outperforms the
state-of-the-art quality metrics. This paper also provides a method that
leverages our quality assessment method to automatically tune the parameter
settings of a denoising algorithm for an input noisy image to produce an
optimal denoising result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05922</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05922</id><created>2018-10-13</created><authors><author><keyname>Fekri-Ershad</keyname><forenames>Shervan</forenames></author></authors><title>Porosity Amount Estimation in Stones Based on Combination of One
  Dimensional Local Binary Patterns and Image Normalization Technique</title><categories>eess.IV cs.CV</categories><comments>16 pages, in Farsi. 9 Figures, Computing Science Journal, Vol. 9,
  2018</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Since now, many approaches has been proposed for surface defect detection
based on image texture analysis techniques. One of the efficient texture
analysis operations is local binary patterns which provides good accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05949</identifier>
 <datestamp>2019-02-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05949</id><created>2018-10-13</created><updated>2019-02-19</updated><authors><author><keyname>Shu</keyname><forenames>Feng</forenames></author><author><keyname>Xu</keyname><forenames>Tingzhen</forenames></author><author><keyname>Hu</keyname><forenames>Jinsong</forenames></author><author><keyname>Yan</keyname><forenames>Shihao</forenames></author></authors><title>Delay-Constrained Covert Communications with A Full-Duplex Receiver</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider delay-constrained covert communications with the
aid of a full-duplex (FD) receiver. Without delay constraints, it has been
shown that the transmit power of artificial noise (AN) at the FD receiver
should be random in order to enhance covert communications. In this work, we
show that transmitting AN with a fixed power indeed improve covert
communications with delay constraints, since in a limited time period the
warden cannot exactly learn its received power. This explicitly shows one
benefit of considering practical delay constraints in the context of covert
communications. We analyze the optimal transmit power of AN for either fixed or
globally optimized transmit power of covert information, based on which we also
determine the specific condition under which transmitting AN by the FD receiver
can aid covert communications and a larger transmit power of AN always leads to
better covert communication performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05964</identifier>
 <datestamp>2018-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05964</id><created>2018-10-14</created><updated>2018-11-13</updated><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>Perceptual Image Quality Assessment through Spectral Analysis of Error
  Representations</title><categories>cs.CV eess.IV</categories><comments>23 pages, 6 figures, 4 tables</comments><acm-class>I.4</acm-class><journal-ref>Signal Processing: Image Communication, Volume 70, 2019, Pages
  37-46,ISSN 0923-5965</journal-ref><doi>10.1016/j.image.2018.09.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the statistics of error signals to assess the
perceived quality of images. Specifically, we focus on the magnitude spectrum
of error images obtained from the difference of reference and distorted images.
Analyzing spectral statistics over grayscale images partially models
interference in spatial harmonic distortion exhibited by the visual system but
it overlooks color information, selective and hierarchical nature of visual
system. To overcome these shortcomings, we introduce an image quality
assessment algorithm based on the Spectral Understanding of Multi-scale and
Multi-channel Error Representations, denoted as SUMMER. We validate the quality
assessment performance over 3 databases with around 30 distortion types. These
distortion types are grouped into 7 main categories as compression artifact,
image noise, color artifact, communication error, blur, global and local
distortions. In total, we benchmark the performance of 17 algorithms along with
the proposed algorithm using 5 performance metrics that measure linearity,
monotonicity, accuracy, and consistency. In addition to experiments with
standard performance metrics, we analyze the distribution of objective and
subjective scores with histogram difference metrics and scatter plots.
Moreover, we analyze the classification performance of quality assessment
algorithms along with their statistical significance tests. Based on our
experiments, SUMMER significantly outperforms majority of the compared methods
in all benchmark categories
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.05989</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.05989</id><created>2018-10-14</created><authors><author><keyname>Gozes</keyname><forenames>Ophir</forenames></author><author><keyname>Greenspan</keyname><forenames>Hayit</forenames></author></authors><title>Lung Structures Enhancement in Chest Radiographs via CT based FCNN
  Training</title><categories>cs.CV cs.AI eess.IV</categories><doi>10.1007/978-3-030-00946-5_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The abundance of overlapping anatomical structures appearing in chest
radiographs can reduce the performance of lung pathology detection by automated
algorithms (CAD) as well as the human reader. In this paper, we present a deep
learning based image processing technique for enhancing the contrast of soft
lung structures in chest radiographs using Fully Convolutional Neural Networks
(FCNN). Two 2D FCNN architectures were trained to accomplish the task: The
first performs 2D lung segmentation which is used for normalization of the lung
area. The second FCNN is trained to extract lung structures. To create the
training images, we employed Simulated X-Ray or Digitally Reconstructed
Radiographs (DRR) derived from 516 scans belonging to the LIDC-IDRI dataset. By
first segmenting the lungs in the CT domain, we are able to create a dataset of
2D lung masks to be used for training the segmentation FCNN. For training the
extraction FCNN, we create DRR images of only voxels belonging to the 3D lung
segmentation which we call &quot;Lung X-ray&quot; and use them as target images. Once the
lung structures are extracted, the original image can be enhanced by fusing the
original input x-ray and the synthesized &quot;Lung X-ray&quot;. We show that our
enhancement technique is applicable to real x-ray data, and display our results
on the recently released NIH Chest X-Ray-14 dataset. We see promising results
when training a DenseNet-121 based architecture to work directly on the lung
enhanced X-ray images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06055</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06055</id><created>2018-10-14</created><authors><author><keyname>Zhao</keyname><forenames>Ruzhang</forenames></author><author><keyname>Fang</keyname><forenames>Yajun</forenames></author><author><keyname>Horn</keyname><forenames>Berthold K. P.</forenames></author></authors><title>A Simple Change Comparison Method for Image Sequences Based on
  Uncertainty Coefficient</title><categories>eess.IV cs.CV</categories><comments>5 pages, 5 figures, 2 tables, accepted as a conference paper at IEEE
  UV 2018, Boston, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For identification of change information in image sequences, most studies
focus on change detection in one image sequence, while few studies have
considered the change level comparison between two different image sequences.
Moreover, most studies require the detection of image information in details,
for example, object detection. Based on Uncertainty Coefficient(UC), this paper
proposes an innovative method CCUC for change comparison between two image
sequences. The proposed method is computationally efficient and simple to
implement. The change comparison stems from video monitoring system. The
limited number of provided screens and a large number of monitoring cameras
require the videos or image sequences ordered by change level. We demonstrate
this new method by applying it on two publicly available image sequences. The
results are able to show the method can distinguish the different change level
for sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06132</identifier>
 <datestamp>2019-04-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06132</id><created>2018-10-14</created><updated>2019-04-19</updated><authors><author><keyname>Pla</keyname><forenames>Pol del Aguila</forenames></author><author><keyname>Saxena</keyname><forenames>Vidit</forenames></author><author><keyname>Jald&#xe9;n</keyname><forenames>Joakim</forenames></author></authors><title>SpotNet - Learned iterations for cell detection in image-based
  immunoassays</title><categories>eess.IV q-bio.QM</categories><comments>5 pages, 4 figures, 2019 IEEE 16th International Symposium on
  Biomedical Imaging (ISBI 2019), Venice, Italy, April 8-11, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate cell detection and counting in the image-based ELISpot and
FluoroSpot immunoassays is a challenging task. Recently proposed methodology
matches human accuracy by leveraging knowledge of the underlying physical
process of these assays and using proximal optimization methods to solve an
inverse problem. Nonetheless, thousands of computationally expensive iterations
are often needed to reach a near-optimal solution. In this paper, we exploit
the structure of the iterations to design a parameterized computation graph,
SpotNet, that learns the patterns embedded within several training images and
their respective cell information. Further, we compare SpotNet to a
convolutional neural network layout customized for cell detection. We show
empirical evidence that, while both designs obtain a detection performance on
synthetic data far beyond that of a human expert, SpotNet is easier to train
and obtains better estimates of particle secretion for each cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06196</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06196</id><created>2018-10-15</created><updated>2018-11-27</updated><authors><author><keyname>Islam</keyname><forenames>Mohammad Tariqul</forenames></author><author><keyname>Ahmed</keyname><forenames>Sk. Tanvir</forenames></author><author><keyname>Shahnaz</keyname><forenames>Celia</forenames></author><author><keyname>Fattah</keyname><forenames>Shaikh Anowarul</forenames></author></authors><title>SPECMAR: Fast Heart Rate Estimation from PPG Signal using a Modified
  Spectral Subtraction Scheme with Composite Motion Artifacts Reference
  Generation</title><categories>eess.SP</categories><comments>Accepted in Medical and Biological Engineering and Computing. This is
  a pre-copyedit version</comments><doi>10.1007/s11517-018-1909-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of heart rate estimation using photoplethysmographic (PPG) signal is
challenging due to the presence of various motion artifacts in the recorded
signals. In this paper, a fast algorithm for heart rate estimation based on
modified SPEctral subtraction scheme utilizing Composite Motion Artifacts
Reference generation (SPECMAR) is proposed using two-channel PPG and three-axis
accelerometer signals. First, the preliminary noise reduction is obtained by
filtering unwanted frequency components from the recorded signals. Next, a
composite motion artifacts reference generation method is developed to be
employed in the proposed SPECMAR algorithm for motion artifacts reduction. The
heart rate is then computed from the noise and motion artifacts reduced PPG
signal. Finally, a heart rate tracking algorithm is proposed considering
neighboring estimates. The performance of the SPECMAR algorithm has been tested
on publicly available PPG database. The average heart rate estimation error is
found to be 2.09 BPM on 23 recordings. The Pearson correlation is 0.9907. Due
to low computational complexity, the method is faster than the comparing
methods. The low estimation error, smooth and fast heart rate tracking makes
SPECMAR an ideal choice to be implemented in wearable devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06203</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06203</id><created>2018-10-15</created><authors><author><keyname>He</keyname><forenames>Di</forenames></author><author><keyname>Jiang</keyname><forenames>Ming</forenames></author><author><keyname>Louis</keyname><forenames>Alfred K.</forenames></author><author><keyname>Maass</keyname><forenames>Peter</forenames></author><author><keyname>Page</keyname><forenames>Thomas</forenames></author></authors><title>Joint bi-modal image reconstruction of DOT and XCT with an extended
  Mumford-Shah functional</title><categories>eess.IV physics.med-ph</categories><comments>29 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature similarity measures are indispensable for joint image reconstruction
in multi-modality medical imaging, which enable joint multi-modal image
reconstruction (JmmIR) by communication of feature information from one
modality to another, and vice versa. In this work, we establish an image
similarity measure in terms of image edges from Tversky's theory of feature
similarity in psychology. For joint bi-modal image reconstruction (JbmIR), it
is found that this image similarity measure is an extended Mumford-Shah
functional with a-priori edge information proposed previously from the
perspective of regularization approach. This image similarity measure consists
of Hausdorff measures of the common and different parts of image edges from
both modalities. By construction, it posits that two images are more similar if
they have more common edges and fewer unique/distinctive features, and will not
force the nonexistent structures to be reconstructed when applied to JbmIR.
With the Gamma-approximation of the JbmIR functional, an alternating
minimization method is proposed for the JbmIR of diffuse optical tomography and
x-ray computed tomography. The performance of the proposed method is evaluated
by three numerical phantoms. It is found that the proposed method improves the
reconstructed image quality by more than 10% compared to single modality image
reconstruction (SmIR) in terms of the structural similarity index measure
(SSIM)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06323</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06323</id><created>2018-10-15</created><authors><author><keyname>Degerli</keyname><forenames>Aysen</forenames></author><author><keyname>Aslan</keyname><forenames>Sinem</forenames></author><author><keyname>Yamac</keyname><forenames>Mehmet</forenames></author><author><keyname>Sankur</keyname><forenames>Bulent</forenames></author><author><keyname>Gabbouj</keyname><forenames>Moncef</forenames></author></authors><title>Compressively Sensed Image Recognition</title><categories>cs.LG cs.CV eess.IV stat.ML</categories><comments>6 pages, submitted/accepted, EUVIP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive Sensing (CS) theory asserts that sparse signal reconstruction is
possible from a small number of linear measurements. Although CS enables
low-cost linear sampling, it requires non-linear and costly reconstruction.
Recent literature works show that compressive image classification is possible
in CS domain without reconstruction of the signal. In this work, we introduce a
DCT base method that extracts binary discriminative features directly from CS
measurements. These CS measurements can be obtained by using (i) a random or a
pseudo-random measurement matrix, or (ii) a measurement matrix whose elements
are learned from the training data to optimize the given classification task.
We further introduce feature fusion by concatenating Bag of Words (BoW)
representation of our binary features with one of the two state-of-the-art
CNN-based feature vectors. We show that our fused feature outperforms the
state-of-the-art in both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06325</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06325</id><created>2018-10-15</created><updated>2019-01-30</updated><authors><author><keyname>Vesperini</keyname><forenames>Fabio</forenames></author><author><keyname>Gabrielli</keyname><forenames>Leonardo</forenames></author><author><keyname>Principi</keyname><forenames>Emanuele</forenames></author><author><keyname>Squartini</keyname><forenames>Stefano</forenames></author></authors><title>Polyphonic Sound Event Detection by using Capsule Neural Networks</title><categories>eess.AS cs.SD</categories><doi>10.1109/JSTSP.2019.2902305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial sound event detection (SED) has the aim to mimic the human ability
to perceive and understand what is happening in the surroundings. Nowadays,
Deep Learning offers valuable techniques for this goal such as Convolutional
Neural Networks (CNNs). The Capsule Neural Network (CapsNet) architecture has
been recently introduced in the image processing field with the intent to
overcome some of the known limitations of CNNs, specifically regarding the
scarce robustness to affine transformations (i.e., perspective, size,
orientation) and the detection of overlapped images. This motivated the authors
to employ CapsNets to deal with the polyphonic-SED task, in which multiple
sound events occur simultaneously. Specifically, we propose to exploit the
capsule units to represent a set of distinctive properties for each individual
sound event. Capsule units are connected through a so-called &quot;dynamic routing&quot;
that encourages learning part-whole relationships and improves the detection
performance in a polyphonic context. This paper reports extensive evaluations
carried out on three publicly available datasets, showing how the CapsNet-based
algorithm not only outperforms standard CNNs but also allows to achieve the
best results with respect to the state of the art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06380</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06380</id><created>2018-10-12</created><authors><author><keyname>Krikheli</keyname><forenames>Michael</forenames></author><author><keyname>Leshem</keyname><forenames>Amir</forenames></author></authors><title>Finite sample performance of linear least squares estimation</title><categories>math.ST eess.SP stat.TH</categories><comments>arXiv admin note: text overlap with arXiv:1710.11594</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Least Squares is a very well known technique for parameter estimation,
which is used even when sub-optimal, because of its very low computational
requirements and the fact that exact knowledge of the noise statistics is not
required. Surprisingly, bounding the probability of large errors with finitely
many samples has been left open, especially when dealing with correlated noise
with unknown covariance. In this paper we analyze the finite sample performance
of the linear least squares estimator. Using these bounds we obtain accurate
bounds on the tail of the estimator's distribution. We show the fast
exponential convergence of the number of samples required to ensure a given
accuracy with high probability. We analyze a sub-Gaussian setting with a fixed
or random design matrix of the linear least squares problem. We also extend the
results to the case of a martingale difference noise sequence. Our analysis
method is simple and uses simple $L_{\infty}$ type bounds on the estimation
error. We also provide probabilistic finite sample bounds on the estimation
error $L_2$ norm. The tightness of the bounds is tested through simulation. We
demonstrate that our results are tighter than previously proposed bounds for
$L_{\infty}$ norm of the error. The proposed bounds make it possible to predict
the number of samples required for least squares estimation even when the least
squares is sub-optimal and is used for computational simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06422</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06422</id><created>2018-09-28</created><authors><author><keyname>Wijayono</keyname><forenames>Andrian</forenames></author><author><keyname>Putra</keyname><forenames>Valentinus Galih Vidia</forenames></author></authors><title>Implementation Of Digital Image Processing And Computation Technology On
  Measurement And Testing Of Various Knit Fabric Parameters</title><categories>eess.IV</categories><comments>40 Pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  One of the big challenges of the industry today is how to produce quality
products, one of which is in the knit fabric industry. The improvement of the
evaluation and quality control processes of non woven production has been
widely developed to support the improvement of the quality of production. The
use of information and computational technology is now widely applied to the
quality control process of textile material production, one of which is the use
of image processing technology in the evaluation process of knit fabric. This
chapter will explain various methods of applying image processing technology in
the field of evaluation and quality control of textile production.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06423</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06423</id><created>2018-09-28</created><authors><author><keyname>Wijayono</keyname><forenames>Andrian</forenames></author><author><keyname>Putra</keyname><forenames>Valentinus Galih Vidia</forenames></author></authors><title>Implementation Of Image Analysis Techniques For Various Textile
  Identification</title><categories>eess.IV</categories><comments>19 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Computer image analysis techniques used to identify textile products are
presented in this chapter, together with a brief review of the historical
development of the method. Automatic and semi-automatic image correction
methods are described, which are often used for identification of textile yarn
products, and can also be used to identify various textile indentification.
  Keywords: digital image acquirement, image modeling, image quality
improvement, image correction methods, median filtration, Laplace filter,
threshold function, autocorrelation, Fourier transform, erosion, dilatation,
digitalisation algorithm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06450</identifier>
 <datestamp>2019-05-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06450</id><created>2018-10-12</created><updated>2019-05-26</updated><authors><author><keyname>Singh</keyname><forenames>Shashank</forenames></author><author><keyname>Roy</keyname><forenames>Amit</forenames></author><author><keyname>P</keyname><forenames>Selvan M.</forenames></author></authors><title>Smart Load Node for Nonsmart Load Under Smart Grid Paradigm: A New Home
  Energy Management System</title><categories>eess.SP</categories><comments>This paper is accepted for inclusion in future issue of IEEE Consumer
  Electronic Magazine. Copyright IEEE 2018. Content presented in this article
  is subjected to Indian Patent Application, Application Number:
  201741039083,Dated: 02/Nov/2017</comments><journal-ref>IEEE Consumer Electronics Magazine, vol. 8, no. 2, pp. 22-27,
  March 2019</journal-ref><doi>10.1109/MCE.2018.2880804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a novel approach for efficient operation of non-smart
household appliances under smart grid environment using the proposed smart load
node (SLN). In real world scenario, there are so many non-smart loads currently
in use and embedding appliance specific intelligence into them to make them as
smart loads will be more expensive compared to the proposed SLN, which is a
common solution for all types of non-smart loads. This makes the proposed
low-cost SLN, which neither requires any infrastructural change in the
electrical wiring of a house nor any constructional change in home appliances
at the manufacturing stage and at the consumer end, as a feasible solution for
intelligent operation of non-smart home appliances under smart grid
environment. The SLNs, which are placed in a home like distributed wireless
sensor nodes, form a home area network (HAN). The HAN includes a load
management unit (LMU) which acts as master for all distributed SLNs. Wi-Fi is
chosen as a medium of communication in the HAN. The LMU incorporates load
management algorithm which is written in Python script.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06451</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06451</id><created>2018-10-12</created><authors><author><keyname>Singh</keyname><forenames>Shashank</forenames></author><author><keyname>L</keyname><forenames>Arun S</forenames></author><author><keyname>P</keyname><forenames>Selvan M</forenames></author></authors><title>Regression Based Approach for Measurement of Current in Single-Phase
  Smart Energy Meter</title><categories>eess.SP</categories><comments>Presented at 2017 IEEE Region 10 Symposium (TENSYMP), Cochin, Kerala,
  India</comments><journal-ref>In Proc. 2017 IEEE Region 10 Symposium (TENSYMP), Cochin, 2017,
  pp. 1-5</journal-ref><doi>10.1109/TENCONSpring.2017.8070103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present scenario rate of utilization of electrical energy is
continuously increasing, to preserve the non-renewable energy resources,
efficient consumption of electrical energy is one of the most important needs
of this era. Smart Energy Meter (SEM) plays a vital role in efficient use of
electrical energy in consumer premises. Accuracy in the measurement of current
is one of the key factors to be considered while measuring energy consumption.
This paper introduces Arduino based low cost single-phase SEM and proposes a
linear regression based technique for accurate measurement of current. The
proposed current measurement approach gives accurate measurement for linear as
well as highly non-linear loads. The SEM built in the laboratory has data
transfer capability and also embedded with Demand Response (DR) feature. Block
Rate Tariff structure used by Tamil Nadu Generation and Distribution Company is
considered to incorporate DR that warns the consumers whenever their energy
consumption reaches the boundary of a block.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06464</identifier>
 <datestamp>2018-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06464</id><created>2018-10-15</created><updated>2018-11-13</updated><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>CSV: Image Quality Assessment Based on Color, Structure, and Visual
  System</title><categories>cs.CV eess.IV</categories><comments>31 pages, 9 figures, 7 tables</comments><acm-class>I.4</acm-class><journal-ref>Signal Processing: Image Communication, Volume 48, 2016, Pages
  92-103, ISSN 0923-5965</journal-ref><doi>10.1016/j.image.2016.08.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a full-reference image quality estimator based on color,
structure, and visual system characteristics denoted as CSV. In contrast to the
majority of existing methods, we quantify perceptual color degradations rather
than absolute pixel-wise changes. We use the CIEDE2000 color difference
formulation to quantify low-level color degradations and the Earth Mover's
Distance between color name descriptors to measure significant color
degradations. In addition to the perceptual color difference, CSV also contains
structural and perceptual differences. Structural feature maps are obtained by
mean subtraction and divisive normalization, and perceptual feature maps are
obtained from contrast sensitivity formulations of retinal ganglion cells. The
proposed quality estimator CSV is tested on the LIVE, the Multiply Distorted
LIVE, and the TID 2013 databases, and it is always among the top two performing
quality estimators in terms of at least ranking, monotonic behavior or
linearity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06466</identifier>
 <datestamp>2018-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06466</id><created>2018-10-15</created><authors><author><keyname>Deane</keyname><forenames>J. H. B.</forenames></author><author><keyname>Dharmasena</keyname><forenames>R. D. I. G.</forenames></author><author><keyname>Gentile</keyname><forenames>G.</forenames></author></authors><title>Power computation for the triboelectric nanogenerator</title><categories>physics.app-ph eess.SP math.CA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider, from a mathematical perspective, the power generated by a
contact-mode triboelectric nanogenerator, an energy harvesting device that has
been well studied recently. We encapsulate the behaviour of the device in a
differential equation, which although linear and of first order, has periodic
coefficients, leading to some interesting mathematical problems. In studying
these, we derive approximate forms for the mean power generated and the current
waveforms, and describe a procedure for computing the Fourier coefficients for
the current, enabling us to show how the power is distributed over the
harmonics. Comparisons with accurate numerics validate our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06510</identifier>
 <datestamp>2019-07-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06510</id><created>2018-10-15</created><authors><author><keyname>Zhong</keyname><forenames>Zijia</forenames></author><author><keyname>Lee</keyname><forenames>Joyoung</forenames></author></authors><title>Simulation Framework for Cooperative Adaptive Cruise Control with
  Empirical DSRC Module</title><categories>eess.SP cs.SY</categories><comments>6 pages, 6 figure, 44th Annual Conference of the IEEE Industrial
  Electronics Society</comments><doi>10.1109/IECON.2018.8592909</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication plays a vital role in the promising performance of
connected and automated vehicle (CAV) technology. This paper proposes a
Vissim-based microscopic traffic simulation framework with an analytical
dedicated short-range communication (DSRC) module for packet reception. Being
derived from ns-2, a packet-level network simulator, the DSRC probability
module takes into account the imperfect wireless communication that occurs in
real-world deployment. Four managed lane deployment strategies are evaluated
using the proposed framework. While the average packet reception rate is above
93\% among all tested scenarios, the results reveal that the reliability of the
vehicle-to-vehicle (V2V) communication can be influenced by the deployment
strategies. Additionally, the proposed framework exhibits desirable scalability
for traffic simulation and it is able to evaluate transportation-network-level
deployment strategies in the near future for CAV technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06545</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06545</id><created>2018-09-24</created><updated>2018-11-21</updated><authors><author><keyname>Poggiolini</keyname><forenames>Pierluigi</forenames></author></authors><title>A generalized GN-model closed-form formula</title><categories>eess.SP physics.app-ph physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GN-model of fiber non-linearity has had quite substantial success in
modern optical telecommunications networks as a design and management tool. A
version of it, capable of handling arbitrary WDM combs and link structures in
closed form, was proposed in 2014. Here we upgrade that formula, to make it
capable of handling frequency-dependent dispersion, frequency-dependent loss
and frequency-dependent gain/loss due to Stimulated Raman Scattering (SRS)
among channels. This way, more challenging and complex network scenarios, like
the ones that are being deployed right now, can be dealt with in real-time, to
the great advantage of management, control and optimization of such networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06603</identifier>
 <datestamp>2019-05-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06603</id><created>2018-10-15</created><updated>2019-03-06</updated><authors><author><keyname>Ramirez</keyname><forenames>Marco A. Mart&#xed;nez</forenames></author><author><keyname>Reiss</keyname><forenames>Joshua D.</forenames></author></authors><title>Modeling of nonlinear audio effects with end-to-end deep neural networks</title><categories>eess.AS cs.LG cs.SD eess.SP</categories><comments>Presented at the 2019 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP), Brighton, UK, May 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of music production, distortion effects are mainly used for
aesthetic reasons and are usually applied to electric musical instruments. Most
existing methods for nonlinear modeling are often either simplified or
optimized to a very specific circuit. In this work, we investigate deep
learning architectures for audio processing and we aim to find a general
purpose end-to-end deep neural network to perform modeling of nonlinear audio
effects. We show the network modeling various nonlinearities and we discuss the
generalization capabilities among different instruments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06631</identifier>
 <datestamp>2018-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06631</id><created>2018-10-15</created><updated>2018-11-13</updated><authors><author><keyname>Temel</keyname><forenames>D.</forenames></author><author><keyname>Prabhushankar</keyname><forenames>M.</forenames></author><author><keyname>AlRegib</keyname><forenames>G.</forenames></author></authors><title>UNIQUE: Unsupervised Image Quality Estimation</title><categories>cs.CV eess.IV</categories><comments>12 pages, 5 figures, 2 tables</comments><acm-class>I.2; I.4; I.5</acm-class><journal-ref>D. Temel, M. Prabhushankar and G. AlRegib, &quot;UNIQUE: Unsupervised
  Image Quality Estimation,&quot; in IEEE Signal Processing Letters, vol. 23, no.
  10, pp. 1414-1418, Oct. 2016</journal-ref><doi>10.1109/LSP.2016.2601119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we estimate perceived image quality using sparse
representations obtained from generic image databases through an unsupervised
learning approach. A color space transformation, a mean subtraction, and a
whitening operation are used to enhance descriptiveness of images by reducing
spatial redundancy; a linear decoder is used to obtain sparse representations;
and a thresholding stage is used to formulate suppression mechanisms in a
visual system. A linear decoder is trained with 7 GB worth of data, which
corresponds to 100,000 8x8 image patches randomly obtained from nearly 1,000
images in the ImageNet 2013 database. A patch-wise training approach is
preferred to maintain local information. The proposed quality estimator UNIQUE
is tested on the LIVE, the Multiply Distorted LIVE, and the TID 2013 databases
and compared with thirteen quality estimators. Experimental results show that
UNIQUE is generally a top performing quality estimator in terms of accuracy,
consistency, linearity, and monotonic behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06635</identifier>
 <datestamp>2018-10-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06635</id><created>2018-10-02</created><authors><author><keyname>Chellapriyadharshini</keyname><forenames>Maharajan</forenames></author><author><keyname>Toffy</keyname><forenames>Anoop</forenames></author><author><keyname>M.</keyname><forenames>Srinivasa Raghavan K.</forenames></author><author><keyname>Ramasubramanian</keyname><forenames>V</forenames></author></authors><title>Semi-supervised and Active-learning Scenarios: Efficient Acoustic Model
  Refinement for a Low Resource Indian Language</title><categories>cs.CL cs.CV cs.SD eess.AS</categories><journal-ref>Proc. Interspeech 2018</journal-ref><doi>10.21437/Interspeech.2018-2486</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of efficient acoustic-model refinement (continuous
retraining) using semi-supervised and active learning for a low resource Indian
language, wherein the low resource constraints are having i) a small labeled
corpus from which to train a baseline `seed' acoustic model and ii) a large
training corpus without orthographic labeling or from which to perform a data
selection for manual labeling at low costs. The proposed semi-supervised
learning decodes the unlabeled large training corpus using the seed model and
through various protocols, selects the decoded utterances with high reliability
using confidence levels (that correlate to the WER of the decoded utterances)
and iterative bootstrapping. The proposed active learning protocol uses
confidence level based metric to select the decoded utterances from the large
unlabeled corpus for further labeling. The semi-supervised learning protocols
can offer a WER reduction, from a poorly trained seed model, by as much as 50%
of the best WER-reduction realizable from the seed model's WER, if the large
corpus were labeled and used for acoustic-model training. The active learning
protocols allow that only 60% of the entire training corpus be manually
labeled, to reach the same performance as the entire data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06652</identifier>
 <datestamp>2019-09-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06652</id><created>2018-10-15</created><updated>2019-09-16</updated><authors><author><keyname>Gordon</keyname><forenames>Ethan K.</forenames></author></authors><title>Design and Control of a Photonic Neural Network Applied to
  High-Bandwidth Classification</title><categories>eess.SP</categories><comments>Princeton University, Electrical Engineering Undergraduate Thesis, 23
  figures, 32 pages + references / appendices. Advisor: Paul Prucnal v2:
  Updated author name</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks can very effectively perform multidimensional nonlinear
classification. However, electronic networks suffer from significant bandwidth
limitations due to carrier lifetimes and capacitive coupling. This project
investigates photonic neural networks that can get around these limitations by
performing both the activation function and weighted addition in the optical
domain using microring resonators. These optical microring resonators provide
both nonlinearity and superior fan-in without compromising bandwidth. The
ability to thermally calibrate networks of cascaded axons and dendrites and
train such a network to solve nonlinear classification problems are
demonstrated using theory and simulations. The former is also demonstrated
experimentally on a two-channel axon cascaded into a two-channel dendrite,
showing good agreement between simulation and experiment. In addition, the use
of transverse modes to increase the size of each photonic layer is examined.
Simulations that determined the optimal waveguide geometry for using these
modes were experimentally validated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06693</identifier>
 <datestamp>2018-10-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06693</id><created>2018-10-15</created><authors><author><keyname>Zhu</keyname><forenames>Jin</forenames></author><author><keyname>Yang</keyname><forenames>Guang</forenames></author><author><keyname>Lio</keyname><forenames>Pietro</forenames></author></authors><title>Lesion Focused Super-Resolution</title><categories>eess.IV cs.CV physics.med-ph</categories><comments>4 pages, 2 figures, 1 table, Accepted as Oral Presentation by the
  SPIE Medical Imaging Conference 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Super-resolution (SR) for image enhancement has great importance in medical
image applications. Broadly speaking, there are two types of SR, one requires
multiple low resolution (LR) images from different views of the same object to
be reconstructed to the high resolution (HR) output, and the other one relies
on the learning from a large amount of training datasets, i.e., LR-HR pairs. In
real clinical environment, acquiring images from multi-views is expensive and
sometimes infeasible. In this paper, we present a novel Generative Adversarial
Networks (GAN) based learning framework to achieve SR from its LR version. By
performing simulation based studies on the Multimodal Brain Tumor Segmentation
Challenge (BraTS) datasets, we demonstrate the efficacy of our method in
application of brain tumor MRI enhancement. Compared to bilinear interpolation
and other state-of-the-art SR methods, our model is lesion focused, which is
not only resulted in better perceptual image quality without blurring, but also
more efficient and directly benefit for the following clinical tasks, e.g.,
lesion detection and abnormality enhancement. Therefore, we can envisage the
application of our SR method to boost image spatial resolution while
maintaining crucial diagnostic information for further clinical tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06730</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06730</id><created>2018-10-15</created><updated>2018-11-07</updated><authors><author><keyname>Tung</keyname><forenames>Tze-Yang</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>SPRT Based Transceiver for Molecular Communications</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving precise synchronisation between transmitters and receivers is
particularly challenging in diffusive molecular communication environments. To
this end, point-to-point molecular communication system design is examined
wherein synchronisation errors are explicitly considered. Two transceiver
design questions are considered: the development of a sequential probability
ratio test-based detector which allows for additional observations in the
presence of uncertainty due to mis-synchronisation at the receiver, and a
modulation design which is optimised for this receiver strategy. The modulation
is based on optimising an approximation for the probability of error for the
detection strategy and directly exploits the structure of the probability of
molecules hitting a receiver within a particular time slot. The proposed
receiver and modulation designs achieve strongly improved asynchronous
detection performance for the same data rate as a decision feedback based
receiver by a factor of 1/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06766</identifier>
 <datestamp>2019-04-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06766</id><created>2018-10-15</created><authors><author><keyname>Ren</keyname><forenames>Haoyu</forenames></author><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author></authors><title>DN-ResNet: Efficient Deep Residual Network for Image Denoising</title><categories>eess.IV cs.CV</categories><journal-ref>Asian Conference of Computer Vision 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deep learning approach to blind denoising of images without complete
knowledge of the noise statistics is considered. We propose DN-ResNet, which is
a deep convolutional neural network (CNN) consisting of several residual blocks
(ResBlocks). With cascade training, DN-ResNet is more accurate and more
computationally efficient than the state of art denoising networks. An
edge-aware loss function is further utilized in training DN-ResNet, so that the
denoising results have better perceptive quality compared to conventional loss
function. Next, we introduce the depthwise separable DN-ResNet (DS-DN-ResNet)
utilizing the proposed Depthwise Seperable ResBlock (DS-ResBlock) instead of
standard ResBlock, which has much less computational cost. DS-DN-ResNet is
incrementally evolved by replacing the ResBlocks in DN-ResNet by DS-ResBlocks
stage by stage. As a result, high accuracy and good computational efficiency
are achieved concurrently. Whereas previous state of art deep learning methods
focused on denoising either Gaussian or Poisson corrupted images, we consider
denoising images having the more practical Poisson with additive Gaussian noise
as well. The results show that DN-ResNets are more efficient, robust, and
perform better denoising than current state of art deep learning methods, as
well as the popular variants of the BM3D algorithm, in cases of blind and
non-blind denoising of images corrupted with Poisson, Gaussian or
Poisson-Gaussian noise. Our network also works well for other image enhancement
task such as compressed image restoration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06811</identifier>
 <datestamp>2019-07-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06811</id><created>2018-10-16</created><updated>2019-07-09</updated><authors><author><keyname>Amhoud</keyname><forenames>El-Mehdi</forenames></author><author><keyname>Trichili</keyname><forenames>Abderrahmen</forenames></author><author><keyname>Ooi</keyname><forenames>Boon S.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>OAM Mode Selection and Space-Time Coding for Turbulence Mitigation in
  FSO Communication</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Free space optical (FSO) communications using orbital angular momentum (OAM)
modes has recently received a considerable interest. Propagating OAM modes
through free space may be subject to atmospheric turbulence (AT) distortions
that cause intermodal crosstalk and power disparities between OAM modes. In
this article, we are interested in 2x2 multiple-input multiple-output (MIMO)
FSO coherent communication systems using OAM. We propose a selection criterion
for OAM modes to minimize the impact of AT. To further improve the obtained
performance, we propose a space-time (ST) coding scheme at the transmitter.
Through numerical simulations of the error probability, we show that the
penalty from AT is completely absorbed for weak AT and considerable coding
gains are obtained in the strong AT regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06865</identifier>
 <datestamp>2020-01-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06865</id><created>2018-10-16</created><updated>2020-01-12</updated><authors><author><keyname>Zhang</keyname><forenames>Jing-Xuan</forenames></author><author><keyname>Ling</keyname><forenames>Zhen-Hua</forenames></author><author><keyname>Liu</keyname><forenames>Li-Juan</forenames></author><author><keyname>Jiang</keyname><forenames>Yuan</forenames></author><author><keyname>Dai</keyname><forenames>Li-Rong</forenames></author></authors><title>Sequence-to-Sequence Acoustic Modeling for Voice Conversion</title><categories>cs.SD eess.AS</categories><comments>Published on IEEE/ACM Transactions on Audio, Speech and Language
  Processing</comments><journal-ref>IEEE/ACM Transactions on Audio, Speech and Language Processing vol
  27 no 3 (2019) 631-644</journal-ref><doi>10.1109/TASLP.2019.2892235</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a neural network named Sequence-to-sequence ConvErsion NeTwork
(SCENT) is presented for acoustic modeling in voice conversion. At training
stage, a SCENT model is estimated by aligning the feature sequences of source
and target speakers implicitly using attention mechanism. At conversion stage,
acoustic features and durations of source utterances are converted
simultaneously using the unified acoustic model. Mel-scale spectrograms are
adopted as acoustic features which contain both excitation and vocal tract
descriptions of speech signals. The bottleneck features extracted from source
speech using an automatic speech recognition (ASR) model are appended as
auxiliary input. A WaveNet vocoder conditioned on Mel-spectrograms is built to
reconstruct waveforms from the outputs of the SCENT model. It is worth noting
that our proposed method can achieve appropriate duration conversion which is
difficult in conventional methods. Experimental results show that our proposed
method obtained better objective and subjective performance than the baseline
methods using Gaussian mixture models (GMM) and deep neural networks (DNN) as
acoustic models. This proposed method also outperformed our previous work which
achieved the top rank in Voice Conversion Challenge 2018. Ablation tests
further confirmed the effectiveness of several components in our proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06876</identifier>
 <datestamp>2019-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06876</id><created>2018-10-16</created><updated>2019-01-05</updated><authors><author><keyname>Laury</keyname><forenames>John</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Lars</forenames></author><author><keyname>Bollen</keyname><forenames>Math H. J</forenames></author></authors><title>A rotary frequency converter model for electromechanical transient
  studies of 16$\frac{2}{3}$ Hz railway systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Railway power systems operating at a nominal frequency below the frequency of
the public grid (50 or 60 Hz) are special in many senses. One is that they
exist in a just few countries around the world. However, for these countries
such low frequency railways are a critical part of their infrastructure.
  The number of published dynamic models as well as stability studies regarding
low frequency railways is small, compared to corresponding publications
regarding 50 Hz/60 Hz public grids. Since there are two main type of low
frequency railways; synchronous and asynchronous, it makes the number of
available useful publications even smaller. One important reason for this is
the small share of such grids on a global scale, resulting in less research and
development man hours spent on low frequency grids.
  This work presents an open model of a (synchronous-synchronous) rotary
frequency converter for electromechanical stability studies in the phasor
domain, based on established synchronous machine models. The proposed model is
designed such that it can be used with the available data for a rotary
frequency converter.
  The behaviour of the model is shown through numerical electromechanical
transient stability simulations of two example cases, where a fault is cleared,
and the subsequent oscillations are shown. The first example is a single-fed
catenary section and the second is doubly-fed catenary section.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06897</identifier>
 <datestamp>2018-10-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06897</id><created>2018-10-16</created><authors><author><keyname>Harb</keyname><forenames>Robert</forenames></author><author><keyname>Pernkopf</keyname><forenames>Franz</forenames></author></authors><title>Sound event detection using weakly-labeled semi-supervised data with
  GCRNNS, VAT and Self-Adaptive Label Refinement</title><categories>cs.SD eess.AS</categories><comments>Accepted at DCASE 2018 Workshop for oral presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a gated convolutional recurrent neural network
based approach to solve task 4, large-scale weakly labelled semi-supervised
sound event detection in domestic environments, of the DCASE 2018 challenge.
Gated linear units and a temporal attention layer are used to predict the onset
and offset of sound events in 10s long audio clips. Whereby for training only
weakly-labelled data is used. Virtual adversarial training is used for
regularization, utilizing both labelled and unlabeled data. Furthermore, we
introduce self-adaptive label refinement, a method which allows unsupervised
adaption of our trained system to refine the accuracy of frame-level class
predictions. The proposed system reaches an overall macro averaged event-based
F-score of 34.6%, resulting in a relative improvement of 20.5% over the
baseline system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.06923</identifier>
 <datestamp>2018-10-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.06923</id><created>2018-10-16</created><authors><author><keyname>Huo</keyname><forenames>Yiming</forenames></author><author><keyname>Dong</keyname><forenames>Xiaodai</forenames></author></authors><title>Millimeter-Wave for Unmanned Aerial Vehicles Networks: Enabling
  Multi-Beam Multi-Stream Communications</title><categories>eess.SP</categories><comments>8 pages, 10 figures. P.S. MmWave for UAV networks and multi-stream
  multi-beam communications, it works ! (from field tests )</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the fifth-generation (5G) mobile networks being actively standardized
and deployed, many new vehicular communications technologies are developed to
support and enrich various application scenarios. Unmanned aerial vehicle (UAV)
enabled communications emerges as one of many promising solutions of
constructing the next-generation highly reconfigurable and mobile networks. In
this article, we first investigate and envision the challenges of future UAV
applications from the net-work, system, and hardware design perspectives, and
then pre-sent a UAV aerial base station (ABS) prototype which works at
millimeter-wave (mmWave) bands and enable multi-beam mul-ti-stream
communications. In terms of the field trial tests of the first UAV-ABS of its
kind in the world, multi-giga-bit-per-second data rate of uplink and downlink
is verified with good stability and reliability against mildly challenging
weather conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07181</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07181</id><created>2018-10-16</created><updated>2018-11-07</updated><authors><author><keyname>Zhao</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Vuran</keyname><forenames>Mehmet C.</forenames></author><author><keyname>Guo</keyname><forenames>Fujuan</forenames></author><author><keyname>Scott</keyname><forenames>Stephen</forenames></author></authors><title>Deep-Waveform: A Learned OFDM Receiver Based on Deep Complex
  Convolutional Networks</title><categories>eess.SP cs.IT cs.NI math.IT</categories><comments>12 pages, 20 figures, manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent explorations of Deep Learning in the physical layer (PHY) of wireless
communication have shown the capabilities of Deep Neuron Networks in tasks like
channel coding, modulation, and parametric estimation. However, it is unclear
if Deep Neuron Networks could also learn the advanced waveforms of current and
next-generation wireless networks, and potentially create new ones. In this
paper, a Deep Complex Convolutional Network (DCCN) without explicit Discrete
Fourier Transform (DFT) is developed as an Orthogonal Frequency-Division
Multiplexing (OFDM) receiver. Compared to existing deep neuron network
receivers composed of fully-connected layers followed by non-linear
activations, the developed DCCN not only contains convolutional layers but is
also almost (and could be fully) linear. Moreover, the developed DCCN not only
learns to convert OFDM waveform with Quadrature Amplitude Modulation (QAM) into
bits under noisy and Rayleigh channels, but also outperforms expert OFDM
receiver based on Linear Minimum Mean Square Error channel estimator with prior
channel knowledge in the low to middle Signal-to-Noise Ratios of Rayleigh
channels. It shows that linear Deep Neuron Networks could learn transformations
in signal processing, thus master advanced waveforms and wireless channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07217</identifier>
 <datestamp>2018-12-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07217</id><created>2018-10-16</created><updated>2018-12-27</updated><authors><author><keyname>Hsu</keyname><forenames>Wei-Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Weiss</keyname><forenames>Ron J.</forenames></author><author><keyname>Zen</keyname><forenames>Heiga</forenames></author><author><keyname>Wu</keyname><forenames>Yonghui</forenames></author><author><keyname>Wang</keyname><forenames>Yuxuan</forenames></author><author><keyname>Cao</keyname><forenames>Yuan</forenames></author><author><keyname>Jia</keyname><forenames>Ye</forenames></author><author><keyname>Chen</keyname><forenames>Zhifeng</forenames></author><author><keyname>Shen</keyname><forenames>Jonathan</forenames></author><author><keyname>Nguyen</keyname><forenames>Patrick</forenames></author><author><keyname>Pang</keyname><forenames>Ruoming</forenames></author></authors><title>Hierarchical Generative Modeling for Controllable Speech Synthesis</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>27 pages, accepted to ICLR 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a neural sequence-to-sequence text-to-speech (TTS) model
which can control latent attributes in the generated speech that are rarely
annotated in the training data, such as speaking style, accent, background
noise, and recording conditions. The model is formulated as a conditional
generative model based on the variational autoencoder (VAE) framework, with two
levels of hierarchical latent variables. The first level is a categorical
variable, which represents attribute groups (e.g. clean/noisy) and provides
interpretability. The second level, conditioned on the first, is a multivariate
Gaussian variable, which characterizes specific attribute configurations (e.g.
noise level, speaking rate) and enables disentangled fine-grained control over
these attributes. This amounts to using a Gaussian mixture model (GMM) for the
latent distribution. Extensive evaluation demonstrates its ability to control
the aforementioned attributes. In particular, we train a high-quality
controllable TTS model on real found data, which is capable of inferring
speaker and style attributes from a noisy utterance and use it to synthesize
clean speech with controllable speaking style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07309</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07309</id><created>2018-10-16</created><authors><author><keyname>Guo</keyname><forenames>Jinxi</forenames></author><author><keyname>Xu</keyname><forenames>Ning</forenames></author><author><keyname>Qian</keyname><forenames>Kailun</forenames></author><author><keyname>Shi</keyname><forenames>Yang</forenames></author><author><keyname>Xu</keyname><forenames>Kaiyuan</forenames></author><author><keyname>Wu</keyname><forenames>Yingnian</forenames></author><author><keyname>Alwan</keyname><forenames>Abeer</forenames></author></authors><title>Deep neural network based i-vector mapping for speaker verification
  using short utterances</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>Submitted to Speech Communication; under final review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text-independent speaker recognition using short utterances is a highly
challenging task due to the large variation and content mismatch between short
utterances. I-vector based systems have become the standard in speaker
verification applications, but they are less effective with short utterances.
In this paper, we first compare two state-of-the-art universal background model
training methods for i-vector modeling using full-length and short utterance
evaluation tasks. The two methods are Gaussian mixture model (GMM) based and
deep neural network (DNN) based methods. The results indicate that the
I-vector_DNN system outperforms the I-vector_GMM system under various
durations. However, the performances of both systems degrade significantly as
the duration of the utterances decreases. To address this issue, we propose two
novel nonlinear mapping methods which train DNN models to map the i-vectors
extracted from short utterances to their corresponding long-utterance
i-vectors. The mapped i-vector can restore missing information and reduce the
variance of the original short-utterance i-vectors. The proposed methods both
model the joint representation of short and long utterance i-vectors by using
autoencoder. Experimental results using the NIST SRE 2010 dataset show that
both methods provide significant improvement and result in a max of 28.43%
relative improvement in Equal Error Rates from a baseline system, when using
deep encoder with residual blocks and adding an additional phoneme vector. When
further testing the best-validated models of SRE10 on the Speaker In The Wild
dataset, the methods result in a 23.12% improvement on arbitrary-duration (1-5
s) short-utterance conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07316</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07316</id><created>2018-10-16</created><authors><author><keyname>Acres</keyname><forenames>Kevin</forenames></author><author><keyname>Barca</keyname><forenames>Jan Carlo</forenames></author></authors><title>Tetrahedra and Relative Directions in Space, Using 2 and 3-Space
  Simplexes for 3-Space Localization</title><categories>eess.SP cs.RO math.AG</categories><comments>28 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research presents a novel method of determining relative bearing and
elevation measurements, to a remote signal, that is suitable for implementation
on small embedded systems - potentially in a GPS denied environment. This is an
important, currently open, problem in a number of areas, particularly in the
field of swarm robotics, where rapid updates of positional information are of
great importance. We achieve our solution by means of a tetrahedral phased
array of receivers at which we measure the phase difference, or time difference
of arrival, of the signal. We then perform an elegant and novel, albeit simple,
series of direct calculations, on this information, in order to derive the
relative bearing and elevation to the signal. This solution opens up a number
of applications where rapidly updated and accurate directional awareness in
3-space is of importance and where the available processing power is limited by
energy or CPU constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07377</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07377</id><created>2018-10-16</created><authors><author><keyname>Zhong</keyname><forenames>Zhenghang</forenames></author><author><keyname>Tang</keyname><forenames>Zhe</forenames></author><author><keyname>Li</keyname><forenames>Xiangxing</forenames></author><author><keyname>Yuan</keyname><forenames>Tiancheng</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Wei</keyname><forenames>Meng</forenames></author><author><keyname>Zhang</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Sheng</keyname><forenames>Renzhi</forenames></author><author><keyname>Grant</keyname><forenames>Naomi</forenames></author><author><keyname>Ling</keyname><forenames>Chongfeng</forenames></author><author><keyname>Huan</keyname><forenames>Xintao</forenames></author><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author><author><keyname>Lee</keyname><forenames>Sanghyuk</forenames></author></authors><title>XJTLUIndoorLoc: A New Fingerprinting Database for Indoor Localization
  and Trajectory Estimation Based on Wi-Fi RSS and Geomagnetic Field</title><categories>cs.LG eess.SP stat.ML</categories><comments>7 pages, 16 figures, 3rd International Workshop on GPU Computing and
  AI (GCA'18)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new location fingerprinting database comprised of
Wi-Fi received signal strength (RSS) and geomagnetic field intensity measured
with multiple devices at a multi-floor building in Xi'an Jiatong-Liverpool
University, Suzhou, China. We also provide preliminary results of localization
and trajectory estimation based on convolutional neural network (CNN) and long
short-term memory (LSTM) network with this database. For localization, we map
RSS data for a reference point to an image-like, two-dimensional array and then
apply CNN which is popular in image and video analysis and recognition. For
trajectory estimation, we use a modified random way point model to efficiently
generate continuous step traces imitating human walking and train a stacked
two-layer LSTM network with the generated data to remember the changing pattern
of geomagnetic field intensity against (x,y) coordinates. Experimental results
demonstrate the usefulness of our new database and the feasibility of the CNN
and LSTM-based localization and trajectory estimation with the database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07502</identifier>
 <datestamp>2019-06-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07502</id><created>2018-10-17</created><updated>2019-06-27</updated><authors><author><keyname>Behjat</keyname><forenames>Hamid</forenames></author><author><keyname>Do&#x11f;an</keyname><forenames>Zafer</forenames></author><author><keyname>Van De Ville</keyname><forenames>Dimitri</forenames></author><author><keyname>S&#xf6;rnmo</keyname><forenames>Leif</forenames></author></authors><title>Domain-Informed Spline Interpolation</title><categories>eess.SP</categories><comments>Accepted version for publication in IEEE Transactions on Signal
  Processing</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 67, no. 15, pp.
  3909-3921, 2019</journal-ref><doi>10.1109/TSP.2019.2922154</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard interpolation techniques are implicitly based on the assumption that
the signal lies on a single homogeneous domain. In contrast, many naturally
occurring signals lie on an inhomogeneous domain, such as brain activity
associated to different brain tissue. We propose an interpolation method that
instead exploits prior information about domain inhomogeneity, characterized by
different, potentially overlapping, subdomains. As proof of concept, the focus
is put on extending conventional shift-invariant B-spline interpolation. Given
a known inhomogeneous domain, B-spline interpolation of a given order is
extended to a domain-informed, shift-variant interpolation. This is done by
constructing a domain-informed generating basis that satisfies stability
properties. We illustrate example constructions of domain-informed generating
basis, and show their property in increasing the coherence between the
generating basis and the given inhomogeneous domain. By advantageously
exploiting domain knowledge, we demonstrate the benefit of domain-informed
interpolation over standard B-spline interpolation through Monte Carlo
simulations across a range of B-spline orders. We also demonstrate the
feasibility of domain-informed interpolation in a neuroimaging application
where the domain information is available by a complementary image contrast.
The results show the benefit of incorporating domain knowledge so that an
interpolant consistent to the anatomy of the brain is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07511</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07511</id><created>2018-10-16</created><authors><author><keyname>Pandey</keyname><forenames>Kaushlendra</forenames></author><author><keyname>Gupta</keyname><forenames>Abhishek</forenames></author></authors><title>Modeling and Analysis of Wildfire Detection using Wireless Sensor
  Network with Poisson Deployment</title><categories>eess.SP</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many forest fire incidences, late detection of the fire has lead to severe
damages to the forest and human property requiring more resources to gain
control over the fire. An early warning and immediate response system can be a
promising solution to avoid such massive losses. This paper considers a network
consisting of multiple wireless sensors randomly deployed throughout the forest
for early prompt detection of fire. We present a framework to model fire
propagation in a forest and analyze the performance of considered wireless
sensor network in terms of fire detection probability. In particular, this
paper models sensor deployment as a Poisson point process (PPP) and models the
forest fire as a dynamic event which expands with time. We also present various
insights to the system including required sensor density and impact of wind
velocity on the detection performance. We show that larger wind velocity may
not necessarily imply bad sensing performance or the requirement of a denser
deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07522</identifier>
 <datestamp>2019-02-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07522</id><created>2018-10-17</created><updated>2019-02-26</updated><authors><author><keyname>Prasad</keyname><forenames>Narayan</forenames></author><author><keyname>Qi</keyname><forenames>Xiao-Feng</forenames></author><author><keyname>Gatherer</keyname><forenames>Alan</forenames></author></authors><title>Optimizing Beams and Bits: A Novel Approach for Massive MIMO
  Base-Station Design</title><categories>eess.SP cs.IT math.IT</categories><comments>Tech. Report. Appeared in part in IEEE ICNC 2019. Added few more
  comments and corrected minor typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of jointly optimizing ADC bit resolution and analog
beamforming over a frequency-selective massive MIMO uplink. We build upon a
popular model to incorporate the impact of low bit resolution ADCs, that
hitherto has mostly been employed over flat-fading systems. We adopt weighted
sum rate (WSR) as our objective and show that WSR maximization under finite
buffer limits and important practical constraints on choices of beams and ADC
bit resolutions can equivalently be posed as constrained submodular set
function maximization. This enables us to design a constant-factor
approximation algorithm. Upon incorporating further enhancements we obtain an
efficient algorithm that significantly outperforms state-of-the-art ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07548</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07548</id><created>2018-10-16</created><authors><author><keyname>Ye</keyname><forenames>Chuang</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author><author><keyname>Velipasalar</keyname><forenames>Senem</forenames></author></authors><title>Deep Learning Based Power Control for Quality-Driven Wireless Video
  Transmissions</title><categories>cs.LG cs.IT eess.IV math.IT stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1707.08232</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, wireless video transmission to multiple users under total
transmission power and minimum required video quality constraints is studied.
In order to provide the desired performance levels to the end-users in
real-time video transmissions while using the energy resources efficiently, we
assume that power control is employed. Due to the presence of interference,
determining the optimal power control is a non-convex problem but can be solved
via monotonic optimization framework. However, monotonic optimization is an
iterative algorithm and can often entail considerable computational complexity,
making it not suitable for real-time applications. To address this, we propose
a learning-based approach that treats the input and output of a resource
allocation algorithm as an unknown nonlinear mapping and a deep neural network
(DNN) is employed to learn this mapping. This learned mapping via DNN can
provide the optimal power level quickly for given channel conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07558</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07558</id><created>2018-10-16</created><authors><author><keyname>Wilding</keyname><forenames>Dean</forenames></author><author><keyname>Soloviev</keyname><forenames>Oleg</forenames></author><author><keyname>Pozzi</keyname><forenames>Paolo</forenames></author><author><keyname>Smith</keyname><forenames>Carlas</forenames></author><author><keyname>Vdovin</keyname><forenames>Gleb</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author></authors><title>Blind single-frame deconvolution by tangential iterative projections
  (TIP)</title><categories>eess.IV</categories><comments>OL original submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deconvolution serves as a computational means of removing the effect of
optical aberrations from recorded images and is employed in many technical and
scientific fields of study. In most imaging scenarios the nature of the
blurring kernel or point-spread function (PSF) of the imaging system is unknown
and both the object and PSF can be estimated using different forms of
mathematical optimisation. The Tangential Iterative Projections (TIP) algorithm
is a multi-frame deconvolution framework where multiple images can be combined
to obtain a single estimate of the object. It is shown here that this framework
may be also used for single-frame deconvolution with a few modifications to the
algorithm. This step from multiple to one frame is non-trivial and greatly
improves the applicability of the TIP framework to most imaging scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07559</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07559</id><created>2018-10-16</created><authors><author><keyname>Yu</keyname><forenames>Y.</forenames></author><author><keyname>Zhao</keyname><forenames>H.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Study of Sparsity-Aware Subband Adaptive Filtering Algorithms with
  Adjustable Penalties</title><categories>eess.SP cs.LG stat.ML</categories><comments>32 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two sparsity-aware normalized subband adaptive filter (NSAF)
algorithms by using the gradient descent method to minimize a combination of
the original NSAF cost function and the l1-norm penalty function on the filter
coefficients. This l1-norm penalty exploits the sparsity of a system in the
coefficients update formulation, thus improving the performance when
identifying sparse systems. Compared with prior work, the proposed algorithms
have lower computational complexity with comparable performance. We study and
devise statistical models for these sparsity-aware NSAF algorithms in the mean
square sense involving their transient and steady -state behaviors. This study
relies on the vectorization argument and the paraunitary assumption imposed on
the analysis filter banks, and thus does not restrict the input signal to being
Gaussian or having another distribution. In addition, we propose to adjust
adaptively the intensity parameter of the sparsity attraction term. Finally,
simulation results in sparse system identification demonstrate the
effectiveness of our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07583</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07583</id><created>2018-10-17</created><authors><author><keyname>Gordon</keyname><forenames>Ethan</forenames></author></authors><title>Mode Division Multiplexing (MDM) Weight Bank Design for Use in Photonic
  Neural Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks provide a powerful tool for applications from classification
and regression to general purpose alternative computing. Photonics have the
potential to provide enormous speed benefits over electronic and software
networks, allowing such networks to be used in real-time applications at radio
frequencies. Mode division multiplexing (MDM) is one method to increase the
total information capacity of a single on-chip waveguide and, by extension, the
information density of the photonic neural network (PNN). This Independent Work
consists of three experimental designs ready for fabrication, each of which
investigates the process of expanding current PNN technology to include MDM.
Experiment 1 determines the optimal waveguide geometry to couple optical power
into different spacial modes within a single waveguide. Experiment 2 combines
MDM and previous wavelength division multiplexing (WDM) technology into a
single weight bank for use as the dendrite of a photonic neuron. Finally,
Experiment 3 puts two full neurons in a folded bus, or &quot;hairpin,&quot; network
topology to provide a platform for training calibration schemes that can be
applied to larger networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07649</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07649</id><created>2018-09-28</created><authors><author><keyname>Wijayono</keyname><forenames>Andrian</forenames></author><author><keyname>Putra</keyname><forenames>Valentinus Galih Vidia</forenames></author></authors><title>Implementation of Digital Image Processing and Computation Technology on
  Measurement and Testing of Various Yarn Parameters</title><categories>eess.IV</categories><comments>62 Pages, Indonesian</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Along with the development of science and technology, the application of
various computational processes in the field of science engineering is
something that is being developed, even today. Currently, many studies have
applied image analysis technology to the field of textile evaluation and
identification, one of which is yarn. Computer image analysis techniques can be
used to identify textile products, especially textile products in the form of
yarn. This chapter has reviewed various methods and application of image
processing technology in the field of identification and evaluation of various
thread parameters, together with a brief review of historical developments, as
well as the need for the application of these methods.
  Keywords: yarn, digital image processing, evaluation and identification,
textiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07650</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07650</id><created>2018-09-28</created><authors><author><keyname>Wijayono</keyname><forenames>Andrian</forenames></author><author><keyname>Irwan</keyname></author><author><keyname>Putra</keyname><forenames>Valentinus Galih Vidia</forenames></author></authors><title>Implementation of Digital Image Processing and Computation Technology on
  Measurement and Testing of Non Woven Fabric Parameters</title><categories>eess.IV</categories><comments>72 pages, Indonesian</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  One of the big challenges of the industry today is how to produce quality
products, one of which is in the non-woven industry. The improvement of the
evaluation and quality control processes of non woven production has been
widely developed to support the improvement of the quality of production. The
use of information and computational technology has now been widely applied to
the quality control process of textile material production, one of which is the
use of image processing technology in the evaluation process of non-woven
materials. This paper will explain various methods of applying image processing
technology in the field of evaluation and quality control of textile
production.
  Keywords: textile fiber, image processing, textile evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07651</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07651</id><created>2018-09-28</created><authors><author><keyname>Wijayono</keyname><forenames>Andrian</forenames></author><author><keyname>Irwan</keyname></author><author><keyname>Rohmah</keyname><forenames>Siti</forenames></author><author><keyname>Putra</keyname><forenames>Valentinus Galih Vidia</forenames></author></authors><title>Implementation of Digital Image Processing and Computation Technology on
  Measurement and Testing of Various Woven Fabric Parameters</title><categories>eess.IV</categories><comments>50 pages, Indonesian</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Quality is one of the important things to be maintained in a weaving
industry. Along with the times, technological developments in the field of
image processing and computing have changed the old method of visual evaluation
of woven fabric to be better. This chapter will explain the implementation of
image and computational processing techniques on woven fabric.
  Keywords: woven fabric, digital image processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07652</identifier>
 <datestamp>2018-10-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07652</id><created>2018-10-16</created><authors><author><keyname>Di Gangi</keyname><forenames>Mattia Antonino</forenames></author><author><keyname>Dess&#xec;</keyname><forenames>Roberto</forenames></author><author><keyname>Cattoni</keyname><forenames>Roldano</forenames></author><author><keyname>Negri</keyname><forenames>Matteo</forenames></author><author><keyname>Turchi</keyname><forenames>Marco</forenames></author></authors><title>Fine-tuning on Clean Data for End-to-End Speech Translation: FBK @ IWSLT
  2018</title><categories>eess.AS cs.CL cs.LG cs.SD stat.ML</categories><comments>6 pages, 2 figures, system description at the 15th International
  Workshop on Spoken Language Translation (IWSLT) 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes FBK's submission to the end-to-end English-German speech
translation task at IWSLT 2018. Our system relies on a state-of-the-art model
based on LSTMs and CNNs, where the CNNs are used to reduce the temporal
dimension of the audio input, which is in general much higher than machine
translation input. Our model was trained only on the audio-to-text parallel
data released for the task, and fine-tuned on cleaned subsets of the original
training corpus. The addition of weight normalization and label smoothing
improved the baseline system by 1.0 BLEU point on our validation set. The final
submission also featured checkpoint averaging within a training run and
ensemble decoding of models trained during multiple runs. On test data, our
best single model obtained a BLEU score of 9.7, while the ensemble obtained a
BLEU score of 10.24.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07810</identifier>
 <datestamp>2019-08-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07810</id><created>2018-10-17</created><updated>2019-08-28</updated><authors><author><keyname>Zhuang</keyname><forenames>Juntang</forenames></author></authors><title>LadderNet: Multi-path networks based on U-Net for medical image
  segmentation</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  U-Net has been providing state-of-the-art performance in many medical image
segmentation problems. Many modifications have been proposed for U-Net, such as
attention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Net
with residual blocks or blocks with dense connections. However, all these
modifications have an encoder-decoder structure with skip connections, and the
number of paths for information flow is limited. We propose LadderNet in this
paper, which can be viewed as a chain of multiple U-Nets. Instead of only one
pair of encoder branch and decoder branch in U-Net, a LadderNet has multiple
pairs of encoder-decoder branches, and has skip connections between every pair
of adjacent decoder and decoder branches in each level. Inspired by the success
of ResNet and R2-UNet, we use modified residual blocks where two convolutional
layers in one block share the same weights. A LadderNet has more paths for
information flow because of skip connections and residual blocks, and can be
viewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence to
an ensemble of FCNs improves segmentation accuracy, while the shared weights
within each residual block reduce parameter number. Semantic segmentation is
essential for retinal disease detection. We tested LadderNet on two benchmark
datasets for blood vessel segmentation in retinal images, and achieved superior
performance over methods in the literature. The implementation is provided
\url{https://github.com/juntang-zhuang/LadderNet}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07856</identifier>
 <datestamp>2019-05-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07856</id><created>2018-10-17</created><updated>2019-05-09</updated><authors><author><keyname>Dean</keyname><forenames>Thomas</forenames></author><author><keyname>Perlstein</keyname><forenames>Jonathan</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Fast Blind MIMO Decoding through Vertex Hopping</title><categories>eess.SP math.OC</categories><comments>submitted to IEEE Transaction on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that efficiently performs blind decoding of MIMO
signals. That is, given no channel state information (CSI) at either the
transmitter or receiver, our algorithm takes a block of samples and returns an
estimate of the underlying data symbols. In prior work, the problem of blind
decoding was formulated as a non-convex optimization problem. In this work, we
present an algorithm that efficiently solves this non-convex problem in
practical settings. This algorithm leverages concepts of linear and
mixed-integer linear programming. Empirically, we show that our technique has
an error performance close to that of zero-forcing with perfect CSI at the
receiver. Initial estimates of the runtime of the algorithm presented in this
work suggest that the real-time blind decoding of MIMO signals is possible for
even modest-sized MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07918</identifier>
 <datestamp>2018-10-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07918</id><created>2018-10-18</created><authors><author><keyname>Kara</keyname><forenames>Ferdi</forenames></author><author><keyname>Kaya</keyname><forenames>Hakan</forenames></author></authors><title>Spatial Multiple Access (SMA): Enhancing performances of MIMO-NOMA
  systems</title><categories>eess.SP cs.IT math.IT</categories><comments>10 pages,4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The error performance of the Non-Orthogonal Multiple Access (NOMA) technique
suffers from the inter-user interference (IUI) although it is a promising
technique for the future wireless systems in terms of the achievable sum rate.
Hence, a multiple access technique design with limited IUI and competitive to
NOMA in terms of spectral efficiency is essential. In this letter, we consider
so-called spatial multiple access (SMA) which is based on applying the
principle of spatial modulation (SM) through the different users' data streams,
as a strong alternative to MIMO-NOMA systems. The analytical expressions of bit
error probability (BEP), ergodic sum rate and outage probability are derived
for the SMA. The derivations are validated via computer simulations. In
addition, the comparison of the SMA system with NOMA is presented. The results
reveal that SMA outperforms to the NOMA in terms of the all performance metrics
(i.e., bit error rate (BER), outage probability and ergodic sum rate) besides
it provides low implementation complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07928</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07928</id><created>2018-10-18</created><updated>2018-11-02</updated><authors><author><keyname>Gannavarpu</keyname><forenames>Rajshekhar</forenames></author><author><keyname>Ambrosini</keyname><forenames>Dario</forenames></author></authors><title>Multi-scale approach for analyzing convective heat transfer flow in
  background-oriented Schlieren technique</title><categories>eess.SP</categories><journal-ref>Gannavarpu Rajshekhar and Dario Ambrosini, &quot;Multi-scale approach
  for analyzing convective heat transfer flow in background-oriented Schlieren
  technique&quot;, Optics and Lasers in Engineering, 110, 415-419, 2018</journal-ref><doi>10.1016/j.optlaseng.2018.07.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces a multi-scale processing method for quantitative study
and visualization of convective heat transfer using diffractive optical element
based background-oriented schlieren technique. The method relies on robust
estimation of phase encoded in the fringe pattern using windowed Fourier
transform and subsequent multi-scale characterization of the obtained phase
using continuous wavelet transform. As the phase is directly mapped to the
refractive index fluctuations caused by the temperature gradients, the
multi-scale inspection provides interesting insights about the underlying heat
flow phenomenon. The performance of the proposed method is demonstrated for
quantitative flow visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.07961</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.07961</id><created>2018-10-18</created><updated>2018-11-04</updated><authors><author><keyname>Mourya</keyname><forenames>Simmi</forenames></author><author><keyname>Kant</keyname><forenames>Sonaal</forenames></author><author><keyname>Kumar</keyname><forenames>Pulkit</forenames></author><author><keyname>Gupta</keyname><forenames>Anubha</forenames></author><author><keyname>Gupta</keyname><forenames>Ritu</forenames></author></authors><title>LeukoNet: DCT-based CNN architecture for the classification of normal
  versus Leukemic blasts in B-ALL Cancer</title><categories>cs.CV cs.LG eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acute lymphoblastic leukemia (ALL) constitutes approximately 25% of the
pediatric cancers. In general, the task of identifying immature leukemic blasts
from normal cells under the microscope is challenging because morphologically
the images of the two cells appear similar. In this paper, we propose a deep
learning framework for classifying immature leukemic blasts and normal cells.
The proposed model combines the Discrete Cosine Transform (DCT) domain features
extracted via CNN with the Optical Density (OD) space features to build a
robust classifier. Elaborate experiments have been conducted to validate the
proposed LeukoNet classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08078</identifier>
 <datestamp>2018-10-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08078</id><created>2018-10-17</created><authors><author><keyname>Farah</keyname><forenames>Joumana</forenames></author><author><keyname>Kilzi</keyname><forenames>Antoine</forenames></author><author><keyname>Nour</keyname><forenames>Charbel Abdel</forenames></author><author><keyname>Douillard</keyname><forenames>Catherine</forenames></author></authors><title>Power Minimization in Distributed Antenna Systems using Non-Orthogonal
  Multiple Access and Mutual Successive Interference Cancellation</title><categories>eess.SP cs.NI</categories><comments>Paper accepted for publication in IEEE Trans. Vehicular Technology,
  14 pages, 5 figures, 2 tables. arXiv admin note: substantial text overlap
  with arXiv:1710.06619</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces new approaches for combining non-orthogonal multiple
access with distributed antenna systems. The study targets a minimization of
the total transmit power in each cell, under user rate and power multiplexing
constraints. Several new suboptimal power allocation techniques are proposed.
They are shown to yield very close performance to an optimal power allocation
scheme. Also, a new approach based on mutual successive interference
cancellation of paired users is proposed. Different techniques are designed for
the joint allocation of subcarriers, antennas, and power, with a particular
care given to maintain a moderate complexity. The coupling of non-orthogonal
multiple access to distributed antenna systems is shown to greatly outperform
any other combination of orthogonal/non-orthogonal multiple access schemes with
distributed or centralized deployment scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08090</identifier>
 <datestamp>2018-10-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08090</id><created>2018-10-18</created><authors><author><keyname>Krishnan</keyname><forenames>Joshin P.</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>Katkovnik</keyname><forenames>Vladimir</forenames></author></authors><title>Dictionary Learning Phase Retrieval from Noisy Diffraction Patterns</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel algorithm for image phase retrieval, i.e., for
recovering complex-valued images from the amplitudes of noisy linear
combinations (often the Fourier transform) of the sought complex images. The
algorithm is developed using the alternating projection framework and is aimed
to obtain high performance for heavily noisy (Poissonian or Gaussian)
observations. The estimation of the target images is reformulated as a sparse
regression, often termed sparse coding, in the complex domain. This is
accomplished by learning a complex domain dictionary from the data it
represents via matrix factorization with sparsity constraints on the code
(i.e., the regression coefficients). Our algorithm, termed dictionary learning
phase retrieval (DLPR), jointly learns the referred to dictionary and
reconstructs the unknown target image. The effectiveness of DLPR is illustrated
through experiments conducted on complex images, simulated and real, where it
shows noticeable advantages over the state-of-the-art competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08098</identifier>
 <datestamp>2018-12-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08098</id><created>2018-10-18</created><updated>2018-12-11</updated><authors><author><keyname>Carlino</keyname><forenames>Luca</forenames></author><author><keyname>Jin</keyname><forenames>Di</forenames></author><author><keyname>Muma</keyname><forenames>Michael</forenames></author><author><keyname>Zoubir</keyname><forenames>Abdelhak M.</forenames></author></authors><title>Robust Distributed Cooperative RSS-based Localization for Directed
  Graphs in Mixed LoS/NLoS Environments</title><categories>eess.SP</categories><comments>16 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The accurate and low-cost localization of sensors using a wireless sensor
network is critically required in a wide range of today's applications. We
propose a novel, robust maximum likelihood-type method for distributed
cooperative received signal strength-based localization in wireless sensor
networks. To cope with mixed LoS/NLoS conditions, we model the measurements
using a two-component Gaussian mixture model. The relevant channel parameters,
including the reference path loss, the path loss exponent and the variance of
the measurement error, for both LoS and NLoS conditions, are assumed to be
unknown deterministic parameters and are adaptively estimated. Unlike existing
algorithms, the proposed method naturally takes into account the (possible)
asymmetry of links between nodes. The proposed approach has a communication
overhead upper-bounded by a quadratic function of the number of nodes and
computational complexity scaling linearly with it. The convergence of the
proposed method is guaranteed for compatible network graphs and compatibility
can be tested a priori by restating the problem as a graph coloring problem.
Simulation results, carried out in comparison to a centralized benchmark
algorithm, demonstrate the good overall performance and high robustness in
mixed LoS/NLoS environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08156</identifier>
 <datestamp>2018-10-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08156</id><created>2018-10-18</created><authors><author><keyname>Sheng</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author></authors><title>A Non-Intrusive Low-Rank Approximation Method for Assessing the
  Probabilistic Available Transfer Capability</title><categories>eess.SP</categories><comments>Eight pages, one figure. Submitted to IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a mathematical formulation of the probabilistic available
transfer capability (PATC) problem is proposed to incorporate uncertainties
from the large-scale renewable energy generation (e.g., wind farms and solar PV
power plants). Moreover, a novel non-intrusive low-rank approximation (LRA) is
developed to assess PATC, which can accurately and efficiently estimate the
probabilistic characteristics (e.g., mean, variance, probability density
function (PDF)) of the PATC. Numerical studies on the IEEE 24-bus reliability
test system (RTS) and IEEE 118-bus system show that the proposed method can
achieve accurate estimations for the probabilistic characteristics of the PATC
with much less computational effort compared to the Latin hypercube sampling
(LHS)-based Monte Carlo simulations (MCS). The proposed LRA-PATC method offers
an efficient and effective way to determine the available transfer capability
so as to fully utilize the transmission assets while maintaining the security
of the grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08169</identifier>
 <datestamp>2018-10-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08169</id><created>2018-10-18</created><authors><author><keyname>Li</keyname><forenames>Dingquan</forenames></author><author><keyname>Jiang</keyname><forenames>Tingting</forenames></author><author><keyname>Jiang</keyname><forenames>Ming</forenames></author></authors><title>Exploiting High-Level Semantics for No-Reference Image Quality
  Assessment of Realistic Blur Images</title><categories>eess.IV cs.CV cs.MM</categories><comments>correct typos, e.g., &quot;avarage&quot; -&gt; &quot;average&quot; in the figure of the
  proposed framework</comments><journal-ref>Proceedings of the 2017 ACM on Multimedia Conference</journal-ref><doi>10.1145/3123266.3123322</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To guarantee a satisfying Quality of Experience (QoE) for consumers, it is
required to measure image quality efficiently and reliably. The neglect of the
high-level semantic information may result in predicting a clear blue sky as
bad quality, which is inconsistent with human perception. Therefore, in this
paper, we tackle this problem by exploiting the high-level semantics and
propose a novel no-reference image quality assessment method for realistic blur
images. Firstly, the whole image is divided into multiple overlapping patches.
Secondly, each patch is represented by the high-level feature extracted from
the pre-trained deep convolutional neural network model. Thirdly, three
different kinds of statistical structures are adopted to aggregate the
information from different patches, which mainly contain some common statistics
(i.e., the mean\&amp;standard deviation, quantiles and moments). Finally, the
aggregated features are fed into a linear regression model to predict the image
quality. Experiments show that, compared with low-level features, high-level
features indeed play a more critical role in resolving the aforementioned
challenging problem for quality estimation. Besides, the proposed method
significantly outperforms the state-of-the-art methods on two realistic blur
image databases and achieves comparable performance on two synthetic blur image
databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08245</identifier>
 <datestamp>2018-10-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08245</id><created>2018-09-14</created><authors><author><keyname>Roko&#x161;</keyname><forenames>O.</forenames></author><author><keyname>Hoefnagels</keyname><forenames>J. P. M.</forenames></author><author><keyname>Peerlings</keyname><forenames>R. H. J.</forenames></author><author><keyname>Geers</keyname><forenames>M. G. D.</forenames></author></authors><title>On Micromechanical Parameter Identification With Integrated DIC and the
  Role of Accuracy in Kinematic Boundary Conditions</title><categories>physics.data-an eess.IV</categories><comments>37 pages, 25 figures, 2 tables, 2 algorithms</comments><journal-ref>International Journal of Solids and Structures 146, 241--259,
  (2018)</journal-ref><doi>10.1016/j.ijsolstr.2018.04.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrated Digital Image Correlation (IDIC) is nowadays a well established
full-field experimental procedure for reliable and accurate identification of
material parameters. It is based on the correlation of a series of images
captured during a mechanical experiment, that are matched by displacement
fields derived from an underlying mechanical model. In recent studies, it has
been shown that when the applied boundary conditions lie outside the employed
field of view, IDIC suffers from inaccuracies. A typical example is a
micromechanical parameter identification inside a Microstructural Volume
Element (MVE), whereby images are usually obtained by electron microscopy or
other microscopy techniques but the loads are applied at a much larger scale.
For any IDIC model, MVE boundary conditions still need to be specified, and any
deviation or fluctuation in these boundary conditions may significantly
influence the quality of identification. Prescribing proper boundary conditions
is generally a challenging task, because the MVE has no free boundary, and the
boundary displacements are typically highly heterogeneous due to the underlying
microstructure. The aim of this paper is therefore first to quantify the
effects of errors in the prescribed boundary conditions on the accuracy of the
identification in a systematic way. To this end, three kinds of mechanical
tests, each for various levels of material contrast ratios and levels of image
noise, are carried out by means of virtual experiments. For simplicity, an
elastic compressible Neo-Hookean constitutive model under plane strain
assumption is adopted. It is shown that a high level of detail is required in
the applied boundary conditions. This motivates an improved boundary condition
application approach, which considers constitutive material parameters as well
as kinematic variables at the boundary of the entire MVE as degrees of freedom
in...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08251</identifier>
 <datestamp>2018-10-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08251</id><created>2018-10-18</created><authors><author><keyname>Yazdani</keyname><forenames>Hassan</forenames></author><author><keyname>Vosoughi</keyname><forenames>Azadeh</forenames></author></authors><title>On Optimal Sensing and Capacity Trade-off in Cognitive Radio Systems
  with Directional Antennas</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cognitive radio system, in which the secondary users (SUs) and
primary users (PUs) coexist. The SUs are equipped with steerable directional
antennas. In our system, the secondary transmitter (SUtx) first senses the
spectrum (with errors) for a duration of {\tau}, and, then transmits data to
the secondary receiver (SUrx) if spectrum is sensed idle. The sensing time as
well as the orientation of SUtx's antenna affect the accuracy of spectrum
sensing and yield a trade-off between spectrum sensing and capacity of the
secondary network. We formulate the ergodic capacity of secondary network which
uses energy detection for spectrum sensing. We obtain optimal SUtx transmit
power, the optimal sensing time {\tau} and the optimal directions of SUtx
transmit antenna and SUrx receive antenna by maximizing the ergodic capacity,
subject to peak transmit power and outage interference probability constraints.
Our simulation results show the effectiveness of these optimizations to
increase the ergodic capacity of the secondary network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08335</identifier>
 <datestamp>2018-10-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08335</id><created>2018-10-18</created><authors><author><keyname>Ruble</keyname><forenames>Macey</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author></authors><title>Multilinear SVD for Millimeter Wave Channel Parameter Estimation</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fifth generation (5G) cellular standards are set to utilize millimeter wave
(mmWave) frequencies, which enable data speeds greater than 10 Gbps and
sub-centimeter localization accuracy. These capabilities rely on accurate
estimates of the channel parameters, which we define as the angle of arrival,
angle of departure, and path distance for each path between the transmitter and
receiver. Estimating the channel parameters in a computationally efficient
manner poses a challenge because it requires estimation of parameters from a
high-dimensional measurement -- particularly for multi-carrier systems since
each subcarrier must be estimated separately. Additionally, channel parameter
estimation must be able to handle hybrid beamforming, which uses a combination
of digital and analog beamforming to reduce the number of required analog to
digital converters. This paper introduces a channel parameter estimation
technique based on the multilinear singular value decomposition (MSVD), a
tensor analog of the singular value decomposition, for massive multiple input
multiple output (MIMO) multi-carrier systems with hybrid beamforming. The MSVD
tensor estimation approach provides a computationally efficient method and is
shown to closely match the Cramer-Rao bound (CRB) of parameter estimates
through simulations. Limitations of channel parameter estimation and
communication waveform effects are also studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08432</identifier>
 <datestamp>2018-10-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08432</id><created>2018-10-19</created><authors><author><keyname>Pla</keyname><forenames>Pol del Aguila</forenames></author><author><keyname>Jald&#xe9;n</keyname><forenames>Joakim</forenames></author></authors><title>Convolutional group-sparse coding and source localization</title><categories>eess.SP</categories><comments>5 pages, 2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP), 15-20 April 2018, Calgary, AB, Canada</comments><journal-ref>2018 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), pp. 2776-2780, 2018</journal-ref><doi>10.1109/ICASSP.2018,8462235</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new interpretation of non-negatively constrained
convolutional coding problems as blind deconvolution problems with spatially
variant point spread function. In this light, we propose an optimization
framework that generalizes our previous work on non-negative group sparsity for
convolutional models. We then link these concepts to source localization
problems that arise in scientific imaging and provide a visual example on an
image derived from data captured by the Hubble telescope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08440</identifier>
 <datestamp>2018-10-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08440</id><created>2018-10-19</created><authors><author><keyname>Perez-Neira</keyname><forenames>Ana I.</forenames></author><author><keyname>Caus</keyname><forenames>Marius</forenames></author><author><keyname>Vazquez</keyname><forenames>Miguel Angel</forenames></author><author><keyname>Alagha</keyname><forenames>Nader</forenames></author></authors><title>NOMA Schemes for Multibeam Satellite Communications</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-orthogonal multiple access (NOMA) schemes are being considered in 5G new
radio developments and beyond. Although seminal papers demonstrated that NOMA
outperforms orthogonal access in terms of capacity and user fairness, the
majority of works have been devoted to the wireless terrestrial arena.
Therefore, it is worth to study how NOMA can be implemented in other types of
communications, as for instance the satellite ones, which are also part of the
5G infrastructure. Although communications through a satellite present a
different architecture than those in the wireless terrestrial links, NOMA can
be an important asset to improve their performance. This article introduces a
general overview of how NOMA can be applied to this different architecture. A
novel taxonomy is presented based on different multibeam transmission schemes
and guidelines that open new avenues for research in this topic are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08554</identifier>
 <datestamp>2018-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08554</id><created>2018-10-19</created><updated>2018-11-13</updated><authors><author><keyname>Pan</keyname><forenames>Huijie</forenames></author><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>HeartBEAT: Heart Beat Estimation through Adaptive Tracking</title><categories>eess.SP</categories><comments>4 pages, 5 figures, 2 tables</comments><journal-ref>H. Pan, D. Temel and G. AlRegib, &quot;HeartBEAT: Heart beat estimation
  through adaptive tracking,&quot; 2016 IEEE-EMBS International Conference on
  Biomedical and Health Informatics (BHI), Las Vegas, NV, 2016, pp. 587-590</journal-ref><doi>10.1109/BHI.2016.7455966</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an algorithm denoted as HeartBEAT that tracks heart
rate from wrist-type photoplethysmography (PPG) signals and simultaneously
recorded three-axis acceleration data. HeartBEAT contains three major parts:
spectrum estimation of PPG signals and acceleration data, elimination of motion
artifacts in PPG signals using recursive least Square (RLS) adaptive filters,
and auxiliary heuristics. We tested HeartBEAT on the 22 datasets provided in
the 2015 IEEE Signal Processing Cup. The first ten datasets were recorded from
subjects performing forearm and upper-arm exercises, jumping, or pushing-up.
The last twelve datasets were recorded from subjects running on tread mills.
The experimental results were compared to the ground truth heart rate, which
comes from simultaneously recorded electrocardiogram (ECG) signals. Compared to
state-of-the-art algorithms, HeartBEAT not only produces comparable Pearson's
correlation and mean absolute error, but also higher Spearman's rho and
Kendall's tau.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08559</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08559</id><created>2018-10-17</created><updated>2018-11-13</updated><authors><author><keyname>Lin</keyname><forenames>Zhong Qiu</forenames></author><author><keyname>Chung</keyname><forenames>Audrey G.</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>EdgeSpeechNets: Highly Efficient Deep Neural Networks for Speech
  Recognition on the Edge</title><categories>eess.AS cs.LG cs.NE cs.SD eess.SP stat.ML</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite showing state-of-the-art performance, deep learning for speech
recognition remains challenging to deploy in on-device edge scenarios such as
mobile and other consumer devices. Recently, there have been greater efforts in
the design of small, low-footprint deep neural networks (DNNs) that are more
appropriate for edge devices, with much of the focus on design principles for
hand-crafting efficient network architectures. In this study, we explore a
human-machine collaborative design strategy for building low-footprint DNN
architectures for speech recognition through a marriage of human-driven
principled network design prototyping and machine-driven design exploration.
The efficacy of this design strategy is demonstrated through the design of a
family of highly-efficient DNNs (nicknamed EdgeSpeechNets) for
limited-vocabulary speech recognition. Experimental results using the Google
Speech Commands dataset for limited-vocabulary speech recognition showed that
EdgeSpeechNets have higher accuracies than state-of-the-art DNNs (with the best
EdgeSpeechNet achieving ~97% accuracy), while achieving significantly smaller
network sizes (as much as 7.8x smaller) and lower computational cost (as much
as 36x fewer multiply-add operations, 10x lower prediction latency, and 16x
smaller memory footprint on a Motorola Moto E phone), making them very
well-suited for on-device edge voice interface applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08581</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08581</id><created>2018-10-19</created><updated>2019-07-10</updated><authors><author><keyname>Isufi</keyname><forenames>Elvin</forenames></author><author><keyname>Loukas</keyname><forenames>Andreas</forenames></author><author><keyname>Perraudin</keyname><forenames>Nathanael</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Forecasting Time Series with VARMA Recursions on Graphs</title><categories>eess.SP cs.SY econ.EM</categories><comments>submitted to the IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2019.2929930</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-based techniques emerged as a choice to deal with the dimensionality
issues in modeling multivariate time series. However, there is yet no complete
understanding of how the underlying structure could be exploited to ease this
task. This work provides contributions in this direction by considering the
forecasting of a process evolving over a graph. We make use of the
(approximate) time-vertex stationarity assumption, i.e., timevarying graph
signals whose first and second order statistical moments are invariant over
time and correlated to a known graph topology. The latter is combined with VAR
and VARMA models to tackle the dimensionality issues present in predicting the
temporal evolution of multivariate time series. We find out that by projecting
the data to the graph spectral domain: (i) the multivariate model estimation
reduces to that of fitting a number of uncorrelated univariate ARMA models and
(ii) an optimal low-rank data representation can be exploited so as to further
reduce the estimation costs. In the case that the multivariate process can be
observed at a subset of nodes, the proposed models extend naturally to Kalman
filtering on graphs allowing for optimal tracking. Numerical experiments with
both synthetic and real data validate the proposed approach and highlight its
benefits over state-of-the-art alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08611</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08611</id><created>2018-10-19</created><authors><author><keyname>Crestel</keyname><forenames>L&#xe9;opold</forenames></author><author><keyname>Esling</keyname><forenames>Philippe</forenames></author><author><keyname>Heng</keyname><forenames>Lena</forenames></author><author><keyname>McAdams</keyname><forenames>Stephen</forenames></author></authors><title>A database linking piano and orchestral MIDI scores with application to
  automatic projective orchestration</title><categories>cs.SD cs.LG eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces the Projective Orchestral Database (POD), a
collection of MIDI scores composed of pairs linking piano scores to their
corresponding orchestrations. To the best of our knowledge, this is the first
database of its kind, which performs piano or orchestral prediction, but more
importantly which tries to learn the correlations between piano and orchestral
scores. Hence, we also introduce the projective orchestration task, which
consists in learning how to perform the automatic orchestration of a piano
score. We show how this task can be addressed using learning methods and also
provide methodological guidelines in order to properly use this database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08691</identifier>
 <datestamp>2019-04-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08691</id><created>2018-10-19</created><updated>2018-11-18</updated><authors><author><keyname>Liang</keyname><forenames>Dawei</forenames></author><author><keyname>Thomaz</keyname><forenames>Edison</forenames></author></authors><title>Audio-Based Activities of Daily Living (ADL) Recognition with
  Large-Scale Acoustic Embeddings from Online Videos</title><categories>cs.HC cs.LG cs.SD eess.AS</categories><comments>18 pages,7 figures; new version: results updates</comments><journal-ref>ACM IMWUT 3(1) 2019 Article 17</journal-ref><doi>10.1145/3314404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, activity sensing and recognition has been shown to play a key
enabling role in a wide range of applications, from sustainability and
human-computer interaction to health care. While many recognition tasks have
traditionally employed inertial sensors, acoustic-based methods offer the
benefit of capturing rich contextual information, which can be useful when
discriminating complex activities. Given the emergence of deep learning
techniques and leveraging new, large-scaled multi-media datasets, this paper
revisits the opportunity of training audio-based classifiers without the
onerous and time-consuming task of annotating audio data. We propose a
framework for audio-based activity recognition that makes use of millions of
embedding features from public online video sound clips. Based on the
combination of oversampling and deep learning approaches, our framework does
not require further feature processing or outliers filtering as in prior work.
We evaluated our approach in the context of Activities of Daily Living (ADL) by
recognizing 15 everyday activities with 14 participants in their own homes,
achieving 64.2% and 83.6% averaged within-subject accuracy in terms of top-1
and top-3 classification respectively. Individual class performance was also
examined in the paper to further study the co-occurrence characteristics of the
activities and the robustness of the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08694</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08694</id><created>2018-10-19</created><authors><author><keyname>Schaake</keyname><forenames>J. C.</forenames></author><author><keyname>Pooser</keyname><forenames>R. C.</forenames></author><author><keyname>Jesse</keyname><forenames>S.</forenames></author></authors><title>Compressive Imaging with Stochastic Spatial Light Modulator</title><categories>eess.IV</categories><comments>4 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a stochastic analog spatial light modulator designed for
compressive imaging applications. We rely on the unpredictable nature of
multi-particle collisions to provide randomization for the particle location.
We demonstrate this concept in an optical imaging system using a single-pixel
camera. This design can be applied to imaging or spectroscopic systems in which
no analog to optical spatial light modulators currently exist or in non-optical
lensless imaging systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08707</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08707</id><created>2018-10-19</created><authors><author><keyname>Fanzeres</keyname><forenames>Leonardo A.</forenames><affiliation>PPGI, DCC/IM, Universidade Federal do Rio de Janeiro</affiliation></author><author><keyname>Vivacqua</keyname><forenames>Adriana S.</forenames><affiliation>PPGI, DCC/IM, Universidade Federal do Rio de Janeiro</affiliation></author><author><keyname>Biscainho</keyname><forenames>Luiz W. P.</forenames><affiliation>DEL/Poli &amp; PEE/COPPE, Universidade Federal do Rio de Janeiro</affiliation></author></authors><title>Mobile Sound Recognition for the Deaf and Hard of Hearing</title><categories>cs.HC cs.AI cs.SD eess.AS</categories><comments>25 pages, 8 figures</comments><msc-class>68U35, 68T37, 68T10</msc-class><acm-class>H.1.2; H.5.2; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human perception of surrounding events is strongly dependent on audio cues.
Thus, acoustic insulation can seriously impact situational awareness. We
present an exploratory study in the domain of assistive computing, eliciting
requirements and presenting solutions to problems found in the development of
an environmental sound recognition system, which aims to assist deaf and hard
of hearing people in the perception of sounds. To take advantage of smartphones
computational ubiquity, we propose a system that executes all processing on the
device itself, from audio features extraction to recognition and visual
presentation of results. Our application also presents the confidence level of
the classification to the user. A test of the system conducted with deaf users
provided important and inspiring feedback from participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08754</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08754</id><created>2018-10-20</created><authors><author><keyname>Fan</keyname><forenames>Yuwei</forenames></author><author><keyname>Bohorquez</keyname><forenames>Cindy Orozco</forenames></author><author><keyname>Ying</keyname><forenames>Lexing</forenames></author></authors><title>BCR-Net: a neural network based on the nonstandard wavelet form</title><categories>math.NA cs.LG eess.SP</categories><comments>17 pages and 9 figures</comments><doi>10.1016/j.jcp.2019.02.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel neural network architecture inspired by the
nonstandard form proposed by Beylkin, Coifman, and Rokhlin in [Communications
on Pure and Applied Mathematics, 44(2), 141-183]. The nonstandard form is a
highly effective wavelet-based compression scheme for linear integral
operators. In this work, we first represent the matrix-vector product algorithm
of the nonstandard form as a linear neural network where every scale of the
multiresolution computation is carried out by a locally connected linear
sub-network. In order to address nonlinear problems, we propose an extension,
called BCR-Net, by replacing each linear sub-network with a deeper and more
powerful nonlinear one. Numerical results demonstrate the efficiency of the new
architecture by approximating nonlinear maps that arise in homogenization
theory and stochastic computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08829</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08829</id><created>2018-10-20</created><updated>2018-10-25</updated><authors><author><keyname>Xian</keyname><forenames>Yin</forenames></author><author><keyname>Gu</keyname><forenames>Hanlin</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Xuhui</forenames></author><author><keyname>Yao</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author></authors><title>Data-Driven Tight Frame for Cryo-EM Image Denoising and Conformational
  Classification</title><categories>stat.CO eess.IV</categories><comments>2018 IEEE Global Signal and Information Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cryo-electron microscope (cryo-EM) is increasingly popular these years.
It helps to uncover the biological structures and functions of macromolecules.
In this paper, we address image denoising problem in cryo-EM. Denoising the
cryo-EM images can help to distinguish different molecular conformations and
improve three dimensional reconstruction resolution. We introduce the use of
data-driven tight frame (DDTF) algorithm for cryo-EM image denoising. The DDTF
algorithm is closely related to the dictionary learning. The advantage of DDTF
algorithm is that it is computationally efficient, and can well identify the
texture and shape of images without using large data samples. Experimental
results on cryo-EM image denoising and conformational classification
demonstrate the power of DDTF algorithm for cryo-EM image denoising and
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08855</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08855</id><created>2018-10-20</created><authors><author><keyname>Beale</keyname><forenames>Kevin</forenames><affiliation>Georgia Institute of Technology</affiliation></author><author><keyname>Chen</keyname><forenames>Jianbo</forenames><affiliation>Rice University</affiliation></author><author><keyname>Kelly</keyname><forenames>Kevin F.</forenames><affiliation>Rice University</affiliation></author><author><keyname>Romberg</keyname><forenames>Justin</forenames><affiliation>Georgia Institute of Technology</affiliation></author></authors><title>A Hardware Realization of Superresolution Combining Random Coding and
  Blurring</title><categories>eess.IV</categories><comments>12 pages, 13 figures, submitted to IEEE Transactions on Computational
  Imaging</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resolution enhancements are often desired in imaging applications where
high-resolution sensor arrays are difficult to obtain. Many computational
imaging methods have been proposed to encode high-resolution scene information
on low-resolution sensors by cleverly modulating light from the scene before it
hits the sensor. These methods often require movement of some portion of the
imaging apparatus or only acquire images up to the resolution of a modulating
element. Here a technique is presented for resolving beyond the resolutions of
both a pointwise-modulating mask element and a sensor array through the
introduction of a controlled blur into the optical pathway. The analysis
contains an intuitive and exact expression for the overall superresolvability
of the system, and arguments are presented to explain how the combination of
random coding and blurring makes the superresolution problem well-posed.
Experimental results demonstrate that a resolution enhancement of approximately
$4\times$ is possible in practice using standard optical components, without
mechanical motion of the imaging apparatus, and without any a priori
assumptions on scene structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08875</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08875</id><created>2018-10-20</created><authors><author><keyname>Warrick</keyname><forenames>Philip</forenames></author><author><keyname>Homsi</keyname><forenames>Masun Nabhan</forenames></author></authors><title>Sleep Arousal Detection from Polysomnography using the Scattering
  Transform and Recurrent Neural Networks</title><categories>cs.LG eess.SP stat.ML</categories><comments>Computing in Cardiology 2018, 4 pages and 5 figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Sleep disorders are implicated in a growing number of health problems. In
this paper, we present a signal-processing/machine learning approach to
detecting arousals in the multi-channel polysomnographic recordings of the
Physionet/CinC Challenge2018 dataset.
  Methods: Our network architecture consists of two components. Inputs were
presented to a Scattering Transform (ST) representation layer which fed a
recurrent neural network for sequence learning using three layers of Long
Short-Term Memory (LSTM). The STs were calculated for each signal with
downsampling parameters chosen to give approximately 1 s time resolution,
resulting in an eighteen-fold data reduction. The LSTM layers then operated at
this downsampled rate.
  Results: The proposed approach detected arousal regions on the 10% random
sample of the hidden test set with an AUROC of 88.0% and an AUPRC of 42.1%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08879</identifier>
 <datestamp>2019-01-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08879</id><created>2018-10-20</created><updated>2019-01-19</updated><authors><author><keyname>Ouyang</keyname><forenames>Chongjun</forenames></author><author><keyname>Ou</keyname><forenames>Zeliang</forenames></author><author><keyname>Yang</keyname><forenames>Hongwen</forenames></author></authors><title>Optimal Transmit Antenna Selection Algorithm in Massive MIMOME Channels</title><categories>eess.SP</categories><comments>This paper will be presented at IEEE Wireless Communications and
  Networking Conference (WCNC), 2019</comments><msc-class>14J60 (Primary) 14F05, 14J26 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the transmit antenna selection in massive multiple-input
multiple-output (MIMO) system over wiretap channel. The transmitter, equipped
with a large-scale antenna array whose size is much larger than that of the
eavesdropper and legitimate receiver, selects a subset of antennas to transmit
messages. An branch-and-bound (BAB) search based algorithm for antenna
selection in independent and identical Rayleigh flat fading channel is proposed
to maximize the secrecy capacity between the transmitter and the legitimate
receiver when the transmit power is equally allocated into the active antennas.
Furthermore, the proposed algorithm is separately applied to two scenarios
which is based on whether the channel side information of the eavesdropper
(CSIE) is available. Simulation results show that the proposed algorithm has
the same performance as the exhaustive search under both scenarios but with
much lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08906</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08906</id><created>2018-10-21</created><authors><author><keyname>Xu</keyname><forenames>Shaofu</forenames></author><author><keyname>Zou</keyname><forenames>Xiuting</forenames></author><author><keyname>Ma</keyname><forenames>Bowen</forenames></author><author><keyname>Chen</keyname><forenames>Jianping</forenames></author><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Zou</keyname><forenames>Weiwen</forenames></author></authors><title>Analog-to-digital conversion revolutionized by deep learning</title><categories>eess.SP physics.app-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the bridge between the analog world and digital computers,
analog-to-digital converters are generally used in modern information systems
such as radar, surveillance, and communications. For the configuration of
analog-to-digital converters in future high-frequency broadband systems, we
introduce a revolutionary architecture that adopts deep learning technology to
overcome tradeoffs between bandwidth, sampling rate, and accuracy. A photonic
front-end provides broadband capability for direct sampling and speed
multiplication. Trained deep neural networks learn the patterns of system
defects, maintaining high accuracy of quantized data in a succinct and adaptive
manner. Based on numerical and experimental demonstrations, we show that the
proposed architecture outperforms state-of-the-art analog-to-digital
converters, confirming the potential of our approach in future
analog-to-digital converter design and performance enhancement of future
information systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08940</identifier>
 <datestamp>2019-12-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08940</id><created>2018-10-21</created><updated>2019-12-18</updated><authors><author><keyname>Jang</keyname><forenames>Hyeryung</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Training Dynamic Exponential Family Models with Causal and Lateral
  Dependencies for Generalized Neuromorphic Computing</title><categories>cs.LG cs.IT eess.SP math.IT stat.ML</categories><comments>Published in IEEE ICASSP 2019. Author's Accepted Manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromorphic hardware platforms, such as Intel's Loihi chip, support the
implementation of Spiking Neural Networks (SNNs) as an energy-efficient
alternative to Artificial Neural Networks (ANNs). SNNs are networks of neurons
with internal analogue dynamics that communicate by means of binary time
series. In this work, a probabilistic model is introduced for a generalized
set-up in which the synaptic time series can take values in an arbitrary
alphabet and are characterized by both causal and instantaneous statistical
dependencies. The model, which can be considered as an extension of exponential
family harmoniums to time series, is introduced by means of a hybrid
directed-undirected graphical representation. Furthermore, distributed learning
rules are derived for Maximum Likelihood and Bayesian criteria under the
assumption of fully observed time series in the training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.08962</identifier>
 <datestamp>2019-07-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.08962</id><created>2018-10-21</created><updated>2019-07-21</updated><authors><author><keyname>Shi</keyname><forenames>Xin</forenames></author><author><keyname>Qiu</keyname><forenames>Robert</forenames></author><author><keyname>Ling</keyname><forenames>Zenan</forenames></author><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>Yang</keyname><forenames>Haosen</forenames></author><author><keyname>He</keyname><forenames>Xing</forenames></author></authors><title>Spatio-Temporal Correlation Analysis of Online Monitoring Data for
  Anomaly Detection and Location in Distribution Networks</title><categories>eess.SP</categories><comments>11 pages, 16 figures, Accepted by IEEE Trans on Smart Grid. arXiv
  admin note: text overlap with arXiv:1801.01669</comments><doi>10.1109/TSG.2019.2929219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The online monitoring data in distribution networks contain rich information
on the running states of the networks. By leveraging the data, this paper
proposes a spatio-temporal correlation analysis approach for anomaly detection
and location in distribution networks. First, spatio-temporal matrix for each
feeder line in a distribution network is formulated and the spectrum of its
covariance matrix is analyzed. The spectrum is complex and exhibits two
aspects: 1) bulk, which arises from random noise or fluctuations and 2) spikes,
which represents factors caused by anomaly signals or fault disturbances. Then,
by connecting the estimation of the number of factors to the limiting empirical
spectral density of covariance matrices of residuals, the spatio-temporal
parameters are accurately estimated, during which free random variable
techniques are used. Based on the estimators, anomaly indicators are designed
to detect and locate the anomalies by exploring the variations of
spatio-temporal correlations in the data. The proposed approach is sensitive to
the anomalies and robust to random fluctuations, which makes it possible for
detecting early anomalies and reducing false alarming rate. Case studies on
both synthetic data and real-world online monitoring data verify the
effectiveness and advantages of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09050</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09050</id><created>2018-10-21</created><updated>2019-02-19</updated><authors><author><keyname>Wang</keyname><forenames>Yun</forenames></author><author><keyname>Li</keyname><forenames>Juncheng</forenames></author><author><keyname>Metze</keyname><forenames>Florian</forenames></author></authors><title>A Comparison of Five Multiple Instance Learning Pooling Functions for
  Sound Event Detection with Weak Labeling</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sound event detection (SED) entails two subtasks: recognizing what types of
sound events are present in an audio stream (audio tagging), and pinpointing
their onset and offset times (localization). In the popular multiple instance
learning (MIL) framework for SED with weak labeling, an important component is
the pooling function. This paper compares five types of pooling functions both
theoretically and experimentally, with special focus on their performance of
localization. Although the attention pooling function is currently receiving
the most attention, we find the linear softmax pooling function to perform the
best among the five. Using this pooling function, we build a neural network
called TALNet. It is the first system to reach state-of-the-art audio tagging
performance on Audio Set, while exhibiting strong localization performance on
the DCASE 2017 challenge at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09052</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09052</id><created>2018-10-21</created><updated>2019-02-19</updated><authors><author><keyname>Wang</keyname><forenames>Yun</forenames></author><author><keyname>Metze</keyname><forenames>Florian</forenames></author></authors><title>Connectionist Temporal Localization for Sound Event Detection with
  Sequential Labeling</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on sound event detection (SED) with weak labeling has mostly focused
on presence/absence labeling, which provides no temporal information at all
about the event occurrences. In this paper, we consider SED with sequential
labeling, which specifies the temporal order of the event boundaries. The
conventional connectionist temporal classification (CTC) framework, when
applied to SED with sequential labeling, does not localize long events well due
to a &quot;peak clustering&quot; problem. We adapt the CTC framework and propose
connectionist temporal localization (CTL), which successfully solves the
problem. Evaluation on a subset of Audio Set shows that CTL closes a third of
the gap between presence/ absence labeling and strong labeling, demonstrating
the usefulness of the extra temporal information in sequential labeling. CTL
also makes it easy to combine sequential labeling with presence/absence
labeling and strong labeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09067</identifier>
 <datestamp>2018-10-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09067</id><created>2018-10-21</created><updated>2018-10-24</updated><authors><author><keyname>Du</keyname><forenames>Zhihao</forenames></author><author><keyname>Zhang</keyname><forenames>Xueliang</forenames></author><author><keyname>Han</keyname><forenames>Jiqing</forenames></author></authors><title>Investigation of Monaural Front-End Processing for Robust ASR without
  Retraining or Joint-Training</title><categories>cs.SD cs.MM eess.AS</categories><comments>5 pages, 0 figures, 4 tables, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, monaural speech separation has been formulated as a
supervised learning problem, which has been systematically researched and shown
the dramatical improvement of speech intelligibility and quality for human
listeners. However, it has not been well investigated whether the methods can
be employed as the front-end processing and directly improve the performance of
a machine listener, i.e., an automatic speech recognizer, without retraining or
joint-training the acoustic model. In this paper, we explore the effectiveness
of the independent front-end processing for the multi-conditional trained ASR
on the CHiME-3 challenge. We find that directly feeding the enhanced features
to ASR can make 36.40% and 11.78% relative WER reduction for the GMM-based and
DNN-based ASR respectively. We also investigate the affect of noisy phase and
generalization ability under unmatched noise condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09078</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09078</id><created>2018-10-22</created><authors><author><keyname>Balemarthy</keyname><forenames>Siddhardha</forenames></author><author><keyname>Sajjanhar</keyname><forenames>Atul</forenames></author><author><keyname>Zheng</keyname><forenames>James Xi</forenames></author></authors><title>Our Practice Of Using Machine Learning To Recognize Species By Voice</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the technology is advancing, audio recognition in machine learning is
improved as well. Research in audio recognition has traditionally focused on
speech. Living creatures (especially the small ones) are part of the whole
ecosystem, monitoring as well as maintaining them are important tasks. Species
such as animals and birds are tending to change their activities as well as
their habitats due to the adverse effects on the environment or due to other
natural or man-made calamities. For those in far deserted areas, we will not
have any idea about their existence until we can continuously monitor them.
Continuous monitoring will take a lot of hard work and labor. If there is no
continuous monitoring, then there might be instances where endangered species
may encounter dangerous situations. The best way to monitor those species are
through audio recognition. Classifying sound can be a difficult task even for
humans. Powerful audio signals and their processing techniques make it possible
to detect audio of various species. There might be many ways wherein audio
recognition can be done. We can train machines either by pre-recorded audio
files or by recording them live and detecting them. The audio of species can be
detected by removing all the background noise and echoes. Smallest sound is
considered as a syllable. Extracting various syllables is the process we are
focusing on which is known as audio recognition in terms of Machine Learning
(ML).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09082</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09082</id><created>2018-10-22</created><authors><author><keyname>Gao</keyname><forenames>Xuanxuan</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Li</keyname><forenames>Geoffrey Ye</forenames></author></authors><title>ComNet: Combination of Deep Learning and Expert Knowledge in OFDM
  Receivers</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a model-driven deep learning (DL) approach that
combines DL with the expert knowledge to replace the existing orthogonal
frequency-division multiplexing (OFDM) receiver in wireless communications.
Different from the data-driven fully connected deep neural network (FC-DNN)
method, we adopt the block-by-block signal processing method that divides the
receiver into channel estimation subnet and signal detection subnet. Each
subnet is constructed by a DNN and uses the existing simple and traditional
solution as initialization. The proposed model-driven DL receiver offers more
accurate channel estimation comparing with the linear minimum mean-squared
error (LMMSE) method and exhibits higher data recovery accuracy comparing with
the existing methods and FC-DNN. Simulation results further demonstrate the
robustness of the proposed approach in terms of signal-to-noise ratio and its
superiority to the FC-DNN approach in the computational complexities or the
memory usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09119</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09119</id><created>2018-10-22</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Lei</keyname><forenames>Mengying</forenames></author><author><keyname>Cui</keyname><forenames>Weigang</forenames></author><author><keyname>Guo</keyname><forenames>Yuzhu</forenames></author><author><keyname>Wei</keyname><forenames>Hua-Liang</forenames></author></authors><title>A Parametric Time Frequency-Conditional Granger Causality Method Using
  Ultra-regularized Orthogonal Least Squares and Multiwavelets for Dynamic
  Connectivity Analysis in EEGs</title><categories>eess.SP q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: This study proposes a new parametric TF (time frequency) CGC
(conditional Granger causality) method for high precision connectivity analysis
over time and frequency in multivariate coupling nonstationary systems, and
applies it to scalp and source EEG signals to reveal dynamic interaction
patterns in oscillatory neocortical sensorimotor networks. Methods: The Geweke
spectral measure is combined with the TVARX (time varying autoregressive with
exogenous input) modelling approach, which uses multiwavelets and ultra
regularized orthogonal least squares (UROLS) algorithm aided by APRESS
(adjustable prediction error sum of squares), to obtain high resolution time
varying CGC representations. The UROLS APRESS algorithm, which adopts both the
regularization technique and the ultra least squares criterion to measure not
only the signal data themselves but also the weak derivatives of them, is a
novel powerful method in constructing time varying models with good
generalization performance, and can accurately track smooth and fast changing
causalities. The generalized measurement based on CGC decomposition is able to
eliminate indirect influences in multivariate systems. Results: The proposed
method is validated on two simulations and then applied to multichannel motor
imagery (MI) EEG signals at scalp and source level, where the predicted
distributions are well recovered with high TF precision, and the detected
connectivity patterns of MI EEG data are physiologically and anatomically
interpretable and yield new insights into the dynamical organization of
oscillatory cortical networks. Conclusion: Experimental results confirm the
effectiveness of the proposed TF CGC method in tracking rapidly varying
causalities of EEG based oscillatory networks. Significance: The novel TF CGC
method is expected to provide important information of neural mechanisms of
perception and cognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09133</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09133</id><created>2018-10-22</created><authors><author><keyname>Koizumi</keyname><forenames>Yuma</forenames></author><author><keyname>Saito</keyname><forenames>Shoichiro</forenames></author><author><keyname>Kawachi</keyname><forenames>Hisashi Uematsum Yuta</forenames></author><author><keyname>Harada</keyname><forenames>Noboru</forenames></author></authors><title>Unsupervised Detection of Anomalous Sound based on Deep Learning and the
  Neyman-Pearson Lemma</title><categories>stat.ML cs.LG cs.SD eess.AS</categories><comments>IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2018</comments><doi>10.1109/TASLP.2018.2877258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel optimization principle and its implementation for
unsupervised anomaly detection in sound (ADS) using an autoencoder (AE). The
goal of unsupervised-ADS is to detect unknown anomalous sound without training
data of anomalous sound. Use of an AE as a normal model is a state-of-the-art
technique for unsupervised-ADS. To decrease the false positive rate (FPR), the
AE is trained to minimize the reconstruction error of normal sounds and the
anomaly score is calculated as the reconstruction error of the observed sound.
Unfortunately, since this training procedure does not take into account the
anomaly score for anomalous sounds, the true positive rate (TPR) does not
necessarily increase. In this study, we define an objective function based on
the Neyman-Pearson lemma by considering ADS as a statistical hypothesis test.
The proposed objective function trains the AE to maximize the TPR under an
arbitrary low FPR condition. To calculate the TPR in the objective function, we
consider that the set of anomalous sounds is the complementary set of normal
sounds and simulate anomalous sounds by using a rejection sampling algorithm.
Through experiments using synthetic data, we found that the proposed method
improved the performance measures of ADS under low FPR conditions. In addition,
we confirmed that the proposed method could detect anomalous sounds in real
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09137</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09137</id><created>2018-10-22</created><authors><author><keyname>Koizumi</keyname><forenames>Yuma</forenames></author><author><keyname>Niwa</keyname><forenames>Kenta</forenames></author><author><keyname>Hioka</keyname><forenames>Yusuke</forenames></author><author><keyname>Kobayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>Haneda</keyname><forenames>Yoichi</forenames></author></authors><title>DNN-based Source Enhancement to Increase Objective Sound Quality
  Assessment Score</title><categories>stat.ML cs.LG cs.SD eess.AS</categories><journal-ref>IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  Vol.26, Issue.10, 2018</journal-ref><doi>10.1109/TASLP.2018.2842156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a training method for deep neural network (DNN)-based source
enhancement to increase objective sound quality assessment (OSQA) scores such
as the perceptual evaluation of speech quality (PESQ). In many conventional
studies, DNNs have been used as a mapping function to estimate time-frequency
masks and trained to minimize an analytically tractable objective function such
as the mean squared error (MSE). Since OSQA scores have been used widely for
sound-quality evaluation, constructing DNNs to increase OSQA scores would be
better than using the minimum-MSE to create high-quality output signals.
However, since most OSQA scores are not analytically tractable, \textit{i.e.},
they are black boxes, the gradient of the objective function cannot be
calculated by simply applying back-propagation. To calculate the gradient of
the OSQA-based objective function, we formulated a DNN optimization scheme on
the basis of \textit{black-box optimization}, which is used for training a
computer that plays a game. For a black-box-optimization scheme, we adopt the
policy gradient method for calculating the gradient on the basis of a sampling
algorithm. To simulate output signals using the sampling algorithm, DNNs are
used to estimate the probability density function of the output signals that
maximize OSQA scores. The OSQA scores are calculated from the simulated output
signals, and the DNNs are trained to increase the probability of generating the
simulated output signals that achieve high OSQA scores. Through several
experiments, we found that OSQA scores significantly increased by applying the
proposed method, even though the MSE was not minimized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09249</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09249</id><created>2018-10-16</created><authors><author><keyname>Xochicale</keyname><forenames>Miguel</forenames></author><author><keyname>Baber</keyname><forenames>Chirs</forenames></author></authors><title>Strengths and Weaknesses of Recurrent Quantification Analysis in the
  context of Human-Humanoid Interaction</title><categories>eess.SP</categories><comments>Preprint</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Movement variability is defined as the variations that occur in motor
performance across multiple repetitions of a task and such behaviour is an
inherent feature within and between each persons' movement. Quantifying
movement variability is still an open problem, particularly when traditional
methods in time domain and frequency domain fail to detect tiny modulations in
frequency or phase for time series. We therefore consider methodologies from
nonlinear dynamics such as reconstructed state space (RSS), uniform time-delay
embedding (UTDE), recurrence plots (RPs) and recurrence quantification analysis
(RQA) metrics. These algorithms require time series measured with sensors that
provide well sampled data with little noise. However, for this study, we are
interested in the analysis of data collected through wearable inertial sensors
(IMUs) and its post-processing effects in each of the nonlinear dynamics
methodologies. Twenty right-handed healthy participants (mean and standard
deviation (SD) age of mean=19.8 (SD=1.39)) performed an experiment in
human-humanoid imitation activities (H-H-I). Then IMUs were attached to the
participants and the humanoid robot performed simple vertical and horizontal
arm movements in normal and faster speed. With four window lengths and three
levels of smoothed time series, we found visual differences in the patterns of
RSSs and RPs. Then using metrics of RQA, we find out that the type of movements
and the level of smoothness affects those metrics. In particular, we found that
entropy values from RQA were well distributed and presenting variation in all
the conditions for time series. Hence, we demonstrated the potential of
nonlinear techniques to quantify human movement variability in the context of
H-H-I can enhance the development of better diagnostic tools for various
applications rehabilitation, sport science or for new forms of human-robot
interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09253</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09253</id><created>2018-10-17</created><authors><author><keyname>Tang</keyname><forenames>Hong</forenames></author><author><keyname>Chen</keyname><forenames>Huaming</forenames></author><author><keyname>Li</keyname><forenames>Ting</forenames></author><author><keyname>Zhong</keyname><forenames>Mingjun</forenames></author></authors><title>Classification of normal/abnormal heart sound recordings based on
  multi-domain features and back propagation neural network</title><categories>eess.SP cs.LG stat.ML</categories><comments>4 pages</comments><journal-ref>2016 Computing in Cardiology Conference (CinC), IEEE, Vancouver,
  BC, 2016, pp. 593-596</journal-ref><doi>10.23919/CIC.2016.7868812</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to classify a single PCG recording as normal or abnormal for
computer-aided diagnosis. The proposed framework for this challenge has four
steps: preprocessing, feature extraction, training and validation. In the
preprocessing step, a recording is segmented into four states, i.e., the first
heart sound, systolic interval, the second heart sound, and diastolic interval
by the Springer Segmentation algorithm. In the feature extraction step, the
authors extract 324 features from multi-domains to perform classification. A
back propagation neural network is used as predication model. The optimal
threshold for distinguishing normal and abnormal is determined by the
statistics of model output for both normal and abnormal. The performance of the
proposed predictor tested by the six training sets is sensitivity 0.812 and
specificity 0.860 (overall accuracy is 0.836). However, the performance reduces
to sensitivity 0.807 and specificity 0.829 (overall accuracy is 0.818) for the
hidden test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09261</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09261</id><created>2018-10-18</created><authors><author><keyname>Ruiz</keyname><forenames>Francisco J. R.</forenames></author><author><keyname>Valera</keyname><forenames>Isabel</forenames></author><author><keyname>Svensson</keyname><forenames>Lennart</forenames></author><author><keyname>Perez-Cruz</keyname><forenames>Fernando</forenames></author></authors><title>Infinite Factorial Finite State Machine for Blind Multiuser Channel
  Estimation</title><categories>eess.SP cs.IT cs.LG math.IT stat.ML</categories><comments>15 pages, 15 figures</comments><journal-ref>IEEE Transactions on Cognitive Communications and Networking, June
  2018, Vol 2, Issue 2, pages 177-191</journal-ref><doi>10.1109/TCCN.2018.2790976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New communication standards need to deal with machine-to-machine
communications, in which users may start or stop transmitting at any time in an
asynchronous manner. Thus, the number of users is an unknown and time-varying
parameter that needs to be accurately estimated in order to properly recover
the symbols transmitted by all users in the system. In this paper, we address
the problem of joint channel parameter and data estimation in a multiuser
communication channel in which the number of transmitters is not known. For
that purpose, we develop the infinite factorial finite state machine model, a
Bayesian nonparametric model based on the Markov Indian buffet that allows for
an unbounded number of transmitters with arbitrary channel length. We propose
an inference algorithm that makes use of slice sampling and particle Gibbs with
ancestor sampling. Our approach is fully blind as it does not require a prior
channel estimation step, prior knowledge of the number of transmitters, or any
signaling information. Our experimental results, loosely based on the LTE
random access channel, show that the proposed approach can effectively recover
the data-generating process for a wide range of scenarios, with varying number
of transmitters, number of receivers, constellation order, channel length, and
signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09273</identifier>
 <datestamp>2018-10-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09273</id><created>2018-10-22</created><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Petruskov&#xe1;</keyname><forenames>Tereza</forenames></author><author><keyname>&#x160;&#xe1;lek</keyname><forenames>Martin</forenames></author><author><keyname>Linhart</keyname><forenames>Pavel</forenames></author></authors><title>Automatic acoustic identification of individual animals: Improving
  generalisation across species and recording conditions</title><categories>cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many animals emit vocal sounds which, independently from the sounds'
function, embed some individually-distinctive signature. Thus the automatic
recognition of individuals by sound is a potentially powerful tool for zoology
and ecology research and practical monitoring. Here we present a general
automatic identification method, that can work across multiple animal species
with various levels of complexity in their communication systems. We further
introduce new analysis techniques based on dataset manipulations that can
evaluate the robustness and generality of a classifier. By using these
techniques we confirmed the presence of experimental confounds in situations
resembling those from past studies. We introduce data manipulations that can
reduce the impact of these confounds, compatible with any classifier. We
suggest that assessment of confounds should become a standard part of future
studies to ensure they do not report over-optimistic results. We provide
annotated recordings used for analyses along with this study and we call for
dataset sharing to be a common practice to enhance development of methods and
comparisons of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09436</identifier>
 <datestamp>2018-10-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09436</id><created>2018-10-19</created><authors><author><keyname>Haerinia</keyname><forenames>Mohammad</forenames></author><author><keyname>Afjei</keyname><forenames>Ebrahim S.</forenames></author></authors><title>Resonant Inductive Coupling as a Potential Means for Wireless Power
  Transfer to Printed Spiral Coil</title><categories>eess.SP</categories><comments>Volume 16 / 2016 - Edition : 2</comments><report-no>Article number:16.2.10</report-no><msc-class>65-05</msc-class><journal-ref>Journal of Electrical Engineering,ISSN: 1582-4594, www.jee.ro,
  2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an inductive coupled wireless power transfer system that
analyses the relationship between induced voltage and distance of resonating
inductance in a printed circuit spiral coils. The resonant frequency produced
by the circuit model of the proposed receiving and transmitting coils are
analyzed by simulation and laboratory experiment. The outcome of the two
results is compared to verify the validity of the proposed inductive coupling
system. Experimental measurements are consistent with simulations over a range
of frequencies spanning the resonance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09444</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09444</id><created>2018-10-21</created><authors><author><keyname>Shimobaba</keyname><forenames>Tomoyoshi</forenames></author><author><keyname>Takahashi</keyname><forenames>Takayuki</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yota</forenames></author><author><keyname>Endo</keyname><forenames>Yutaka</forenames></author><author><keyname>Shiraki</keyname><forenames>Atsushi</forenames></author><author><keyname>Nishitsuji</keyname><forenames>Takashi</forenames></author><author><keyname>Hoshikawa</keyname><forenames>Naoto</forenames></author><author><keyname>Kakue</keyname><forenames>Takashi</forenames></author><author><keyname>Ito</keyname><forenames>Tomoyosh</forenames></author></authors><title>Digital holographic particle volume reconstruction using a deep neural
  network</title><categories>eess.IV cs.CV</categories><doi>10.1364/AO.58.001900</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a particle volume reconstruction directly from an in-line
hologram using a deep neural network. Digital holographic volume reconstruction
conventionally uses multiple diffraction calculations to obtain sectional
reconstructed images from an in-line hologram, followed by detection of the
lateral and axial positions, and the sizes of particles by using focus metrics.
However, the axial resolution is limited by the numerical aperture of the
optical system, and the processes are time-consuming. The method proposed here
can simultaneously detect the lateral and axial positions, and the particle
sizes via a deep neural network (DNN). We numerically investigated the
performance of the DNN in terms of the errors in the detected positions and
sizes. The calculation time is faster than conventional diffracted-based
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09479</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09479</id><created>2018-10-22</created><authors><author><keyname>N.</keyname><forenames>Bharath Raj</forenames></author><author><keyname>N</keyname><forenames>Venkateswaran</forenames></author></authors><title>Single Image Haze Removal using a Generative Adversarial Network</title><categories>cs.CV eess.IV</categories><comments>13 Pages, 8 Figures and 2 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single image haze removal is an under constrained problem due to lack of
depth information. It is usually performed by estimating the transmission map
directly or by using a prior. Other methods use predictive models to estimate
the transmission map and perform guided dehazing. In this paper, we propose a
conditional GAN, that can directly remove haze from an image, without
explicitly estimating transmission map or haze relevant features. We find that,
only one module, comprising of the generator and discriminator is enough. We
replaced the classic U-Net with the Tiramisu model, yielding much higher
parameter efficiency and performance. We also observe that the performance
during inference is dependent on the diversity of the dataset used for
training. Experiments on synthetic and real world hazy images prove that our
model performs competitively with the state of the art models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09517</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09517</id><created>2018-10-02</created><authors><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Liu</keyname><forenames>Boxiao</forenames></author><author><keyname>Li</keyname><forenames>Yongfu</forenames></author><author><keyname>Wang</keyname><forenames>Guoxing</forenames></author><author><keyname>Lian</keyname><forenames>Yong</forenames></author></authors><title>A High Accuracy and High Sensitivity System Architecture for Electrical
  Impedance Tomography System</title><categories>physics.med-ph cs.ET eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A high accuracy and high sensitivity system architecture is proposed for the
read-out circuit of electrical impedance tomography system-on-chip. The
switched ratiometric technique is applied in the proposed architecture. The
proposed system architecture minimizes the device noise by processing signals
from both read-out electrodes and the stimulus. The quantized signals are
post-processed in the digital processing unit for proper signal demodulation
and impedance ratio calculation. Our proposed architecture improves the
sensitivity of the read-out circuit, cancels out the gain fluctuations in the
system, and overcomes the effects of motion artifacts on measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09698</identifier>
 <datestamp>2019-05-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09698</id><created>2018-10-23</created><updated>2019-05-18</updated><authors><author><keyname>Huang</keyname><forenames>Changcun</forenames></author></authors><title>The Interpretation of Linear Prediction by Interpolation Framework and
  Several following Results</title><categories>eess.SP</categories><comments>v2-v3: typos corrected and Theorem 4 revised; v4: Lemma 1 and Theorem
  1 generalized; v5-v6: paper simplified and revised</comments><msc-class>65D05, 42A10, 42A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a general interpretation of Linear Prediction (LP) by
interpolation framework different from the perspective of statistics. This
interpretation is proved to be useful by several following results, such as:
The mechanism of widely used least square estimation of LP coefficients can be
explained more intuitively. In data modeling, LP coefficients cannot
distinguish signals spanned by the same interpolation bases. Two new general LP
constructive methods instead of least square estimation are presented with
their upper bounds of approximation error and some properties given; one is
based on DCT-1 and the other is based on difference operator. We also establish
the relationship between LP and Taylor series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09707</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09707</id><created>2018-10-23</created><authors><author><keyname>Pla</keyname><forenames>Pol del Aguila</forenames></author><author><keyname>Jald&#xe9;n</keyname><forenames>Joakim</forenames></author></authors><title>Cell detection on image-based immunoassays</title><categories>eess.IV q-bio.QM</categories><comments>5 pages, 2018 IEEE 15th International Symposium on Biomedical Imaging
  (ISBI 2018), Washington, DC, USA, April 4-7, 2018</comments><journal-ref>2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI
  2018), pp. 431-435, 2018</journal-ref><doi>10.1109/ISBI.2018.8363609</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell detection and counting in the image-based ELISPOT and Fluorospot
immunoassays is considered a bottleneck. The task has remained hard to
automatize, and biomedical researchers often have to rely on results that are
not accurate. Previously proposed solutions are heuristic, and data-based
solutions are subject to a lack of objective ground truth data. In this paper,
we analyze a partial differential equations model for ELISPOT, Fluorospot, and
assays of similar design. This leads us to a mathematical observation model for
the images generated by these assays. We use this model to motivate a
methodology for cell detection. Finally, we provide a real-data example that
suggests that this cell detection methodology and a human expert perform
comparably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09708</identifier>
 <datestamp>2019-12-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09708</id><created>2018-10-23</created><authors><author><keyname>Mirabilii</keyname><forenames>Daniele</forenames></author><author><keyname>Habets</keyname><forenames>Emanu&#xeb;l A. P.</forenames></author></authors><title>On the difference-to-sum power ratio of speech and wind noise based on
  the Corcos model</title><categories>eess.AS cs.SD eess.SP</categories><comments>5 pages, 3 figures, IEEE-ICSEE Eilat-Israel conference (special
  session)</comments><doi>10.1109/ICSEE.2018.8645977</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The difference-to-sum power ratio was proposed and used to suppress wind
noise under specific acoustic conditions. In this contribution, a general
formulation of the difference-to-sum power ratio associated with a mixture of
speech and wind noise is proposed and analyzed. In particular, it is assumed
that the complex coherence of convective turbulence can be modelled by the
Corcos model. In contrast to the work in which the power ratio was first
presented, the employed Corcos model holds for every possible air stream
direction and takes into account the lateral coherence decay rate. The obtained
expression is subsequently validated with real data for a dual microphone
set-up. Finally, the difference-to- sum power ratio is exploited as a spatial
feature to indicate the frame-wise presence of wind noise, obtaining improved
detection performance when compared to an existing multi-channel wind noise
detection approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09757</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09757</id><created>2018-10-23</created><authors><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Wang</keyname><forenames>Xiangdong</forenames></author><author><keyname>Long</keyname><forenames>Zhou</forenames></author><author><keyname>Tian</keyname><forenames>Tian</forenames></author><author><keyname>Gao</keyname><forenames>Mingming</forenames></author><author><keyname>Yun</keyname><forenames>Xiaoping</forenames></author><author><keyname>Qian</keyname><forenames>Yueliang</forenames></author><author><keyname>Li</keyname><forenames>Jintao</forenames></author></authors><title>Estimation of Spatial-Temporal Gait Parameters based on the Fusion of
  Inertial and Film-Pressure Signals</title><categories>eess.SP</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gait analysis (GA) has been widely used in physical activity monitoring and
clinical contexts, and the estimation of the spatial-temporal gait parameters
is of primary importance for GA. With the quick development of smart tiny
sensors, GA methods based on wearable devices have become more popular
recently. However, most existing wearable GA methods focus on data analysis
from inertial sensors. In this paper, we firstly present a two-foot-worn
in-shoe system (Gaitboter) based on low-cost, wearable and multimodal sensors'
fusion for GA, comprising an inertial sensor and eight film-pressure sensors
with each foot for gait raw data collection while walking. Secondly, a GA
algorithm for estimating the spatial-temporal parameters of gait is proposed.
The algorithm fully uses the fusion of two kinds of sensors' signals: inertial
sensor and film-pressure sensor, in order to estimate the spatial-temporal gait
parameters, such as stance phase, swing phase, double stance phase, cadence,
step time, stride time, stride length, velocity. Finally, to verify the
effectiveness of this system and algorithm of the paper, an experiment is
performed with 27 stoke patients from local hospital that the spatial-temporal
gait parameters obtained with the system and the algorithm are compared with a
GA tool used in medical laboratories. And the experimental results show that it
achieves very good consistency and correlation between the proposed system and
the compared GA tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09785</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09785</id><created>2018-10-23</created><authors><author><keyname>D&#xe9;fossez</keyname><forenames>Alexandre</forenames><affiliation>FAIR, PSL, SIERRA</affiliation></author><author><keyname>Zeghidour</keyname><forenames>Neil</forenames><affiliation>PSL, FAIR, LSCP</affiliation></author><author><keyname>Usunier</keyname><forenames>Nicolas</forenames><affiliation>FAIR</affiliation></author><author><keyname>Bottou</keyname><forenames>L&#xe9;on</forenames><affiliation>FAIR</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>DI-ENS, PSL, SIERRA</affiliation></author></authors><title>SING: Symbol-to-Instrument Neural Generator</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><proxy>ccsd</proxy><journal-ref>Conference on Neural Information Processing Systems (NIPS), Dec
  2018, Montr{\'e}al, Canada</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress in deep learning for audio synthesis opens the way to models
that directly produce the waveform, shifting away from the traditional paradigm
of relying on vocoders or MIDI synthesizers for speech or music generation.
Despite their successes, current state-of-the-art neural audio synthesizers
such as WaveNet and SampleRNN suffer from prohibitive training and inference
times because they are based on autoregressive models that generate audio
samples one at a time at a rate of 16kHz. In this work, we study the more
computationally efficient alternative of generating the waveform frame-by-frame
with large strides. We present SING, a lightweight neural audio synthesizer for
the original task of generating musical notes given desired instrument, pitch
and velocity. Our model is trained end-to-end to generate notes from nearly
1000 instruments with a single decoder, thanks to a new loss function that
minimizes the distances between the log spectrograms of the generated and
target waveforms. On the generalization task of synthesizing notes for pairs of
pitch and instrument not seen during training, SING produces audio with
significantly improved perceptual quality compared to a state-of-the-art
autoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is
about 32 times faster for training and 2, 500 times faster for inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09812</identifier>
 <datestamp>2018-10-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09812</id><created>2018-10-23</created><updated>2018-10-24</updated><authors><author><keyname>Liu</keyname><forenames>Fan</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author></authors><title>Hybrid Beamforming With Sub-arrayed MIMO Radar: Enabling Joint Sensing
  and Communication at mmWave Band</title><categories>eess.SP</categories><comments>5 pages, 2 figures, submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a beamforming design for dual-functional
radar-communication (DFRC) systems at the millimeter wave (mmWave) band, where
hybrid beamforming and sub-arrayed MIMO radar techniques are jointly exploited.
We assume that a base station (BS) is serving a user equipment (UE) located in
a Non-Line-of-Sight (NLoS) channel, which in the meantime actively detects
multiple targets located in a Line-of-Sight (LoS) channel. Given the optimal
communication beamformer and the desired radar beampattern, we propose to
design the analog and digital beamformers under non-convex constant-modulus
(CM) and power constraints, such that the weighted summation of the
communication and radar beamforming errors is minimized. The formulated
optimization problem can be decomposed into three subproblems, and is solved by
the alternating minimization approach. Numerical simulations verify the
feasibility of the proposed beamforming design, and show that our approach
offers a favorable performance tradeoff between sensing and communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09850</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09850</id><created>2018-10-20</created><authors><author><keyname>Salman</keyname><forenames>Tara</forenames></author><author><keyname>Badawy</keyname><forenames>Ahmed</forenames></author><author><keyname>Elfouly</keyname><forenames>Tarek M.</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Estimating the Number of Sources: An Efficient Maximization Approach</title><categories>cs.IT eess.SP math.IT</categories><comments>2015 International Wireless Communications and Mobile Computing
  Conference (IWCMC)</comments><doi>10.1109/IWCMC.2015.7289082</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the number of sources received by an antenna array have been well
known and investigated since the starting of array signal processing. Accurate
estimation of such parameter is critical in many applications that involve
prior knowledge of the number of received signals. Information theo- retic
approaches such as Akaikes information criterion (AIC) and minimum description
length (MDL) have been used extensively even though they are complex and show
bad performance at some stages. In this paper, a new algorithm for estimating
the number of sources is presented. This algorithm exploits the estimated
eigenvalues of the auto correlation coefficient matrix rather than the auto
covariance matrix, which is conventionally used, to estimate the number of
sources. We propose to use either of a two simply estimated decision
statistics, which are the moving increment and moving standard deviation as
metric to estimate the number of sources. Then process a simple calculation of
the increment or standard deviation of eigenvalues to find the number of
sources at the location of the maximum value. Results showed that our proposed
algorithms have a better performance in comparison to the popular and more
computationally expensive AIC and MDL at low SNR values and low number of
collected samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09907</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09907</id><created>2018-10-23</created><authors><author><keyname>Ali</keyname><forenames>Anum</forenames></author><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Linear Receivers in Non-stationary Massive MIMO Channels with Visibility
  Regions</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a massive MIMO system with large arrays, the channel becomes spatially
non-stationary. We study the impact of spatial non-stationarity characterized
by visibility regions (VRs) where the channel energy is significant on a
portion of the array. Relying on a channel model based on VRs, we provide
expressions of the signal-to-interference-plus-noise ratio (SINR) of conjugate
beamforming (CB) and zero-forcing (ZF) precoders. We also provide an
approximate deterministic equivalent of the SINR of ZF precoders. We identify
favorable and unfavorable multi-user configurations of the VRs and compare the
performance of both stationary and non-stationary channels through analysis and
numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09911</identifier>
 <datestamp>2019-06-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09911</id><created>2018-10-23</created><updated>2019-06-03</updated><authors><author><keyname>Gu</keyname><forenames>Yunjie</forenames></author><author><keyname>Li</keyname><forenames>Yitong</forenames></author><author><keyname>Green</keyname><forenames>Timothy C.</forenames></author></authors><title>Interpreting Frame Transformations as Diagonalization of Harmonic
  Transfer Functions</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of ac electrical systems can be performed via frame transformations
in the time-domain or via harmonic transfer functions (HTFs) in the
frequency-domain. The two approaches each have unique advantages but are hard
to reconcile because the coupling effect in the frequency-domain leads to
infinite dimensional HTF matrices that need to be truncated. This paper
explores the relation between the two representations and shows that applying a
similarity transformation to an HTF matrix creates a direct equivalence to a
frame transformation on the input-output signals. Under certain conditions,
such similarity transformations have a diagonalizing effect which, essentially,
reduces the HTF matrix order from infinity to two or one, making the matrix
tractable mathematically without truncation or approximation. This theory is
applied to a droop-controlled voltage source inverter as an illustrative
example. A stability criterion is derived in the frequency-domain which agrees
with the conventional state-space model but offers greater insights into the
mechanism of instability in terms of the negative damping (non-passivity) under
droop control. The paper not only establishes a unified view in theory but also
offers an effective practical tool for stability assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09929</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09929</id><created>2018-10-03</created><authors><author><keyname>Hassan</keyname><forenames>Hussein F.</forenames></author><author><keyname>Abou-Loukh</keyname><forenames>Sadiq J.</forenames></author><author><keyname>Ibraheem</keyname><forenames>Ibraheem Kasim</forenames></author></authors><title>Teleoperated Robotic Arm Movement Using EMG Signal With Wearable MYO
  Armband</title><categories>eess.SP cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this research is to move the robotic arm (5DoF) in
real-time, based on the surface Electromyography (sEMG) signals, as obtained
from the wireless Myo gesture armband to distinguish seven hand movements. The
sEMG signals are biomedical signals that estimate and record the electrical
signals produced in muscles through their contraction and relaxation,
representing neuromuscular activities. Therefore, controlling the robotic arm
via the muscles of the human arm using sEMG signals is considered to be one of
the most significant methods. The wireless Myo gesture armband is used to
record sEMG signals from the forearm. In order to analyze these signals, the
pattern recognition system is employed, which consists of three main parts:
segmentation, feature extraction, and classification. Overlap technique is
chosen for segmenting part of the signal. Six time domain features (MAV, WL,
RMS, AR, ZC, and SSC) are extracted from each segment. The classifiers (SVM,
LDA, and KNN) are employed to enable comparison between them in order to obtain
optimum accuracy of the system. The results show that the SVM achieves higher
system accuracy at 96.57 %, compared to LDA reaching 96.01 %, and 92.67 %
accuracy achieved by KNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09933</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09933</id><created>2018-10-02</created><authors><author><keyname>Chowdhury</keyname><forenames>Anirban</forenames></author><author><keyname>Sarkar</keyname><forenames>Siddhartha</forenames></author><author><keyname>Das</keyname><forenames>Saikat</forenames></author><author><keyname>Bhaumik</keyname><forenames>Subhasis</forenames></author></authors><title>RFID based Automated Car Theft Detection and Arresting system</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we are presenting a new approach to car theft detection and
arresting system using RFID technology. The main purpose of this paper is to
establish the concept and the architecture of the whole system. The system
which we are able to develop still now is basically a demonstration model or a
prototype of the system. The key thing in this system is the passive RFID tags
which will be hidden inside the car and act as a unique identification number
for the car. The information of all such tags will be maintained by a
centralized server. In case of any car being theft, a report should be logged
into the server by an authorized user and a network of RFID scanners installed
in different check post, traffic signal or toll plazas in the city will search
for the reported tag. Once it is found necessary security system would be
activated for arresting the car.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09937</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09937</id><created>2018-10-20</created><authors><author><keyname>Salman</keyname><forenames>Tara</forenames></author><author><keyname>Badawy</keyname><forenames>Ahmed</forenames></author><author><keyname>Elfouly</keyname><forenames>Tarek M.</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>Non-data-aided SNR Estimation for QPSK Modulation in AWGN Channel</title><categories>eess.SP</categories><comments>2014 IEEE 10th International Conference on Wireless and Mobile
  Computing, Networking and Communications (WiMob)</comments><doi>10.1109/WiMOB.2014.6962233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal-to-noise ratio (SNR) estimation is an important parameter that is
required in any receiver or communication systems. It can be computed either by
a pilot signal data-aided approach in which the transmitted signal would be
known to the receiver, or without any knowledge of the transmitted signal,
which is a non-data-aided (NDA) estimation approach. In this paper, a NDA SNR
estimation algorithm for QPSK signal is proposed. The proposed algorithm
modifies the existing Signal- to-Variation Ratio (SVR) SNR estimation algorithm
in the aim to reduce its bias and mean square error in case of negative SNR
values at low number of samples of it. We first present the existing SVR
algorithm and then show the mathematical derivation of the new NDA algorithm.
In addition, we compare our algorithm to two baselines estimation methods,
namely the M2M4 and SVR algorithms, using different test cases. Those test
cases include low SNR values, extremely high SNR values and low number of
samples. Results showed that our algorithm had a better performance compared to
second and fourth moment estimation (M2M4) and original SVR algorithms in terms
of normalized mean square error (NMSE) and bias estimation while keeping almost
the same complexity as the original algorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09940</identifier>
 <datestamp>2018-10-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09940</id><created>2018-10-22</created><authors><author><keyname>Basu</keyname><forenames>Kaustav</forenames></author><author><keyname>Padhee</keyname><forenames>Malhar</forenames></author><author><keyname>Roy</keyname><forenames>Sohini</forenames></author><author><keyname>Pal</keyname><forenames>Anamitra</forenames></author><author><keyname>Sen</keyname><forenames>Arunabha</forenames></author><author><keyname>Rhodes</keyname><forenames>Matthew</forenames></author><author><keyname>Keel</keyname><forenames>Brian</forenames></author></authors><title>Health Monitoring of Critical Power System Equipments using Identifying
  Codes</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High voltage power transformers are one of the most critical equipments in
the electric power grid. A sudden failure of a power transformer can
significantly disrupt bulk power delivery. Before a transformer reaches its
critical failure state, there are indicators which, if monitored periodically,
can alert an operator that the transformer is heading towards a failure. One of
the indicators is the signal to noise ratio (SNR) of the voltage and current
signals in substations located in the vicinity of the transformer. During
normal operations, the width of the SNR band is small. However, when the
transformer heads towards a failure, the widths of the bands increase, reaching
their maximum just before the failure actually occurs. This change in width of
the SNR can be observed by sensors, such as phasor measurement units (PMUs)
located nearby. Identifying Code is a mathematical tool that enables one to
uniquely identify one or more {\em objects of interest}, by generating a unique
signature corresponding to those objects, which can then be detected by a
sensor. In this paper, we first describe how Identifying Code can be utilized
for detecting failure of power transformers. Then, we apply this technique to
determine the fewest number of sensors needed to uniquely identify failing
transformers in different test systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.09977</identifier>
 <datestamp>2020-01-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.09977</id><created>2018-10-23</created><updated>2019-02-28</updated><authors><author><keyname>Rosenfeld</keyname><forenames>Bleema</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Rajendran</keyname><forenames>Bipin</forenames></author></authors><title>Learning First-to-Spike Policies for Neuromorphic Control Using Policy
  Gradients</title><categories>cs.LG eess.SP stat.ML</categories><comments>Submitted for conference publication</comments><doi>10.1109/SPAWC.2019.8815546</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Neural Networks (ANNs) are currently being used as function
approximators in many state-of-the-art Reinforcement Learning (RL) algorithms.
Spiking Neural Networks (SNNs) have been shown to drastically reduce the energy
consumption of ANNs by encoding information in sparse temporal binary spike
streams, hence emulating the communication mechanism of biological neurons. Due
to their low energy consumption, SNNs are considered to be important candidates
as co-processors to be implemented in mobile devices. In this work, the use of
SNNs as stochastic policies is explored under an energy-efficient
first-to-spike action rule, whereby the action taken by the RL agent is
determined by the occurrence of the first spike among the output neurons. A
policy gradient-based algorithm is derived considering a Generalized Linear
Model (GLM) for spiking neurons. Experimental results demonstrate the
capability of online trained SNNs as stochastic policies to gracefully trade
energy consumption, as measured by the number of spikes, and control
performance. Significant gains are shown as compared to the standard approach
of converting an offline trained ANN into an SNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10002</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10002</id><created>2018-10-22</created><updated>2018-10-25</updated><authors><author><keyname>Masada</keyname><forenames>Kristen</forenames></author><author><keyname>Bunescu</keyname><forenames>Razvan</forenames></author></authors><title>Chord Recognition in Symbolic Music: A Segmental CRF Model,
  Segment-Level Features, and Comparative Evaluations on Classical and Popular
  Music</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a new approach to harmonic analysis that is trained to segment
music into a sequence of chord spans tagged with chord labels. Formulated as a
semi-Markov Conditional Random Field (semi-CRF), this joint segmentation and
labeling approach enables the use of a rich set of segment-level features, such
as segment purity and chord coverage, that capture the extent to which the
events in an entire segment of music are compatible with a candidate chord
label. The new chord recognition model is evaluated extensively on three
corpora of classical music and a newly created corpus of rock music.
Experimental results show that the semi-CRF model performs substantially better
than previous approaches when trained on a sufficient number of labeled
examples and remains competitive when the amount of training data is limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10062</identifier>
 <datestamp>2019-06-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10062</id><created>2018-10-23</created><authors><author><keyname>Sapsanis</keyname><forenames>Christos</forenames></author></authors><title>Recognition of basic hand movements using Electromyography</title><categories>eess.SP</categories><comments>123 pages, 109 figures</comments><journal-ref>Universtity of Patras, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work was to identify six basic movements of the hand using
two systems. Being an interdisciplinary topic, there has been conducted
studying in the anatomy of forearm muscles, biosignals, the method of
electromyography (EMG) and methods of pattern recognition. Moreover, the signal
contained enough noise and had to be analyzed, using EMD, to extract features
and to reduce its dimensionality, using RELIEF and PCA, to improve the success
rate of classification. The first part uses an EMG system of Delsys initially
for an individual and then for six people with the average successful
classification, for these six movements at rates of over 80%. The second part
involves the construction of an autonomous system EMG using an Arduino
microcontroller, EMG sensors and electrodes, which are arranged in an elastic
glove. Classification results in this case reached 75% of success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10108</identifier>
 <datestamp>2018-10-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10108</id><created>2018-10-23</created><authors><author><keyname>Ahmadi</keyname><forenames>Mehdi</forenames></author><author><keyname>Nest</keyname><forenames>Timothy</forenames></author><author><keyname>Abdelnaim</keyname><forenames>Mostafa</forenames></author><author><keyname>Le</keyname><forenames>Thanh-Dung</forenames></author></authors><title>Reproducing AmbientGAN: Generative models from lossy measurements</title><categories>eess.SP cs.LG</categories><comments>This work was submitted as final project for the course IFT6135:
  Representation Learning - A Deep Learning Course, University of Montreal,
  Winter 2018</comments><journal-ref>ICLR 2018 Reproducibility Challenge</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, Generative Adversarial Networks (GANs) have shown
substantial progress in modeling complex distributions of data. These networks
have received tremendous attention since they can generate implicit
probabilistic models that produce realistic data using a stochastic procedure.
While such models have proven highly effective in diverse scenarios, they
require a large set of fully-observed training samples. In many applications
access to such samples are difficult or even impractical and only noisy or
partial observations of the desired distribution is available. Recent research
has tried to address the problem of incompletely observed samples to recover
the distribution of the data. \citep{zhu2017unpaired} and
\citep{yeh2016semantic} proposed methods to solve ill-posed inverse problem
using cycle-consistency and latent-space mappings in adversarial networks,
respectively. \citep{bora2017compressed} and \citep{kabkab2018task} have
applied similar adversarial approaches to the problem of compressed sensing. In
this work, we focus on a new variant of GAN models called AmbientGAN, which
incorporates a measurement process (e.g. adding noise, data removal and
projection) into the GAN training. While in the standard GAN, the discriminator
distinguishes a generated image from a real image, in AmbientGAN model the
discriminator has to separate a real measurement from a simulated measurement
of a generated image. The results shown by \citep{bora2018ambientgan} are quite
promising for the problem of incomplete data, and have potentially important
implications for generative approaches to compressed sensing and ill-posed
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10192</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10192</id><created>2018-10-24</created><updated>2018-10-24</updated><authors><author><keyname>Zhang</keyname><forenames>Zhongyang</forenames><affiliation>Missouri S&amp;T EMC Laboratory, Rolla, MO, USA</affiliation></author><author><keyname>Zhang</keyname><forenames>Ling</forenames><affiliation>Missouri S&amp;T EMC Laboratory, Rolla, MO, USA</affiliation></author><author><keyname>Sun</keyname><forenames>Ze</forenames><affiliation>Missouri S&amp;T EMC Laboratory, Rolla, MO, USA</affiliation></author><author><keyname>Erickson</keyname><forenames>Nicholas</forenames><affiliation>Missouri S&amp;T EMC Laboratory, Rolla, MO, USA</affiliation></author><author><keyname>From</keyname><forenames>Ryan</forenames><affiliation>Boeing Company, St. Louis, MO, USA</affiliation></author><author><keyname>Fan</keyname><forenames>Jun</forenames><affiliation>Missouri S&amp;T EMC Laboratory, Rolla, MO, USA</affiliation></author></authors><title>Solving Poisson's Equation using Deep Learning in Particle Simulation of
  PN Junction</title><categories>physics.comp-ph cs.AI eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulating the dynamic characteristics of a PN junction at the microscopic
level requires solving the Poisson's equation at every time step. Solving at
every time step is a necessary but time-consuming process when using the
traditional finite difference (FDM) approach. Deep learning is a powerful
technique to fit complex functions. In this work, deep learning is utilized to
accelerate solving Poisson's equation in a PN junction. The role of the
boundary condition is emphasized in the loss function to ensure a better
fitting. The resulting I-V curve for the PN junction, using the deep learning
solver presented in this work, shows a perfect match to the I-V curve obtained
using the finite difference method, with the advantage of being 10 times faster
at every time step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10274</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10274</id><created>2018-10-24</created><updated>2018-11-03</updated><authors><author><keyname>Pons</keyname><forenames>Jordi</forenames></author><author><keyname>Serr&#xe0;</keyname><forenames>Joan</forenames></author><author><keyname>Serra</keyname><forenames>Xavier</forenames></author></authors><title>Training neural audio classifiers with few data</title><categories>cs.SD cs.AI cs.LG eess.AS</categories><comments>Code: https://github.com/jordipons/neural-classifiers-with-few-audio/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate supervised learning strategies that improve the training of
neural network audio classifiers on small annotated collections. In particular,
we study whether (i) a naive regularization of the solution space, (ii)
prototypical networks, (iii) transfer learning, or (iv) their combination, can
foster deep learning models to better leverage a small amount of training
examples. To this end, we evaluate (i-iv) for the tasks of acoustic event
recognition and acoustic scene classification, considering from 1 to 100
labeled examples per class. Results indicate that transfer learning is a
powerful strategy in such scenarios, but prototypical networks show promising
results when one does not count with external or validation data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10408</identifier>
 <datestamp>2018-10-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10408</id><created>2018-10-24</created><authors><author><keyname>Cui</keyname><forenames>Jingjing</forenames></author><author><keyname>Liu</keyname><forenames>Yuanwei</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author></authors><title>Multi-Agent Reinforcement Learning Based Resource Allocation for UAV
  Networks</title><categories>eess.SP</categories><comments>30 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unmanned aerial vehicles (UAVs) are capable of serving as aerial base
stations (BSs) for providing both cost-effective and on-demand wireless
communications. This article investigates dynamic resource allocation of
multiple UAVs enabled communication networks with the goal of maximizing
long-term rewards. More particularly, each UAV communicates with a ground user
by automatically selecting its communicating users, power levels and
subchannels without any information exchange among UAVs. To model the
uncertainty of environments, we formulate the long-term resource allocation
problem as a stochastic game for maximizing the expected rewards, where each
UAV becomes a learning agent and each resource allocation solution corresponds
to an action taken by the UAVs. Afterwards, we develop a multi-agent
reinforcement learning (MARL) framework that each agent discovers its best
strategy according to its local observations using learning. More specifically,
we propose an agent-independent method, for which all agents conduct a decision
algorithm independently but share a common structure based on Q-learning.
Finally, simulation results reveal that: 1) appropriate parameters for
exploitation and exploration are capable of enhancing the performance of the
proposed MARL based resource allocation algorithm; 2) the proposed MARL
algorithm provides acceptable performance compared to the case with complete
information exchanges among UAVs. By doing so, it strikes a good tradeoff
between performance gains and information exchange overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10571</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10571</id><created>2018-10-24</created><authors><author><keyname>Krishnan</keyname><forenames>Joshin P.</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9; M.</forenames></author></authors><title>Patch-based Interferometric Phase Estimation via Mixture of Gaussian
  Density Modelling &amp; Non-local Averaging in the Complex Domain</title><categories>eess.SP</categories><comments>British Machine Vision Conference, 2017</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses interferometric phase (InPhase) image denoising, i.e.,
the denoising of phase modulo-2p images from sinusoidal 2p-periodic and noisy
observations. The wrapping discontinuities present in the InPhase images, which
are to be preserved carefully, make InPhase denoising a challenging inverse
problem. We propose a novel two-step algorithm to tackle this problem by
exploiting the non-local self-similarity of the InPhase images. In the first
step, the patches of the phase images are modelled using Mixture of Gaussian
(MoG) densities in the complex domain. An Expectation Maximization(EM)
algorithm is formulated to learn the parameters of the MoG from the noisy data.
The learned MoG is used as a prior for estimating the InPhase images from the
noisy images using Minimum Mean Square Error (MMSE) estimation. In the second
step, an additional exploitation of non-local self-similarity is done by
performing a type of non-local mean filtering. Experiments conducted on
simulated and real (MRI and InSAR) datasets show results which are competitive
with the state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10597</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10597</id><created>2018-10-24</created><authors><author><keyname>Burton</keyname><forenames>Jake</forenames></author><author><keyname>Frank</keyname><forenames>David</forenames></author><author><keyname>Saleh</keyname><forenames>Madhi</forenames></author><author><keyname>Navab</keyname><forenames>Nassir</forenames></author><author><keyname>Bear</keyname><forenames>Helen L.</forenames></author></authors><title>The speaker-independent lipreading play-off; a survey of lipreading
  machines</title><categories>cs.CV eess.AS</categories><comments>To appear at the third IEEE International Conference on Image
  Processing, Applications and Systems 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lipreading is a difficult gesture classification task. One problem in
computer lipreading is speaker-independence. Speaker-independence means to
achieve the same accuracy on test speakers not included in the training set as
speakers within the training set. Current literature is limited on
speaker-independent lipreading, the few independent test speaker accuracy
scores are usually aggregated within dependent test speaker accuracies for an
averaged performance. This leads to unclear independent results. Here we
undertake a systematic survey of experiments with the TCD-TIMIT dataset using
both conventional approaches and deep learning methods to provide a series of
wholly speaker-independent benchmarks and show that the best
speaker-independent machine scores 69.58% accuracy with CNN features and an SVM
classifier. This is less than state of the art speaker-dependent lipreading
machines, but greater than previously reported in independence experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10662</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10662</id><created>2018-10-24</created><authors><author><keyname>Zong</keyname><forenames>Zefang</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Qi</forenames></author></authors><title>Multi-Channel Auto-Encoder for Speech Emotion Recognition</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring emotion status from users' queries plays an important role to
enhance the capacity in voice dialogues applications. Even though several
related works obtained satisfactory results, the performance can still be
further improved. In this paper, we proposed a novel framework named
multi-channel auto-encoder (MTC-AE) on emotion recognition from acoustic
information. MTC-AE contains multiple local DNNs based on different low-level
descriptors with different statistics functions that are partly concatenated
together, by which the structure is enabled to consider both local and global
features simultaneously. Experiment based on a benchmark dataset IEMOCAP shows
that our method significantly outperforms the existing state-of-the-art
results, achieving $64.8\%$ leave-one-speaker-out unweighted accuracy, which is
$2.4\%$ higher than the best result on this dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10724</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10724</id><created>2018-10-25</created><authors><author><keyname>Guo</keyname><forenames>Shuaishuai</forenames></author><author><keyname>Zhang</keyname><forenames>Haixia</forenames></author><author><keyname>Zhang</keyname><forenames>Peng</forenames></author><author><keyname>Zhao</keyname><forenames>Pengjie</forenames></author><author><keyname>Wang</keyname><forenames>Leiyu</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Generalized Beamspace Modulation Using Multiplexing: A Breakthrough in
  mmWave MIMO</title><categories>eess.SP</categories><comments>Conference submitted to ICC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial multiplexing (SMX) multiple-input multiple-output (MIMO) over the
best beamspace was considered as the best solution for millimeter wave (mmWave)
communications regarding spectral efficiency (SE), referred as the best
beamspace selection (BBS) solution. The equivalent MIMO water-filling (WF-MIMO)
channel capacity was treated as an unsurpassed SE upper bound. Recently,
researchers have proposed various schemes trying to approach the benchmark and
the performance bound. But, are they the real limit of mmWave MIMO systems with
reduced radio-frequency (RF) chains? In this paper, we challenge the benchmark
and the corresponding bound by proposing a better transmission scheme that
achieves higher SE, namely the Generalized Beamspace Modulation using
Multiplexing (GBMM). Inspired by the concept of spatial modulation, besides the
selected beamspace, the selection operation is used to carry information. We
prove that GBMM is superior to BBS in terms of SE and can break through the
well known `upper bound'. That is, GBMM renews the upper bound of the SE. We
investigate SE-oriented precoder activation probability optimization,
fully-digital precoder design, optimal power allocation and hybrid precoder
design for GBMM. A gradient ascent algorithm is developed to find the optimal
solution, which is applicable in all signal-to-noise-ratio (SNR) regimes. The
best solution is derived in the high SNR regime. Additionally, we investigate
the hybrid receiver design and deduce the minimum number of receive RF chains
configured to gain from GBMM in achievable SE. We propose a coding approach to
realize the optimized precoder activation. An extension to mmWave broadband
communications is also discussed. Comparisons with the benchmark (i.e., WF-MIMO
channel capacity) are made under different system configurations to show the
superiority of GBMM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10725</identifier>
 <datestamp>2019-07-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10725</id><created>2018-10-25</created><updated>2019-07-19</updated><authors><author><keyname>Hosseini</keyname><forenames>Mahdi S.</forenames></author><author><keyname>Plataniotis</keyname><forenames>Konstantinos N.</forenames></author></authors><title>Convolutional Deblurring for Natural Imaging</title><categories>eess.IV cs.CV</categories><comments>15 pages, for publication in IEEE Transaction Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel design of image deblurring in the form of
one-shot convolution filtering that can directly convolve with naturally
blurred images for restoration. The problem of optical blurring is a common
disadvantage to many imaging applications that suffer from optical
imperfections. Despite numerous deconvolution methods that blindly estimate
blurring in either inclusive or exclusive forms, they are practically
challenging due to high computational cost and low image reconstruction
quality. Both conditions of high accuracy and high speed are prerequisites for
high-throughput imaging platforms in digital archiving. In such platforms,
deblurring is required after image acquisition before being stored, previewed,
or processed for high-level interpretation. Therefore, on-the-fly correction of
such images is important to avoid possible time delays, mitigate computational
expenses, and increase image perception quality. We bridge this gap by
synthesizing a deconvolution kernel as a linear combination of Finite Impulse
Response (FIR) even-derivative filters that can be directly convolved with
blurry input images to boost the frequency fall-off of the Point Spread
Function (PSF) associated with the optical blur. We employ a Gaussian low-pass
filter to decouple the image denoising problem for image edge deblurring.
Furthermore, we propose a blind approach to estimate the PSF statistics for two
Gaussian and Laplacian models that are common in many imaging pipelines.
Thorough experiments are designed to test and validate the efficiency of the
proposed method using 2054 naturally blurred images across six imaging
applications and seven state-of-the-art deconvolution methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10727</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10727</id><created>2018-10-25</created><updated>2018-11-07</updated><authors><author><keyname>Kida</keyname><forenames>Yusuke</forenames></author><author><keyname>Tran</keyname><forenames>Dung</forenames></author><author><keyname>Omachi</keyname><forenames>Motoi</forenames></author><author><keyname>Taniguchi</keyname><forenames>Toru</forenames></author><author><keyname>Fujita</keyname><forenames>Yuya</forenames></author></authors><title>Speaker Selective Beamformer with Keyword Mask Estimation</title><categories>eess.AS cs.LG cs.SD</categories><comments>Accepted by SLT2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of automatic speech recognition (ASR) of a
target speaker in background speech. The novelty of our approach is that we
focus on a wakeup keyword, which is usually used for activating ASR systems
like smart speakers. The proposed method firstly utilizes a DNN-based mask
estimator to separate the mixture signal into the keyword signal uttered by the
target speaker and the remaining background speech. Then the separated signals
are used for calculating a beamforming filter to enhance the subsequent
utterances from the target speaker. Experimental evaluations show that the
trained DNN-based mask can selectively separate the keyword and background
speech from the mixture signal. The effectiveness of the proposed method is
also verified with Japanese ASR experiments, and we confirm that the character
error rates are significantly improved by the proposed method for both
simulated and real recorded test sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10755</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10755</id><created>2018-10-25</created><authors><author><keyname>Tan</keyname><forenames>Zhenyu</forenames></author><author><keyname>Liu</keyname><forenames>Yu</forenames></author><author><keyname>Sun</keyname><forenames>Hongbo</forenames></author><author><keyname>Cui</keyname><forenames>Bai</forenames></author></authors><title>Fault Diagnosis and Bad Data Detection of Power Transmission Network - A
  Time Domain Approach</title><categories>eess.SP</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault analysis and bad data are often processed in separate manners. In this
paper it is proved that fault as well as bad current measurement data can be
modeled as control failure for the power transmission network and any fault on
the transmission line can be treated as multiple bad data. Subsequently a
linear observer theory is designed in order to identify the fault type and bad
data simultaneously. The state space model based observer theory allows a
particular failure mode manifest itself as residual which remains in a fixed
direction. Moreover coordinate transformation is performed to allow the
residual for each failure mode to generate specific geometry characteristic in
separate output dimensions. The design approach based on the observer theory is
presented in this paper. The design allows 1) bad data detection for current
measurement, and 2) fault location, and fault resistance estimation (as a
byproduct) where the fault location accuracy is not affected by fault
resistance. However it loses freedom in designing the eigenvalues in the
excessive subspace. While the theoretical framework is general, the analysis
and design are dedicated to transmission lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10788</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10788</id><created>2018-10-25</created><updated>2018-11-19</updated><authors><author><keyname>Elvander</keyname><forenames>Filip</forenames></author><author><keyname>Haasler</keyname><forenames>Isabel</forenames></author><author><keyname>Jakobsson</keyname><forenames>Andreas</forenames></author><author><keyname>Karlsson</keyname><forenames>Johan</forenames></author></authors><title>Non-Coherent Sensor Fusion via Entropy Regularized Optimal Mass
  Transport</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a method for information fusion in source localization
applications. The method utilizes the concept of optimal mass transport in
order to construct estimates of the spatial spectrum using a convex barycenter
formulation. We introduce an entropy regularization term to the convex
objective, which allows for low-complexity iterations of the solution algorithm
and thus makes the proposed method applicable also to higher-dimensional
problems. We illustrate the proposed method's inherent robustness to
misalignment and miscalibration of the sensor arrays using numerical examples
of localization in two dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10871</identifier>
 <datestamp>2018-12-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10871</id><created>2018-10-24</created><authors><author><keyname>French</keyname><forenames>Rebecca</forenames></author><author><keyname>Gigan</keyname><forenames>Sylvain</forenames></author><author><keyname>Muskens</keyname><forenames>Otto L.</forenames></author></authors><title>Snapshot fiber spectral imaging using speckle correlations and
  compressive sensing</title><categories>eess.IV physics.ins-det physics.optics</categories><doi>10.1364/OE.26.032302</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Snapshot spectral imaging is rapidly gaining interest for remote sensing
applications. Acquiring spatial and spectral data within one image promotes
fast measurement times, and reduces the need for stabilized scanning imaging
systems. Many current snapshot technologies, which rely on gratings or prisms
to characterize wavelength information, are difficult to reduce in size for
portable hyperspectral imaging. Here, we show that a multicore multimode fiber
can be used as a compact spectral imager with sub-nanometer resolution, by
encoding spectral information within a monochrome CMOS camera. We characterize
wavelength-dependent speckle patterns for up to 3000 fiber cores over a broad
wavelength range. A clustering algorithm is employed in combination with
l$_{1}$-minimization to limit data collection at the acquisition stage for the
reconstruction of spectral images that are sparse in the wavelength domain. We
also show that in the non-compressive regime these techniques are able to
accurately reconstruct broadband information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10884</identifier>
 <datestamp>2019-04-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10884</id><created>2018-10-25</created><updated>2019-04-10</updated><authors><author><keyname>Jung</keyname><forenames>Jee-weon</forenames></author><author><keyname>Heo</keyname><forenames>Hee-soo</forenames></author><author><keyname>Shim</keyname><forenames>Hye-jin</forenames></author><author><keyname>Yu</keyname><forenames>Ha-jin</forenames></author></authors><title>Short utterance compensation in speaker verification via cosine-based
  teacher-student learning of speaker embeddings</title><categories>eess.AS cs.AI cs.SD</categories><comments>5 pages, 2 figures, submitted to Interspeech 2019 as a conference
  paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The short duration of an input utterance is one of the most critical threats
that degrade the performance of speaker verification systems. This study aimed
to develop an integrated text-independent speaker verification system that
inputs utterances with short duration of 2 seconds or less. We propose an
approach using a teacher-student learning framework for this goal, applied to
short utterance compensation for the first time in our knowledge. The core
concept of the proposed system is to conduct the compensation throughout the
network that extracts the speaker embedding, mainly in phonetic-level, rather
than compensating via a separate system after extracting the speaker embedding.
In the proposed architecture, phonetic-level features where each feature
represents a segment of 130 ms are extracted using convolutional layers. A
layer of gated recurrent units extracts an utterance-level feature using
phonetic-level features. The proposed approach also adopts a new objective
function for teacher-student learning that considers both Kullback-Leibler
divergence of output layers and cosine distance of speaker embeddings layers.
Experiments were conducted using deep neural networks that take raw waveforms
as input, and output speaker embeddings on VoxCeleb1 dataset. The proposed
model could compensate approximately 65 \% of the performance degradation due
to the shortened duration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10902</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10902</id><created>2018-10-22</created><authors><author><keyname>Lugosch</keyname><forenames>Loren</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Learning from the Syndrome</title><categories>cs.IT cs.LG eess.SP math.IT</categories><comments>Accepted to Asilomar 2018 - special session on &quot;Machine Learning for
  Wireless Systems&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the syndrome loss, an alternative loss function
for neural error-correcting decoders based on a relaxation of the syndrome. The
syndrome loss penalizes the decoder for producing outputs that do not
correspond to valid codewords. We show that training with the syndrome loss
yields decoders with consistently lower frame error rate for a number of short
block codes, at little additional cost during training and no additional cost
during inference. The proposed method does not depend on knowledge of the
transmitted codeword, making it a promising tool for online adaptation to
changing channel conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10957</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10957</id><created>2018-10-25</created><authors><author><keyname>Jindal</keyname><forenames>Ishan</forenames></author><author><keyname>Nokleby</keyname><forenames>Matthew</forenames></author></authors><title>Tensor Matched Kronecker-Structured Subspace Detection for Missing
  Information</title><categories>cs.IT eess.SP math.IT stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider the problem of detecting whether a tensor signal having many
missing entities lies within a given low dimensional Kronecker-Structured (KS)
subspace. This is a matched subspace detection problem. Tensor matched subspace
detection problem is more challenging because of the intertwined signal
dimensions. We solve this problem by projecting the signal onto the Kronecker
structured subspace, which is a Kronecker product of different subspaces
corresponding to each signal dimension. Under this framework, we define the KS
subspaces and the orthogonal projection of the signal onto the KS subspace. We
prove that reliable detection is possible as long as the cardinality of the
missing signal is greater than the dimensions of the KS subspace by bounding
the residual energy of the sampling signal with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10964</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10964</id><created>2018-10-17</created><authors><author><keyname>Sajedian</keyname><forenames>Iman</forenames></author><author><keyname>Badloe</keyname><forenames>Trevon</forenames></author><author><keyname>Rho</keyname><forenames>Junsuk</forenames></author></authors><title>Finding the best design parameters for optical nanostructures using
  reinforcement learning</title><categories>eess.IV cs.LG physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a novel machine learning model has emerged in the field of
reinforcement learning known as deep Q-learning. This model is capable of
finding the best possible solution in systems consisting of millions of
choices, without ever experiencing it before, and has been used to beat the
best human minds at complex games such as, Go and chess, which both have a huge
number of possible decisions and outcomes for each move. With a human-level
intelligence, it has been solved the problems that no other machine learning
model could do before. Here, we show the steps needed for implementing this
model on an optical problem. We investigated the colour generation by
dielectric nanostructures and show that this model can find geometrical
properties that can generate a much deeper red, green and blue colours compared
to the ones found by human researchers. This technique can easily be extended
to predict and find the best design parameters for other optical structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10973</identifier>
 <datestamp>2018-10-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10973</id><created>2018-10-21</created><authors><author><keyname>Salim</keyname><forenames>Maryam</forenames></author><author><keyname>Ozen</keyname><forenames>Ali Caglar</forenames></author><author><keyname>Bock</keyname><forenames>Michael</forenames></author><author><keyname>Atalar</keyname><forenames>Ergin</forenames></author></authors><title>Active Decoupling of Transmit and Receive Coils for Full-Duplex MRI</title><categories>physics.med-ph eess.IV eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: Concurrent excitation and acquisition in MRI is a method to
acquire MRI signal from tissues with very short transverse relaxation time.
Since transmit power is many orders of magnitude larger than receive signal, a
weak coupling dominates the MR signal during CEA. Thus, appropriate decoupling
between transmit and receive coils is required. In this study, two controllable
decoupling designs are investigated for achieving isolation between coils.
Methods: A modified version of isolation concept used in the full-duplex radios
in communication systems is applied to acquire MRI signal using CEA. In our new
method, a small copy of RF transmit signal is attenuated and delayed to
generate the same coupling signal which is available in the receiver coil. Then
it is subtracted from the receive signal to detect the MRI signal. The proposed
decoupling method is developed and implemented in two designs: Semi-Automatic
and Fully-Automatic Controllable Decoupling Designs. Results: Using
Semi-Automatic Controllable Decoupling Design, decoupling of more than 75 dB is
achieved. Fully-Automatic Controllable Decoupling Design provides more than 100
dB decoupling between coils which is good enough for detecting MRI signals
during excitation from tissues with very short transverse relaxation time.
Conclusion: This study shows feasibility of applying full duplex electronics to
decouple transmit and receive coils for CEA in a clinical MRI system.
Significance: These designs can automatically tune the cancellation circuit and
it is a potential tool for recovering signal from tissues with very short T2 in
clinical MR systems with a minor hardware modification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.10989</identifier>
 <datestamp>2018-12-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.10989</id><created>2018-10-25</created><updated>2018-12-17</updated><authors><author><keyname>Sheng</keyname><forenames>Leyuan</forenames></author><author><keyname>Pavlovskiy</keyname><forenames>Evgeniy N.</forenames></author></authors><title>Reducing over-smoothness in speech synthesis using Generative
  Adversarial Networks</title><categories>cs.SD eess.AS</categories><comments>Accepted by Siberian Symposium on Data Science and Engineering
  (SSDSE) 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech synthesis is widely used in many practical applications. In recent
years, speech synthesis technology has developed rapidly. However, one of the
reasons why synthetic speech is unnatural is that it often has over-smoothness.
In order to improve the naturalness of synthetic speech, we first extract the
mel-spectrogram of speech and convert it into a real image, then take the
over-smooth mel-spectrogram image as input, and use image-to-image translation
Generative Adversarial Networks(GANs) framework to generate a more realistic
mel-spectrogram. Finally, the results show that this method greatly reduces the
over-smoothness of synthesized speech and is more close to the mel-spectrogram
of real speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11137</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11137</id><created>2018-10-25</created><updated>2019-06-24</updated><authors><author><keyname>Bhown</keyname><forenames>Ashutosh</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soham</forenames></author><author><keyname>Yang</keyname><forenames>Sean</forenames></author><author><keyname>Chandak</keyname><forenames>Shubham</forenames></author><author><keyname>Fischer-Hwang</keyname><forenames>Irena</forenames></author><author><keyname>Tatwawadi</keyname><forenames>Kedar</forenames></author><author><keyname>Fan</keyname><forenames>Judith</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Towards improved lossy image compression: Human image reconstruction
  with public-domain images</title><categories>eess.IV cs.CV cs.IT cs.MM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lossy image compression has been studied extensively in the context of
typical loss functions such as RMSE, MS-SSIM, etc. However, compression at low
bitrates generally produces unsatisfying results. Furthermore, the availability
of massive public image datasets appears to have hardly been exploited in image
compression. Here, we present a paradigm for eliciting human image
reconstruction in order to perform lossy image compression. In this paradigm,
one human describes images to a second human, whose task is to reconstruct the
target image using publicly available images and text instructions. The
resulting reconstructions are then evaluated by human raters on the Amazon
Mechanical Turk platform and compared to reconstructions obtained using
state-of-the-art compressor WebP. Our results suggest that prioritizing
semantic visual elements may be key to achieving significant improvements in
image compression, and that our paradigm can be used to develop a more
human-centric loss function.
  The images, results and additional data are available at
https://compression.stanford.edu/human-compression
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11145</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11145</id><created>2018-10-25</created><updated>2019-02-22</updated><authors><author><keyname>Rapp</keyname><forenames>Joshua</forenames></author><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Dawson</keyname><forenames>Robin M. A.</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author></authors><title>Dead Time Compensation for High-Flux Ranging</title><categories>eess.SP</categories><comments>Revision with added estimation results, references, and figures, and
  modified appendices</comments><doi>10.1109/TSP.2019.2914891</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dead time effects have been considered a major limitation for fast data
acquisition in various time-correlated single photon counting applications,
since a commonly adopted approach for dead time mitigation is to operate in the
low-flux regime where dead time effects can be ignored. Through the application
of lidar ranging, this work explores the empirical distribution of detection
times in the presence of dead time and demonstrates that an accurate
statistical model can result in reduced ranging error with shorter data
acquisition time when operating in the high-flux regime. Specifically, we show
that the empirical distribution of detection times converges to the stationary
distribution of a Markov chain. Depth estimation can then be performed by
passing the empirical distribution through a filter matched to the stationary
distribution. Moreover, based on the Markov chain model, we formulate the
recovery of arrival distribution from detection distribution as a nonlinear
inverse problem and solve it via provably convergent mathematical optimization.
By comparing per-detection Fisher information for depth estimation from high-
and low-flux detection time distributions, we provide an analytical basis for
possible improvement of ranging performance resulting from the presence of dead
time. Finally, we demonstrate the effectiveness of our formulation and
algorithm via simulations of lidar ranging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11215</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11215</id><created>2018-10-26</created><authors><author><keyname>Nguyen</keyname><forenames>Huy H.</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Echizen</keyname><forenames>Isao</forenames></author></authors><title>Capsule-Forensics: Using Capsule Networks to Detect Forged Images and
  Videos</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in media generation techniques have made it easier for
attackers to create forged images and videos. State-of-the-art methods enable
the real-time creation of a forged version of a single video obtained from a
social network. Although numerous methods have been developed for detecting
forged images and videos, they are generally targeted at certain domains and
quickly become obsolete as new kinds of attacks appear. The method introduced
in this paper uses a capsule network to detect various kinds of spoofs, from
replay attacks using printed images or recorded videos to computer-generated
videos using deep convolutional neural networks. It extends the application of
capsule networks beyond their original intention to the solving of inverse
graphics problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11217</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11217</id><created>2018-10-26</created><authors><author><keyname>Xu</keyname><forenames>Ziyi</forenames></author><author><keyname>Strake</keyname><forenames>Maximilian</forenames></author><author><keyname>Fingscheidt</keyname><forenames>Tim</forenames></author></authors><title>Concatenated Identical DNN (CI-DNN) to Reduce Noise-Type Dependence in
  DNN-Based Speech Enhancement</title><categories>eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating time-frequency domain masks for speech enhancement using deep
learning approaches has recently become a popular field of research. In this
paper, we propose a mask-based speech enhancement framework by using
concatenated identical deep neural networks (CI-DNNs). The idea is that a
single DNN is trained under multiple input and output signal-to-noise power
ratio (SNR) conditions, using targets that provide a moderate SNR gain with
respect to the input and therefore achieve a balance between speech component
quality and noise suppression. We concatenate this single DNN several times
without any retraining to provide enough noise attenuation. Simulation results
show that our proposed CI-DNN outperforms enhancement methods using classical
spectral weighting rules w.r.t. total speech quality and speech
intelligibility. Moreover, our approach shows similar or even a little bit
better performance with much fewer trainable parameters compared with a
noisy-target single DNN approach of the same size. A comparison to the
conventional clean-target single DNN approach shows that our proposed CI-DNN is
better in speech component quality and much better in residual noise component
quality. Most importantly, our new CI-DNN generalized best to an unseen noise
type, if compared to the other tested deep learning approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11312</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11312</id><created>2018-10-26</created><updated>2019-05-01</updated><authors><author><keyname>Liu</keyname><forenames>Can</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Li</keyname><forenames>Yongzhao</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>Zhang</keyname><forenames>Hailin</forenames></author></authors><title>Omnidirectional Quasi-Orthogonal Space-Time Block Coded Massive MIMO
  Systems</title><categories>eess.SP</categories><comments>6 pages, 5 figures, submitted to IEEE communication letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common signals in public channels of cellular systems are usually transmitted
omnidirectionally from the base station (BS). In recent years, both discrete
and consecutive omnidirectional space-time block codings (STBC) have been
proposed for massive multiple-input multiple-output (MIMO) systems with a
uniform linear array (ULA) configuration to ensure cell-wide coverage. In these
systems, constant received signal power at discrete angles, or constant
received signal sum power in a few consecutive time slots at any angle is
achieved. In addition, equal-power transmission per antenna and full spatial
diversity can be achieved as well. In this letter, by utilizing the property of
orthogonal complementary codes (OCCs), a new consecutive omnidirectional
quasi-orthogonal STBC (QOSTBC) design is proposed, in which constant received
sum power at any angle can be realized with equal-power transmission per
antenna through one STBC transmission, and a higher diversity order of 4 can be
achieved. In addition, the proposed design can be further extended to the
uniform planar array (UPA) configuration with the two-dimensional OCCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11314</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11314</id><created>2018-10-26</created><authors><author><keyname>Bagheri</keyname><forenames>Hossein</forenames></author><author><keyname>Schmitt</keyname><forenames>Michael</forenames></author><author><keyname>Zhu</keyname><forenames>Xiao Xiang</forenames></author></authors><title>Fusion of Urban TanDEM-X raw DEMs using variational models</title><categories>eess.IV</categories><comments>This is the pre-acceptance version, to read the final version, please
  go to IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing on IEEE Xplore</comments><journal-ref>IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing, 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a new global Digital Elevation Model (DEM) with pixel spacing of
0.4 arcseconds and relative height accuracy finer than 2m for flat areas
(slopes &lt; 20%) and better than 4m for rugged terrain (slopes &gt; 20%) was created
through the TanDEM-X mission. One important step of the chain of global DEM
generation is to mosaic and fuse multiple raw DEM tiles to reach the target
height accuracy. Currently, Weighted Averaging (WA) is applied as a fast and
simple method for TanDEM-X raw DEM fusion in which the weights are computed
from height error maps delivered from the Interferometric TanDEM-X Processor
(ITP). However, evaluations show that WA is not the perfect DEM fusion method
for urban areas especially in confrontation with edges such as building
outlines. The main focus of this paper is to investigate more advanced
variational approaches such as TV-L1 and Huber models. Furthermore, we also
assess the performance of variational models for fusing raw DEMs produced from
data takes with different baseline configurations and height of ambiguities.
The results illustrate the high efficiency of variational models for TanDEM-X
raw DEM fusion in comparison to WA. Using variational models could improve the
DEM quality by up to 2m particularly in inner-city subsets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11335</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11335</id><created>2018-10-26</created><authors><author><keyname>Yi</keyname><forenames>Jirong</forenames></author><author><keyname>Le</keyname><forenames>Anh Duc</forenames></author><author><keyname>Wang</keyname><forenames>Tianming</forenames></author><author><keyname>Wu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Outlier Detection using Generative Models with Theoretical Performance
  Guarantees</title><categories>cs.IT cs.CV eess.IV math.IT math.OC stat.ML</categories><comments>38 Pages, 15 Figures, 10 Lemmas or Theorems with Proofs</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper considers the problem of recovering signals from compressed
measurements contaminated with sparse outliers, which has arisen in many
applications. In this paper, we propose a generative model neural network
approach for reconstructing the ground truth signals under sparse outliers. We
propose an iterative alternating direction method of multipliers (ADMM)
algorithm for solving the outlier detection problem via $\ell_1$ norm
minimization, and a gradient descent algorithm for solving the outlier
detection problem via squared $\ell_1$ norm minimization. We establish the
recovery guarantees for reconstruction of signals using generative models in
the presence of outliers, and give an upper bound on the number of outliers
allowed for recovery. Our results are applicable to both the linear generator
neural network and the nonlinear generator neural network with an arbitrary
number of layers. We conduct extensive experiments using variational
auto-encoder and deep convolutional generative adversarial networks, and the
experimental results show that the signals can be successfully reconstructed
under outliers using our approach. Our approach outperforms the traditional
Lasso and $\ell_2$ minimization approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11352</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11352</id><created>2018-10-26</created><updated>2018-10-31</updated><authors><author><keyname>Yang</keyname><forenames>Xuerui</forenames></author><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Zhou</keyname><forenames>Xi</forenames></author></authors><title>A novel pyramidal-FSMN architecture with lattice-free MMI for speech
  recognition</title><categories>cs.SD eess.AS</categories><comments>5 pages, 3 figures, 2 tables. 2019 ICASSP submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Feedforward Sequential Memory Network (DFSMN) has shown superior
performance on speech recognition tasks. Based on this work, we propose a novel
network architecture which introduces pyramidal memory structure to represent
various context information in different layers. Additionally, res-CNN layers
are added in the front to extract more sophisticated features as well. Together
with lattice-free maximum mutual information (LF-MMI) and cross entropy (CE)
joint training criteria, experimental results show that this approach achieves
word error rates (WERs) of 3.62% and 10.89% respectively on Librispeech and
LDC97S62 (Switchboard 300 hours) corpora. Furthermore, Recurrent neural network
language model (RNNLM) rescoring is applied and a WER of 2.97% is obtained on
Librispeech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11359</identifier>
 <datestamp>2019-12-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11359</id><created>2018-10-26</created><updated>2019-12-16</updated><authors><author><keyname>Diaz-Guerra</keyname><forenames>David</forenames></author><author><keyname>Miguel</keyname><forenames>Antonio</forenames></author><author><keyname>Beltran</keyname><forenames>Jose R.</forenames></author></authors><title>gpuRIR: A Python Library for Room Impulse Response Simulation with GPU
  Acceleration</title><categories>eess.AS cs.SD</categories><comments>Submitted to Multimedia Tools and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Image Source Method (ISM) is one of the most employed techniques to
calculate acoustic Room Impulse Responses (RIRs), however, its computational
complexity grows fast with the reverberation time of the room and its
computation time can be prohibitive for some applications where a huge number
of RIRs are needed. In this paper, we present a new implementation that
dramatically improves the computation speed of the ISM by using Graphic
Processing Units (GPUs) to parallelize both the simulation of multiple RIRs and
the computation of the images inside each RIR. Additional speedups were
achieved by exploiting the mixed precision capabilities of the newer GPUs and
by using lookup tables. We provide a Python library under GNU license that can
be easily used without any knowledge about GPU programming and we show that it
is about 100 times faster than other state of the art CPU libraries. It may
become a powerful tool for many applications that need to perform a large
number of acoustic simulations, such as training machine learning systems for
audio signal processing, or for real-time room acoustics simulations for
immersive multimedia systems, such as augmented or virtual reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11360</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11360</id><created>2018-10-26</created><authors><author><keyname>Huang</keyname><forenames>Yongwei</forenames></author><author><keyname>Zhou</keyname><forenames>Mingkang</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>New Designs on MVDR Robust Adaptive Beamforming Based on Optimal
  Steering Vector Estimation</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2918997</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robust adaptive beamforming design problem based on estimation of the
signal of interest steering vector is considered in the paper. In this case,
the optimal beamformer is obtained by computing the sample matrix inverse and
an optimal estimate of the signal of interest steering vector. The common
criteria to find the best estimate of the steering vector are the beamformer
output SINR and output power, while the constraints assume as little as
possible prior inaccurate knowledge about the signal of interest, the
propagation media, and the antenna array. Herein, a new beamformer output power
maximization problem is formulated and solved subject to a double-sided norm
perturbation constraint, a similarity constraint, and a quadratic constraint
that guarantees that the direction-of-arrival (DOA) of the signal of interest
is away from the DOA region of all linear combinations of the interference
steering vectors. In the new robust design, the prior information required
consists of some allowable error norm bounds, the approximate knowledge of the
antenna array geometry, and the angular sector of the signal of interest. It
turns out that the array output power maximization problem is a non-convex QCQP
problem with inhomogeneous constraints. However, we show that the problem is
still solvable, and develop efficient algorithms for finding globally optimal
estimate of the signal of interest steering vector. The results are generalized
to the case where an ellipsoidal constraint is considered, and sufficient
conditions for the global optimality are derived. In addition, a new quadratic
constraint on the actual signal steering vector is proposed in order to improve
the array performance. To validate our results, simulation examples are
presented, and they demonstrate the improved performance of the new robust
beamformers in terms of the output SINR as well as the output power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11390</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11390</id><created>2018-10-26</created><authors><author><keyname>Zhang</keyname><forenames>Zhan</forenames></author><author><keyname>Wei</keyname><forenames>Ping</forenames></author><author><keyname>Deng</keyname><forenames>Lijuan</forenames></author><author><keyname>Zhang</keyname><forenames>Huaguo</forenames></author></authors><title>Joint Estimation of DOA and Frequency with Sub-Nyquist Sampling in a
  Binary Array Radar System</title><categories>eess.SP cs.IT math.IT</categories><comments>6 pages, 2 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, several array radar structures combined with sub-Nyquist techniques
and corresponding algorithms have been extensively studied. Carrier frequency
and direction-of-arrival (DOA) estimations of multiple narrow-band signals
received by array radars at the sub-Nyquist rates are considered in this paper.
We propose a new sub-Nyquist array radar architecture (a binary array radar
separately connected to a multi-coset structure with M branches) and an
efficient joint estimation algorithm which can match frequencies up with
corresponding DOAs. We further come up with a delay pattern augmenting method,
by which the capability of the number of identifiable signals can increase from
M-1 to Q-1 (Q is extended degrees of freedom). We further conclude that the
minimum total sampling rate 2MB is sufficient to identify $ {K \leq Q-1}$
narrow-band signals of maximum bandwidth $B$ inside. The effectiveness and
performance of the estimation algorithm together with the augmenting method
have been verified by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11413</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11413</id><created>2018-10-26</created><authors><author><keyname>Bagheri</keyname><forenames>Hossein</forenames></author><author><keyname>Schmitt</keyname><forenames>Michael</forenames></author><author><keyname>d'Angelo</keyname><forenames>Pablo</forenames></author><author><keyname>Zhu</keyname><forenames>Xiao Xiang</forenames></author></authors><title>A Framework for SAR-Optical Stereogrammetry over Urban Areas</title><categories>eess.IV</categories><comments>This is the pre-acceptance version, to read the final version, please
  go to ISPRS Journal of Photogrammetry and Remote Sensing on ScienceDirect</comments><journal-ref>ISPRS Journal of Photogrammetry and Remote Sensing, 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, numerous remote sensing satellites provide a huge volume of
diverse earth observation data. As these data show different features regarding
resolution, accuracy, coverage, and spectral imaging ability, fusion techniques
are required to integrate the different properties of each sensor and produce
useful information. For example, synthetic aperture radar (SAR) data can be
fused with optical imagery to produce 3D information using stereogrammetric
methods. The main focus of this study is to investigate the possibility of
applying a stereogrammetry pipeline to very-high-resolution (VHR) SAR-optical
image pairs. For this purpose, the applicability of semi-global matching is
investigated in this unconventional multi-sensor setting. To support the image
matching by reducing the search space and accelerating the identification of
correct, reliable matches, the possibility of establishing an epipolarity
constraint for VHR SAR-optical image pairs is investigated as well. In
addition, it is shown that the absolute geolocation accuracy of VHR optical
imagery with respect to VHR SAR imagery such as provided by TerraSAR-X can be
improved by a multi-sensor block adjustment formulation based on rational
polynomial coefficients. Finally, the feasibility of generating point clouds
with a median accuracy of about 2m is demonstrated and confirms the potential
of 3D reconstruction from SAR-optical image pairs over urban areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11415</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11415</id><created>2018-10-26</created><authors><author><keyname>Bagheri</keyname><forenames>Hossein</forenames></author><author><keyname>Schmitt</keyname><forenames>Michael</forenames></author><author><keyname>Zhu</keyname><forenames>Xiao Xiang</forenames></author></authors><title>Fusion of TanDEM-X and Cartosat-1 Elevation Data Supported by
  NeuralNetwork-Predicted Weight Maps</title><categories>eess.IV</categories><comments>This is the pre-acceptance version, to read the final version, please
  go to ISPRS Journal of Photogrammetry and Remote Sensing on ScienceDirect</comments><journal-ref>ISPRS Journal of Photogrammetry and Remote Sensing, Volume 144,
  October 2018, Pages 285-297</journal-ref><doi>10.1016/j.isprsjprs.2018.07.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the bistatic SAR interferometry mission TanDEM-X provided a global
terrain map with unprecedented accuracy. However, visual inspection and
empirical assessment of TanDEM-X elevation data against high-resolution ground
truth illustrates that the quality of the DEM decreases in urban areas because
of SAR-inherent imaging properties. One possible solution for an enhancement of
the TanDEM-X DEM quality is to fuse it with other elevation data derived from
high-resolution optical stereoscopic imagery, such as that provided by the
Cartosat-1 mission. This is usually done by Weighted Averaging (WA) of
previously aligned DEM cells. The main contribution of this paper is to develop
a method to efficiently predict weight maps in order to achieve optimized
fusion results. The prediction is modeled using a fully connected Artificial
Neural Network (ANN). The idea of this ANN is to extract suitable features from
DEMs that relate to height residuals in training areas and then to
automatically learn the pattern of the relationship between height errors and
features. The results show the DEM fusion based on the ANN-predicted weights
improves the qualities of the study DEMs. Apart from increasing the absolute
accuracy of Cartosat-1 DEM by DEM fusion, the relative accuracy (respective to
reference LiDAR data) ofDEMs is improved by up to 50% in urban areas and 22% in
non-urban areas while the improvement by them-based method does not exceed 20%
and 10% in urban and non-urban areas respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11451</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11451</id><created>2018-10-25</created><authors><author><keyname>Hains</keyname><forenames>G.</forenames></author><author><keyname>Suijlen</keyname><forenames>W.</forenames></author><author><keyname>Liang</keyname><forenames>W.</forenames></author><author><keyname>Wu</keyname><forenames>Z.</forenames></author></authors><title>5Gperf: signal processing performance for 5G</title><categories>eess.SP cs.DC</categories><report-no>PADAL-TR-2018-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 5Gperf project was conducted by Huawei research teams in 2016-17. It was
concerned with the acceleration of signal-processing algorithms for a 5G
base-station prototype. It improved on already optimized SIMD-parallel CPU
algorithms and designed a new software tool for higher programmer productivity
when converting MATLAB code to optimized C
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11452</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11452</id><created>2018-10-24</created><authors><author><keyname>sankar</keyname><forenames>A. Bharathi</forenames></author><author><keyname>Seyezhai</keyname><forenames>R.</forenames></author></authors><title>Analysis and Development of SiC MOSFET Boost Converter as Solar PV
  Pre-regulator</title><categories>physics.app-ph cond-mat.mes-hall eess.SP physics.ins-det</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Renewable energy source such as photovoltaic (PV) cell generates power from
the sun light by converting solar power to electrical power with no moving
parts and less maintenance. A single photovoltaic cell produces voltage of low
level. In order to boost up the voltage, a DC-DC boost converter is used. In
order to use this DC-DC converter for high voltage and high frequency
applications, Silicon Carbide (SiC) device is most preferred because of larger
current carrying capability, higher voltage blocking capability, high operating
temperature and less static and dynamic losses than the traditional silicon
(Si) power switches. In the proposed work, the static and dynamic
characteristics of SiC MOSFET for different temperatures are observed. A SiC
MOSFET based boost converter is investigated which is powered by PV source.
This DC - DC converter is controlled using a Pulse-Width Method (PWM) and the
duty cycle d is calculated for tracking the maximum power point using
incremental conductance algorithm of the PV systems implemented in FPGA.
Simulation studies are carried out in MATLAB/SIMULINK.A prototype of the SiC
converter is built and the results are verified experimentally. The performance
parameters of the proposed converter such as output voltage ripple input
current ripple and losses are computed and it is compared with the classical
silicon (Si) MOSFET converter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11453</identifier>
 <datestamp>2018-10-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11453</id><created>2018-10-25</created><authors><author><keyname>Aljohani</keyname><forenames>Tawfiq M</forenames></author></authors><title>Analysis of the Smart Grid as a System of Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The energy grid is currently undergoing a historic change of state from the
traditional structure where a utility owns the generation, transmission and
distribution services into an integrated smart grid in a monopolistic market
which introduce consumers as active players in managing and controlling the
power. This evolution adds more complexity to the energy scene as it requires
an unprecedented partnership of different fields and technologies to establish
successful integration of components with a bidirectional transfer of
information and energy. The Smart Grid Interoperability Panel (SGIP) was
incorporated in late 2009 to oversee the efforts in defining a set of standards
and interoperability layers to ensure a secure integration and operation of
various elements of the system. This paper discusses the smart grid concept as
a system-of-systems (SoS); analyzes the interoperability framework models
developed by SGIP and the European Union Smart Grid Cooperation Group
respectively; presents the stakeholders as domains and zones associated with
the interoperability layers; and illustrates other architectural concepts such
as fault-tolerance, software integration platform and design characteristics of
smart grid as an elegant system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11474</identifier>
 <datestamp>2019-08-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11474</id><created>2018-10-27</created><updated>2019-08-21</updated><authors><author><keyname>Wang</keyname><forenames>Ning</forenames></author><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Chen</keyname><forenames>Mo</forenames></author><author><keyname>Bao</keyname><forenames>Bocheng</forenames></author></authors><title>Generating Multi-Scroll Chua's Attractors via Simplified
  Piecewise-Linear Chua's Diode</title><categories>nlin.CD eess.SP</categories><comments>14 pages, 15 figures</comments><msc-class>37G35</msc-class><journal-ref>IEEE Transactions on Circuits and Systems I: Regular Papers, 2019</journal-ref><doi>10.1109/TCSI.2019.2933365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High implementation complexity of multi-scroll circuit is a bottleneck
problem in real chaos-based communication. Especially, in multi-scroll Chua's
circuit, the simplified implementation of piecewise-linear resistors with
multiple segments is difficult due to their intricate irregular breakpoints and
slopes. To solve the challenge, this paper presents a systematic scheme for
synthesizing a Chua's diode with multi-segment piecewise-linearity, which is
achieved by cascading even-numbered passive nonlinear resistors with
odd-numbered ones via a negative impedance converter. The traditional voltage
mode op-amps are used to implement nonlinear resistors. As no extra DC bias
voltage is employed, the scheme can be implemented by much simpler circuits.
The voltage-current characteristics of the obtained Chua's diode are analyzed
theoretically and verified by numerical simulations. Using the Chua's diode and
a second-order active Sallen-Key high-pass filter, a new inductor-free Chua's
circuit is then constructed to generate multi-scroll chaotic attractors.
Different number of scrolls can be generated by changing the number of passive
nonlinear resistor cells or adjusting two coupling parameters. Besides, the
system can be scaled by using different power supplies, satisfying the
low-voltage low-power requirement of integrated circuit design. The circuit
simulations and hardware experiments both confirmed the feasibility of the
designed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11520</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11520</id><created>2018-10-26</created><updated>2018-10-30</updated><authors><author><keyname>Oh</keyname><forenames>Jaehoon</forenames></author><author><keyname>Kim</keyname><forenames>Duyeon</forenames></author><author><keyname>Yun</keyname><forenames>Se-Young</forenames></author></authors><title>Spectrogram-channels u-net: a source separation model viewing each
  channel as the spectrogram of each source</title><categories>cs.SD cs.LG eess.AS eess.SP stat.ML</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sound source separation has attracted attention from Music Information
Retrieval(MIR) researchers, since it is related to many MIR tasks such as
automatic lyric transcription, singer identification, and voice conversion. In
this paper, we propose an intuitive spectrogram-based model for source
separation by adapting U-Net. We call it Spectrogram-Channels U-Net, which
means each channel of the output corresponds to the spectrogram of separated
source itself. The proposed model can be used for not only singing voice
separation but also multi-instrument separation by changing only the number of
output channels. In addition, we propose a loss function that balances volumes
between different sources. Finally, we yield performance that is
state-of-the-art on both separation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11528</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11528</id><created>2018-10-15</created><authors><author><keyname>Hammouti</keyname><forenames>Hajar El</forenames></author><author><keyname>Benjillali</keyname><forenames>Mustapha</forenames></author><author><keyname>Shihada</keyname><forenames>Basem</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Learn to Fly: A Distributed Mechanism for Joint 3D Placement and Users
  Association in UAVs-assisted Networks</title><categories>eess.SP cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the joint 3D placement of unmanned aerial vehicles
(UAVs) and UAVs-users association under bandwidth limitation and quality of
service constraint. In particular, in order to allow to UAVs to dynamically
improve their 3D locations in a distributed fashion while maximizing the
network's sum-rate, we break the underlying optimization into 3 subproblems
where we separately solve the 2D UAVs positioning, the altitude optimization,
and the UAVs-users association. First, given fixed 3D positions of UAVs, we
propose a fully distributed matching based association that alleviates the
bottlenecks of bandwidth allocation and guarantees the required quality of
service. Next, to address the 2D positions of UAVs, we adopt a modified version
of K-means algorithm, with a distributed implementation, where UAVs dynamically
change their 2D positions in order to reach the barycenter of the cluster that
is composed of the served ground users. In order to optimize the UAVs
altitudes, we study a naturally defined game-theoretic version of the problem
and show that under fixed UAVs 2D coordinates, a predefined association scheme,
and limited-interference, the UAVs altitudes game is a non-cooperative
potential game where the players (UAVs) can maximize the limited-interference
sum-rate by only optimizing a local utility function. Therefore, we adopt the
best response dynamics to reach a Nash equilibrium of the game which is also a
local optimum of the social welfare function. Our simulation results show that,
using the proposed approach, the network's sum rate of the studied scenario is
improved as compared with the trivial case where the classical version of
K-means is adopted and users are assigned, at each iteration, to the closest
UAV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11573</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11573</id><created>2018-10-26</created><authors><author><keyname>Noman</keyname><forenames>Fuad</forenames></author><author><keyname>Ting</keyname><forenames>Chee-Ming</forenames></author><author><keyname>Salleh</keyname><forenames>Sh-Hussain</forenames></author><author><keyname>Ombao</keyname><forenames>Hernando</forenames></author></authors><title>Short-segment heart sound classification using an ensemble of deep
  convolutional neural networks</title><categories>cs.SD cs.LG eess.AS eess.SP stat.ML</categories><comments>8 pages, 1 figure, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a framework based on deep convolutional neural networks
(CNNs) for automatic heart sound classification using short-segments of
individual heart beats. We design a 1D-CNN that directly learns features from
raw heart-sound signals, and a 2D-CNN that takes inputs of two- dimensional
time-frequency feature maps based on Mel-frequency cepstral coefficients
(MFCC). We further develop a time-frequency CNN ensemble (TF-ECNN) combining
the 1D-CNN and 2D-CNN based on score-level fusion of the class probabilities.
On the large PhysioNet CinC challenge 2016 database, the proposed CNN models
outperformed traditional classifiers based on support vector machine and hidden
Markov models with various hand-crafted time- and frequency-domain features.
Best classification scores with 89.22% accuracy and 89.94% sensitivity were
achieved by the ECNN, and 91.55% specificity and 88.82% modified accuracy by
the 2D-CNN alone on the test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11575</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11575</id><created>2018-10-26</created><updated>2019-11-12</updated><authors><author><keyname>Zou</keyname><forenames>Qing</forenames></author><author><keyname>Poddar</keyname><forenames>Sunrita</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>Sampling of Planar Curves: Theory and Fast Algorithms</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2954508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a continuous domain framework for the recovery of a planar curve
from a few samples. We model the curve as the zero level set of a trigonometric
polynomial. We show that the exponential feature maps of the points on the
curve lie on a low-dimensional subspace. We show that the null-space vector of
the feature matrix can be used to uniquely identify the curve, given a
sufficient number of samples. The worst-case theoretical guarantees show that
the number of samples required for unique recovery depends on the bandwidth of
the underlying trigonometric polynomial, which is a measure of the complexity
of the curve. We introduce an iterative algorithm that relies on the low-rank
property of the feature maps to recover the curves when the samples are noisy
or when the true bandwidth of the curve is unknown. We also demonstrate the
preliminary utility of the proposed curve representation in the context of
image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11609</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11609</id><created>2018-10-27</created><updated>2018-10-30</updated><authors><author><keyname>Narayanan</keyname><forenames>H.</forenames></author><author><keyname>Narayanan</keyname><forenames>Hariharan</forenames></author></authors><title>On the linear static output feedback problem: the annihilating
  polynomial approach</title><categories>eess.SP math.OC</categories><comments>24 pages</comments><msc-class>15A03, 15A04, 15A18, 93B52, 93B55, 93C05, 93C35, 93D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the fundamental open problems in control theory is that of the
stabilization of a linear time invariant dynamical system through static output
feedback. We are given a linear dynamical system defined through \begin{align*}
  \mydot{w} &amp;= Aw + Bu
  y &amp;= Cw . \end{align*} The problem is to find, if it exists, a feedback
$u=Ky$ such that the matrix $A+BKC$ has all its eigenvalues in the complex left
half plane and, if such a feedback does not exist, to prove that it does not.
Substantial progress has not been made on the computational aspect of the
solution to this problem.
  In this paper we consider instead `which annihilating polynomials can a
matrix of the form $A+BKC$ possess?'.
  We give a simple solution to this problem when the system has either a single
input or a single output. For the multi input - multi output case, we use these
ideas to characterize the annihilating polynomials when $K$ has rank one, and
suggest possible computational solutions for general $K.$
  We also present some numerical evidence for the plausibility of this approach
for the general case as well as for the problem of shifting the eigenvalues of
the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11614</identifier>
 <datestamp>2019-07-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11614</id><created>2018-10-27</created><updated>2019-07-19</updated><authors><author><keyname>Yu</keyname><forenames>Siwei</forenames></author><author><keyname>Ma</keyname><forenames>Jianwei</forenames></author><author><keyname>Wang</keyname><forenames>Wenlong</forenames></author></authors><title>Deep learning for denoising</title><categories>physics.geo-ph cs.LG eess.SP</categories><comments>59 pages, 27 figures</comments><msc-class>86A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared with traditional seismic noise attenuation algorithms that depend on
signal models and their corresponding prior assumptions, removing noise with a
deep neural network is trained based on a large training set, where the inputs
are the raw datasets and the corresponding outputs are the desired clean data.
After the completion of training, the deep learning method achieves adaptive
denoising with no requirements of (i) accurate modelings of the signal and
noise, or (ii) optimal parameters tuning. We call this intelligent denoising.
We use a convolutional neural network as the basic tool for deep learning. In
random and linear noise attenuation, the training set is generated with
artificially added noise. In the multiple attenuation step, the training set is
generated with acoustic wave equation. Stochastic gradient descent is used to
solve the optimal parameters for the convolutional neural network. The runtime
of deep learning on a graphics processing unit for denoising has the same order
as the $f-x$ deconvolution method. Synthetic and field results show the
potential applications of deep learning in automatic attenuation of random
noise (with unknown variance), linear noise, and multiples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11633</identifier>
 <datestamp>2019-09-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11633</id><created>2018-10-27</created><authors><author><keyname>Tachella</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Altmann</keyname><forenames>Yoann</forenames></author><author><keyname>Ren</keyname><forenames>Ximing</forenames></author><author><keyname>McCarthy</keyname><forenames>Aongus</forenames></author><author><keyname>Buller</keyname><forenames>Gerald S.</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steve</forenames></author></authors><title>Bayesian 3D Reconstruction of Complex Scenes from Single-Photon Lidar
  Data</title><categories>eess.IV</categories><journal-ref>SIAM Journal on Imaging Sciences 2019 12:1, 521-550</journal-ref><doi>10.1137/18M1183972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Light detection and ranging (Lidar) data can be used to capture the depth and
intensity profile of a 3D scene. This modality relies on constructing, for each
pixel, a histogram of time delays between emitted light pulses and detected
photon arrivals. In a general setting, more than one surface can be observed in
a single pixel. The problem of estimating the number of surfaces, their
reflectivity and position becomes very challenging in the low-photon regime
(which equates to short acquisition times) or relatively high background levels
(i.e., strong ambient illumination). This paper presents a new approach to 3D
reconstruction using single-photon, single-wavelength Lidar data, which is
capable of identifying multiple surfaces in each pixel. Adopting a Bayesian
approach, the 3D structure to be recovered is modelled as a marked point
process and reversible jump Markov chain Monte Carlo (RJ-MCMC) moves are
proposed to sample the posterior distribution of interest. In order to promote
spatial correlation between points belonging to the same surface, we propose a
prior that combines an area interaction process and a Strauss process. New
RJ-MCMC dilation and erosion updates are presented to achieve an efficient
exploration of the configuration space. To further reduce the computational
load, we adopt a multiresolution approach, processing the data from a coarse to
the finest scale. The experiments performed with synthetic and real data show
that the algorithm obtains better reconstructions than other recently published
optimization algorithms for lower execution times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11641</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11641</id><created>2018-10-27</created><updated>2018-11-05</updated><authors><author><keyname>Hafner</keyname><forenames>Frank</forenames></author><author><keyname>Bhuiyan</keyname><forenames>Amran</forenames></author><author><keyname>Kooij</keyname><forenames>Julian F. P.</forenames></author><author><keyname>Granger</keyname><forenames>Eric</forenames></author></authors><title>A Cross-Modal Distillation Network for Person Re-identification in
  RGB-Depth</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person re-identification involves the recognition over time of individuals
captured using multiple distributed sensors. With the advent of powerful deep
learning methods able to learn discriminant representations for visual
recognition, cross-modal person re-identification based on different sensor
modalities has become viable in many challenging applications in, e.g.,
autonomous driving, robotics and video surveillance. Although some methods have
been proposed for re-identification between infrared and RGB images, few
address depth and RGB images. In addition to the challenges for each modality
associated with occlusion, clutter, misalignment, and variations in pose and
illumination, there is a considerable shift across modalities since data from
RGB and depth images are heterogeneous. In this paper, a new cross-modal
distillation network is proposed for robust person re-identification between
RGB and depth sensors. Using a two-step optimization process, the proposed
method transfers supervision between modalities such that similar structural
features are extracted from both RGB and depth modalities, yielding a
discriminative mapping to a common feature space. Our experiments investigate
the influence of the dimensionality of the embedding space, compares transfer
learning from depth to RGB and vice versa, and compares against other
state-of-the-art cross-modal re-identification methods. Results obtained with
BIWI and RobotPKU datasets indicate that the proposed method can successfully
transfer descriptive structural features from the depth modality to the RGB
modality. It can significantly outperform state-of-the-art conventional methods
and deep neural networks for cross-modal sensing between RGB and depth, with no
impact on computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11705</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11705</id><created>2018-10-27</created><authors><author><keyname>Li</keyname><forenames>Heju</forenames></author><author><keyname>Chen</keyname><forenames>Xukai</forenames></author><author><keyname>Du</keyname><forenames>Haohua</forenames></author><author><keyname>He</keyname><forenames>Xin</forenames></author><author><keyname>Qian</keyname><forenames>Jianwei</forenames></author><author><keyname>Wan</keyname><forenames>Peng-Jun</forenames></author><author><keyname>Yang</keyname><forenames>Panlong</forenames></author></authors><title>Wi-Motion: A Robust Human Activity Recognition Using WiFi Signals</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has shown that human motions and positions can be recognized
through WiFi signals. The key intuition is that different motions and positions
introduce different multi-path distortions in WiFi signals and generate
different patterns in the time-series of channel state information (CSI). In
this paper, we propose Wi-Motion, a WiFi-based human activities recognition
system. Unlike existing systems, Wi-Motion adopts the amplitude and phase
information extracted from the CSI sequence to construct the classifiers
respectively, and combines the results using a combination strategy based on
posterior probability. As the simulation results shows, Wi-Motion can recognize
six human activities with the mean accuracy of 98:4%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11707</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11707</id><created>2018-10-27</created><authors><author><keyname>Xiao</keyname><forenames>Ning</forenames></author><author><keyname>Yang</keyname><forenames>Panlong</forenames></author><author><keyname>Yan</keyname><forenames>Yubo</forenames></author><author><keyname>Zhou</keyname><forenames>Hao</forenames></author><author><keyname>Li</keyname><forenames>Xiang-Yang</forenames></author><author><keyname>Du</keyname><forenames>Haohua</forenames></author></authors><title>From Communication to Sensing : Recognizing and Counting Repetitive
  Motions with Wireless Backscattering</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently several ground-breaking RF-based motion recognition systems were
proposed to detect and/or recognize macro/micro human movements. These systems
often suffer from various interferences caused by multiple-users moving
simultaneously, resulting in extremely low recognition accuracy. To tackle this
challenge, we propose a novel system, called Motion-Fi, which marries battery
free wireless backscattering and device-free sensing. Motion-Fi is an accurate,
interference tolerable motion-recognition system, which counts repetitive
motions without using scenario-dependent templates or profiles and enables
multi-users performing certain motions simultaneously because of the relatively
short transmission range of backscattered signals. Although the repetitive
motions are fairly well detectable through the backscattering signals in
theory, in reality they get blended into various other system noises during the
motion. Moreover, irregular motion patterns among users will lead to expensive
computation cost for motion recognition. We build a backscattering wireless
platform to validate our design in various scenarios for over 6 months when
different persons, distances and orientations are incorporated. In our
experiments, the periodicity in motions could be recognized without any
learning or training process, and the accuracy of counting such motions can be
achieved within 5% count error. With little efforts in learning the patterns,
our method could achieve 93.1% motion-recognition accuracy for a variety of
motions. Moreover, by leveraging the periodicity of motions, the recognition
accuracy could be further improved to nearly 100% with only 3 repetitions. Our
experiments also show that the motions of multiple persons separating by around
2 meters cause little accuracy reduction in the counting process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11717</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11717</id><created>2018-10-27</created><authors><author><keyname>Kobayashi</keyname><forenames>Ricardo Tadashi</forenames></author><author><keyname>Abrao</keyname><forenames>Taufik</forenames></author></authors><title>FBMC Prototype Filter Design via Convex Optimization</title><categories>eess.SP</categories><comments>Paper accepted to the IEEE Transactions on Vehicular Technology. 29
  pages, 4 tables, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a prototype filter design for Filter Bank
MultiCarrier (FBMC) systems based on convex optimization, aiming superior
spectrum features while maintaining a high symbol reconstruction quality.
Initially, the proposed design is written as a non-convex Quadratically
Constrained Quadratic Programming (QCQP), which is relaxed into a convex QCQP
guided by a line search. Through the resulting problem, we design three
prototype filters: Type-I, II and III. In particular, the Type-II filter shows
a slightly better performance than the classical Mirabasi-Martin design, while
Type-I and III filters offer a much more contained spectrum than most of the
prototype filters suitable for FBMC applications. Furthermore, numerical
results corroborate the effectiveness of the designed filters as the proposed
filters offer fast decay and contained spectrum while not jeopardizing symbol
reconstruction in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11725</identifier>
 <datestamp>2019-08-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11725</id><created>2018-10-27</created><updated>2019-08-28</updated><authors><author><keyname>Medra</keyname><forenames>Mostafa</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew W.</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj</forenames></author></authors><title>Total Power Minimization: Joint Antenna Selection and Beamforming Design</title><categories>eess.SP cs.IT math.IT</categories><comments>The previous version had some flaws in the simulations. The weighted
  one-norm regularization parameter was not tuned correctly providing wrong gap
  between fractional programming and weighted one-norm. Accordingly in this
  revised paper (accepted in Asilomar 2019), we focused on the original problem
  using weighted one-norm only, and left the fractional programming part to
  future work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the total power minimization problem when we have
signal-to-interference-plus-noise ratio (SINR) constraints. The consumed power
in the circuits depends on the number of active antennas, which can be modeled
using zero-norm. Due to the difficulty of dealing with the non-convex
zero-norm, we used the standard alternate weighted one-norm approach. We
addressed the total power minimization for a narrowband system with and without
per-antenna power constraints (PAPCs). We derived iterative closed-form
expressions in both cases. Then we analysed the case when we have multiple
bands operating at the same time. Analogous closed-form expressions are
provided. Our simulation results show that significant gains can be obtained in
terms of the total power required compared to standard methods that do not take
into account the circuit power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11793</identifier>
 <datestamp>2019-08-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11793</id><created>2018-10-28</created><updated>2019-08-18</updated><authors><author><keyname>Yakura</keyname><forenames>Hiromu</forenames></author><author><keyname>Sakuma</keyname><forenames>Jun</forenames></author></authors><title>Robust Audio Adversarial Example for a Physical Attack</title><categories>cs.LG cs.CR cs.SD eess.AS stat.ML</categories><comments>Accepted to IJCAI 2019</comments><doi>10.24963/ijcai.2019/741</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to generate audio adversarial examples that can attack a
state-of-the-art speech recognition model in the physical world. Previous work
assumes that generated adversarial examples are directly fed to the recognition
model, and is not able to perform such a physical attack because of
reverberation and noise from playback environments. In contrast, our method
obtains robust adversarial examples by simulating transformations caused by
playback or recording in the physical world and incorporating the
transformations into the generation process. Evaluation and a listening
experiment demonstrated that our adversarial examples are able to attack
without being noticed by humans. This result suggests that audio adversarial
examples generated by the proposed method may become a real threat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11800</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11800</id><created>2018-10-28</created><updated>2018-10-30</updated><authors><author><keyname>Chen</keyname><forenames>Da</forenames></author><author><keyname>Huang</keyname><forenames>Qiwei</forenames></author><author><keyname>Feng</keyname><forenames>Hui</forenames></author><author><keyname>Zhao</keyname><forenames>Qing</forenames></author><author><keyname>Hu</keyname><forenames>Bo</forenames></author></authors><title>Active Anomaly Detection with Switching Cost</title><categories>eess.SP</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of anomaly detection among multiple processes is considered
within the framework of sequential design of experiments. The objective is an
active inference strategy consisting of a selection rule governing which
process to probe at each time, a stopping rule on when to terminate the
detection, and a decision rule on the final detection outcome. The performance
measure is the Bayes risk that takes into account of not only sample complexity
and detection errors, but also costs associated with switching across
processes. While the problem is a partially observable Markov decision process
to which optimal solutions are generally intractable, a low-complexity
deterministic policy is shown to be asymptotically optimal and offer
significant performance improvement over existing methods in the finite regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11846</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11846</id><created>2018-10-28</created><updated>2019-02-19</updated><authors><author><keyname>Valin</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Skoglund</keyname><forenames>Jan</forenames></author></authors><title>LPCNet: Improving Neural Speech Synthesis Through Linear Prediction</title><categories>eess.AS cs.LG cs.SD</categories><comments>ICASSP 2019, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural speech synthesis models have recently demonstrated the ability to
synthesize high quality speech for text-to-speech and compression applications.
These new models often require powerful GPUs to achieve real-time operation, so
being able to reduce their complexity would open the way for many new
applications. We propose LPCNet, a WaveRNN variant that combines linear
prediction with recurrent neural networks to significantly improve the
efficiency of speech synthesis. We demonstrate that LPCNet can achieve
significantly higher quality than WaveRNN for the same network size and that
high quality LPCNet speech synthesis is achievable with a complexity under 3
GFLOPS. This makes it easier to deploy neural synthesis applications on
lower-power devices, such as embedded systems and mobile phones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11939</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11939</id><created>2018-10-28</created><authors><author><keyname>Shen</keyname><forenames>Yu-Han</forenames></author><author><keyname>He</keyname><forenames>Ke-Xin</forenames></author><author><keyname>Zhang</keyname><forenames>Wei-Qiang</forenames></author></authors><title>Learning How to Listen: A Temporal-Frequential Attention Model for Sound
  Event Detection</title><categories>cs.SD eess.AS</categories><comments>5 pages, to be submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a temporal-frequential attention model for sound
event detection (SED). Our network learns how to listen with two attention
models: a temporal attention model and a frequential attention model. Proposed
system learns when to listen using the temporal attention model while it learns
where to listen on the frequency axis using the frequential attention model.
With these two models, we attempt to make our system pay more attention to
important frames or segments and important frequency components for sound event
detection. Our proposed method is demonstrated on the task 2 of Detection and
Classification of Acoustic Scenes and Events (DCASE) 2017 Challenge and
achieves competitive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11944</identifier>
 <datestamp>2018-12-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11944</id><created>2018-10-28</created><authors><author><keyname>Wang</keyname><forenames>Yongchao</forenames></author><author><keyname>Wang</keyname><forenames>Yanjiao</forenames></author><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author></authors><title>Optimized Signal Distortion for PAPR Reduction of OFDM Signals with
  IFFT/FFT Complexity via ADMM Approaches</title><categories>eess.SP cs.IT math.IT</categories><comments>15 pages, 7 figures</comments><doi>10.1109/TSP.2018.2880711</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose two low-complexity optimization methods to reduce
peak-to-average power ratio (PAPR) values of orthogonal frequency division
multiplexing (OFDM) signals via alternating direction method of multipliers
(ADMM). First, we formulate a non-convex signal distortion optimization model
based on minimizing data carrier distortion such that the constraints are
placed on PAPR and the power of free carriers. Second, to obtain the model's
approximate optimal solution efficiently, we design two low-complexity ADMM
algorithms, named ADMM-Direct and ADMM-Relax respectively. Third, we show that,
in ADMM-Direct/-Relax, all the optimization subproblems can be solved
semi-analytically and the computational complexity in each iteration is roughly
O(lNlog2(lN)), where l and N are over sampling factor and carrier number
respectively. Moreover, we show that the resulting solution of ADMM-Direct is
guaranteed to be some Karush-Kuhn-Tucker (KKT) point of the non-convex model
when the iteration algorithm is convergent. For ADMM-Relax, we prove that it
has theoretically guaranteed convergence and can approach arbitrarily close to
some KKT point of the model if proper parameters are chosen. Simulation results
demonstrate the effectiveness of the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11945</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11945</id><created>2018-10-29</created><updated>2018-10-30</updated><authors><author><keyname>Takaki</keyname><forenames>Shinji</forenames></author><author><keyname>Nakashika</keyname><forenames>Toru</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author></authors><title>STFT spectral loss for training a neural speech waveform model</title><categories>eess.AS cs.CL cs.SD stat.ML</categories><comments>Submitted to the 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a new loss using short-time Fourier transform (STFT)
spectra for the aim of training a high-performance neural speech waveform model
that predicts raw continuous speech waveform samples directly. Not only
amplitude spectra but also phase spectra obtained from generated speech
waveforms are used to calculate the proposed loss. We also mathematically show
that training of the waveform model on the basis of the proposed loss can be
interpreted as maximum likelihood training that assumes the amplitude and phase
spectra of generated speech waveforms following Gaussian and von Mises
distributions, respectively. Furthermore, this paper presents a simple network
architecture as the speech waveform model, which is composed of uni-directional
long short-term memories (LSTMs) and an auto-regressive structure. Experimental
results showed that the proposed neural model synthesized high-quality speech
waveforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11946</identifier>
 <datestamp>2019-04-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11946</id><created>2018-10-29</created><updated>2019-04-26</updated><authors><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Takaki</keyname><forenames>Shinji</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author></authors><title>Neural source-filter-based waveform model for statistical parametric
  speech synthesis</title><categories>eess.AS cs.SD stat.ML</categories><comments>Submitted to ICASSP 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural waveform models such as the WaveNet are used in many recent
text-to-speech systems, but the original WaveNet is quite slow in waveform
generation because of its autoregressive (AR) structure. Although faster non-AR
models were recently reported, they may be prohibitively complicated due to the
use of a distilling training method and the blend of other disparate training
criteria. This study proposes a non-AR neural source-filter waveform model that
can be directly trained using spectrum-based training criteria and the
stochastic gradient descent method. Given the input acoustic features, the
proposed model first uses a source module to generate a sine-based excitation
signal and then uses a filter module to transform the excitation signal into
the output speech waveform. Our experiments demonstrated that the proposed
model generated waveforms at least 100 times faster than the AR WaveNet and the
quality of its synthetic speech is close to that of speech generated by the AR
WaveNet. Ablation test results showed that both the sine-wave excitation signal
and the spectrum-based training criteria were essential to the performance of
the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11960</identifier>
 <datestamp>2019-02-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11960</id><created>2018-10-29</created><updated>2019-02-14</updated><authors><author><keyname>Yasuda</keyname><forenames>Yusuke</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Takaki</keyname><forenames>Shinji</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author></authors><title>Investigation of enhanced Tacotron text-to-speech synthesis systems with
  self-attention for pitch accent language</title><categories>eess.AS cs.CL cs.SD stat.ML</categories><comments>to be appeared at ICASSP 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  End-to-end speech synthesis is a promising approach that directly converts
raw text to speech. Although it was shown that Tacotron2 outperforms classical
pipeline systems with regards to naturalness in English, its applicability to
other languages is still unknown. Japanese could be one of the most difficult
languages for which to achieve end-to-end speech synthesis, largely due to its
character diversity and pitch accents. Therefore, state-of-the-art systems are
still based on a traditional pipeline framework that requires a separate text
analyzer and duration model. Towards end-to-end Japanese speech synthesis, we
extend Tacotron to systems with self-attention to capture long-term
dependencies related to pitch accents and compare their audio quality with
classical pipeline systems under various conditions to show their pros and
cons. In a large-scale listening test, we investigated the impacts of the
presence of accentual-type labels, the use of force or predicted alignments,
and acoustic features used as local condition parameters of the Wavenet
vocoder. Our results reveal that although the proposed systems still do not
match the quality of a top-line pipeline system for Japanese, we show important
stepping stones towards end-to-end Japanese speech synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11968</identifier>
 <datestamp>2019-04-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11968</id><created>2018-10-29</created><updated>2019-04-01</updated><authors><author><keyname>Berk</keyname><forenames>Aaron</forenames></author><author><keyname>Plan</keyname><forenames>Yaniv</forenames></author><author><keyname>Yilmaz</keyname><forenames>&#xd6;zg&#xfc;r</forenames></author></authors><title>Sensitivity of $\ell_{1}$ minimization to parameter choice</title><categories>cs.IT eess.SP math.IT math.OC</categories><msc-class>90C31, 90C47, 94A15, 60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of generalized LASSO is a common technique for recovery of structured
high-dimensional signals. Each generalized LASSO program has a governing
parameter whose optimal value depends on properties of the data. At this
optimal value, compressed sensing theory explains why LASSO programs recover
structured high-dimensional signals with minimax order-optimal error.
Unfortunately in practice, the optimal choice is generally unknown and must be
estimated. Thus, we investigate stability of each LASSO program with respect to
its governing parameter. Our goal is to aid the practitioner in answering the
following question: given real data, which LASSO program should be used? We
take a step towards answering this by analyzing the case where the measurement
matrix is identity (the so-called proximal denoising setup) and we use
$\ell_{1}$ regularization. For each LASSO program, we specify settings in which
that program is provably unstable with respect to its governing parameter. We
support our analysis with detailed numerical simulations. For example, there
are settings where a 0.1% underestimate of a LASSO parameter can increase the
error significantly; and a 50% underestimate can cause the error to increase by
a factor of $10^{9}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.11990</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.11990</id><created>2018-10-29</created><authors><author><keyname>Ferguson</keyname><forenames>Eric L.</forenames></author><author><keyname>Williams</keyname><forenames>Stefan B.</forenames></author><author><keyname>Jin</keyname><forenames>Craig T.</forenames></author></authors><title>Improved multipath time delay estimation using cepstrum subtraction</title><categories>cs.SD eess.AS</categories><comments>Final predraft submitted to 2019 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP 2019), in Brighton, UK, May
  2019. 5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a motor-powered vessel travels past a fixed hydrophone in a multipath
environment, a Lloyd's mirror constructive/destructive interference pattern is
observed in the output spectrogram. The power cepstrum detects the periodic
structure of the Lloyd's mirror pattern by generating a sequence of pulses
(rahmonics) located at the fundamental quefrency (periodic time) and its
multiples. This sequence is referred to here as the `rahmonic component' of the
power cepstrum. The fundamental quefrency, which is the reciprocal of the
frequency difference between adjacent interference fringes, equates to the
multipath time delay. The other component of the power cepstrum is the
non-rahmonic (extraneous) component, which combines with the rahmonic component
to form the (total) power cepstrum. A data processing technique, termed
`cepstrum subtraction', is described. This technique suppresses the extraneous
component of the power cepstrum, leaving the rahmonic component that contains
the desired multipath time delay information. This technique is applied to real
acoustic recordings of motor-vessel transits in a shallow water environment,
where the broadband noise radiated by the vessel arrives at the hydrophone via
a direct ray path and a time-delayed multipath. The results show that cepstrum
subtraction improves multipath time delay estimation by a factor of two for the
at-sea experiment.
  keywords - time delay estimation, underwater acoustics, cepstrum, source
localization, autocorrelation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12001</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12001</id><created>2018-10-29</created><updated>2018-10-30</updated><authors><author><keyname>Zhou</keyname><forenames>Xinpei</forenames></author><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Zhou</keyname><forenames>Xi</forenames></author></authors><title>Cascaded CNN-resBiLSTM-CTC: An End-to-End Acoustic Model For Speech
  Recognition</title><categories>eess.AS cs.CL cs.SD</categories><comments>5 pages, 1 figure, 4 tables. Submitted to 2019 ICASSP (International
  Conference on Acoustics, Speech, and Signal Processing)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic speech recognition (ASR) tasks are resolved by end-to-end deep
learning models, which benefits us by less preparation of raw data, and easier
transformation between languages. We propose a novel end-to-end deep learning
model architecture namely cascaded CNN-resBiLSTM-CTC. In the proposed model, we
add residual blocks in BiLSTM layers to extract sophisticated phoneme and
semantic information together, and apply cascaded structure to pay more
attention mining information of hard negative samples. By applying both simple
Fast Fourier Transform (FFT) technique and n-gram language model (LM) rescoring
method, we manage to achieve word error rate (WER) of 3.41% on LibriSpeech test
clean corpora. Furthermore, we propose a new batch-varied method to speed up
the training process in length-varied tasks, which result in 25% less training
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12020</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12020</id><created>2018-10-29</created><updated>2018-11-01</updated><authors><author><keyname>Yuan</keyname><forenames>Zhe</forenames></author><author><keyname>Lyu</keyname><forenames>Zhuoran</forenames></author><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Zhou</keyname><forenames>Xi</forenames></author></authors><title>An improved hybrid CTC-Attention model for speech recognition</title><categories>cs.SD eess.AS</categories><comments>Submitted to the 2019 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP), Brighton, UK, May 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, end-to-end speech recognition with a hybrid model consisting of the
connectionist temporal classification(CTC) and the attention encoder-decoder
achieved state-of-the-art results. In this paper, we propose a novel CTC
decoder structure based on the experiments we conducted and explore the
relation between decoding performance and the depth of encoder. We also apply
attention smoothing mechanism to acquire more context information for
subword-based decoding. Taken together, these strategies allow us to achieve a
word error rate(WER) of 4.43% without LM and 3.34% with RNN-LM on the
test-clean subset of the LibriSpeech corpora, which by far are the best
reported WERs for end-to-end ASR systems on this dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12039</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12039</id><created>2018-10-29</created><authors><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>1-Bit Massive MIMO Downlink Based on Constructive Interference</title><categories>eess.SP cs.IT math.IT</categories><comments>5 pages, EUSIPCO 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on the multiuser massive multiple-input single-output
(MISO) downlink with low-cost 1-bit digital-to-analog converters (DACs) for PSK
modulation, and propose a low-complexity refinement process that is applicable
to any existing 1-bit precoding approaches based on the constructive
interference (CI) formulation. With the decomposition of the signals along the
detection thresholds, we first formulate a simple symbol-scaling method as the
performance metric. The low-complexity refinement approach is subsequently
introduced, where we aim to improve the introduced symbol-scaling performance
metric by modifying the transmit signal on one antenna at a time. Numerical
results validate the effectiveness of the proposed refinement method on
existing approaches for massive MIMO with 1-bit DACs, and the performance
improvements are most significant for the low-complexity quantized zero-forcing
(ZF) method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12044</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12044</id><created>2018-10-29</created><authors><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Liu</keyname><forenames>Fan</forenames></author></authors><title>Hybrid Analog-Digital Precoding for Interference Exploitation</title><categories>eess.SP cs.IT math.IT</categories><comments>5 pages, EUSIPCO 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the multi-user massive multiple-input-single-output (MISO) and focus
on the downlink systems where the base station (BS) employs hybrid
analog-digital precoding with low-cost 1-bit digital-to-analog converters
(DACs). In this paper, we propose a hybrid downlink transmission scheme where
the analog precoder is formed based on the SVD decomposition. In the digital
domain, instead of designing a linear transmit precoding matrix, we directly
design the transmit signals by exploiting the concept of constructive
interference. The optimization problem is then formulated based on the geometry
of the modulation constellations and is shown to be non-convex. We relax the
above optimization and show that the relaxed optimization can be transformed
into a linear programming that can be efficiently solved. Numerical results
validate the superiority of the proposed scheme for the hybrid massive MIMO
downlink systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12051</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12051</id><created>2018-10-29</created><authors><author><keyname>Bollepalli</keyname><forenames>Bajibabu</forenames></author><author><keyname>Juvela</keyname><forenames>Lauri</forenames></author><author><keyname>Alku</keyname><forenames>Paavo</forenames></author></authors><title>Speaking style adaptation in Text-To-Speech synthesis using
  Sequence-to-sequence models with attention</title><categories>cs.SD cs.CL eess.AS</categories><comments>5 pages, 5 figures. Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, there are increasing interests in text-to-speech (TTS) synthesis
to use sequence-to-sequence models with attention. These models are end-to-end
meaning that they learn both co-articulation and duration properties directly
from text and speech. Since these models are entirely data-driven, they need
large amounts of data to generate synthetic speech with good quality. However,
in challenging speaking styles, such as Lombard speech, it is difficult to
record sufficiently large speech corpora. Therefore, in this study we propose a
transfer learning method to adapt a sequence-to-sequence based TTS system of
normal speaking style to Lombard style. Moreover, we experiment with a WaveNet
vocoder in synthesis of Lombard speech. We conducted subjective evaluations to
assess the performance of the adapted TTS systems. The subjective evaluation
results indicated that an adaptation system with the WaveNet vocoder clearly
outperformed the conventional deep neural network based TTS system in synthesis
of Lombard speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12075</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12075</id><created>2018-10-29</created><authors><author><keyname>Ouyang</keyname><forenames>Chongjun</forenames></author><author><keyname>Ou</keyname><forenames>Zeliang</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Yang</keyname><forenames>Hongwen</forenames></author></authors><title>Asymptotic Upper Capacity Bound for Receive Antenna Selection in Massive
  MIMO Systems</title><categories>eess.SP</categories><comments>Submitted to ICC 2019</comments><msc-class>34K25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the receive antenna selection in massive multiple-input
multiple-output (MIMO) system. The receiver, equipped with a large-scale
antenna array whose size is much larger than that of the transmitter, selects a
subset of antennas to receive messages. A low-complexity asymptotic
approximated upper capacity bound is derived in the limit of massive MIMO
systems over independent and identical distributed flat fading Rayleigh
channel, assuming that the channel side information (CSI) is only available at
the receiver. Furthermore, the asymptotic theory is separately applied to two
scenarios which is based on whether the total amount of the selected antennas
exceed that of the transmit antennas. Besides analytical derivations,
simulation results are provided to demonstrate the approximation precision of
the asymptotic results and the tightness of the capacity bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12093</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12093</id><created>2018-10-22</created><authors><author><keyname>Zhang</keyname><forenames>Junwei</forenames></author><author><keyname>Wen</keyname><forenames>Yuanhui</forenames></author><author><keyname>Tan</keyname><forenames>Heyun</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Shen</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Maochun</forenames></author><author><keyname>Zhu</keyname><forenames>Jiangbo</forenames></author><author><keyname>Guo</keyname><forenames>Changjian</forenames></author><author><keyname>Chen</keyname><forenames>Yujie</forenames></author><author><keyname>Li</keyname><forenames>Zhaohui</forenames></author><author><keyname>Yu</keyname><forenames>Siyuan</forenames></author></authors><title>80-Channel WDM-MDM Transmission over 50-km Ring-Core Fiber Using a
  Compact OAM DEMUX and Modular 4x4 MIMO Equalization</title><categories>eess.SP</categories><comments>3 pages,2 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  8-OAM modes each carrying 10 wavelengths with 2.56-Tbit/s aggregated capacity
and 10.24-bit/s/Hz spectral efficiency have been transmitted over 50-km
specially designed ring-core fiber, using a compact OAM mode sorter and only
modular 4x4 MIMO equalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12126</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12126</id><created>2018-10-29</created><authors><author><keyname>Angelini</keyname><forenames>Federico</forenames></author><author><keyname>Fu</keyname><forenames>Zeyu</forenames></author><author><keyname>Long</keyname><forenames>Yang</forenames></author><author><keyname>Shao</keyname><forenames>Ling</forenames></author><author><keyname>Naqvi</keyname><forenames>Syed Mohsen</forenames></author></authors><title>ActionXPose: A Novel 2D Multi-view Pose-based Algorithm for Real-time
  Human Action Recognition</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ActionXPose, a novel 2D pose-based algorithm for posture-level
Human Action Recognition (HAR). The proposed approach exploits 2D human poses
provided by OpenPose detector from RGB videos. ActionXPose aims to process
poses data to be provided to a Long Short-Term Memory Neural Network and to a
1D Convolutional Neural Network, which solve the classification problem.
ActionXPose is one of the first algorithms that exploits 2D human poses for
HAR. The algorithm has real-time performance and it is robust to camera
movings, subject proximity changes, viewpoint changes, subject appearance
changes and provide high generalization degree. In fact, extensive simulations
show that ActionXPose can be successfully trained using different datasets at
once. State-of-the-art performance on popular datasets for posture-related HAR
problems (i3DPost, KTH) are provided and results are compared with those
obtained by other methods, including the selected ActionXPose baseline.
Moreover, we also proposed two novel datasets called MPOSE and ISLD recorded in
our Intelligent Sensing Lab, to show ActionXPose generalization performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12136</identifier>
 <datestamp>2019-07-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12136</id><created>2018-10-29</created><updated>2019-06-29</updated><authors><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Zhang</keyname><forenames>Sixin</forenames></author><author><keyname>Rochette</keyname><forenames>Gaspar</forenames></author></authors><title>Phase Harmonic Correlations and Convolutional Neural Networks</title><categories>eess.SP cs.LG stat.ML</categories><comments>26 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major issue in harmonic analysis is to capture the phase dependence of
frequency representations, which carries important signal properties. It seems
that convolutional neural networks have found a way. Over time-series and
images, convolutional networks often learn a first layer of filters which are
well localized in the frequency domain, with different phases. We show that a
rectifier then acts as a filter on the phase of the resulting coefficients. It
computes signal descriptors which are local in space, frequency and phase. The
non-linear phase filter becomes a multiplicative operator over phase harmonics
computed with a Fourier transform along the phase. We prove that it defines a
bi-Lipschitz and invertible representation. The correlations of phase harmonics
coefficients characterise coherent structures from their phase dependence
across frequencies. For wavelet filters, we show numerically that signals
having sparse wavelet coefficients can be recovered from few phase harmonic
correlations, which provide a compressive representation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12137</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12137</id><created>2018-10-26</created><authors><author><keyname>Fu</keyname><forenames>Wenzhi</forenames></author><author><keyname>Yang</keyname><forenames>Jianlei</forenames></author><author><keyname>Dai</keyname><forenames>Pengcheng</forenames></author><author><keyname>Chen</keyname><forenames>Yiran</forenames></author><author><keyname>Zhao</keyname><forenames>Weisheng</forenames></author></authors><title>A Scalable Pipelined Dataflow Accelerator for Object Region Proposals on
  FPGA Platform</title><categories>cs.DC eess.IV</categories><comments>accepted by FPT 2018 Conference</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Region proposal is critical for object detection while it usually poses a
bottleneck in improving the computation efficiency on traditional control-flow
architectures. We have observed region proposal tasks are potentially suitable
for performing pipelined parallelism by exploiting dataflow driven
acceleration. In this paper, a scalable pipelined dataflow accelerator is
proposed for efficient region proposals on FPGA platform. The accelerator
processes image data by a streaming manner with three sequential stages:
resizing, kernel computing and sorting. First, Ping-Pong cache strategy is
adopted for rotation loading in resize module to guarantee continuous output
streaming. Then, a multiple pipelines architecture with tiered memory is
utilized in kernel computing module to complete the main computation tasks.
Finally, a bubble-pushing heap sort method is exploited in sorting module to
find the top-k largest candidates efficiently. Our design is implemented with
high level synthesis on FPGA platforms, and experimental results on VOC2007
datasets show that it could achieve about 3.67X speedups than traditional
desktop CPU platform and &gt;250X energy efficiency improvement than embedded ARM
platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12138</identifier>
 <datestamp>2019-10-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12138</id><created>2018-10-29</created><updated>2019-10-10</updated><authors><author><keyname>Marafioti</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Perraudin</keyname><forenames>Nathana&#xeb;l</forenames></author><author><keyname>Holighaus</keyname><forenames>Nicki</forenames></author><author><keyname>Majdak</keyname><forenames>Piotr</forenames></author></authors><title>A context encoder for audio inpainting</title><categories>cs.SD cs.LG eess.AS</categories><comments>Published in IEEE TASLP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the ability of deep neural networks (DNNs) to restore missing audio
content based on its context, i.e., inpaint audio gaps. We focus on a condition
which has not received much attention yet: gaps in the range of tens of
milliseconds. We propose a DNN structure that is provided with the signal
surrounding the gap in the form of time-frequency (TF) coefficients. Two DNNs
with either complex-valued TF coefficient output or magnitude TF coefficient
output were studied by separately training them on inpainting two types of
audio signals (music and musical instruments) having 64-ms long gaps. The
magnitude DNN outperformed the complex-valued DNN in terms of signal-to-noise
ratios and objective difference grades. Although, for instruments, a reference
inpainting obtained through linear predictive coding performed better in both
metrics, it performed worse than the magnitude DNN for music. This demonstrates
the potential of the magnitude DNN, in particular for inpainting signals that
are more complex than single instrument sounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12139</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12139</id><created>2018-10-29</created><authors><author><keyname>Gasulla</keyname><forenames>Ivana</forenames></author><author><keyname>Garcia</keyname><forenames>Sergi</forenames></author><author><keyname>Barrera</keyname><forenames>David</forenames></author><author><keyname>Hervas</keyname><forenames>Javier</forenames></author><author><keyname>Sales</keyname><forenames>Salvador</forenames></author></authors><title>Fiber-distributed Signal Processing: Where the Space Dimension Comes
  into Play</title><categories>eess.SP physics.app-ph</categories><comments>3 pages</comments><journal-ref>OSA Advanced Photonics Congress, New Orleans, US, paper W1D. 1,
  2017</journal-ref><doi>10.1364/PS.2017.PW1D.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present how to implement fiber-distributed signal processing in the
context of fiber-wireless communications by using different multicore fibers,
providing both radio access distribution and microwave photonics signal
processing in the same fiber medium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12154</identifier>
 <datestamp>2019-02-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12154</id><created>2018-10-29</created><updated>2019-02-01</updated><authors><author><keyname>Teng</keyname><forenames>Chieh-Fang</forenames></author><author><keyname>Wu</keyname><forenames>Chen-Hsi</forenames></author><author><keyname>Ho</keyname><forenames>Kuan-Shiuan</forenames></author><author><keyname>Wu</keyname><forenames>An-Yeu</forenames></author></authors><title>Low-complexity Recurrent Neural Network-based Polar Decoder with Weight
  Quantization Mechanism</title><categories>eess.SP cs.LG</categories><comments>5 pages, accepted by the 2019 International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes have drawn much attention and been adopted in 5G New Radio (NR)
due to their capacity-achieving performance. Recently, as the emerging deep
learning (DL) technique has breakthrough achievements in many fields, neural
network decoder was proposed to obtain faster convergence and better
performance than belief propagation (BP) decoding. However, neural networks are
memory-intensive and hinder the deployment of DL in communication systems. In
this work, a low-complexity recurrent neural network (RNN) polar decoder with
codebook-based weight quantization is proposed. Our test results show that we
can effectively reduce the memory overhead by 98% and alleviate computational
complexity with slight performance loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12157</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12157</id><created>2018-10-29</created><authors><author><keyname>Gasulla</keyname><forenames>Ivana</forenames></author><author><keyname>Garcia</keyname><forenames>Sergi</forenames></author><author><keyname>Barrera</keyname><forenames>David</forenames></author><author><keyname>Hervas</keyname><forenames>Javier</forenames></author><author><keyname>Sales</keyname><forenames>Salvador</forenames></author></authors><title>Space-division multiplexing for fiber-wireless communications</title><categories>eess.SP</categories><comments>4 pages, 20th International Conference on Transparent Optical
  Networks (ICTON), Girona (Spain), 2017. arXiv admin note: text overlap with
  arXiv:1810.12139</comments><doi>10.1109/ICTON.2017.8025132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We envision the application of optical Space-division Multiplexing (SDM) to
the next generation fiber-wireless communications as a firm candidate to
increase the end user capacity and provide adaptive radiofrequency-photonic
interfaces. This approach relies on the concept of fiber-distributed signal
processing, where the SDM fiber provides not only radio access distribution but
also broadband microwave photonics signal processing. In particular, we present
two different SDM fiber technologies: dispersion-engineered heterogeneous
multicore fiber links and multicavity devices built upon the selective
inscription of gratings in homogeneous multicore fibers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12170</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12170</id><created>2018-10-29</created><authors><author><keyname>Alon</keyname><forenames>Uri</forenames></author><author><keyname>Pundak</keyname><forenames>Golan</forenames></author><author><keyname>Sainath</keyname><forenames>Tara N.</forenames></author></authors><title>Contextual Speech Recognition with Difficult Negative Training Examples</title><categories>eess.AS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving the representation of contextual information is key to unlocking
the potential of end-to-end (E2E) automatic speech recognition (ASR). In this
work, we present a novel and simple approach for training an ASR context
mechanism with difficult negative examples. The main idea is to focus on proper
nouns (e.g., unique entities such as names of people and places) in the
reference transcript, and use phonetically similar phrases as negative
examples, encouraging the neural model to learn more discriminative
representations. We apply our approach to an end-to-end contextual ASR model
that jointly learns to transcribe and select the correct context items, and
show that our proposed method gives up to $53.1\%$ relative improvement in word
error rate (WER) across several benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12173</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12173</id><created>2018-10-29</created><authors><author><keyname>Garcia</keyname><forenames>Sergi</forenames></author><author><keyname>Gasulla</keyname><forenames>Ivana</forenames></author></authors><title>RF photonic delay lines using space-division multiplexing</title><categories>eess.SP physics.app-ph</categories><comments>10 pages</comments><journal-ref>Steep Dispersion Engineering and Opto-Atomic Precision Metrology
  XI 10548, SPIE Photonics West 2018, San Francisco, US, 105480J, 2018</journal-ref><doi>10.1117/12.2299169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review our last work on dispersion-engineered heterogeneous multicore
fiber links designed to act as tunable true time delay lines for radiofrequency
signals. This approach allows the realization of fiber distributed signal
processing in the context of fiber-wireless communications, providing both
radiofrequency access distribution and signal processing in the same fiber
medium. We show how to design trench-assisted heterogeneous multicore fibers to
fulfil the requirements for sampled true time delay line operation while
assuring a low level of crosstalk, bend sensitivity and tolerance to possible
fabrication errors. The performance of the designed radiofrequency photonic
delay lines is evaluated in the context of tunable microwave signal filtering
and optical beamforming for phased array antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12175</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12175</id><created>2018-10-29</created><authors><author><keyname>Garcia</keyname><forenames>Sergi</forenames></author><author><keyname>Urena</keyname><forenames>Mario</forenames></author><author><keyname>Guillem</keyname><forenames>Ruben</forenames></author><author><keyname>Gasulla</keyname><forenames>Ivana</forenames></author></authors><title>Multicore fiber delay line performance against bending and twisting
  effects</title><categories>eess.SP</categories><comments>3 pages, 44th European Conference on Optical Communication (ECOC
  2018), Rome, Italy, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report the experimental evaluation of bending and twisting effects on the
propagation and radiofrequency signal processing performance of multicore
fibers. We demonstrate that twisting minimizes to a large extend the group
delay fluctuations over a bent multicore fiber.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12187</identifier>
 <datestamp>2019-07-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12187</id><created>2018-10-29</created><updated>2019-06-28</updated><authors><author><keyname>Llu&#xed;s</keyname><forenames>Francesc</forenames></author><author><keyname>Pons</keyname><forenames>Jordi</forenames></author><author><keyname>Serra</keyname><forenames>Xavier</forenames></author></authors><title>End-to-end music source separation: is it possible in the waveform
  domain?</title><categories>cs.SD cs.LG eess.AS</categories><comments>In proceedings of INTERSPEECH 2019. Code:
  https://github.com/francesclluis/source-separation-wavenet and demo:
  http://jordipons.me/apps/end-to-end-music-source-separation/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the currently successful source separation techniques use the
magnitude spectrogram as input, and are therefore by default omitting part of
the signal: the phase. To avoid omitting potentially useful information, we
study the viability of using end-to-end models for music source separation ---
which take into account all the information available in the raw audio signal,
including the phase. Although during the last decades end-to-end music source
separation has been considered almost unattainable, our results confirm that
waveform-based models can perform similarly (if not better) than a
spectrogram-based deep learning model. Namely: a Wavenet-based model we propose
and Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep
learning model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12189</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12189</id><created>2018-10-29</created><updated>2019-02-18</updated><authors><author><keyname>Anavangot</keyname><forenames>Vijay</forenames></author><author><keyname>Kumar</keyname><forenames>Animesh</forenames></author></authors><title>Novel Near-Optimal Scalar Quantizers with Exponential Decay Rate and
  Global Convergence</title><categories>eess.SP cs.IT math.IT</categories><comments>11 pages, 4 figures, shorter version accepted in ICASSP-19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern distributed real-time signal sensing/monitoring systems require
quantization for efficient signal representation. These distributed sensors
often have inherent computational and energy limitations. Motivated by this
concern, we propose a novel quantization scheme called approximate Lloyd-Max
that is nearly-optimal. Assuming a continuous and finite support probability
distribution of the source, we show that our quantizer converges to the
classical Lloyd-Max quantizer with increasing bitrate. We also show that our
approximate Lloyd-Max quantizer converges exponentially fast with the number of
iterations. The proposed quantizer is modified to account for a relatively new
quantization model which has envelope constraints, termed as the envelope
quantizer. With suitable modifications we show optimality and convergence
properties for the equivalent approximate envelope quantizer. We illustrate our
results using extensive simulations for different finite support source
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12204</identifier>
 <datestamp>2020-01-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12204</id><created>2018-10-29</created><updated>2020-01-16</updated><authors><author><keyname>Z&#xe1;vi&#x161;ka</keyname><forenames>Pavel</forenames></author><author><keyname>Rajmic</keyname><forenames>Pavel</forenames></author><author><keyname>Mokr&#xfd;</keyname><forenames>Ond&#x159;ej</forenames></author><author><keyname>Pr&#x16f;&#x161;a</keyname><forenames>Zden&#x11b;k</forenames></author></authors><title>A Proper version of Synthesis-based Sparse Audio Declipper</title><categories>eess.AS</categories><journal-ref>ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), Brighton, United Kingdom, 2019, pp.
  591-595</journal-ref><doi>10.1109/ICASSP.2019.8682348</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Methods based on sparse representation have found great use in the recovery
of audio signals degraded by clipping. The state of the art in declipping has
been achieved by the SPADE algorithm by Kiti\'c et. al. (LVA/ICA2015). Our
recent study (LVA/ICA2018) has shown that although the original S-SPADE can be
improved such that it converges significantly faster than the A-SPADE, the
restoration quality is significantly worse. In the present paper, we propose a
new version of S-SPADE. Experiments show that the novel version of S-SPADE
outperforms its old version in terms of restoration quality, and that it is
comparable with the A-SPADE while being even slightly faster than A-SPADE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12247</identifier>
 <datestamp>2019-01-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12247</id><created>2018-10-29</created><updated>2019-01-17</updated><authors><author><keyname>Hawthorne</keyname><forenames>Curtis</forenames></author><author><keyname>Stasyuk</keyname><forenames>Andriy</forenames></author><author><keyname>Roberts</keyname><forenames>Adam</forenames></author><author><keyname>Simon</keyname><forenames>Ian</forenames></author><author><keyname>Huang</keyname><forenames>Cheng-Zhi Anna</forenames></author><author><keyname>Dieleman</keyname><forenames>Sander</forenames></author><author><keyname>Elsen</keyname><forenames>Erich</forenames></author><author><keyname>Engel</keyname><forenames>Jesse</forenames></author><author><keyname>Eck</keyname><forenames>Douglas</forenames></author></authors><title>Enabling Factorized Piano Music Modeling and Generation with the MAESTRO
  Dataset</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Examples available at https://goo.gl/magenta/maestro-examples</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating musical audio directly with neural networks is notoriously
difficult because it requires coherently modeling structure at many different
timescales. Fortunately, most music is also highly structured and can be
represented as discrete note events played on musical instruments. Herein, we
show that by using notes as an intermediate representation, we can train a
suite of models capable of transcribing, composing, and synthesizing audio
waveforms with coherent musical structure on timescales spanning six orders of
magnitude (~0.1 ms to ~100 s), a process we call Wave2Midi2Wave. This large
advance in the state of the art is enabled by our release of the new MAESTRO
(MIDI and Audio Edited for Synchronous TRacks and Organization) dataset,
composed of over 172 hours of virtuosic piano performances captured with fine
alignment (~3 ms) between note labels and audio waveforms. The networks and the
dataset together present a promising approach toward creating new expressive
and interpretable neural models of music.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12249</identifier>
 <datestamp>2019-02-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12249</id><created>2018-10-29</created><updated>2019-02-14</updated><authors><author><keyname>Ono</keyname><forenames>Shunsuke</forenames></author></authors><title>Efficient Constrained Signal Reconstruction by Randomized Epigraphical
  Projection</title><categories>math.OC eess.SP</categories><comments>To be presented at ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a randomized optimization framework for constrained
signal reconstruction, where the word &quot;constrained&quot; implies that data-fidelity
is imposed as a hard constraint instead of adding a data-fidelity term to an
objective function to be minimized. Such formulation facilitates the selection
of regularization terms and hyperparameters, but due to the non-separability of
the data-fidelity constraint, it does not suit block-coordinate-wise
randomization as is. To resolve this, we give another expression of the
data-fidelity constraint via epigraphs, which enables to design a randomized
solver based on a stochastic proximal algorithm with randomized epigraphical
projection. Our method is very efficient especially when the problem involves
non-structured large matrices. We apply our method to CT image reconstruction,
where the advantage of our method over the deterministic counterpart is
demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12271</identifier>
 <datestamp>2018-10-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12271</id><created>2018-10-29</created><authors><author><keyname>Song</keyname><forenames>WenZhan</forenames></author><author><keyname>Li</keyname><forenames>Fangyu</forenames></author><author><keyname>Valero</keyname><forenames>Maria</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author></authors><title>Toward Creating Subsurface Camera</title><categories>eess.SP physics.geo-ph</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the framework and architecture of Subsurface Camera (SAMERA)
is envisioned and described for the first time. A SAMERA is a geophysical
sensor network that senses and processes geophysical sensor signals, and
computes a 3D subsurface image in-situ in real-time. The basic mechanism is:
geophysical waves propagating/reflected/refracted through subsurface enter a
network of geophysical sensors, where a 2D or 3D image is computed and
recorded; a control software may be connected to this network to allow view of
the 2D/3D image and adjustment of settings such as resolution, filter,
regularization and other algorithm parameters. System prototypes based on
seismic imaging have been designed. SAMERA technology is envisioned as a game
changer to transform many subsurface survey and monitoring applications,
including oil/gas exploration and production, subsurface infrastructures and
homeland security, wastewater and CO2 sequestration, earthquake and volcano
hazard monitoring. The system prototypes for seismic imaging have been built.
Creating SAMERA requires an interdisciplinary collaboration and transformation
of sensor networks, signal processing, distributed computing, and geophysical
imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12355</identifier>
 <datestamp>2019-01-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12355</id><created>2018-10-29</created><updated>2019-01-14</updated><authors><author><keyname>Schl&#xfc;ter</keyname><forenames>Matthias</forenames></author><author><keyname>Otte</keyname><forenames>Christoph</forenames></author><author><keyname>Saathoff</keyname><forenames>Thore</forenames></author><author><keyname>Gessert</keyname><forenames>Nils</forenames></author><author><keyname>Schlaefer</keyname><forenames>Alexander</forenames></author></authors><title>Feasibility of a markerless tracking system based on optical coherence
  tomography</title><categories>physics.med-ph eess.IV</categories><comments>Accepted at SPIE Medical Imaging 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clinical tracking systems are popular but typically require specific tracking
markers. During the last years, scanning speed of optical coherence tomography
(OCT) has increased to A-scan rates above 1 MHz allowing to acquire volume
scans of moving objects. Thorefore, we propose a markerless tracking system
based on OCT to obtain small volumetric images including information of
sub-surface structures at high spatio-temporal resolution. In contrast to
conventional vision based approaches, this allows identifying natural landmarks
even for smooth and homogeneous surfaces. We describe the optomechanical setup
and process flow to evaluate OCT volumes for translations and accordingly
adjust the position of the field-of-view to follow moving samples. While our
current setup is still preliminary, we demonstrate tracking of motion
transversal to the OCT beam of up to 20 mm/s with errors around 0.2 mm and even
better for some scenarios. Tracking is evaluated on a clearly structured and on
a homogeneous phantom as well as on actual tissue samples. The results show
that OCT is promising for fast and precise tracking of smooth, monochromatic
objects in medical scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12431</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12431</id><created>2018-10-29</created><authors><author><keyname>Ghasemi</keyname><forenames>Mohammad Amin</forenames></author><author><keyname>Foroushani</keyname><forenames>Hossein Mohammadian</forenames></author><author><keyname>Parniani</keyname><forenames>Mostafa</forenames></author></authors><title>Partial Shading Detection and Smooth Maximum Power Point Tracking of PV
  Arrays under PSC</title><categories>eess.SP</categories><comments>12 pages, 11 figures, 4 tables, EEE Transactions on Power Electronics
  31.9 (2016): 6281-6292</comments><journal-ref>M. A. Ghasemi, H. M. Foroushani, M. Parniani, &quot;Partial shading
  detection and smooth maximum power point tracking of PV arrays under PSC&quot;,
  IEEE Trans. Power Electron., vol. 31, no. 9, pp. 6281-6292, Sep. 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important issues in the operation of a photovoltaic (PV)
system is extracting maximum power from the PV array, especially in partial
shading condition (PSC). Under PSC, P-V characteristic of PV arrays will have
multiple peak points, only one of which is global maximum. Conventional maximum
power point tracking (MPPT) methods are not able to extract maximum power in
this condition. In this paper, a novel two-stage MPPT method is presented to
overcome this drawback. In the first stage, a method is proposed to determine
the occurrence of PSC, and in the second stage, using a new algorithm that is
based on ramp change of the duty cycle and continuous sampling from the P-V
characteristic of the array, global maximum power point of array is reached.
P&amp;O algorithm is then re-activated to trace small changes of the new MPP. Open
loop operation of the proposed method makes its implementation cheap and
simple. The method is robust in the face of changing environmental conditions
and array characteristics, and has minimum negative impact on the connected
power system. Simulations in Matlab/Simulink and experimental results validate
the performance of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12457</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12457</id><created>2018-10-29</created><authors><author><keyname>Rao</keyname><forenames>Milind</forenames></author><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Distributed Convex Optimization With Limited Communications</title><categories>cs.DC cs.LG cs.MA eess.SP</categories><comments>Extended version of submission to IEEE ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a distributed convex optimization algorithm, termed
\emph{distributed coordinate dual averaging} (DCDA) algorithm, is proposed. The
DCDA algorithm addresses the scenario of a large distributed optimization
problem with limited communication among nodes in the network. Currently known
distributed subgradient methods, such as the distributed dual averaging or the
distributed alternating direction method of multipliers algorithms, assume that
nodes can exchange messages of large cardinality. Such network communication
capabilities are not valid in many scenarios of practical relevance. In the
DCDA algorithm, on the other hand, communication of each coordinate of the
optimization variable is restricted over time. For the proposed algorithm, we
bound the rate of convergence under different communication protocols and
network architectures. We also consider the extensions to the case of imperfect
gradient knowledge and the case in which transmitted messages are corrupted by
additive noise or are quantized. Relevant numerical simulations are also
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12473</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12473</id><created>2018-10-29</created><authors><author><keyname>Souza</keyname><forenames>Roberto</forenames></author><author><keyname>Frayne</keyname><forenames>Richard</forenames></author></authors><title>A Hybrid Frequency-domain/Image-domain Deep Network for Magnetic
  Resonance Image Reconstruction</title><categories>eess.IV cs.LG eess.SP stat.ML</categories><comments>8 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Decreasing magnetic resonance (MR) image acquisition times can potentially
reduce procedural cost and make MR examinations more accessible. Compressed
sensing (CS)-based image reconstruction methods, for example, decrease MR
acquisition time by reconstructing high-quality images from data that were
originally sampled at rates inferior to the Nyquist-Shannon sampling theorem.
In this work we propose a hybrid architecture that works both in the k-space
(or frequency-domain) and the image (or spatial) domains. Our network is
composed of a complex-valued residual U-net in the k-space domain, an inverse
Fast Fourier Transform (iFFT) operation, and a real-valued U-net in the image
domain. Our experiments demonstrated, using MR raw k-space data, that the
proposed hybrid approach can potentially improve CS reconstruction compared to
deep-learning networks that operate only in the image domain. In this study we
compare our method with four previously published deep neural networks and
examine their ability to reconstruct images that are subsequently used to
generate regional volume estimates. We evaluated undersampling ratios of 75%
and 80%. Our technique was ranked second in the quantitative analysis, but
qualitative analysis indicated that our reconstruction performed the best in
hard to reconstruct regions, such as the cerebellum. All images reconstructed
with our method were successfully post-processed, and showed good volumetry
agreement compared with the fully sampled reconstruction measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12538</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12538</id><created>2018-10-30</created><authors><author><keyname>Mei</keyname><forenames>Kunqiang</forenames></author><author><keyname>Hu</keyname><forenames>Bin</forenames></author><author><keyname>Fei</keyname><forenames>Baowei</forenames></author><author><keyname>Qin</keyname><forenames>Binjie</forenames></author></authors><title>Phase asymmetry guided adaptive fractional-order total variation and
  diffusion for feature-preserving ultrasound despeckling</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is essential for ultrasound despeckling to remove speckle noise while
simultaneously preserving edge features for accurate diagnosis and analysis in
many applications. To preserve real edges such as ramp edges and low contrast
edges, we first detect edges using a phase-based measure called phase asymmetry
(PAS), which can distinguish small differences in transition border regions and
varies from $0$ to $1$, taking $0$ in ideal smooth regions and taking $1$ at
ideal step edges. We further propose three strategies to properly preserve
edges. First, in observing that fractional-order anisotropic diffusion (FAD)
filter has good performance in smooth regions while the fractional-order TV
(FTV) filter performs better at edges, we leverage the PAS metric to keep a
balance between FAD filter and FTV filter for achieving the best performance of
preserving ramp edges. Second, considering that the FAD filter fails to protect
low contrast edges by solely integrating gradient information into the
diffusion coefficient, we integrate the PAS metric into the diffusion
coefficient to properly preserve low contrast edges. Finally, different from
fixed fractional order diffusion filters neglecting the differences between
smooth regions and transition border regions, an adaptive fractional order is
implemented based on the PAS metric to enhance edges. The experimental results
show that our method outperforms other state-of-the-art ultrasound despeckling
filters in both speckle reduction and feature preservation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12566</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12566</id><created>2018-10-30</created><authors><author><keyname>Chen</keyname><forenames>Yi-Chen</forenames></author><author><keyname>Shen</keyname><forenames>Chia-Hao</forenames></author><author><keyname>Huang</keyname><forenames>Sung-Feng</forenames></author><author><keyname>Lee</keyname><forenames>Hung-yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>Almost-unsupervised Speech Recognition with Close-to-zero Resource Based
  on Phonetic Structures Learned from Very Small Unpaired Speech and Text Data</title><categories>cs.CL cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Producing a large amount of annotated speech data for training ASR systems
remains difficult for more than 95% of languages all over the world which are
low-resourced. However, we note human babies start to learn the language by the
sounds of a small number of exemplar words without hearing a large amount of
data. We initiate some preliminary work in this direction in this paper. Audio
Word2Vec is used to obtain embeddings of spoken words which carry phonetic
information extracted from the signals. An autoencoder is used to generate
embeddings of text words based on the articulatory features for the phoneme
sequences. Both sets of embeddings for spoken and text words describe similar
phonetic structures among words in their respective latent spaces. A mapping
relation from the audio embeddings to text embeddings actually gives the
word-level ASR. This can be learned by aligning a small number of spoken words
and the corresponding text words in the embedding spaces. In the initial
experiments only 200 annotated spoken words and one hour of speech data without
annotation gave a word accuracy of 27.5%, which is low but a good starting
point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12568</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12568</id><created>2018-10-30</created><authors><author><keyname>Zhang</keyname><forenames>Xi</forenames></author><author><keyname>Wu</keyname><forenames>Xiaolin</forenames></author></authors><title>Nonlinear Prediction of Multidimensional Signals via Deep Regression
  with Applications to Image Coding</title><categories>eess.IV cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (DCNN) have enjoyed great successes in
many signal processing applications because they can learn complex, non-linear
causal relationships from input to output. In this light, DCNNs are well suited
for the task of sequential prediction of multidimensional signals, such as
images, and have the potential of improving the performance of traditional
linear predictors. In this research we investigate how far DCNNs can push the
envelop in terms of prediction precision. We propose, in a case study, a
two-stage deep regression DCNN framework for nonlinear prediction of
two-dimensional image signals. In the first-stage regression, the proposed deep
prediction network (PredNet) takes the causal context as input and emits a
prediction of the present pixel. Three PredNets are trained with the regression
objectives of minimizing $\ell_1$, $\ell_2$ and $\ell_\infty$ norms of
prediction residuals, respectively. The second-stage regression combines the
outputs of the three PredNets to generate an even more precise and robust
prediction. The proposed deep regression model is applied to lossless
predictive image coding, and it outperforms the state-of-the-art linear
predictors by appreciable margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12598</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12598</id><created>2018-10-30</created><authors><author><keyname>Juvela</keyname><forenames>Lauri</forenames></author><author><keyname>Bollepalli</keyname><forenames>Bajibabu</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Alku</keyname><forenames>Paavo</forenames></author></authors><title>Waveform generation for text-to-speech synthesis using pitch-synchronous
  multi-scale generative adversarial networks</title><categories>eess.AS cs.SD stat.ML</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state-of-the-art in text-to-speech synthesis has recently improved
considerably due to novel neural waveform generation methods, such as WaveNet.
However, these methods suffer from their slow sequential inference process,
while their parallel versions are difficult to train and even more expensive
computationally. Meanwhile, generative adversarial networks (GANs) have
achieved impressive results in image generation and are making their way into
audio applications; parallel inference is among their lucrative properties. By
adopting recent advances in GAN training techniques, this investigation studies
waveform generation for TTS in two domains (speech signal and glottal
excitation). Listening test results show that while direct waveform generation
with GAN is still far behind WaveNet, a GAN-based glottal excitation model can
achieve quality and voice similarity on par with a WaveNet vocoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12614</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12614</id><created>2018-10-30</created><authors><author><keyname>Pellegrini</keyname><forenames>Thomas</forenames></author><author><keyname>Farinas</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Delpech</keyname><forenames>Estelle</forenames></author><author><keyname>Lancelot</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>The Airbus Air Traffic Control speech recognition 2018 challenge:
  towards ATC automatic transcription and call sign detection</title><categories>cs.SD eess.AS</categories><comments>5 pages, 4 tables, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe the outcomes of the challenge organized and run by
Airbus and partners in 2018. The challenge consisted of two tasks applied to
Air Traffic Control (ATC) speech in English: 1) automatic speech-to-text
transcription, 2) call sign detection (CSD). The registered participants were
provided with 40 hours of speech along with manual transcriptions. Twenty-two
teams submitted predictions on a five hour evaluation set. ATC speech
processing is challenging for several reasons: high speech rate,
foreign-accented speech with a great diversity of accents, noisy communication
channels. The best ranked team achieved a 7.62% Word Error Rate and a 82.41%
CSD F1-score. Transcribing pilots' speech was found to be twice as harder as
controllers' speech. Remaining issues towards solving ATC ASR are also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12642</identifier>
 <datestamp>2019-02-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12642</id><created>2018-10-30</created><updated>2019-02-25</updated><authors><author><keyname>Phaye</keyname><forenames>Sai Samarth R</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author><author><keyname>Wang</keyname><forenames>Ye</forenames></author></authors><title>SubSpectralNet - Using Sub-Spectrogram based Convolutional Neural
  Networks for Acoustic Scene Classification</title><categories>cs.SD eess.AS</categories><comments>Accepted to IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustic Scene Classification (ASC) is one of the core research problems in
the field of Computational Sound Scene Analysis. In this work, we present
SubSpectralNet, a novel model which captures discriminative features by
incorporating frequency band-level differences to model soundscapes. Using
mel-spectrograms, we propose the idea of using band-wise crops of the input
time-frequency representations and train a convolutional neural network (CNN)
on the same. We also propose a modification in the training method for more
efficient learning of the CNN models. We first give a motivation for using
sub-spectrograms by giving intuitive and statistical analyses and finally we
develop a sub-spectrogram based CNN architecture for ASC. The system is
evaluated on the public ASC development dataset provided for the &quot;Detection and
Classification of Acoustic Scenes and Events&quot; (DCASE) 2018 Challenge. Our best
model achieves an improvement of +14% in terms of classification accuracy with
respect to the DCASE 2018 baseline system. Code and figures are available at
https://github.com/ssrp/SubSpectralNet
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12656</identifier>
 <datestamp>2019-08-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12656</id><created>2018-10-30</created><updated>2019-08-22</updated><authors><author><keyname>Chen</keyname><forenames>Li-Wei</forenames></author><author><keyname>Lee</keyname><forenames>Hung-Yi</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author></authors><title>Generative Adversarial Networks for Unpaired Voice Transformation on
  Impaired Speech</title><categories>eess.AS cs.LG cs.SD</categories><comments>Published as a conference paper at INTERSPEECH 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on using voice conversion (VC) to improve the speech
intelligibility of surgical patients who have had parts of their articulators
removed. Due to the difficulty of data collection, VC without parallel data is
highly desired. Although techniques for unparallel VC, for example, CycleGAN,
have been developed, they usually focus on transforming the speaker identity,
and directly transforming the speech of one speaker to that of another speaker
and as such do not address the task here. In this paper, we propose a new
approach for unparallel VC. The proposed approach transforms impaired speech to
normal speech while preserving the linguistic content and speaker
characteristics. To our knowledge, this is the first end-to-end GAN-based
unsupervised VC model applied to impaired speech. The experimental results show
that the proposed approach outperforms CycleGAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12675</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12675</id><created>2018-10-30</created><updated>2019-02-18</updated><authors><author><keyname>Venkatakrishnan</keyname><forenames>Singanallur</forenames></author><author><keyname>Wohlberg</keyname><forenames>Brendt</forenames></author></authors><title>Convolutional Dictionary Regularizers for Tomographic Inversion</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a growing interest in the use of data-driven regularizers to
solve inverse problems associated with computational imaging systems. The
convolutional sparse representation model has recently gained attention, driven
by the development of fast algorithms for solving the dictionary learning and
sparse coding problems for sufficiently large images and data sets.
Nevertheless, this model has seen very limited application to tomographic
reconstruction problems. In this paper, we present a model-based tomographic
reconstruction algorithm using a learnt convolutional dictionary as a
regularizer. The key contribution is the use of a data-dependent weighting
scheme for the l1 regularization to construct an effective denoising method
that is integrated into the inversion using the Plug-and-Play reconstruction
framework. Using simulated data sets we demonstrate that our approach can
improve performance over traditional regularizers based on a Markov random
field model and a patch-based sparse representation model for sparse and
limited-view tomographic data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12677</identifier>
 <datestamp>2019-05-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12677</id><created>2018-10-30</created><updated>2019-05-22</updated><authors><author><keyname>Chen</keyname><forenames>Liyan</forenames></author><author><keyname>Cheng</keyname><forenames>Samuel</forenames></author><author><keyname>He</keyname><forenames>Kanghang</forenames></author><author><keyname>Stankovic</keyname><forenames>Lina</forenames></author><author><keyname>Stankovic</keyname><forenames>Vladimir</forenames></author></authors><title>Undirected graphs: is the shift-enabled condition trivial or necessary?</title><categories>eess.SP</categories><comments>5 pages, 1 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has recently been shown that, contrary to the wide belief that a
shift-enabled condition (necessary for any shift-invariant filter to be
representable by a graph shift matrix) can be ignored because any
non-shift-enabled matrix can be converted to a shift-enabled matrix, such a
conversion in general may not hold for a directed graph with non-symmetric
shift matrix. This letter extends this prior work, focusing on undirected
graphs where the shift matrix is generally symmetric. We show that while, in
this case, the shift matrix can be converted to satisfy the original
shift-enabled condition, the converted matrix is not associated with the
original graph, that is, it does not capture anymore the structure of the graph
signal. We show via a counterexample, that a non-shift-enabled matrix cannot be
converted to a shift-enabled one and still maintain the topological structure
of the underlying graph, which is necessary to facilitate localized signal
processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12679</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12679</id><created>2018-10-30</created><updated>2018-11-21</updated><authors><author><keyname>Alvarado</keyname><forenames>Pablo A.</forenames></author><author><keyname>&#xc1;lvarez</keyname><forenames>Mauricio A.</forenames></author><author><keyname>Stowell</keyname><forenames>Dan</forenames></author></authors><title>Sparse Gaussian Process Audio Source Separation Using Spectrum Priors in
  the Time-Domain</title><categories>eess.AS cs.LG cs.SD eess.SP stat.ML</categories><comments>Paper submitted to the 44th International Conference on Acoustics,
  Speech, and Signal Processing, ICASSP 2019. To be held in Brighton, United
  Kingdom, between May 12 and May 17, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian process (GP) audio source separation is a time-domain approach that
circumvents the inherent phase approximation issue of spectrogram based
methods. Furthermore, through its kernel, GPs elegantly incorporate prior
knowledge about the sources into the separation model. Despite these compelling
advantages, the computational complexity of GP inference scales cubically with
the number of audio samples. As a result, source separation GP models have been
restricted to the analysis of short audio frames. We introduce an efficient
application of GPs to time-domain audio source separation, without compromising
performance. For this purpose, we used GP regression, together with spectral
mixture kernels, and variational sparse GPs. We compared our method with
LD-PSDTF (positive semi-definite tensor factorization), KL-NMF
(Kullback-Leibler non-negative matrix factorization), and IS-NMF (Itakura-Saito
NMF). Results show that the proposed method outperforms these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12722</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12722</id><created>2018-10-30</created><authors><author><keyname>Lerato</keyname><forenames>Lerato</forenames></author><author><keyname>Niesler</keyname><forenames>Thomas</forenames></author></authors><title>Feature Trajectory Dynamic Time Warping for Clustering of Speech
  Segments</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic time warping (DTW) can be used to compute the similarity between two
sequences of generally differing length. We propose a modification to DTW that
performs individual and independent pairwise alignment of feature trajectories.
The modified technique, termed feature trajectory dynamic time warping (FTDTW),
is applied as a similarity measure in the agglomerative hierarchical clustering
of speech segments. Experiments using MFCC and PLP parametrisations extracted
from TIMIT and from the Spoken Arabic Digit Dataset (SADD) show consistent and
statistically significant improvements in the quality of the resulting clusters
in terms of F-measure and normalised mutual information (NMI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12730</identifier>
 <datestamp>2018-12-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12730</id><created>2018-10-29</created><updated>2018-12-01</updated><authors><author><keyname>Fang</keyname><forenames>Fuming</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Echizen</keyname><forenames>Isao</forenames></author></authors><title>Audiovisual speaker conversion: jointly and simultaneously transforming
  facial expression and acoustic characteristics</title><categories>eess.AS cs.CL cs.LG cs.SD stat.ML</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An audiovisual speaker conversion method is presented for simultaneously
transforming the facial expressions and voice of a source speaker into those of
a target speaker. Transforming the facial and acoustic features together makes
it possible for the converted voice and facial expressions to be highly
correlated and for the generated target speaker to appear and sound natural. It
uses three neural networks: a conversion network that fuses and transforms the
facial and acoustic features, a waveform generation network that produces the
waveform from both the converted facial and acoustic features, and an image
reconstruction network that outputs an RGB facial image also based on both the
converted features. The results of experiments using an emotional audiovisual
database showed that the proposed method achieved significantly higher
naturalness compared with one that separately transformed acoustic and facial
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12735</identifier>
 <datestamp>2019-10-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12735</id><created>2018-10-30</created><updated>2019-10-02</updated><authors><author><keyname>Saade</keyname><forenames>Alaa</forenames></author><author><keyname>Coucke</keyname><forenames>Alice</forenames></author><author><keyname>Caulier</keyname><forenames>Alexandre</forenames></author><author><keyname>Dureau</keyname><forenames>Joseph</forenames></author><author><keyname>Ball</keyname><forenames>Adrien</forenames></author><author><keyname>Bluche</keyname><forenames>Th&#xe9;odore</forenames></author><author><keyname>Leroy</keyname><forenames>David</forenames></author><author><keyname>Doumouro</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Gisselbrecht</keyname><forenames>Thibault</forenames></author><author><keyname>Caltagirone</keyname><forenames>Francesco</forenames></author><author><keyname>Lavril</keyname><forenames>Thibaut</forenames></author><author><keyname>Primet</keyname><forenames>Ma&#xeb;l</forenames></author></authors><title>Spoken Language Understanding on the Edge</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>arXiv admin note: text overlap with arXiv:1805.10190</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of performing Spoken Language Understanding (SLU) on
small devices typical of IoT applications. Our contributions are twofold.
First, we outline the design of an embedded, private-by-design SLU system and
show that it has performance on par with cloud-based commercial solutions.
Second, we release the datasets used in our experiments in the interest of
reproducibility and in the hope that they can prove useful to the SLU
community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12743</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12743</id><created>2018-10-28</created><authors><author><keyname>Tran</keyname><forenames>Loc Hoang</forenames></author><author><keyname>Hoang</keyname><forenames>Trang</forenames></author><author><keyname>Huynh</keyname><forenames>Bui Hoang Nam</forenames></author></authors><title>Hypergraph based semi-supervised learning algorithms applied to speech
  recognition problem: a novel approach</title><categories>stat.ML cs.LG cs.SD eess.AS</categories><comments>11 pages, 1 figure, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:1212.0388</comments><msc-class>05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most network-based speech recognition methods are based on the assumption
that the labels of two adjacent speech samples in the network are likely to be
the same. However, assuming the pairwise relationship between speech samples is
not complete. The information a group of speech samples that show very similar
patterns and tend to have similar labels is missed. The natural way overcoming
the information loss of the above assumption is to represent the feature data
of speech samples as the hypergraph. Thus, in this paper, the three
un-normalized, random walk, and symmetric normalized hypergraph Laplacian based
semi-supervised learning methods applied to hypergraph constructed from the
feature data of speech samples in order to predict the labels of speech samples
are introduced. Experiment results show that the sensitivity performance
measures of these three hypergraph Laplacian based semi-supervised learning
methods are greater than the sensitivity performance measures of the Hidden
Markov Model method (the current state of the art method applied to speech
recognition problem) and graph based semi-supervised learning methods (i.e. the
current state of the art network-based method for classification problems)
applied to network created from the feature data of speech samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12757</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12757</id><created>2018-10-26</created><authors><author><keyname>Keren</keyname><forenames>Gil</forenames></author><author><keyname>Han</keyname><forenames>Jing</forenames></author><author><keyname>Schuller</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Scaling Speech Enhancement in Unseen Environments with Noise Embeddings</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><journal-ref>The Fifth CHiME Challenge Workshop, 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of speech enhancement generalisation to unseen
environments by performing two manipulations. First, we embed an additional
recording from the environment alone, and use this embedding to alter
activations in the main enhancement subnetwork. Second, we scale the number of
noise environments present at training time to 16,784 different environments.
Experiment results show that both manipulations reduce word error rates of a
pretrained speech recognition system and improve enhancement quality according
to a number of performance measures. Specifically, our best model reduces the
word error rate from 34.04% on noisy speech to 15.46% on the enhanced speech.
Enhanced audio samples can be found in
https://speechenhancement.page.link/samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12759</identifier>
 <datestamp>2019-06-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12759</id><created>2018-10-26</created><authors><author><keyname>Saavedra</keyname><forenames>Gabriel</forenames></author><author><keyname>Liga</keyname><forenames>Gabriele</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author></authors><title>Volterra-assisted Optical Phase Conjugation: a Hybrid Optical-Digital
  Scheme For Fiber Nonlinearity Compensation</title><categories>eess.SP</categories><doi>10.1109/JLT.2019.2907821</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mitigation of optical fiber nonlinearity is an active research field in the
area of optical communications, due to the resulting marked improvement in
transmission performance. Following the resurgence of optical coherent
detection, digital nonlinearity compensation (NLC) schemes such as digital
backpropagation (DBP) and Volterra equalization have received much attention.
Alternatively, optical NLC, and specifically optical phase conjugation (OPC),
has been proposed to relax the digital signal processing complexity. In this
work, a novel hybrid optical-digital NLC scheme combining OPC and a Volterra
equalizer is proposed, termed Volterra-Assisted OPC (VAO). It has a twofold
advantage: it overcomes the OPC limitation in asymmetric links and
substantially enhances the performance of Volterra equalizers. The proposed
scheme is shown to outperform both OPC and Volterra equalization alone by up to
4.2 dB in a 1000 km EDFA-amplified fiber link. Moreover, VAO is also
demonstrated to be very robust when applied to long-transmission distances,
with a 2.5 dB gain over OPC-only systems at 3000 km. VAO combines the
advantages of both optical and digital NLC offering a promising trade-off
between performance and complexity for future high-speed optical communication
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12760</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12760</id><created>2018-10-30</created><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Gurram</keyname><forenames>Prudhvi</forenames></author><author><keyname>Whipps</keyname><forenames>Gene</forenames></author></authors><title>Quickest Detection Of Deviations From Periodic Statistical Behavior</title><categories>math.ST cs.IT eess.SP math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of stochastic processes called independent and periodically
identically distributed (i.p.i.d.) processes is defined to capture periodically
varying statistical behavior. Algorithms are proposed to detect changes in such
i.p.i.d. processes. It is shown that the algorithms can be computed recursively
and are asymptotically optimal. This problem has applications in anomaly
detection in traffic data, social network data, and neural data, where periodic
statistical behavior has been observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12764</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12764</id><created>2018-10-26</created><authors><author><keyname>Ruddlesden</keyname><forenames>Michael</forenames></author><author><keyname>Zhang</keyname><forenames>Jinshuai</forenames></author><author><keyname>Zhao</keyname><forenames>Tianrui</forenames></author><author><keyname>Wang</keyname><forenames>Wen</forenames></author><author><keyname>Su</keyname><forenames>Lei</forenames></author></authors><title>Single-shot image retrieval through a multimode fiber using a genetic
  algorithm</title><categories>eess.IV physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we present a genetic algorithm-based approach for image
retrieval through a multimode fiber in a reference-less system. Due to mode
interference, when an image is illuminated at one side of a multimode fiber,
the transmitted light forms a noise-like speckle pattern at the other end. With
the use of a prior-measured transmission matrix of the fiber, a speckle pattern
is calculated using a random input mask. By optimizing the input mask to
achieve a high correlation coefficient of experimental and calculated patterns,
the input mask is optimized into an image with high similarity to the original
image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12817</identifier>
 <datestamp>2019-08-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12817</id><created>2018-10-30</created><updated>2019-08-20</updated><authors><author><keyname>Hafiene</keyname><forenames>Yosra</forenames></author><author><keyname>Fadili</keyname><forenames>Jalal</forenames></author><author><keyname>Elmoataz</keyname><forenames>Abderrahim</forenames></author></authors><title>Nonlocal $p$-Laplacian Variational problems on graphs</title><categories>math.NA cs.NA eess.IV eess.SP math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a nonlocal variational problem which consists of
minimizing in $L^2$ the sum of a quadratic data fidelity and a regularization
term corresponding to the $L^p$-norm of the nonlocal gradient. In particular,
we study convergence of the numerical solution to a discrete version of this
nonlocal variational problem to the unique solution of the continuum one. To do
so, we derive an error bound and highlight the role of the initial data and the
kernel governing the nonlocal interactions. When applied to variational problem
on graphs, this error bound allows us to show the consistency of the
discretized variational problem as the number of vertices goes to infinity.
More precisely, for networks in convergent graph sequences (simple and weighted
deterministic dense graphs as well as random inhomogeneous graphs), we prove
convergence and provide rate of convergence of solutions for the discrete
models to the solution of the continuum problem as the number of vertices
grows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12865</identifier>
 <datestamp>2018-10-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12865</id><created>2018-10-30</created><authors><author><keyname>Lara</keyname><forenames>Pedro</forenames></author><author><keyname>Tarrataca</keyname><forenames>Lu&#xed;s D. T. J.</forenames></author><author><keyname>Haddad</keyname><forenames>Diego B.</forenames></author></authors><title>Exact Expectation Analysis of the Deficient-Length LMS Algorithm</title><categories>eess.SP cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic models that predict adaptive filtering algorithms performance
usually employ several assumptions in order to simplify the analysis. Although
these simplifications facilitate the recursive update of the statistical
quantities of interest, they by themselves may hamper the modeling accuracy.
This paper simultaneously avoids for the first time the employment of two
ubiquitous assumptions often adopted in the analysis of the least mean squares
algorithm. The first of them is the so-called independence assumption, which
presumes statistical independence between adaptive coefficients and input data.
The second one assumes a sufficient-order configuration, in which the lengths
of the unknown plant and the adaptive filter are equal. State equations that
characterize both the mean and mean square performance of the deficient-length
configuration without using the independence assumption are provided. The
devised analysis, encompassing both transient and steady-state regimes, is not
restricted neither to white nor to Gaussian input signals and is able to
provide a proper step size upper bound that guarantees stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12871</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12871</id><created>2018-10-30</created><updated>2019-02-19</updated><authors><author><keyname>Ajjanagadde</keyname><forenames>Ganesh</forenames></author><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Yedidia</keyname><forenames>Adam</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author></authors><title>Near-Optimal Coded Apertures for Imaging via Nazarov's Theorem</title><categories>eess.IV eess.SP</categories><comments>Changed plot to a log-log scale, minor typos corrected, minor changes
  in wording. Author affiliation updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the fundamental limits of coded aperture imaging systems up
to universal constants by drawing upon a theorem of Nazarov regarding Fourier
transforms. Our work is performed under a simple propagation and sensor model
that accounts for thermal and shot noise, scene correlation, and exposure time.
Focusing on mean square error as a measure of linear reconstruction quality, we
show that appropriate application of a theorem of Nazarov leads to essentially
optimal coded apertures, up to a constant multiplicative factor in exposure
time. Additionally, we develop a heuristically efficient algorithm to generate
such patterns that explicitly takes into account scene correlations. This
algorithm finds apertures that correspond to local optima of a certain
potential on the hypercube, yet are guaranteed to be tight. Finally, for i.i.d.
scenes, we show improvements upon prior work by using spectrally flat sequences
with bias. The development focuses on 1D apertures for conceptual clarity; the
natural generalizations to 2D are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12947</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12947</id><created>2018-10-30</created><updated>2019-02-18</updated><authors><author><keyname>Hsieh</keyname><forenames>Tsung-Han</forenames></author><author><keyname>Su</keyname><forenames>Li</forenames></author><author><keyname>Yang</keyname><forenames>Yi-Hsuan</forenames></author></authors><title>A Streamlined Encoder/Decoder Architecture for Melody Extraction</title><categories>eess.AS cs.SD</categories><comments>This is a pre-print version of an ICASSP 2019 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Melody extraction in polyphonic musical audio is important for music signal
processing. In this paper, we propose a novel streamlined encoder/decoder
network that is designed for the task. We make two technical contributions.
First, drawing inspiration from a state-of-the-art model for semantic
pixel-wise segmentation, we pass through the pooling indices between pooling
and un-pooling layers to localize the melody in frequency. We can achieve
result close to the state-of-the-art with much fewer convolutional layers and
simpler convolution modules. Second, we propose a way to use the bottleneck
layer of the network to estimate the existence of a melody line for each time
frame, and make it possible to use a simple argmax function instead of ad-hoc
thresholding to get the final estimation of the melody line. Our experiments on
both vocal melody extraction and general melody extraction validate the
effectiveness of the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12988</identifier>
 <datestamp>2019-08-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12988</id><created>2018-10-30</created><authors><author><keyname>Rahnama</keyname><forenames>Oscar</forenames></author><author><keyname>Cavallari</keyname><forenames>Tommaso</forenames></author><author><keyname>Golodetz</keyname><forenames>Stuart</forenames></author><author><keyname>Walker</keyname><forenames>Simon</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>R$^3$SGM: Real-time Raster-Respecting Semi-Global Matching for
  Power-Constrained Systems</title><categories>eess.IV cs.CV</categories><comments>Accepted in FPT 2018 as Oral presentation, 8 pages, 6 figures, 4
  tables</comments><journal-ref>2018 International Conference on Field-Programmable Technology
  (FPT)</journal-ref><doi>10.1109/FPT.2018.00025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stereo depth estimation is used for many computer vision applications. Though
many popular methods strive solely for depth quality, for real-time mobile
applications (e.g. prosthetic glasses or micro-UAVs), speed and power
efficiency are equally, if not more, important. Many real-world systems rely on
Semi-Global Matching (SGM) to achieve a good accuracy vs. speed balance, but
power efficiency is hard to achieve with conventional hardware, making the use
of embedded devices such as FPGAs attractive for low-power applications.
However, the full SGM algorithm is ill-suited to deployment on FPGAs, and so
most FPGA variants of it are partial, at the expense of accuracy. In a non-FPGA
context, the accuracy of SGM has been improved by More Global Matching (MGM),
which also helps tackle the streaking artifacts that afflict SGM. In this
paper, we propose a novel, resource-efficient method that is inspired by MGM's
techniques for improving depth quality, but which can be implemented to run in
real time on a low-power FPGA. Through evaluation on multiple datasets (KITTI
and Middlebury), we show that in comparison to other real-time capable stereo
approaches, we can achieve a state-of-the-art balance between accuracy, power
efficiency and speed, making our approach highly desirable for use in real-time
systems with limited power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.12999</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.12999</id><created>2018-10-30</created><authors><author><keyname>Sadat</keyname><forenames>Sayed Abdullah</forenames></author><author><keyname>Sreesobha</keyname><forenames>E.</forenames></author><author><keyname>Prasad</keyname><forenames>P. V. N.</forenames></author></authors><title>Power Factor Correction of Inductive Loads using PLC</title><categories>eess.SP</categories><comments>8 pages, 12 figures, a conference paper</comments><journal-ref>International Conference on Electrical Engineering and Computer
  Sciences, Hong Kong, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an automatic power factor correction for variable
inductive loads, most dominantly induction motors (IM) utilizing the
Programmable Logic Controllers (PLC). This hardware implementation of a 3{\O}
Inductive load system focuses on the automatic correction of power factor using
PLC. With the help of PLC, different performance parameters current level, real
power, and inductive power are obtained and logged in the PC. Using PLC
program, according to the control strategy to obtain a pre-specified power
factor a set of capacitors sized in a binary rate will be switched on or off
with the help of switching relays and contactors. This PLC control strategy
relies on a lookup table which is prepared based on two input parameters - peak
current and power factor, at a constant voltage. From these parameters, PLC
will calculate the reactive power of the system and accordingly the right
sequence of the capacitors are switched on in order to compensate for reactive
power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13024</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13024</id><created>2018-10-30</created><updated>2019-02-18</updated><authors><author><keyname>Li</keyname><forenames>Qiujia</forenames></author><author><keyname>Ness</keyname><forenames>Preben</forenames></author><author><keyname>Ragni</keyname><forenames>Anton</forenames></author><author><keyname>Gales</keyname><forenames>Mark</forenames></author></authors><title>Bi-Directional Lattice Recurrent Neural Networks for Confidence
  Estimation</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>Accepted by ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard approach to mitigate errors made by an automatic speech
recognition system is to use confidence scores associated with each predicted
word. In the simplest case, these scores are word posterior probabilities
whilst more complex schemes utilise bi-directional recurrent neural network
(BiRNN) models. A number of upstream and downstream applications, however, rely
on confidence scores assigned not only to 1-best hypotheses but to all words
found in confusion networks or lattices. These include but are not limited to
speaker adaptation, semi-supervised training and information retrieval.
Although word posteriors could be used in those applications as confidence
scores, they are known to have reliability issues. To make improved confidence
scores more generally available, this paper shows how BiRNNs can be extended
from 1-best sequences to confusion network and lattice structures. Experiments
are conducted using one of the Cambridge University submissions to the IARPA
OpenKWS 2016 competition. The results show that confusion network and
lattice-based BiRNNs can provide a significant improvement in confidence
estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13025</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13025</id><created>2018-10-30</created><authors><author><keyname>Ragni</keyname><forenames>Anton</forenames></author><author><keyname>Li</keyname><forenames>Qiujia</forenames></author><author><keyname>Gales</keyname><forenames>Mark</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author></authors><title>Confidence Estimation and Deletion Prediction Using Bidirectional
  Recurrent Neural Networks</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>Accepted as a conference paper at 2018 IEEE Workshop on Spoken
  Language Technology (SLT 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard approach to assess reliability of automatic speech
transcriptions is through the use of confidence scores. If accurate, these
scores provide a flexible mechanism to flag transcription errors for upstream
and downstream applications. One challenging type of errors that recognisers
make are deletions. These errors are not accounted for by the standard
confidence estimation schemes and are hard to rectify in the upstream and
downstream processing. High deletion rates are prominent in limited resource
and highly mismatched training/testing conditions studied under IARPA Babel and
Material programs. This paper looks at the use of bidirectional recurrent
neural networks to yield confidence estimates in predicted as well as deleted
words. Several simple schemes are examined for combination. To assess
usefulness of this approach, the combined confidence score is examined for
untranscribed data selection that favours transcriptions with lower deletion
errors. Experiments are conducted using IARPA Babel/Material program languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13038</identifier>
 <datestamp>2019-07-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13038</id><created>2018-10-30</created><authors><author><keyname>Dong</keyname><forenames>Jonathan</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Gigan</keyname><forenames>Sylvain</forenames></author></authors><title>Spectral Method for Multiplexed Phase Retrieval and Application in
  Optical Imaging in Complex Media</title><categories>eess.IV physics.optics</categories><journal-ref>ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</journal-ref><doi>10.1109/ICASSP.2019.8682329</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a generalized version of phase retrieval called multiplexed
phase retrieval. We want to recover the phase of amplitude-only measurements
from linear combinations of them. This corresponds to the case in which
multiple incoherent sources are sampled jointly, and one would like to recover
their individual contributions. We show that a recent spectral method developed
for phase retrieval can be generalized to this setting, and that its
performance follows a phase transition behavior. We apply this new technique to
light focusing at depth in a complex medium. Experimentally, although we only
have access to the sum of the intensities on multiple targets, we are able to
separately focus on each ones, thus opening potential applications in deep
fluorescence imaging and light delivery
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13048</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13048</id><created>2018-10-30</created><authors><author><keyname>Lai</keyname><forenames>Cheng-I</forenames></author><author><keyname>Abad</keyname><forenames>Alberto</forenames></author><author><keyname>Richmond</keyname><forenames>Korin</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author><author><keyname>Dehak</keyname><forenames>Najim</forenames></author><author><keyname>King</keyname><forenames>Simon</forenames></author></authors><title>Attentive Filtering Networks for Audio Replay Attack Detection</title><categories>eess.AS cs.CL cs.SD stat.ML</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An attacker may use a variety of techniques to fool an automatic speaker
verification system into accepting them as a genuine user. Anti-spoofing
methods meanwhile aim to make the system robust against such attacks. The
ASVspoof 2017 Challenge focused specifically on replay attacks, with the
intention of measuring the limits of replay attack detection as well as
developing countermeasures against them. In this work, we propose our replay
attacks detection system - Attentive Filtering Network, which is composed of an
attention-based filtering mechanism that enhances feature representations in
both the frequency and time domains, and a ResNet-based classifier. We show
that the network enables us to visualize the automatically acquired feature
representations that are helpful for spoofing detection. Attentive Filtering
Network attains an evaluation EER of 8.99$\%$ on the ASVspoof 2017 Version 2.0
dataset. With system fusion, our best system further obtains a 30$\%$ relative
improvement over the ASVspoof 2017 enhanced baseline system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13053</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13053</id><created>2018-10-30</created><updated>2019-02-18</updated><authors><author><keyname>Venkatakrishnan</keyname><forenames>S. V.</forenames></author><author><keyname>Dessieux</keyname><forenames>Luc</forenames></author><author><keyname>Bingham</keyname><forenames>Philip</forenames></author></authors><title>Wavelength-resolved neutron tomography for crystalline materials</title><categories>eess.IV physics.app-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wavelength-resolved (WR) neutron transmission tomography is an emerging
technique to characterize engineering materials. While tomographic
reconstruction for amorphous samples is straightforward, it is challenging to
reconstruct samples with single-crystal domains because the attenuation of the
sample varies as a function of its orientation with respect to the incident
beam due to Bragg scattering. In this paper, we present an algorithm that can
reconstruct samples with single-crystal domains from WR neutron tomographic
measurements. In particular, we use a model-based iterative reconstruction
(MBIR) technique that reconstructs the volume by identifying and leaving out
the regions of the measurement that are affected by Bragg scatter. We combine
the output of the MBIR method with an algorithm that matches the reconstruction
to the identified Bragg scatter to reconstruct a feature that corresponds to
the local crystallography of the sample being measured. Using simulated data,
we demonstrate how our algorithm can reconstruct materials with single-crystal
domains, thereby adding a powerful new capability for WR neutron imaging
instruments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13066</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13066</id><created>2018-10-30</created><authors><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author><author><keyname>Marques</keyname><forenames>Antonio G.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Connecting the Dots: Identifying Network Structure via Graph Signal
  Processing</title><categories>eess.SP</categories><doi>10.1109/MSP.2018.2890143</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network topology inference is a prominent problem in Network Science. Most
graph signal processing (GSP) efforts to date assume that the underlying
network is known, and then analyze how the graph's algebraic and spectral
characteristics impact the properties of the graph signals of interest. Such an
assumption is often untenable beyond applications dealing with e.g., directly
observable social and infrastructure networks; and typically adopted graph
construction schemes are largely informal, distinctly lacking an element of
validation. This tutorial offers an overview of graph learning methods
developed to bridge the aforementioned gap, by using information available from
graph signals to infer the underlying graph topology. Fairly mature statistical
approaches are surveyed first, where correlation analysis takes center stage
along with its connections to covariance selection and high-dimensional
regression for learning Gaussian graphical models. Recent GSP-based network
inference frameworks are also described, which postulate that the network
exists as a latent underlying structure, and that observations are generated as
a result of a network process defined in such a graph. A number of arguably
more nascent topics are also briefly outlined, including inference of dynamic
networks, nonlinear models of pairwise interaction, as well as extensions to
directed graphs and their relation to causal inference. All in all, this paper
introduces readers to challenges and opportunities for signal processing
research in emerging topic areas at the crossroads of modeling, prediction, and
control of complex behavior arising in networked systems that evolve over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13088</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13088</id><created>2018-10-30</created><updated>2018-11-05</updated><authors><author><keyname>Yin</keyname><forenames>Yan</forenames></author><author><keyname>Prieto</keyname><forenames>Ramon</forenames></author><author><keyname>Wang</keyname><forenames>Bin</forenames></author><author><keyname>Zhou</keyname><forenames>Jianwei</forenames></author><author><keyname>Gu</keyname><forenames>Yiwei</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Lin</keyname><forenames>Hui</forenames></author></authors><title>Attention-based sequence-to-sequence model for speech recognition:
  development of state-of-the-art system on LibriSpeech and its application to
  non-native English</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has shown that attention-based sequence-to-sequence models
such as Listen, Attend, and Spell (LAS) yield comparable results to
state-of-the-art ASR systems on various tasks. In this paper, we describe the
development of such a system and demonstrate its performance on two tasks:
first we achieve a new state-of-the-art word error rate of 3.43% on the test
clean subset of LibriSpeech English data; second on non-native English speech,
including both read speech and spontaneous speech, we obtain very competitive
results compared to a conventional system built with the most updated Kaldi
recipe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13091</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13091</id><created>2018-10-30</created><updated>2018-11-01</updated><authors><author><keyname>Luo</keyname><forenames>Ne</forenames></author><author><keyname>Jiang</keyname><forenames>Dongwei</forenames></author><author><keyname>Zhao</keyname><forenames>Shuaijiang</forenames></author><author><keyname>Gong</keyname><forenames>Caixia</forenames></author><author><keyname>Zou</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Xiangang</forenames></author></authors><title>Towards End-to-End Code-Switching Speech Recognition</title><categories>cs.CL eess.AS</categories><comments>5 pages, submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code-switching speech recognition has attracted an increasing interest
recently, but the need for expert linguistic knowledge has always been a big
issue. End-to-end automatic speech recognition (ASR) simplifies the building of
ASR systems considerably by predicting graphemes or characters directly from
acoustic input. In the mean time, the need of expert linguistic knowledge is
also eliminated, which makes it an attractive choice for code-switching ASR.
This paper presents a hybrid CTC-Attention based end-to-end Mandarin-English
code-switching (CS) speech recognition system and studies the effect of hybrid
CTC-Attention based models, different modeling units, the inclusion of language
identification and different decoding strategies on the task of code-switching
ASR. On the SEAME corpus, our system achieves a mixed error rate (MER) of
34.24%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13104</identifier>
 <datestamp>2019-08-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13104</id><created>2018-10-31</created><updated>2019-08-04</updated><authors><author><keyname>Karamatl&#x131;</keyname><forenames>Ertu&#x11f;</forenames></author><author><keyname>Cemgil</keyname><forenames>Ali Taylan</forenames></author><author><keyname>K&#x131;rb&#x131;z</keyname><forenames>Serap</forenames></author></authors><title>Audio Source Separation Using Variational Autoencoders and Weak Class
  Supervision</title><categories>cs.SD cs.LG eess.AS</categories><comments>Accepted version</comments><journal-ref>IEEE Signal Processing Letters 26 (2019) 1349-1353</journal-ref><doi>10.1109/LSP.2019.2929440</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a source separation method that is trained by
observing the mixtures and the class labels of the sources present in the
mixture without any access to isolated sources. Since our method does not
require source class labels for every time-frequency bin but only a single
label for each source constituting the mixture signal, we call this scenario as
weak class supervision. We associate a variational autoencoder (VAE) with each
source class within a non-negative (compositional) model. Each VAE provides a
prior model to identify the signal from its associated class in a sound
mixture. After training the model on mixtures, we obtain a generative model for
each source class and demonstrate our method on one-second mixtures of
utterances of digits from 0 to 9. We show that the separation performance
obtained by source class supervision is as good as the performance obtained by
source signal supervision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13107</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13107</id><created>2018-10-31</created><authors><author><keyname>Tjandra</keyname><forenames>Andros</forenames></author><author><keyname>Sakti</keyname><forenames>Sakriani</forenames></author><author><keyname>Nakamura</keyname><forenames>Satoshi</forenames></author></authors><title>End-to-End Feedback Loss in Speech Chain Framework via Straight-Through
  Estimator</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The speech chain mechanism integrates automatic speech recognition (ASR) and
text-to-speech synthesis (TTS) modules into a single cycle during training. In
our previous work, we applied a speech chain mechanism as a semi-supervised
learning. It provides the ability for ASR and TTS to assist each other when
they receive unpaired data and let them infer the missing pair and optimize the
model with reconstruction loss. If we only have speech without transcription,
ASR generates the most likely transcription from the speech data, and then TTS
uses the generated transcription to reconstruct the original speech features.
However, in previous papers, we just limited our back-propagation to the
closest module, which is the TTS part. One reason is that back-propagating the
error through the ASR is challenging due to the output of the ASR are discrete
tokens, creating non-differentiability between the TTS and ASR. In this paper,
we address this problem and describe how to thoroughly train a speech chain
end-to-end for reconstruction loss using a straight-through estimator (ST).
Experimental results revealed that, with sampling from ST-Gumbel-Softmax, we
were able to update ASR parameters and improve the ASR performances by 11\%
relative CER reduction compared to the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13109</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13109</id><created>2018-10-31</created><authors><author><keyname>Chetupalli</keyname><forenames>Srikanth Raj</forenames></author><author><keyname>Bhowmick</keyname><forenames>Anirban</forenames></author><author><keyname>Sreenivas</keyname><forenames>Thippur V.</forenames></author></authors><title>Latent variable approach to diarization of audio recordings using ad-hoc
  randomly placed mobile devices</title><categories>eess.AS cs.SD</categories><comments>Paper Submitted to the International Conference on Acoustics Speech
  and Signal Processing (ICASSP) 2019 to be held in Brighton, UK between May
  12-17, 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diarization of audio recordings from ad-hoc mobile devices using spatial
information is considered in this paper. A two-channel synchronous recording is
assumed for each mobile device, which is used to compute directional statistics
separately at each device in a frame-wise manner. The recordings across the
mobile devices are asynchronous, but a coarse synchronization is performed by
aligning the signals using acoustic events, or real-time clock. Direction
statistics computed for all the devices, are then modeled jointly using a
Dirichlet mixture model, and the posterior probability over the mixture
components is used to derive the diarization information. Experiments on real
life recordings using mobile phones show a diarization error rate of less than
14%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13137</identifier>
 <datestamp>2020-01-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13137</id><created>2018-10-31</created><updated>2019-06-18</updated><authors><author><keyname>Mokr&#xfd;</keyname><forenames>Ond&#x159;ej</forenames></author><author><keyname>Z&#xe1;vi&#x161;ka</keyname><forenames>Pavel</forenames></author><author><keyname>Rajmic</keyname><forenames>Pavel</forenames></author><author><keyname>Vesel&#xfd;</keyname><forenames>V&#xed;t&#x11b;zslav</forenames></author></authors><title>Introducing SPAIN (SParse Audio INpainter)</title><categories>cs.SD eess.AS math.OC</categories><journal-ref>2019 27th European Signal Processing Conference (EUSIPCO)</journal-ref><doi>10.23919/EUSIPCO.2019.8902560</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel sparsity-based algorithm for audio inpainting is proposed. It is an
adaptation of the SPADE algorithm by Kiti\'c et al., originally developed for
audio declipping, to the task of audio inpainting. The new SPAIN (SParse Audio
INpainter) comes in synthesis and analysis variants. Experiments show that both
A-SPAIN and S-SPAIN outperform other sparsity-based inpainting algorithms.
Moreover, A-SPAIN performs on a par with the state-of-the-art method based on
linear prediction in terms of the SNR, and, for larger gaps, SPAIN is even
slightly better in terms of the PEMO-Q psychoacoustic criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13183</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13183</id><created>2018-10-31</created><authors><author><keyname>Novotny</keyname><forenames>Ondrej</forenames></author><author><keyname>Plchot</keyname><forenames>Oldrich</forenames></author><author><keyname>Glembek</keyname><forenames>Ondrej</forenames></author><author><keyname>Burget</keyname><forenames>Lukas</forenames></author><author><keyname>Matejka</keyname><forenames>Pavel</forenames></author></authors><title>Discriminatively Re-trained i-vector Extractor for Speaker Recognition</title><categories>eess.AS cs.SD</categories><comments>5 pages, 1 figure, submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we revisit discriminative training of the i-vector extractor
component in the standard speaker verification (SV) system. The motivation of
our research lies in the robustness and stability of this large generative
model, which we want to preserve, and focus its power towards any intended SV
task. We show that after generative initialization of the i-vector extractor,
we can further refine it with discriminative training and obtain i-vectors that
lead to better performance on various benchmarks representing different
acoustic domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13201</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13201</id><created>2018-10-31</created><updated>2018-11-08</updated><authors><author><keyname>Deshpande</keyname><forenames>Sanket</forenames></author><author><keyname>Kapoor</keyname><forenames>Lucky</forenames></author><author><keyname>Kamat</keyname><forenames>Shivangi</forenames></author><author><keyname>Bheesette</keyname><forenames>Satyanarayana</forenames></author><author><keyname>Pal</keyname><forenames>Dipankar</forenames></author></authors><title>Design and Qualification of an Airborne, Cosmic Ray Flux Measurement
  System</title><categories>physics.ins-det astro-ph.IM eess.SP hep-ex</categories><comments>12 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper presents the design and qualification tests of an airborne
experimental setup to determine cosmic ray-flux in the lower stratospheric
regions of the earth's atmosphere. The concept of coincidence is implemented to
preferentially detect cosmic rays and reject noise and particles that are
incident at large angles but otherwise have similar characteristics, and are
therefore inseparable from the particles of interest by conventional detection
techniques. The experiment is designed to measure cosmic ray flux at two
altitudes extending to a maximum height of 30 km from mean-sea-level. The
experimental setup is to be lifted using a High Altitude Balloon (HAB). The
setup is designed and tested to withstand extreme temperature and pressure
conditions during the flight in the stratosphere. It includes a cosmic ray
telescope, a data acquisition system, a power supply systems, and peripheral
sensors. In the present endeavor, the payload design and results from
qualification tests are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13322</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13322</id><created>2018-10-31</created><authors><author><keyname>Lunglmayr</keyname><forenames>Michael</forenames></author><author><keyname>Wiesinger</keyname><forenames>Daniel</forenames></author><author><keyname>Haselmayr</keyname><forenames>Werner</forenames></author></authors><title>A stochastic computing architecture for iterative estimation</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic computing (SC) is a promising candidate for fault tolerant
computing in digital circuits. We present a novel stochastic computing
estimation architecture allowing to solve a large group of estimation problems
including least squares estimation as well as sparse estimation. This allows
utilizing the high fault tolerance of stochastic computing for implementing
estimation algorithms. The presented architecture is based on the recently
proposed linearized-Bregman-based Sparse Kaczmarz algorithm. To realize this
architecture, we develop a shrink function in stochastic computing and
analytically describe its error probability. We compare the stochastic
computing architecture to a fixed-point binary implementation and present
bit-true simulation results as well as synthesis results demonstrating the
feasibility of the proposed architecture for practical implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13338</identifier>
 <datestamp>2018-11-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13338</id><created>2018-10-31</created><authors><author><keyname>Tukuljac</keyname><forenames>Helena Peic</forenames><affiliation>EPFL</affiliation></author><author><keyname>Deleforge</keyname><forenames>Antoine</forenames><affiliation>MULTISPEECH</affiliation></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>PANAMA</affiliation></author></authors><title>MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval</title><categories>cs.SD cs.AI eess.AS</categories><proxy>ccsd</proxy><journal-ref>Thirty-second Conference on Neural Information Processing Systems
  (NIPS 2018), Dec 2018, Montr{\'e}al, Canada</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the general problem of blind echo retrieval, i.e., given
M sensors measuring in the discrete-time domain M mixtures of K delayed and
attenuated copies of an unknown source signal, can the echo locations and
weights be recovered? This problem has broad applications in fields such as
sonars, seismol-ogy, ultrasounds or room acoustics. It belongs to the broader
class of blind channel identification problems, which have been intensively
studied in signal processing. Existing methods in the literature proceed in two
steps: (i) blind estimation of sparse discrete-time filters and (ii) echo
information retrieval by peak-picking on filters. The precision of these
methods is fundamentally limited by the rate at which the signals are sampled:
estimated echo locations are necessary on-grid, and since true locations never
match the sampling grid, the weight estimation precision is impacted. This is
the so-called basis-mismatch problem in compressed sensing. We propose a
radically different approach to the problem, building on the framework of
finite-rate-of-innovation sampling. The approach operates directly in the
parameter-space of echo locations and weights, and enables near-exact blind and
off-grid echo retrieval from discrete-time measurements. It is shown to
outperform conventional methods by several orders of magnitude in precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1810.13407</identifier>
 <datestamp>2018-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1810.13407</id><created>2018-10-31</created><updated>2018-11-12</updated><authors><author><keyname>Tang</keyname><forenames>Hao</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>On The Inductive Bias of Words in Acoustics-to-Word Models</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustics-to-word models are end-to-end speech recognizers that use words as
targets without relying on pronunciation dictionaries or graphemes. These
models are notoriously difficult to train due to the lack of linguistic
knowledge. It is also unclear how the amount of training data impacts the
optimization and generalization of such models. In this work, we study the
optimization and generalization of acoustics-to-word models under different
amounts of training data. In addition, we study three types of inductive bias,
leveraging a pronunciation dictionary, word boundary annotations, and
constraints on word durations. We find that constraining word durations leads
to the most improvement. Finally, we analyze the word embedding space learned
by the model, and find that the space has a structure dominated by the
pronunciation of words. This suggests that the contexts of words, instead of
their phonetic structure, should be the future focus of inductive bias in
acoustics-to-word models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00002</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00002</id><created>2018-10-30</created><authors><author><keyname>Prenger</keyname><forenames>Ryan</forenames></author><author><keyname>Valle</keyname><forenames>Rafael</forenames></author><author><keyname>Catanzaro</keyname><forenames>Bryan</forenames></author></authors><title>WaveGlow: A Flow-based Generative Network for Speech Synthesis</title><categories>cs.SD cs.AI cs.LG eess.AS stat.ML</categories><comments>5 pages, 1 figure, 1 table, 13 equations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose WaveGlow: a flow-based network capable of generating
high quality speech from mel-spectrograms. WaveGlow combines insights from Glow
and WaveNet in order to provide fast, efficient and high-quality audio
synthesis, without the need for auto-regression. WaveGlow is implemented using
only a single network, trained using only a single cost function: maximizing
the likelihood of the training data, which makes the training procedure simple
and stable. Our PyTorch implementation produces audio samples at a rate of more
than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers
audio quality as good as the best publicly available WaveNet implementation.
All code will be made publicly available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00003</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00003</id><created>2018-10-31</created><updated>2018-11-02</updated><authors><author><keyname>Nagarajan</keyname><forenames>Bhalaji</forenames></author><author><keyname>Oruganti</keyname><forenames>V Ramana Murthy</forenames></author></authors><title>Deep Net Features for Complex Emotion Recognition</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Conflict of interest</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper investigates the influence of different acoustic features,
audio-events based features and automatic speech translation based lexical
features in complex emotion recognition such as curiosity. Pretrained networks,
namely, AudioSet Net, VoxCeleb Net and Deep Speech Net trained extensively for
different speech based applications are studied for this objective. Information
from deep layers of these networks are considered as descriptors and encoded
into feature vectors. Experimental results on the EmoReact dataset consisting
of 8 complex emotions show the effectiveness, yielding highest F1 score of 0.85
as against the baseline of 0.69 in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00006</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00006</id><created>2018-10-31</created><authors><author><keyname>Ramsay</keyname><forenames>David B.</forenames></author><author><keyname>Kilgour</keyname><forenames>Kevin</forenames></author><author><keyname>Roblek</keyname><forenames>Dominik</forenames></author><author><keyname>Sharifi</keyname><forenames>Matthew</forenames></author></authors><title>Low-Dimensional Bottleneck Features for On-Device Continuous Speech
  Recognition</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low power digital signal processors (DSPs) typically have a very limited
amount of memory in which to cache data. In this paper we develop efficient
bottleneck feature (BNF) extractors that can be run on a DSP, and retrain a
baseline large-vocabulary continuous speech recognition (LVCSR) system to use
these BNFs with only a minimal loss of accuracy. The small BNFs allow the DSP
chip to cache more audio features while the main application processor is
suspended, thereby reducing the overall battery usage. Our presented system is
able to reduce the footprint of standard, fixed point DSP spectral features by
a factor of 10 without any loss in word error rate (WER) and by a factor of 64
with only a 5.8% relative increase in WER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00074</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00074</id><created>2018-10-31</created><authors><author><keyname>Alemazkoor</keyname><forenames>Negin</forenames></author><author><keyname>Meidani</keyname><forenames>Hadi</forenames></author></authors><title>Efficient Collection of Connected Vehicles Data with Precision
  Guarantees</title><categories>stat.AP eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connected vehicles disseminate detailed data, including their position and
speed, at a very high frequency. Such data can be used for accurate real-time
analysis, prediction and control of transportation systems. The outstanding
challenge for such analysis is how to continuously collect and process
extremely large volumes of data. To address this challenge, efficient
collection of data is critical to prevent overburdening the communication
systems and overreaching computational and memory capacity. In this work, we
propose an efficient data collection scheme that selects and transmits only a
small subset of data to alleviate data transmission burden. As a demonstration,
we have used the proposed approach to select data points to be transmitted from
10,000 connected vehicles trips available in the Safety Pilot Model Deployment
dataset. The presented results show that collection ratio can be as small as
0.05 depending on the required precision. Moreover, a simulation study was
performed to evaluate the travel time estimation accuracy using the proposed
data collection approach. Results show that the proposed data collection
approach can significantly improve travel time estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00078</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00078</id><created>2018-10-31</created><authors><author><keyname>Dionelis</keyname><forenames>Nikolaos</forenames></author></authors><title>On Single-Channel Speech Enhancement and On Non-Linear Modulation-Domain
  Kalman Filtering</title><categories>cs.SD eess.AS</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report focuses on algorithms that perform single-channel speech
enhancement. The author of this report uses modulation-domain Kalman filtering
algorithms for speech enhancement, i.e. noise suppression and dereverberation,
in [1], [2], [3], [4] and [5]. Modulation-domain Kalman filtering can be
applied for both noise and late reverberation suppression and in [2], [1], [3]
and [4], various model-based speech enhancement algorithms that perform
modulation-domain Kalman filtering are designed, implemented and tested. The
model-based enhancement algorithm in [2] estimates and tracks the speech phase.
The short-time-Fourier-transform-based enhancement algorithm in [5] uses the
active speech level estimator presented in [6]. This report describes how
different algorithms perform speech enhancement and the algorithms discussed in
this report are addressed to researchers interested in monaural speech
enhancement. The algorithms are composed of different processing blocks and
techniques [7]; understanding the implementation choices made during the system
design is important because this provides insights that can assist the
development of new algorithms. Index Terms - Speech enhancement,
dereverberation, denoising, Kalman filter, minimum mean squared error
estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00079</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00079</id><created>2018-10-31</created><authors><author><keyname>Chen</keyname><forenames>Jiaming</forenames></author><author><keyname>Valehi</keyname><forenames>Ali</forenames></author><author><keyname>Razi</keyname><forenames>Abolfazl</forenames></author></authors><title>Predictive Modeling of Biomedical Signals Using Controlled Spatial
  Transformation</title><categories>eess.SP</categories><comments>13 pages, 7 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important paradigm in smart health is developing diagnosis tools and
monitoring a patient's heart activity through processing Electrocardiogram
(ECG) signals is a key example, sue to high mortality rate of heart-related
disease. However, current heart monitoring devices suffer from two important
drawbacks: i) failure in capturing inter-patient variability, and ii)
incapability of identifying heart abnormalities ahead of time to take effective
preventive and therapeutic interventions.
  This paper proposed a novel predictive signal processing method to solve
these issues. We propose a two-step classification framework for ECG signals,
where a global classifier recognizes severe abnormalities by comparing the
signal against a universal reference model. The seemingly normal signals are
then passed through a personalized classifier, to recognize mild but
informative signal morphology distortions. The key idea is to develop a novel
deviation analysis based on a controlled nonlinear transformation to capture
significant deviations of the signal towards any of predefined abnormality
classes. Here, we embrace the proven but overlooked fact that certain features
of ECG signals reflect underlying cardiac abnormalities before the occurrences
of cardiac disease. The proposed method achieves a classification accuracy of
96.6% and provides a unique feature of predictive analysis by providing
warnings before critical heart conditions. In particular, the chance of
observing a severe problem (a red alarm) is raised by about 5% to 10% after
observing a yellow alarm of the same type. Although we used this methodology to
provide early precaution messages to elderly and high-risk heart-patients, the
proposed method is general and applicable to similar bio-medical signal
processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00080</identifier>
 <datestamp>2019-01-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00080</id><created>2018-10-18</created><updated>2019-01-13</updated><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Dyck</keyname><forenames>Ondrej E.</forenames></author><author><keyname>Oxley</keyname><forenames>Mark P.</forenames></author><author><keyname>Lupini</keyname><forenames>Andrew R.</forenames></author><author><keyname>McInnes</keyname><forenames>Leland</forenames></author><author><keyname>Healy</keyname><forenames>John</forenames></author><author><keyname>Jesse</keyname><forenames>Stephen</forenames></author><author><keyname>Kalinin</keyname><forenames>Sergei V.</forenames></author></authors><title>Manifold Learning of Four-dimensional Scanning Transmission Electron
  Microscopy</title><categories>eess.IV cond-mat.mtrl-sci physics.data-an stat.ML</categories><journal-ref>npj Computational Materials volume 5, Article number: 5 (2019)</journal-ref><doi>10.1038/s41524-018-0139-y</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Four-dimensional scanning transmission electron microscopy (4D-STEM) of local
atomic diffraction patterns is emerging as a powerful technique for probing
intricate details of atomic structure and atomic electric fields. However,
efficient processing and interpretation of large volumes of data remain
challenging, especially for two-dimensional or light materials because the
diffraction signal recorded on the pixelated arrays is weak. Here we employ
data-driven manifold leaning approaches for straightforward visualization and
exploration analysis of the 4D-STEM datasets, distilling real-space neighboring
effects on atomically resolved deflection patterns from single-layer graphene,
with single dopant atoms, as recorded on a pixelated detector. These extracted
patterns relate to both individual atom sites and sublattice structures,
effectively discriminating single dopant anomalies via multi-mode views. We
believe manifold learning analysis will accelerate physics discoveries coupled
between data-rich imaging mechanisms and materials such as ferroelectric,
topological spin and van der Waals heterostructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00162</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00162</id><created>2018-10-31</created><authors><author><keyname>Wang</keyname><forenames>Yu-An</forenames></author><author><keyname>Huang</keyname><forenames>Yu-Kai</forenames></author><author><keyname>Lin</keyname><forenames>Tzu-Chuan</forenames></author><author><keyname>Su</keyname><forenames>Shang-Yu</forenames></author><author><keyname>Chen</keyname><forenames>Yun-Nung</forenames></author></authors><title>Modeling Melodic Feature Dependency with Modularized Variational
  Auto-Encoder</title><categories>cs.AI cs.LG cs.MM cs.SD eess.AS</categories><comments>The first three authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic melody generation has been a long-time aspiration for both AI
researchers and musicians. However, learning to generate euphonious melodies
has turned out to be highly challenging. This paper introduces 1) a new variant
of variational autoencoder (VAE), where the model structure is designed in a
modularized manner in order to model polyphonic and dynamic music with domain
knowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly
models the dependency between melodic features. The proposed framework is
capable of generating distinct melodies that sounds natural, and the
experiments for evaluating generated music clips show that the proposed model
outperforms the baselines in human evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00183</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00183</id><created>2018-10-31</created><authors><author><keyname>Narayanaswamy</keyname><forenames>Vivek Sivaraman</forenames></author><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author><author><keyname>Song</keyname><forenames>Huan</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Designing an Effective Metric Learning Pipeline for Speaker Diarization</title><categories>stat.ML cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art speaker diarization systems utilize knowledge from external
data, in the form of a pre-trained distance metric, to effectively determine
relative speaker identities to unseen data. However, much of recent focus has
been on choosing the appropriate feature extractor, ranging from pre-trained
$i-$vectors to representations learned via different sequence modeling
architectures (e.g. 1D-CNNs, LSTMs, attention models), while adopting
off-the-shelf metric learning solutions. In this paper, we argue that,
regardless of the feature extractor, it is crucial to carefully design a metric
learning pipeline, namely the loss function, the sampling strategy and the
discrimnative margin parameter, for building robust diarization systems.
Furthermore, we propose to adopt a fine-grained validation process to obtain a
comprehensive evaluation of the generalization power of metric learning
pipelines. To this end, we measure diarization performance across different
language speakers, and variations in the number of speakers in a recording.
Using empirical studies, we provide interesting insights into the effectiveness
of different design choices and make recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00220</identifier>
 <datestamp>2019-07-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00220</id><created>2018-11-01</created><updated>2019-06-29</updated><authors><author><keyname>Iquebal</keyname><forenames>Ashif Sikandar</forenames></author><author><keyname>Bukkapatnam</keyname><forenames>Satish</forenames></author></authors><title>Consistent estimation of the max-flow problem: Towards unsupervised
  image segmentation</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in the image-based diagnostics of complex biological and
manufacturing processes have brought unsupervised image segmentation to the
forefront of enabling automated, on the fly decision making. However, most
existing unsupervised segmentation approaches are either computationally
complex or require manual parameter selection (e.g., flow capacities in
max-flow/min-cut segmentation). In this work, we present a fully unsupervised
segmentation approach using a continuous max-flow formulation over the image
domain while optimally estimating the flow parameters from the image
characteristics. More specifically, we show that the maximum a posteriori
estimate of the image labels can be formulated as a continuous max-flow problem
given the flow capacities are known. The flow capacities are then iteratively
obtained by employing a novel Markov random field prior over the image domain.
We present theoretical results to establish the posterior consistency of the
flow capacities. We compare the performance of our approach on two real-world
case studies including brain tumor image segmentation and defect identification
in additively manufactured components using electron microscopic images.
Comparative results with several state-of-the-art supervised as well as
unsupervised methods suggest that the present method performs statistically
similar to the supervised methods, but results in more than 90% improvement in
the Dice score when compared to the state-of-the-art unsupervised methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00223</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00223</id><created>2018-11-01</created><authors><author><keyname>Kim</keyname><forenames>Jong Wook</forenames></author><author><keyname>Bittner</keyname><forenames>Rachel</forenames></author><author><keyname>Kumar</keyname><forenames>Aparna</forenames></author><author><keyname>Bello</keyname><forenames>Juan Pablo</forenames></author></authors><title>Neural Music Synthesis for Flexible Timbre Control</title><categories>cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent success of raw audio waveform synthesis models like WaveNet
motivates a new approach for music synthesis, in which the entire process ---
creating audio samples from a score and instrument information --- is modeled
using generative neural networks. This paper describes a neural music synthesis
model with flexible timbre controls, which consists of a recurrent neural
network conditioned on a learned instrument embedding followed by a WaveNet
vocoder. The learned embedding space successfully captures the diverse
variations in timbres within a large dataset and enables timbre control and
morphing by interpolating between instruments in the embedding space. The
synthesis quality is evaluated both numerically and perceptually, and an
interactive web demo is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00224</identifier>
 <datestamp>2019-02-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00224</id><created>2018-11-01</created><authors><author><keyname>Navidi</keyname><forenames>Thomas</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>A Two-layer Decentralized Control Architecture for DER Coordination</title><categories>eess.SP cs.SY</categories><comments>Published in IEEE Conference on Decision and Control 2018</comments><doi>10.1109/CDC.2018.8619201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a two-layer distributed energy resource (DER)
coordination architecture that allows for separate ownership of data, operates
with data subjected to a large buffering delay, and employs a new measure of
power quality. The two-layer architecture comprises a centralized model
predictive controller (MPC) and several decentralized MPCs each operating
independently with no direct communication between them and with infrequent
communication with the centralized controller. The goal is to minimize a
combination of total energy cost and a measure of power quality while obeying
cyber-physical constraints. The global controller utilizes a fast AC optimal
power flow (OPF) solver and extensive parallelization to scale the solution to
large networks. Each local controller attempts to maximize arbitrage profit
while following the load profile and constraints dictated by the global
controller. Extensive simulations are performed for two distribution networks
under a wide variety of possible storage and solar penetrations enabled by the
controller speed. The simulations show that (i) the two-layer architecture can
achieve tenfold improvement in power quality relative to no coordination, while
capturing nearly all of the available arbitrage profit for a moderate amount of
storage penetration, and (ii) both power quality and arbitrage profits are
optimized when the solar and storage are distributed more widely over the
network, hence it is more effective to install storage closer to the consumer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00244</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00244</id><created>2018-11-01</created><authors><author><keyname>Islam</keyname><forenames>Mohammad Tariqul</forenames></author><author><keyname>Saha</keyname><forenames>Dipayan</forenames></author><author><keyname>Rahman</keyname><forenames>S. M. Mahbubur</forenames></author><author><keyname>Ahmad</keyname><forenames>M. Omair</forenames></author><author><keyname>Swamy</keyname><forenames>M. N. S.</forenames></author></authors><title>A Variational Step for Reduction of Mixed Gaussian-Impulse Noise from
  Images</title><categories>eess.IV</categories><comments>ICECE, Dhaka, Bangladesh, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reduction of mixed noise is an ill posed problem for the occurrence of
contrasting distributions of noise in the image. The mixed noise that is
usually encountered is the simultaneous presence of additive white Gaussian
noise (AWGN) and impulse noise (IN). A standard approach to denoise an image
with such corruption is to apply a rank order filter (ROF) followed by an
efficient linear filter to remove the residual noise. However, ROF cannot
completely remove the heavy tail of the noise distribution originating from the
IN and thus the denoising performance can be suboptimal. In this paper, we
present a variational step to remove the heavy tail of the noise distribution.
Through experiments, it is shown that this approach can significantly improve
the denoising performance of mixed AWGN-IN using well-established methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00283</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00283</id><created>2018-11-01</created><authors><author><keyname>Liu</keyname><forenames>Penghuan</forenames></author></authors><title>A unified joint reconstruction approach in structured illumination
  microscopy using unknown speckle patterns</title><categories>eess.IV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structured illumination microscopy using unknown speckle patterns has
shown the capacity to surpass the Abbe's diffraction barrier, giving the
possibility to design cheap and versatile SIM devices. However, the
state-of-the-art joint reconstruction methods in this framework has a
relatively low contrast in super-resolution part in comparison to conventional
SIM and the hyper-parameter is not easy to tune. In this paper, a unified joint
reconstruction approach is proposed with the hyper-parameter proportional to
the noise level. Different regularization terms could be evaluated under the
same model. Moreover, the degradation entailed by out-of-focus light could be
solved in speckle illumination setup easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00301</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00301</id><created>2018-11-01</created><authors><author><keyname>Wang</keyname><forenames>Dezhi</forenames></author><author><keyname>Zhang</keyname><forenames>Lilun</forenames></author><author><keyname>Bao</keyname><forenames>Changchun</forenames></author><author><keyname>Xu</keyname><forenames>Kele</forenames></author><author><keyname>Zhu</keyname><forenames>Boqing</forenames></author><author><keyname>Kong</keyname><forenames>Qiuqiang</forenames></author></authors><title>Weakly supervised CRNN system for sound event detection with large-scale
  unlabeled in-domain data</title><categories>cs.SD eess.AS</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sound event detection (SED) is typically posed as a supervised learning
problem requiring training data with strong temporal labels of sound events.
However, the production of datasets with strong labels normally requires
unaffordable labor cost. It limits the practical application of supervised SED
methods. The recent advances in SED approaches focuses on detecting sound
events by taking advantages of weakly labeled or unlabeled training data. In
this paper, we propose a joint framework to solve the SED task using
large-scale unlabeled in-domain data. In particular, a state-of-the-art general
audio tagging model is first employed to predict weak labels for unlabeled
data. On the other hand, a weakly supervised architecture based on the
convolutional recurrent neural network (CRNN) is developed to solve the strong
annotations of sound events with the aid of the unlabeled data with predicted
labels. It is found that the SED performance generally increases as more
unlabeled data is added into the training. To address the noisy label problem
of unlabeled data, an ensemble strategy is applied to increase the system
robustness. The proposed system is evaluated on the SED dataset of DCASE 2018
challenge. It reaches a F1-score of 21.0%, resulting in an improvement of 10%
over the baseline system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00334</identifier>
 <datestamp>2019-02-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00334</id><created>2018-11-01</created><updated>2019-02-20</updated><authors><author><keyname>Damsk&#xe4;gg</keyname><forenames>Eero-Pekka</forenames></author><author><keyname>Juvela</keyname><forenames>Lauri</forenames></author><author><keyname>Thuillier</keyname><forenames>Etienne</forenames></author><author><keyname>V&#xe4;lim&#xe4;ki</keyname><forenames>Vesa</forenames></author></authors><title>Deep Learning for Tube Amplifier Emulation</title><categories>eess.AS cs.SD</categories><comments>Accepted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analog audio effects and synthesizers often owe their distinct sound to
circuit nonlinearities. Faithfully modeling such significant aspect of the
original sound in virtual analog software can prove challenging. The current
work proposes a generic data-driven approach to virtual analog modeling and
applies it to the Fender Bassman 56F-A vacuum-tube amplifier. Specifically, a
feedforward variant of the WaveNet deep neural network is trained to carry out
a regression on audio waveform samples from input to output of a SPICE model of
the tube amplifier. The output signals are pre-emphasized to assist the model
at learning the high-frequency content. The results of a listening test suggest
that the proposed model accurately emulates the reference device. In
particular, the model responds to user control changes, and faithfully
restitutes the range of sonic characteristics found across the configurations
of the original device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00338</identifier>
 <datestamp>2019-08-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00338</id><created>2018-11-01</created><updated>2019-08-03</updated><authors><author><keyname>Zou</keyname><forenames>Qin</forenames></author><author><keyname>Wang</keyname><forenames>Yanling</forenames></author><author><keyname>Wang</keyname><forenames>Qian</forenames></author><author><keyname>Zhao</keyname><forenames>Yi</forenames></author><author><keyname>Li</keyname><forenames>Qingquan</forenames></author></authors><title>Deep Learning Based Gait Recognition Using Smartphones in the Wild</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparing with other biometrics, gait has advantages of being unobtrusive and
difficult to conceal. Inertial sensors such as accelerometer and gyroscope are
often used to capture gait dynamics. Nowadays, these inertial sensors have
commonly been integrated in smartphones and widely used by average person,
which makes it very convenient and inexpensive to collect gait data. In this
paper, we study gait recognition using smartphones in the wild. Unlike
traditional methods that often require the person to walk along a specified
road and/or at a normal walking speed, the proposed method collects inertial
gait data under a condition of unconstraint without knowing when, where, and
how the user walks. To obtain a high performance of person identification and
authentication, deep-learning techniques are presented to learn and model the
gait biometrics from the walking data. Specifically, a hybrid deep neural
network is proposed for robust gait feature representation, where features in
the space domain and in the time domain are successively abstracted by a
convolutional neural network and a recurrent neural network. In the
experiments, two datasets collected by smartphones on a total of 118 subjects
are used for evaluations. Experiments show that the proposed method achieves
over 93.5% and 93.7% accuracy in person identification and authentication,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00348</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00348</id><created>2018-11-01</created><authors><author><keyname>Zhang</keyname><forenames>Haitong</forenames></author><author><keyname>Zhang</keyname><forenames>Junbo</forenames></author><author><keyname>Wang</keyname><forenames>Yujun</forenames></author></authors><title>Sequence-to-sequence Models for Small-Footprint Keyword Spotting</title><categories>cs.SD eess.AS</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a sequence-to-sequence model for keyword spotting
(KWS). Compared with other end-to-end architectures for KWS, our model
simplifies the pipelines of production-quality KWS system and satisfies the
requirement of high accuracy, low-latency, and small-footprint. We also
evaluate the performances of different encoder architectures, which include
LSTM and GRU. Experiments on the real-world wake-up data show that our approach
outperforms the recently proposed attention-based end-to-end model.
Specifically speaking, with 73K parameters, our sequence-to-sequence model
achieves $\sim$3.05\% false rejection rate (FRR) at 0.1 false alarm (FA) per
hour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00350</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00350</id><created>2018-11-01</created><updated>2018-11-03</updated><authors><author><keyname>Zhang</keyname><forenames>Haitong</forenames></author><author><keyname>Zhang</keyname><forenames>Junbo</forenames></author><author><keyname>Wang</keyname><forenames>Yujun</forenames></author></authors><title>End-to-end Models with auditory attention in Multi-channel Keyword
  Spotting</title><categories>cs.SD eess.AS</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an attention-based end-to-end model for
multi-channel keyword spotting (KWS), which is trained to optimize the KWS
result directly. As a result, our model outperforms the baseline model with
signal pre-processing techniques in both the clean and noisy testing data. We
also found that multi-task learning results in a better performance when the
training and testing data are similar. Transfer learning and multi-target
spectral mapping can dramatically enhance the robustness to the noisy
environment. At 0.1 false alarm (FA) per hour, the model with transfer learning
and multi-target mapping gain an absolute 30% improvement in the wake-up rate
in the noisy data with SNR about -20.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00403</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00403</id><created>2018-11-01</created><updated>2019-04-15</updated><authors><author><keyname>Kamper</keyname><forenames>Herman</forenames></author></authors><title>Truly unsupervised acoustic word embeddings using weak top-down
  constraints in encoder-decoder models</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>5 pages, 3 figures, 2 tables; accepted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate unsupervised models that can map a variable-duration speech
segment to a fixed-dimensional representation. In settings where unlabelled
speech is the only available resource, such acoustic word embeddings can form
the basis for &quot;zero-resource&quot; speech search, discovery and indexing systems.
Most existing unsupervised embedding methods still use some supervision, such
as word or phoneme boundaries. Here we propose the encoder-decoder
correspondence autoencoder (EncDec-CAE), which, instead of true word segments,
uses automatically discovered segments: an unsupervised term discovery system
finds pairs of words of the same unknown type, and the EncDec-CAE is trained to
reconstruct one word given the other as input. We compare it to a standard
encoder-decoder autoencoder (AE), a variational AE with a prior over its latent
embedding, and downsampling. EncDec-CAE outperforms its closest competitor by
24% relative in average precision on two languages in a word discrimination
task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00409</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00409</id><created>2018-11-01</created><authors><author><keyname>Eisen</keyname><forenames>Mark</forenames></author><author><keyname>Rashid</keyname><forenames>Mohammad M.</forenames></author><author><keyname>Gatsis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Cavalcanti</keyname><forenames>Dave</forenames></author><author><keyname>Himayat</keyname><forenames>Nageen</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Control Aware Radio Resource Allocation in Low Latency Wireless Control
  Systems</title><categories>eess.SP cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of allocating radio resources over wireless
communication links to control a series of independent wireless control
systems. Low-latency transmissions are necessary in enabling time-sensitive
control systems to operate over wireless links with high reliability. Achieving
fast data rates over wireless links thus comes at the cost of reliability in
the form of high packet error rates compared to wired links due to channel
noise and interference. However, the effect of the communication link errors on
the control system performance depends dynamically on the control system state.
We propose a novel control-communication co-design approach to the low-latency
resource allocation problem. We incorporate control and channel state
information to make scheduling decisions over time on frequency, bandwidth and
data rates across the next-generation Wi-Fi based wireless communication links
that close the control loops. Control systems that are closer to instability or
further from a desired range in a given control cycle are given higher packet
delivery rate targets to meet. Rather than a simple priority ranking, we derive
precise packet error rate targets for each system needed to satisfy stability
targets and make scheduling decisions to meet such targets while reducing total
transmission time. The resulting Control-Aware Low Latency Scheduling (CALLS)
method is tested in numerous simulation experiments that demonstrate its
effectiveness in meeting control-based goals under tight latency constraints
relative to control-agnostic scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00417</identifier>
 <datestamp>2018-11-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00417</id><created>2018-10-31</created><authors><author><keyname>Cao</keyname><forenames>Trang Ngoc</forenames></author><author><keyname>Ahmadzadeh</keyname><forenames>Arman</forenames></author><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Wicke</keyname><forenames>Wayan</forenames></author><author><keyname>Yeoh</keyname><forenames>Phee Lep</forenames></author><author><keyname>Evans</keyname><forenames>Jamie</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Diffusive Mobile MC for Controlled-Release Drug Delivery with Absorbing
  Receiver</title><categories>physics.med-ph eess.SP</categories><comments>7 pages, 4 figures, 1 table. Submitted to the 2019 IEEE International
  Conference on Communications (IEEE ICC 2019) in October 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nanoparticle drug carriers play an important role in facilitating efficient
targeted drug delivery, i.e., improving treatment success and reducing drug
costs and side effects. However, the mobility of nanoparticle drug carriers
poses a challenge in designing drug delivery systems. Moreover, healing results
critically depend on the rate and time duration of drug absorption. Therefore,
in this paper, we aim to design a controlled-release drug delivery system with
a mobile drug carrier that minimizes the total amount of released drugs while
ensuring a desired rate of drug absorption during a prescribed time period. We
model the mobile drug carrier as a mobile transmitter, the targeted diseased
cells as an absorbing receiver, and the channel between the transceivers as a
time-variant channel since the carrier mobility results in a time-variant
absorption rate of the drug molecules. Based on this, we develop a molecular
communication (MC) framework to design the controlled-release drug delivery
system. In particular, we develop new analytical expressions for the mean,
variance, probability density function, and cumulative distribution function of
the channel impulse response (CIR). Equipped with the statistical analysis of
the CIR, we design and evaluate the performance of the controlled-release drug
delivery system. Numerical results show significant savings in the amount of
released drugs compared to a constant-release rate design and reveal the
necessity of accounting for drug carrier mobility for reliable drug delivery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00454</identifier>
 <datestamp>2019-06-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00454</id><created>2018-11-01</created><authors><author><keyname>Grais</keyname><forenames>Emad M.</forenames></author><author><keyname>Wierstorf</keyname><forenames>Hagen</forenames></author><author><keyname>Ward</keyname><forenames>Dominic</forenames></author><author><keyname>Mason</keyname><forenames>Russell</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Referenceless Performance Evaluation of Audio Source Separation using
  Deep Neural Networks</title><categories>cs.SD cs.LG cs.MM eess.AS</categories><msc-class>68T01, 68T10, 68T45, 62H25</msc-class><acm-class>H.5.5; I.5; I.2.6; I.4.3; I.4; I.2</acm-class><journal-ref>This paper will be presented at EUSIPCO 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current performance evaluation for audio source separation depends on
comparing the processed or separated signals with reference signals. Therefore,
common performance evaluation toolkits are not applicable to real-world
situations where the ground truth audio is unavailable. In this paper, we
propose a performance evaluation technique that does not require reference
signals in order to assess separation quality. The proposed technique uses a
deep neural network (DNN) to map the processed audio into its quality score.
Our experiment results show that the DNN is capable of predicting the
sources-to-artifacts ratio from the blind source separation evaluation toolkit
without the need for reference signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00572</identifier>
 <datestamp>2019-08-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00572</id><created>2018-11-01</created><updated>2019-08-16</updated><authors><author><keyname>Mohades</keyname><forenames>Mohamad Mahdi</forenames></author><author><keyname>Kahaei</keyname><forenames>Mohammad Hossein</forenames></author></authors><title>Matrix Completion with Side Information using Manifold Optimization</title><categories>math.OC eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the Matrix Completion (MC) problem based on manifold optimization by
incorporating the side information under which the columns of the intended
matrix are drawn from a union of low dimensional subspaces. It is proved that
this side information leads us to construct new manifolds, as $\it{embedded}$
submanifold of the manifold of constant rank matrices, using which the MC
problem is solved more accurately.
  The required geometrical properties of the aforementioned manifold are then
presented for matrix completion. Simulation results show that the proposed
method outperforms some recent techniques either based on side information or
not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00577</identifier>
 <datestamp>2019-09-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00577</id><created>2018-11-01</created><updated>2019-09-18</updated><authors><author><keyname>Chamon</keyname><forenames>Luiz F. O.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Functional Nonlinear Sparse Models</title><categories>cs.LG eess.SP math.OC stat.ML</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal processing is rich in inherently continuous and often nonlinear
applications, such as spectral estimation, optical imaging, and
super-resolution microscopy, in which sparsity plays a key role in obtaining
state-of-the-art results. Coping with the infinite dimensionality and
non-convexity of these estimation problems typically involves discretization
and convex relaxations, e.g., using atomic norms. Nevertheless, grid mismatch
and other coherence issues imply that discretized sparse signals are often no
longer sparse. Even if they are, recovering sparse solutions using convex
relaxations requires assumptions that may be hard to meet in practice. Finally,
problems involving nonlinear measurements remain non-convex even after relaxing
the sparsity objective. We address these issues by directly tackling the
continuous, nonlinear problem cast as a sparse functional optimization program.
We prove that when these problems are non-atomic, they have no duality gap and
can therefore be solved efficiently using duality and~(stochastic) convex
optimization methods. We illustrate the wide range of applications of this
approach by formulating and solving problems from nonlinear spectral estimation
and robust classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00687</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00687</id><created>2018-11-01</created><authors><author><keyname>Amalladinne</keyname><forenames>Vamsi K.</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author><author><keyname>Chamberland</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author></authors><title>Asynchronous Neighbor Discovery Using Coupled Compressive Sensing</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The neighbor discovery paradigm finds wide application in Internet of Things
networks, where the number of active devices is orders of magnitude smaller
than the total device population. Designing low-complexity schemes for
asynchronous neighbor discovery has recently gained significant attention from
the research community. Concurrently, a divide-and-conquer framework, referred
to as coupled compressive sensing, has been introduced for the synchronous
massive random access channel. This work adapts this novel algorithm to the
problem of asynchronous neighbor discovery with unknown transmission delays.
Simulation results suggest that the proposed scheme requires much fewer
transmissions to achieve a performance level akin to that of state-of-the-art
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00707</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00707</id><created>2018-11-01</created><authors><author><keyname>Li</keyname><forenames>Jason</forenames></author><author><keyname>Gadde</keyname><forenames>Ravi</forenames></author><author><keyname>Ginsburg</keyname><forenames>Boris</forenames></author><author><keyname>Lavrukhin</keyname><forenames>Vitaly</forenames></author></authors><title>Training Neural Speech Recognition Systems with Synthetic Speech
  Augmentation</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Pre-print. Work in progress, 5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building an accurate automatic speech recognition (ASR) system requires a
large dataset that contains many hours of labeled speech samples produced by a
diverse set of speakers. The lack of such open free datasets is one of the main
issues preventing advancements in ASR research. To address this problem, we
propose to augment a natural speech dataset with synthetic speech. We train
very large end-to-end neural speech recognition models using the LibriSpeech
dataset augmented with synthetic speech. These new models achieve state of the
art Word Error Rate (WER) for character-level based models without an external
language model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00738</identifier>
 <datestamp>2019-02-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00738</id><created>2018-11-02</created><updated>2019-02-25</updated><authors><author><keyname>Liu</keyname><forenames>Quanying</forenames></author><author><keyname>Nakahira</keyname><forenames>Yorie</forenames></author><author><keyname>Mohideen</keyname><forenames>Ahkeel</forenames></author><author><keyname>Dai</keyname><forenames>Adam</forenames></author><author><keyname>Choi</keyname><forenames>Sunghoon</forenames></author><author><keyname>Pan</keyname><forenames>Angelina</forenames></author><author><keyname>Ho</keyname><forenames>Dimitar M.</forenames></author><author><keyname>Doyle</keyname><forenames>John C.</forenames></author></authors><title>WheelCon: A wheel control-based gaming platform for studying human
  sensorimotor control</title><categories>math.OC eess.SP q-bio.NC</categories><comments>20 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feedback control theory has been extensively implemented to theoretically
model human sensorimotor control. However, experimental platforms capable of
manipulating important components of multiple feedback loops lack development.
This paper describes the WheelCon, which is an open source platform aimed at
resolving such insufficiencies. WheelCon enables safely simulation of the
canonical sensorimotor task such as riding a mountain bike down a steep,
twisting, bumpy trail etc., with provided only a computer, standard display,
and an inexpensive gaming steering wheel with a force feedback motor. The
platform provides flexibility, as will be demonstrated in the demos provided,
so that researchers may manipulate the disturbances, delay, and quantization
(data rate) in the layered feedback loops, including a high-level advanced plan
layer and a low-level delayed reflex layer. In this paper, we illustrate
WheelCon's graphical user interface (GUI), the input and output of existing
demos, and how to design new games. In addition, we present the basic feedback
model, and we show the testing results from our demo games which align well
with prediction from the model. In short, the platform is featured as cheap,
simple to use, and flexible to program for effective sensorimotor neuroscience
research and control engineering education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00747</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00747</id><created>2018-11-02</created><updated>2018-11-04</updated><authors><author><keyname>Williams</keyname><forenames>Simon</forenames></author><author><keyname>Suvorov</keyname><forenames>Arthur George</forenames></author><author><keyname>Fu</keyname><forenames>Wang Zeng</forenames></author><author><keyname>Moran</keyname><forenames>Bill</forenames></author></authors><title>Information Geometry of Sensor Configuration</title><categories>cs.IT eess.SP math.IT math.ST stat.TH</categories><comments>submitted to Information Geometry 2018-11-02</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In problems of parameter estimation from sensor data, the Fisher Information
provides a measure of the performance of the sensor; effectively, in an
infinitesimal sense, how much information about the parameters can be obtained
from the measurements. From the geometric viewpoint, it is a Riemannian metric
on the manifold of parameters of the observed system. In this paper we consider
the case of parameterized sensors and answer the question, &quot;How best to
reconfigure a sensor (vary the parameters of the sensor) to optimize the
information collected?&quot; A change in the sensor parameters results in a
corresponding change to the metric. We show that the change in information due
to reconfiguration exactly corresponds to the natural metric on the infinite
dimensional space of Riemannian metrics on the parameter manifold, restricted
to finite-dimensional sub-manifold determined by the sensor parameters. The
distance measure on this configuration manifold is shown to provide optimal,
dynamic sensor reconfiguration based on an information criterion. Geodesics on
the configuration manifold are shown to optimize the information gain but only
if the change is made at a certain rate. An example of configuring two
bearings-only sensors to optimally locate a target is developed in detail to
illustrate the mathematical machinery, with Fast-Marching methods employed to
efficiently calculate the geodesics and illustrate the practicality of using
this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00863</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00863</id><created>2018-10-23</created><authors><author><keyname>Fageot</keyname><forenames>Julien</forenames></author><author><keyname>Uhlmann</keyname><forenames>Virginie</forenames></author><author><keyname>P&#xfc;sp&#xf6;ki</keyname><forenames>Zsuzsanna</forenames></author><author><keyname>Beck</keyname><forenames>Benjamin</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author><author><keyname>Depeursinge</keyname><forenames>Adrien</forenames></author></authors><title>Principled Design and Implementation of Steerable Detectors</title><categories>eess.IV stat.ME</categories><msc-class>68U10, 65T40, 65D07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a complete pipeline for the detection of patterns of interest in
an image. In our approach, the patterns are assumed to be adequately modeled by
a known template, and are located at unknown position and orientation. We
propose a continuous-domain additive image model, where the analyzed image is
the sum of the template and an isotropic background signal with self-similar
isotropic power-spectrum. The method is able to learn an optimal steerable
filter fulfilling the SNR criterion based on one single template and background
pair, that therefore strongly responds to the template, while optimally
decoupling from the background model. The proposed filter then allows for a
fast detection process, with the unknown orientation estimation through the use
of steerability properties. In practice, the implementation requires to
discretize the continuous-domain formulation on polar grids, which is performed
using radial B-splines. We demonstrate the practical usefulness of our method
on a variety of template approximation and pattern detection experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00873</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00873</id><created>2018-10-30</created><authors><author><keyname>Bose</keyname><forenames>Sumon Kumar</forenames></author><author><keyname>Kar</keyname><forenames>Bapi</forenames></author><author><keyname>Roy</keyname><forenames>Mohendra</forenames></author><author><keyname>Gopalakrishnan</keyname><forenames>Pradeep Kumar</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author></authors><title>ADEPOS: Anomaly Detection based Power Saving for Predictive Maintenance
  using Edge Computing</title><categories>eess.SP</categories><comments>Submitted to ASP-DAC 2019, Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In industry 4.0, predictive maintenance(PM) is one of the most important
applications pertaining to the Internet of Things(IoT). Machine learning is
used to predict the possible failure of a machine before the actual event
occurs. However, the main challenges in PM are (a) lack of enough data from
failing machines, and (b) paucity of power and bandwidth to transmit sensor
data to cloud throughout the lifetime of the machine. Alternatively, edge
computing approaches reduce data transmission and consume low energy. In this
paper, we propose Anomaly Detection based Power Saving(ADEPOS) scheme using
approximate computing through the lifetime of the machine. In the beginning of
the machines life, low accuracy computations are used when the machine is
healthy. However, on the detection of anomalies, as time progresses, the system
is switched to higher accuracy modes. We show using the NASA bearing dataset
that using ADEPOS, we need 8.8X less neurons on average and based on
post-layout results, the resultant energy savings are 6.4 to 6.65X
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00879</identifier>
 <datestamp>2019-12-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00879</id><created>2018-11-02</created><updated>2019-12-12</updated><authors><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Thompson</keyname><forenames>Andrew</forenames></author></authors><title>CHIRRUP: a practical algorithm for unsourced multiple access</title><categories>eess.SP</categories><journal-ref>Information and Inference, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsourced multiple access abstracts grantless simultaneous communication of a
large number of devices (messages) each of which transmits (is transmitted)
infrequently. It provides a model for machine-to-machine communication in the
Internet of Things (IoT), including the special case of radio-frequency
identification (RFID), as well as neighbor discovery in ad hoc wireless
networks. This paper presents a fast algorithm for unsourced multiple access
that scales to $2^{100}$ devices (arbitrary $100$ bit messages). The primary
building block is multiuser detection of binary chirps which are simply
codewords in the second order Reed Muller code. The chirp detection algorithm
originally presented by Howard et al. is enhanced and integrated into a peeling
decoder designed for a patching and slotting framework. In terms of both energy
per bit and number of transmitted messages, the proposed algorithm is within a
factor of $2$ of state of the art approaches. A significant advantage of our
algorithm is its computational efficiency. We prove that the worst-case
complexity of the basic chirp reconstruction algorithm is
$\mathcal{O}[nK(\log_2 n + K)]$, where $n$ is the codeword length and $K$ is
the number of active users, and we report computing times for our algorithm.
Our performance and computing time results represent a benchmark against which
other practical algorithms can be measured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00882</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00882</id><created>2018-10-31</created><updated>2019-04-18</updated><authors><author><keyname>An</keyname><forenames>Yi</forenames></author><author><keyname>Huang</keyname><forenames>Liangjin</forenames></author><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Leng</keyname><forenames>Jinyong</forenames></author><author><keyname>Yang</keyname><forenames>Lijia</forenames></author><author><keyname>Zhou</keyname><forenames>Pu</forenames></author></authors><title>Learning to decompose the modes in few-mode fibers with deep
  convolutional neural network</title><categories>eess.SP physics.optics</categories><journal-ref>Optics Express Vol. 27, Issue 7, pp. 10127-10137 (2019)</journal-ref><doi>10.1364/OE.27.010127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce deep learning technique to perform complete mode decomposition
for few-mode optical fiber for the first time. Our goal is to learn a fast and
accurate mapping from near-field beam profiles to the complete mode
coefficients, including both modal amplitudes and phases. We train the
convolutional neural network with simulated beam patterns, and evaluate the
network on both of the simulated beam data and the real beam data. In simulated
beam data testing, the correlation between the reconstructed and the ideal beam
profiles can achieve 0.9993 and 0.995 for 3-mode case and 5-mode case
respectively. While in the real 3-mode beam data testing, the average
correlation is 0.9912 and the mode decomposition can be potentially performed
at 33 Hz frequency on Graphic Processing Unit, indicating real-time processing
ability. The quantitative evaluations demonstrate the superiority of our deep
learning based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00883</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00883</id><created>2018-10-31</created><authors><author><keyname>Liu</keyname><forenames>Bin</forenames></author><author><keyname>Nie</keyname><forenames>Shuai</forenames></author><author><keyname>Zhang</keyname><forenames>Yaping</forenames></author><author><keyname>Liang</keyname><forenames>Shan</forenames></author><author><keyname>Liu</keyname><forenames>Wenju</forenames></author></authors><title>Deep Segment Attentive Embedding for Duration Robust Speaker
  Verification</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LSTM-based speaker verification usually uses a fixed-length local segment
randomly truncated from an utterance to learn the utterance-level speaker
embedding, while using the average embedding of all segments of a test
utterance to verify the speaker, which results in a critical mismatch between
testing and training. This mismatch degrades the performance of speaker
verification, especially when the durations of training and testing utterances
are very different. To alleviate this issue, we propose the deep segment
attentive embedding method to learn the unified speaker embeddings for
utterances of variable duration. Each utterance is segmented by a sliding
window and LSTM is used to extract the embedding of each segment. Instead of
only using one local segment, we use the whole utterance to learn the
utterance-level embedding by applying an attentive pooling to the embeddings of
all segments. Moreover, the similarity loss of segment-level embeddings is
introduced to guide the segment attention to focus on the segments with more
speaker discriminations, and jointly optimized with the similarity loss of
utterance-level embeddings. Systematic experiments on Tongdun and VoxCeleb show
that the proposed method significantly improves robustness of duration variant
and achieves the relative Equal Error Rate reduction of 50% and 11.54% ,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00936</identifier>
 <datestamp>2018-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00936</id><created>2018-11-02</created><authors><author><keyname>Bhatt</keyname><forenames>Gaurav</forenames></author><author><keyname>Gupta</keyname><forenames>Akshita</forenames></author><author><keyname>Arora</keyname><forenames>Aditya</forenames></author><author><keyname>Raman</keyname><forenames>Balasubramanian</forenames></author></authors><title>Acoustic Features Fusion using Attentive Multi-channel Deep Architecture</title><categories>cs.SD eess.AS</categories><comments>Accepted in CHiME'18 (Interspeech Workshop)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel deep fusion architecture for audio
classification tasks. The multi-channel model presented is formed using deep
convolution layers where different acoustic features are passed through each
channel. To enable dissemination of information across the channels, we
introduce attention feature maps that aid in the alignment of frames. The
output of each channel is merged using interaction parameters that non-linearly
aggregate the representative features. Finally, we evaluate the performance of
the proposed architecture on three benchmark datasets:- DCASE-2016 and LITIS
Rouen (acoustic scene recognition), and CHiME-Home (tagging). Our experimental
results suggest that the architecture presented outperforms the standard
baselines and achieves outstanding performance on the task of acoustic scene
recognition and audio tagging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.00971</identifier>
 <datestamp>2019-05-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.00971</id><created>2018-11-02</created><updated>2019-05-27</updated><authors><author><keyname>Balevi</keyname><forenames>Eren</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>One-Bit OFDM Receivers via Deep Learning</title><categories>cs.IT cs.LG eess.SP math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops novel deep learning-based architectures and design
methodologies for an orthogonal frequency division multiplexing (OFDM) receiver
under the constraint of one-bit complex quantization. Single bit quantization
greatly reduces complexity and power consumption, but makes accurate channel
estimation and data detection difficult. This is particularly true for
multicarrier waveforms, which have high peak-to-average ratio in the time
domain and fragile subcarrier orthogonality in the frequency domain. The severe
distortion for one-bit quantization typically results in an error floor even at
moderately low signal-to-noise-ratio (SNR) such as 5 dB. For channel estimation
(using pilots), we design a novel generative supervised deep neural network
(DNN) that can be trained with a reasonable number of pilots. After channel
estimation, a neural network-based receiver -- specifically, an autoencoder --
jointly learns a precoder and decoder for data symbol detection. Since
quantization prevents end-to-end training, we propose a two-step sequential
training policy for this model. With synthetic data, our deep learning-based
channel estimation can outperform least squares (LS) channel estimation for
unquantized (full-resolution) OFDM at average SNRs up to 14 dB. For data
detection, our proposed design achieves lower bit error rate (BER) in fading
than unquantized OFDM at average SNRs up to 10 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01092</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01092</id><created>2018-11-02</created><updated>2019-02-18</updated><authors><author><keyname>Phan</keyname><forenames>Huy</forenames></author><author><keyname>Ch&#xe9;n</keyname><forenames>Oliver Y.</forenames></author><author><keyname>Koch</keyname><forenames>Philipp</forenames></author><author><keyname>Pham</keyname><forenames>Lam</forenames></author><author><keyname>McLoughlin</keyname><forenames>Ian</forenames></author><author><keyname>Mertins</keyname><forenames>Alfred</forenames></author><author><keyname>De Vos</keyname><forenames>Maarten</forenames></author></authors><title>Unifying Isolated and Overlapping Audio Event Detection with Multi-Label
  Multi-Task Convolutional Recurrent Neural Networks</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Accepted for the 44th International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a multi-label multi-task framework based on a convolutional
recurrent neural network to unify detection of isolated and overlapping audio
events. The framework leverages the power of convolutional recurrent neural
network architectures; convolutional layers learn effective features over which
higher recurrent layers perform sequential modelling. Furthermore, the output
layer is designed to handle arbitrary degrees of event overlap. At each time
step in the recurrent output sequence, an output triple is dedicated to each
event category of interest to jointly model event occurrence and temporal
boundaries. That is, the network jointly determines whether an event of this
category occurs, and when it occurs, by estimating onset and offset positions
at each recurrent time step. We then introduce three sequential losses for
network training: multi-label classification loss, distance estimation loss,
and confidence loss. We demonstrate good generalization on two datasets:
ITC-Irst for isolated audio event detection, and TUT-SED-Synthetic-2016 for
overlapping audio event detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01095</identifier>
 <datestamp>2019-05-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01095</id><created>2018-11-02</created><updated>2019-05-08</updated><authors><author><keyname>Phan</keyname><forenames>Huy</forenames></author><author><keyname>Ch&#xe9;n</keyname><forenames>Oliver Y.</forenames></author><author><keyname>Koch</keyname><forenames>Philipp</forenames></author><author><keyname>Pham</keyname><forenames>Lam</forenames></author><author><keyname>McLoughlin</keyname><forenames>Ian</forenames></author><author><keyname>Mertins</keyname><forenames>Alfred</forenames></author><author><keyname>De Vos</keyname><forenames>Maarten</forenames></author></authors><title>Beyond Equal-Length Snippets: How Long is Sufficient to Recognize an
  Audio Scene?</title><categories>cs.SD cs.LG eess.AS</categories><comments>Accepted to 2019 AES Conference on Audio Forensics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the variability in characteristics of audio scenes, some scenes can
naturally be recognized earlier than others. In this work, rather than using
equal-length snippets for all scene categories, as is common in the literature,
we study to which temporal extent an audio scene can be reliably recognized
given state-of-the-art models. Moreover, as model fusion with deep network
ensemble is prevalent in audio scene classification, we further study whether,
and if so, when model fusion is necessary for this task. To achieve these
goals, we employ two single-network systems relying on a convolutional neural
network and a recurrent neural network for classification as well as early
fusion and late fusion of these networks. Experimental results on the
LITIS-Rouen dataset show that some scenes can be reliably recognized with a few
seconds while other scenes require significantly longer durations. In addition,
model fusion is shown to be the most beneficial when the signal length is
short.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01133</identifier>
 <datestamp>2019-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01133</id><created>2018-11-02</created><updated>2019-11-20</updated><authors><author><keyname>As'ad</keyname><forenames>Hala</forenames></author><author><keyname>Bouchard</keyname><forenames>Martin</forenames></author><author><keyname>Kamkar-Parsi</keyname><forenames>Homayoun</forenames></author></authors><title>A Robust Target Linearly Constrained Minimum Variance Beamformer With
  Spatial Cues Preservation for Binaural Hearing Aids</title><categories>eess.AS cs.SD</categories><comments>15 pages, 16 figures</comments><journal-ref>IEEE/ACM Transactions on Audio, Speech and Language Processing
  (TASLP). 2019 Oct 1; 27(10):1549-63</journal-ref><doi>10.1109/TASLP.2019.2924321</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, a binaural beamforming algorithm for hearing aid applications
is introduced.The beamforming algorithm is designed to be robust to some error
in the estimate of the target speaker direction. The algorithm has two main
components: a robust target linearly constrained minimum variance (TLCMV)
algorithm based on imposing two constraints around the estimated direction of
the target signal, and a post-processor to help with the preservation of
binaural cues. The robust TLCMV provides a good level of noise reduction and
low level of target distortion under realistic conditions. The post-processor
enhances the beamformer abilities to preserve the binaural cues for both
diffuse-like background noise and directional interferers (competing speakers),
while keeping a good level of noise reduction. The introduced algorithm does
not require knowledge or estimation of the directional interferers' directions
nor the second-order statistics of noise-only components. The introduced
algorithm requires an estimate of the target speaker direction, but it is
designed to be robust to some deviation from the estimated direction. Compared
with recently proposed state-of-the-art methods, comprehensive evaluations are
performed under complex realistic acoustic scenarios generated in both anechoic
and mildly reverberant environments, considering a mismatch between estimated
and true sources direction of arrival. Mismatch between the anechoic
propagation models used for the design of the beamformers and the mildly
reverberant propagation models used to generate the simulated directional
signals is also considered. The results illustrate the robustness of the
proposed algorithm to such mismatches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01143</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01143</id><created>2018-11-02</created><updated>2019-02-18</updated><authors><author><keyname>Hung</keyname><forenames>Yun-Ning</forenames></author><author><keyname>Chen</keyname><forenames>Yi-An</forenames></author><author><keyname>Yang</keyname><forenames>Yi-Hsuan</forenames></author></authors><title>Multitask learning for frame-level instrument recognition</title><categories>cs.SD eess.AS</categories><comments>This is a pre-print version of an ICASSP 2019 paper</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  For many music analysis problems, we need to know the presence of instruments
for each time frame in a multi-instrument musical piece. However, such a
frame-level instrument recognition task remains difficult, mainly due to the
lack of labeled datasets. To address this issue, we present in this paper a
large-scale dataset that contains synthetic polyphonic music with frame-level
pitch and instrument labels. Moreover, we propose a simple yet novel network
architecture to jointly predict the pitch and instrument for each frame. With
this multitask learning method, the pitch information can be leveraged to
predict the instruments, and also the other way around. And, by using the
so-called pianoroll representation of music as the main target output of the
model, our model also predicts the instruments that play each individual note
event. We validate the effectiveness of the proposed method for framelevel
instrument recognition by comparing it with its singletask ablated versions and
three state-of-the-art methods. We also demonstrate the result of the proposed
method for multipitch streaming with real-world music. For reproducibility, we
will share the code to crawl the data and to implement the proposed model at:
https://github.com/biboamy/ instrument-streaming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01152</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01152</id><created>2018-11-02</created><authors><author><keyname>Abdul-Rashid</keyname><forenames>Ramadan</forenames></author><author><keyname>Al-Shaikhi</keyname><forenames>Ali</forenames></author><author><keyname>Masoud</keyname><forenames>Ahmad</forenames></author></authors><title>Accurate, Energy-Efficient, Decentralized, Single-Hop, Asynchronous Time
  Synchronization Protocols for Wireless Sensor Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns with the synchronization of infrastructure impoverished
sensor networks under harsh conditions. It suggests three novel asynchronous,
decentralized, energyefficient time synchronization protocols. The protocols
require only single hop, sparse communication with unlabeled neighboring nodes
of the network to determine accurately the time of the gateway node. The time
of a node is considered as a dynamical variable of a discrete system whose
evolution is asynchronously activated/inhibited by another dynamical switching
system. The protocols are termed: Timed Sequential Asynchronous Update (TSAU),
Unidirectional Asynchronous Flooding (UAF) and the Bidirectional Asynchronous
Flooding (UAF). Along with intensive simulation, the protocols are implemented
and tested on the MicaZ sensor node platform. A comprehensive evaluation of the
energy consumption, memory requirements, convergence time, local and global
synchronization errors of the proposed protocols are carried-out against
Flooding Time Synchronization Protocol (FTSP) and Flooding Proportional
Integral Time Synchronization Protocol (FloodPISync). All the analysis show
that these protocols outperform the well known protocols. Also, being
asynchronous, they are more realistic relative to the synchronous ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01174</identifier>
 <datestamp>2020-02-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01174</id><created>2018-11-03</created><updated>2019-04-08</updated><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Chakraborty</keyname><forenames>Deep</forenames></author><author><keyname>Tembine</keyname><forenames>Hamidou</forenames></author><author><keyname>Olaleye</keyname><forenames>Olaitan</forenames></author></authors><title>Nonparallel Emotional Speech Conversion</title><categories>cs.LG eess.AS stat.ML</categories><comments>submitted to INTERSPEECH 2019, 5 pages, 6 figures</comments><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><doi>10.21437/Interspeech.2019-2878</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a nonparallel data-driven emotional speech conversion method. It
enables the transfer of emotion-related characteristics of a speech signal
while preserving the speaker's identity and linguistic content. Most existing
approaches require parallel data and time alignment, which is not available in
most real applications. We achieve nonparallel training based on an
unsupervised style transfer technique, which learns a translation model between
two distributions instead of a deterministic one-to-one mapping between paired
examples. The conversion model consists of an encoder and a decoder for each
emotion domain. We assume that the speech signal can be decomposed into an
emotion-invariant content code and an emotion-related style code in latent
space. Emotion conversion is performed by extracting and recombining the
content code of the source speech and the style code of the target emotion. We
tested our method on a nonparallel corpora with four emotions. Both subjective
and objective evaluations show the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01222</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01222</id><created>2018-11-03</created><authors><author><keyname>Bhattacharjee</keyname><forenames>Mrinmoy</forenames></author><author><keyname>Prasanna</keyname><forenames>S. R. M.</forenames></author><author><keyname>Guha</keyname><forenames>Prithwijit</forenames></author></authors><title>Time-Frequency Audio Features for Speech-Music Classification</title><categories>eess.AS cs.SD</categories><comments>4 pages, 16 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Distinct striation patterns are observed in the spectrograms of speech and
music. This motivated us to propose three novel time-frequency features for
speech-music classification. These features are extracted in two stages. First,
a preset number of prominent spectral peak locations are identified from the
spectra of each frame. These important peak locations obtained from each frame
are used to form Spectral peak sequences (SPS) for an audio interval. In second
stage, these SPS are treated as time series data of frequency locations. The
proposed features are extracted as periodicity, average frequency and
statistical attributes of these spectral peak sequences. Speech-music
categorization is performed by learning binary classifiers on these features.
We have experimented with Gaussian mixture models, support vector machine and
random forest classifiers. Our proposal is validated on four datasets and
benchmarked against three baseline approaches. Experimental results establish
the validity of our proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01233</identifier>
 <datestamp>2020-02-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01233</id><created>2018-11-03</created><updated>2020-02-26</updated><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author></authors><title>Deep Ad-hoc Beamforming</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Far-field speech processing is an important and challenging problem. In this
paper, we propose \textit{deep ad-hoc beamforming}, a deep-learning-based
multichannel speech enhancement framework based on ad-hoc microphone arrays, to
address the problem. It contains three novel components. First, it combines
\textit{ad-hoc microphone arrays} with deep-learning-based multichannel speech
enhancement, which reduces the probability of the occurrence of far-field
acoustic environments significantly. Second, it groups the microphones around
the speech source to a local microphone array by a supervised channel selection
framework based on deep neural networks. Third, it develops a simple time
synchronization framework to synchronize the channels that have different time
delay. Besides the above novelties and advantages, the proposed model is also
trained in a single-channel fashion, so that it can easily employ new
development of speech processing techniques. Its test stage is also flexible in
incorporating any number of microphones without retraining or modifying the
framework. We have developed many implementations of the proposed framework and
conducted an extensive experiment in scenarios where the locations of the
speech sources are far-field, random, and blind to the microphones. Results on
speech enhancement tasks show that our method outperforms its counterpart that
works with linear microphone arrays by a considerable margin in both diffuse
noise reverberant environments and point source noise reverberant environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01251</identifier>
 <datestamp>2019-02-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01251</id><created>2018-11-03</created><updated>2019-02-26</updated><authors><author><keyname>Casebeer</keyname><forenames>Jonah</forenames></author><author><keyname>Wang</keyname><forenames>Zhepei</forenames></author><author><keyname>Smaragdis</keyname><forenames>Paris</forenames></author></authors><title>Multi-View Networks For Multi-Channel Audio Classification</title><categories>cs.SD eess.AS</categories><comments>5 pages, 7 figures, Accepted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the idea of multi-view networks for sound
classification with multiple sensors. We show how one can build a multi-channel
sound recognition model trained on a fixed number of channels, and deploy it to
scenarios with arbitrary (and potentially dynamically changing) number of input
channels and not observe degradation in performance. We demonstrate that at
inference time you can safely provide this model all available channels as it
can ignore noisy information and leverage new information better than standard
baseline approaches. The model is evaluated in both an anechoic environment and
in rooms generated by a room acoustics simulator. We demonstrate that this
model can generalize to unseen numbers of channels as well as unseen room
geometries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01257</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01257</id><created>2018-11-03</created><authors><author><keyname>Porter</keyname><forenames>Richard</forenames></author><author><keyname>Tadic</keyname><forenames>Vladislav</forenames></author><author><keyname>Achim</keyname><forenames>Alin</forenames></author></authors><title>Recovery of compressively sensed ultrasound images with structured
  Sparse Bayesian Learning</title><categories>eess.SP eess.IV</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of recovering compressively sensed
ultrasound images. We build on prior work, and consider a number of existing
approaches that we consider to be the state-of-the-art. The methods we consider
take advantage of a number of assumptions on the signals including those of
temporal and spatial correlation, block structure, prior knowledge of the
support, and non-Gaussianity. We conduct a series of intensive tests to
quantify the performance of these methods. We find that by altering the
parameters of the structured Sparse Bayesian Learning approaches considered, we
can significantly improve the objective quality of the reconstructed images.
The results we achieve are a significant improvement upon previously proposed
reconstruction techniques. In addition, we further show that by careful choice
of parameters, we can obtain near-optimal results whilst requiring only a small
fraction of the computational time needed for the best reconstruction quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01307</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01307</id><created>2018-11-03</created><authors><author><keyname>Chung</keyname><forenames>Yu-An</forenames></author><author><keyname>Weng</keyname><forenames>Wei-Hung</forenames></author><author><keyname>Tong</keyname><forenames>Schrasing</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>Towards Unsupervised Speech-to-Text Translation</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for building speech-to-text translation (ST) systems
using only monolingual speech and text corpora, in other words, speech
utterances from a source language and independent text from a target language.
As opposed to traditional cascaded systems and end-to-end architectures, our
system does not require any labeled data (i.e., transcribed source audio or
parallel source and target text corpora) during training, making it especially
applicable to language pairs with very few or even zero bilingual resources.
The framework initializes the ST system with a cross-modal bilingual dictionary
inferred from the monolingual corpora, that maps every source speech segment
corresponding to a spoken word to its target text translation. For unseen
source speech utterances, the system first performs word-by-word translation on
each speech segment in the utterance. The translation is improved by leveraging
a language model and a sequence denoising autoencoder to provide prior
knowledge about the target language. Experimental results show that our
unsupervised system achieves comparable BLEU scores to supervised end-to-end
models despite the lack of supervision. We also provide an ablation analysis to
examine the utility of each component in our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01340</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01340</id><created>2018-11-04</created><authors><author><keyname>Younus</keyname><forenames>Safwan Hafeedh</forenames></author><author><keyname>Al-Hameed</keyname><forenames>Aubida A.</forenames></author><author><keyname>Hussein</keyname><forenames>Ahmed Taha</forenames></author><author><keyname>Alresheedi</keyname><forenames>Mohammed T.</forenames></author><author><keyname>Elmirghani</keyname><forenames>Jaafar M. H.</forenames></author></authors><title>Subcarrier Multiplexing for Parallel Data Transmission in Indoor Visible
  Light Communication Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an indoor visible light communication (VLC) system in
conjunction with an imaging receiver with parallel data transmission (spatial
multiplexing) to decrease the effects of inter-symbol interference (ISI). To
distinguish between light units (transmitters) and to match the light units
used to convey the data with the pixels of the imaging receiver, we propose the
use of subcarrier multiplexing (SCM) tones. Each light unit transmission is
multiplexed with a unique tone. At the receiver, a SCM tone decision system is
utilized to measure the power level of each SCM tone and consequently associate
each pixel with a light unit. In addition, the level of co-channel interference
(CCI) between light units is estimated using the SCM tones. Our proposed system
is examined in two indoor environments taking into account reflective
components (first and second order reflections). The results show that this
system has the potential to achieve an aggregate data rate of 8 Gb/s with a bit
error rate (BER) of 10-6 for each light unit, using simple on-off-keying (OOK).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01341</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01341</id><created>2018-11-04</created><authors><author><keyname>Younus</keyname><forenames>Safwan Hafeedh</forenames></author><author><keyname>Al-Hameed</keyname><forenames>Aubida A.</forenames></author><author><keyname>Hussein</keyname><forenames>Ahmed Taha</forenames></author><author><keyname>Alresheedi</keyname><forenames>Mohammed T.</forenames></author><author><keyname>Elmirghani</keyname><forenames>Jaafar M. H.</forenames></author></authors><title>WDM for Multi-user Indoor VLC Systems with SCM</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system that employs wavelength division multiplexing (WDM) in conjunction
with subcarrier multiplexing (SCM) tones is proposed to realize high data rate
multi-user indoor visible light communication (VLC). The SCM tones, which are
unmodulated signals, are used to identify each light unit, to find the optimum
light unit for each user and to calculate the level of the co-channel
interference (CCI). WDM is utilized to attain a high data rate for each user.
In this paper, multicolour (four colours) laser diodes (LDs) are utilized as
sources of lighting and data communication. One of the WDM colours is used to
convey the SCM tones at the beginning of the connection to set up the
connection among receivers and light units (to find the optimum light unit for
each user). To evaluate the performance of our VLC system, we propose two types
of receivers: an array of non-imaging receivers (NI-R) and an array of
non-imaging angle diversity receivers (NI-ADR). In this paper, we consider the
effects of diffuse reflections, CCI and mobility on the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01376</identifier>
 <datestamp>2019-02-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01376</id><created>2018-11-04</created><updated>2019-02-25</updated><authors><author><keyname>Mametani</keyname><forenames>Kohki</forenames></author><author><keyname>Kato</keyname><forenames>Tsuneo</forenames></author><author><keyname>Yamamoto</keyname><forenames>Seiichi</forenames></author></authors><title>Investigating context features hidden in End-to-End TTS</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Accepted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have introduced end-to-end TTS, which integrates the
production of context and acoustic features in statistical parametric speech
synthesis. As a result, a single neural network replaced laborious feature
engineering with automated feature learning. However, little is known about
what types of context information end-to-end TTS extracts from text input
before synthesizing speech, and the previous knowledge about context features
is barely utilized. In this work, we first point out the model similarity
between end-to-end TTS and parametric TTS. Based on the similarity, we evaluate
the quality of encoder outputs from an end-to-end TTS system against eight
criteria that are derived from a standard set of context information used in
parametric TTS. We conduct experiments using an evaluation procedure that has
been newly developed in the machine learning literature for quantitative
analysis of neural representations, while adapting it to the TTS domain.
Experimental results show that the encoder outputs reflect both linguistic and
phonetic contexts, such as vowel reduction at phoneme level, lexical stress at
syllable level, and part-of-speech at word level, possibly due to the joint
optimization of context and acoustic features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01381</identifier>
 <datestamp>2018-11-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01381</id><created>2018-11-04</created><updated>2018-11-28</updated><authors><author><keyname>Wu</keyname><forenames>Shaohan</forenames></author><author><keyname>Hughes</keyname><forenames>Brian L.</forenames></author></authors><title>A Hybrid Approach to Joint Estimation of Channel and Antenna impedance</title><categories>eess.SP</categories><comments>6 pages, two columns, 6 figures. References updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a hybrid approach to joint estimation of channel
information and antenna impedance, for single-input, single-output channels.
Based on observation of training sequences via synchronously switched load at
the receiver, we derive joint maximum a posteriori and maximum-likelihood
(MAP/ML) estimators for channel and impedance over multiple packets. We
investigate important properties of these estimators, e.g., bias and
efficiency. We also explore the performance of these estimators through
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01393</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01393</id><created>2018-11-04</created><authors><author><keyname>Khalid</keyname><forenames>Maryam</forenames></author><author><keyname>Amin</keyname><forenames>Osama</forenames></author><author><keyname>Ahmed</keyname><forenames>Sajid</forenames></author><author><keyname>Shihada</keyname><forenames>Basem</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Communication Through Breath: Aerosol Transmission</title><categories>eess.SP cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exhaled breath can be used in retrieving information and creating innovative
communication systems. It contains several volatile organic compounds (VOCs)
and biological entities that can act as health biomarkers. For instance, the
breath of infected human contains a nonnegligible amount of pathogenic aerosol
that can spread or remain suspended in the atmosphere. Therefore, the exhaled
breath can be exploited as a source's message in a communication setup to
remotely scan the bio-information via an aerosol transmission channel. An
overview of the basic configuration is presented along with a description of
system components with a particular emphasis on channel modeling. Furthermore,
the challenges that arise in theoretical analysis and system development are
highlighted. Finally, several open issues are discussed to concretize the
proposed communication concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01436</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01436</id><created>2018-11-04</created><authors><author><keyname>Moser</keyname><forenames>Bernhard</forenames></author></authors><title>On Quasi-Isometry of Threshold-Based Sampling</title><categories>eess.SP</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><msc-class>54E35, 54E40, 94A20</msc-class><doi>10.1109/TSP.2019.2919415</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of isometry for threshold-based sampling such as
integrate-and-fire (IF) or send-on-delta (SOD) is addressed. While for uniform
sampling the Parseval theorem provides isometry and makes the Euclidean metric
canonical, there is no analogy for threshold-based sampling. The relaxation of
the isometric postulate to quasi-isometry, however, allows the discovery of the
underlying metric structure of threshold-based sampling. This paper
characterizes this metric structure making Hermann Weyl's discrepancy measure
canonical for threshold-based sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01447</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01447</id><created>2018-11-04</created><authors><author><keyname>Moser</keyname><forenames>Bernhard</forenames></author></authors><title>Estimating the Signal Reconstruction Error from Threshold-Based Sampling
  Without Knowing the Original Signal</title><categories>eess.SP</categories><comments>3rd International Conference on Event-Based Control, Communication
  and Signal Processing (EBCCSP), 2017</comments><msc-class>94A12, 94A20, 30L05</msc-class><doi>10.1109/EBCCSP.2017.8022834</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of estimating the accuracy of signal reconstruction from
threshold-based sampling, by only taking the sampling output into account, is
addressed. The approach is based on re-sampling the reconstructed signal and
the application of a distance measure in the output space which satisfies the
condition of quasi-isometry. The quasi-isometry property allows to estimate the
reconstruction accuracy from the matching accuracy between the sign sequences
resulting from sampling and the re-sampling after reconstruction. This approach
is exemplified by means of leaky integrate-and-fire. It is shown that this
approach can be used for parameter tuning for optimizing the reconstruction
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01470</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01470</id><created>2018-11-04</created><updated>2019-02-01</updated><authors><author><keyname>Jafari</keyname><forenames>Rana</forenames></author><author><keyname>Jones</keyname><forenames>Travis</forenames></author><author><keyname>Trebino</keyname><forenames>Rick</forenames></author></authors><title>100% Reliable Algorithm for Second-Harmonic-Generation
  Frequency-Resolved Optical Gating</title><categories>physics.optics eess.SP</categories><doi>10.1364/OE.27.002112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate a novel algorithmic approach for the
second-harmonic-generation (SHG) frequency-resolved optical gating (FROG)
ultrashort-pulse-measurement technique that always converges and, for complex
pulses, is also much faster. It takes advantage of the Paley-Wiener Theorem to
retrieve the precise pulse spectrum (half the desired information) directly
from the measured trace. It also uses a multi-grid approach, permitting the
algorithm to operate on smaller arrays for early iterations and on the complete
array for only the final few iterations. We tested this approach on more than
25,000 randomly generated complex pulses, yielding SHG FROG traces to which
noise was added, and have achieved convergence to the correct pulse in all
cases. Moreover, convergence occurs in less than half the time for extremely
large traces corresponding to extremely complex pulses with time-bandwidth
products up to 100.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01514</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01514</id><created>2018-11-05</created><authors><author><keyname>Gonzalez</keyname><forenames>Emmanuel A.</forenames></author></authors><title>Complex variables for fractional-order systems</title><categories>eess.SP</categories><comments>4 pages</comments><msc-class>26A33</msc-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper discusses and summarizes some results on complex variables that
are very useful in fractional-order systems analysis and design, specifically
when the system is analyzed in the frequency domain. The author hopes that this
document will serve as a handy reference when performing computations with
complex variables, especially when working within the Laplace and Fourier
domains. The reader can refer to Table I for the summary of these formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01531</identifier>
 <datestamp>2018-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01531</id><created>2018-11-05</created><updated>2018-11-09</updated><authors><author><keyname>Tzinis</keyname><forenames>Efthymios</forenames></author><author><keyname>Venkataramani</keyname><forenames>Shrikant</forenames></author><author><keyname>Smaragdis</keyname><forenames>Paris</forenames></author></authors><title>Unsupervised Deep Clustering for Source Separation: Direct Learning from
  Mixtures using Spatial Information</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Submitted to ICASSP 2019 (v1: November 5th 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a monophonic source separation system that is trained by only
observing mixtures with no ground truth separation information. We use a deep
clustering approach which trains on multi-channel mixtures and learns to
project spectrogram bins to source clusters that correlate with various spatial
features. We show that using such a training process we can obtain separation
performance that is as good as making use of ground truth separation
information. Once trained, this system is capable of performing sound
separation on monophonic inputs, despite having learned how to do so using
multi-channel recordings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01554</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01554</id><created>2018-11-05</created><authors><author><keyname>Pawar</keyname><forenames>Sankalp</forenames></author><author><keyname>Semper</keyname><forenames>Sebastian</forenames></author><author><keyname>R&#xf6;mer</keyname><forenames>Florian</forenames></author></authors><title>Combining Matrix Design for 2D DoA Estimation with Compressive Antenna
  Arrays using Stochastic Gradient Descent</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, compressive antenna arrays have been considered for DoA estimation
with reduced hardware complexity. By utilizing compressive sensing, such arrays
employ a linear combining network to combine signals from a larger set of
antenna elements in the analog RF domain. In this paper, we develop a design
approach based on the minimization of error between spatial correlation
function (SCF) of the compressive and the uncompressed array resulting in the
estimation performance of the two arrays to be as close as possible. The
proposed design is based on grid-free stochastic gradient descent (SGD)
optimization. In addition to a low computational cost for the proposed method,
we show numerically that the resulting combining matrices perform better than
the ones generated by a previous approach and combining matrices generated from
a Gaussian ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01566</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01566</id><created>2018-11-05</created><authors><author><keyname>Jarosik</keyname><forenames>Piotr</forenames></author><author><keyname>Byra</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Lewandowski</keyname><forenames>Marcin</forenames></author></authors><title>WaveFlow - Towards Integration of Ultrasound Processing with Deep
  Learning</title><categories>eess.SP</categories><comments>2018 IEEE International Ultrasonics Symposium</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The ultimate goal of this work is a real-time processing framework for
ultrasound image reconstruction augmented with machine learning. To attain
this, we have implemented WaveFlow - a set of ultrasound data acquisition and
processing tools for TensorFlow. WaveFlow includes: ultrasound Environments
(connection points between the input raw ultrasound data source and TensorFlow)
and signal processing Operators (ops) library. Raw data can be processed in
real-time using algorithms available both in TensorFlow and WaveFlow.
Currently, WaveFlow provides ops for B-mode image reconstruction (beamforming),
signal processing and quantitative ultrasound. The ops were implemented both
for the CPU and GPU, as well as for built-in automated tests and benchmarks. To
demonstrate WaveFlow's performance, ultrasound data were acquired from wire and
cyst phantoms and elaborated using selected sequences of the ops. We
implemented and evaluated: Delay-and-Sum beamformer, synthetic transmit
aperture imaging (STAI), plane-wave imaging (PWI), envelope detection algorithm
and dynamic range clipping. The benchmarks were executed on the NVidia Titan X
GPU integrated in the USPlatform research scanner (us4us Ltd., Poland). We
achieved B-mode image reconstruction frame rates of 55 fps, 17 fps for the STAI
and the PWI algorithms, respectively. The results showed the feasibility of
real-time ultrasound image reconstruction using WaveFlow operators in the
TensorFlow framework. WaveFlow source code can be found at
github.com/waveflow-team/waveflow
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01574</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01574</id><created>2018-11-05</created><authors><author><keyname>Liu</keyname><forenames>Kaihui</forenames></author><author><keyname>Wang</keyname><forenames>Jiayi</forenames></author><author><keyname>Xing</keyname><forenames>Zhengli</forenames></author><author><keyname>Yang</keyname><forenames>Linxiao</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author></authors><title>Low-Rank Phase Retrieval via Variational Bayesian Learning</title><categories>stat.ML cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of low-rank phase retrieval whose
objective is to estimate a complex low-rank matrix from magnitude-only
measurements. We propose a hierarchical prior model for low-rank phase
retrieval, in which a Gaussian-Wishart hierarchical prior is placed on the
underlying low-rank matrix to promote the low-rankness of the matrix. Based on
the proposed hierarchical model, a variational expectation-maximization (EM)
algorithm is developed. The proposed method is less sensitive to the choice of
the initialization point and works well with random initialization. Simulation
results are provided to illustrate the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01609</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01609</id><created>2018-11-05</created><updated>2018-11-16</updated><authors><author><keyname>Kameoka</keyname><forenames>Hirokazu</forenames></author><author><keyname>Tanaka</keyname><forenames>Kou</forenames></author><author><keyname>Kaneko</keyname><forenames>Takuhiro</forenames></author><author><keyname>Hojo</keyname><forenames>Nobukatsu</forenames></author></authors><title>ConvS2S-VC: Fully convolutional sequence-to-sequence voice conversion</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Submitted to ICASSP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a voice conversion method based on fully convolutional
sequence-to-sequence (seq2seq) learning. The present method, which we call
&quot;ConvS2S-VC&quot;, learns the mapping between source and target speech feature
sequences using a fully convolutional seq2seq model with an attention
mechanism. Owing to the nature of seq2seq learning, our method is particularly
noteworthy in that it allows the flexible conversion of not only the voice
characteristics but also the pitch contour and duration of the input speech.
The current model consists of six networks, namely source and target encoders,
a target decoder, source and target reconstructors and a postnet, which are
designed using dilated causal convolution networks with gated linear units.
Subjective evaluation experiments revealed that the proposed method obtained
higher sound quality and speaker similarity than a baseline method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01644</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01644</id><created>2018-11-05</created><authors><author><keyname>R</keyname><forenames>Pradeep</forenames></author><author><keyname>K</keyname><forenames>Sreenivasa Rao</forenames></author></authors><title>Manner of Articulation Detection using Connectionist Temporal
  Classification to Improve Automatic Speech Recognition Performance</title><categories>eess.AS cs.SD</categories><comments>5 pages, 4 figures, ICASSP-2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventionally, the manner of articulations in speech signal are derived
using discriminative signal processing techniques or deep learning approaches.
However, training such complex systems involves feature extraction, phoneme
force alignment and deep neural network training. In our work, we initially
detect the manner of articulations without phoneme alignment using an
end-to-end manner of articulation modeling based on connectionist temporal
classification (CTC). The manner of articulation knowledge is deployed in the
conventional character CTC path to regenerate the new character CTC path. The
modified manner based character CTC is evaluated on open source speech datasets
such as AN4, LibriSpeech and TEDLIUM-2 and it outperforms over the baseline
character CTC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01690</identifier>
 <datestamp>2019-05-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01690</id><created>2018-11-02</created><updated>2019-05-22</updated><authors><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author><author><keyname>Astudillo</keyname><forenames>Ramon</forenames></author><author><keyname>Hayashi</keyname><forenames>Tomoki</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author></authors><title>Cycle-consistency training for end-to-end speech recognition</title><categories>cs.CL cs.SD eess.AS</categories><comments>Submitted to ICASSP'19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method to train end-to-end automatic speech recognition
(ASR) models using unpaired data. Although the end-to-end approach can
eliminate the need for expert knowledge such as pronunciation dictionaries to
build ASR systems, it still requires a large amount of paired data, i.e.,
speech utterances and their transcriptions. Cycle-consistency losses have been
recently proposed as a way to mitigate the problem of limited paired data.
These approaches compose a reverse operation with a given transformation, e.g.,
text-to-speech (TTS) with ASR, to build a loss that only requires unsupervised
data, speech in this example. Applying cycle consistency to ASR models is not
trivial since fundamental information, such as speaker traits, are lost in the
intermediate text bottleneck. To solve this problem, this work presents a loss
that is based on the speech encoder state sequence instead of the raw speech
signal. This is achieved by training a Text-To-Encoder model and defining a
loss based on the encoder reconstruction error. Experimental results on the
LibriSpeech corpus show that the proposed cycle-consistency training reduced
the word error rate by 14.7% from an initial model trained with 100-hour paired
data, using an additional 360 hours of audio data without transcriptions. We
also investigate the use of text-only data mainly for language modeling to
further improve the performance in the unpaired data training scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01720</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01720</id><created>2018-11-01</created><updated>2018-11-15</updated><authors><author><keyname>Lin</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Weizi</forenames></author><author><keyname>Peeta</keyname><forenames>Srinivas</forenames></author></authors><title>Capture and Recovery of Connected Vehicle Data: A Compressive Sensing
  Approach</title><categories>eess.SP</categories><comments>This extended abstract is accepted in Transportation Research Board
  98th Annual Meeting, 2019. According to TRB's policy, the full report (i.e.,
  arXiv:1806.10046) can be submitted to another journal for publication. Here,
  we provide this text to interested readers for concise reading</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Connected vehicles (CVs) can capture and transmit detailed data through
vehicle-to-vehicle and vehicle-to-infrastructure communications, which bring
new opportunities to improve the safety, mobility, and sustainability of
transportation systems. However, the potential data explosion is likely to
over-burden storage and communication systems. We design a compressive sensing
(CS) approach which allows CVs to capture and compress data in real-time and
later recover the original data accurately and efficiently. We have evaluated
our approach using two case studies. In the first case study, the CS approach
is applied to re-capture 10 million CV Basic Safety Message (BSM) speed samples
from the Safety Pilot Model Deployment program. The recovery performances of
our approach regarding several BSM variables are explored in detail. In the
second case study, a freeway traffic simulation model is built to evaluate the
impact of our approach on travel time estimation. Multiple scenarios with
various CV market penetration rates, On-board Unit (OBU) capacities,
compression ratios, arrival rate patterns, and data capture rates are
simulated. The results show the potential of saving large amounts of OBU
hardware cost. Furthermore, our approach can greatly improve the accuracy of
travel time estimation when CVs are in traffic congestion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01733</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01733</id><created>2018-11-02</created><authors><author><keyname>Zhou</keyname><forenames>Cheng</forenames></author><author><keyname>Tian</keyname><forenames>Tian</forenames></author><author><keyname>Gao</keyname><forenames>Chao</forenames></author><author><keyname>Gong</keyname><forenames>Wenli</forenames></author><author><keyname>Song</keyname><forenames>Lijun</forenames></author></authors><title>Multi-resolution Progressive Computational Ghost Imaging</title><categories>eess.IV physics.optics</categories><doi>10.1088/2040-8986/ab1471</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ghost imaging needs massive measurements to obtain an image with good
visibility and the imaging speed is usually very low. In order to realize
real-time high-resolution ghost imaging of a target which is located in a
scenario with a large field of view (FOV), we propose a high-speed
multi-resolution progressive computational ghost imaging approach. The target
area is firstly locked by a low-resolution image with a small number of
measurements, then high-resolution imaging of the target can be obtained by
only modulating the light fields corresponding to the target area. The
experiments verify the feasibility of the approach. The influence of detection
signal-to-noise ratio on the quality of multi-resolution progressive
computational ghost imaging is also investigated experimentally. This approach
may be applied to some practical application scenarios such as ground-to-air or
air-to-air imaging with a large FOV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01850</identifier>
 <datestamp>2019-05-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01850</id><created>2018-11-05</created><updated>2019-05-09</updated><authors><author><keyname>Slizovskaia</keyname><forenames>Olga</forenames></author><author><keyname>Kim</keyname><forenames>Leo</forenames></author><author><keyname>Haro</keyname><forenames>Gloria</forenames></author><author><keyname>Gomez</keyname><forenames>Emilia</forenames></author></authors><title>End-to-End Sound Source Separation Conditioned On Instrument Labels</title><categories>cs.SD cs.LG eess.AS</categories><comments>5 pages, 2 figures, 2 tables, ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can we perform an end-to-end music source separation with a variable number
of sources using a deep learning model? We present an extension of the
Wave-U-Net model which allows end-to-end monaural source separation with a
non-fixed number of sources. Furthermore, we propose multiplicative
conditioning with instrument labels at the bottleneck of the Wave-U-Net and
show its effect on the separation results. This approach leads to other types
of conditioning such as audio-visual source separation and score-informed
source separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01909</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01909</id><created>2018-11-05</created><authors><author><keyname>Ardeshiri</keyname><forenames>Ghazaleh</forenames></author><author><keyname>Yazdani</keyname><forenames>Hassan</forenames></author><author><keyname>Vosoughi</keyname><forenames>Azadeh</forenames></author></authors><title>Optimal Local Thresholds for Distributed Detection in Energy Harvesting
  Wireless Sensor Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless sensor network, consisting of K heterogeneous sensors
and a fusion center (FC), that is tasked with solving a binary distributed
detection problem. Each sensor is capable of harvesting and storing energy for
communication with the FC. For energy efficiency, a sensor transmits only if
the sensor test statistic exceeds a local threshold {\theta}k, its channel gain
exceeds a minimum threshold, and its battery state can afford the transmission.
Our proposed transmission model at each sensor is motivated by the channel
inversion power control strategy in the wireless communication community.
Considering a constraint on the average energy of transmit symbols, we study
the optimal {\theta}_k's that optimize two detection performance metrics: (i)
the detection probability P_D at the FC, assuming that the FC utilizes the
optimal fusion rule based on Neyman-Pearson optimality criterion, and (ii)
Kullback-Leibler distance (KL) between the two distributions of the received
signals at the FC conditioned by each hypothesis. Our numerical results
indicate that {\theta}k's obtained from maximizing the KL distance are
near-optimal. Finding these thresholds is computationally efficient, as it
requires only K one-dimensional searches, as opposed to a K-dimensional search
required to find the thresholds that maximize P_D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01917</identifier>
 <datestamp>2018-11-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01917</id><created>2018-11-05</created><authors><author><keyname>Jeon</keyname><forenames>Charles</forenames></author><author><keyname>Ghods</keyname><forenames>Ramina</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Optimal Data Detection in Large MIMO</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large multiple-input multiple-output (MIMO) appears in massive multi-user
MIMO and randomly-spread code-division multiple access (CDMA)-based wireless
systems. In order to cope with the excessively high complexity of optimal data
detection in such systems, a variety of efficient yet sub-optimal algorithms
have been proposed in the past. In this paper, we propose a data detection
algorithm that is computationally efficient and optimal in a sense that it is
able to achieve the same error-rate performance as the individually optimal
(IO) data detector under certain assumptions on the MIMO system matrix and
constellation alphabet. Our algorithm, which we refer to as LAMA (short for
large MIMO AMP), builds on complex-valued Bayesian approximate message passing
(AMP), which enables an exact analytical characterization of the performance
and complexity in the large-system limit via the state-evolution framework. We
derive optimality conditions for LAMA and investigate performance/complexity
trade-offs. As a byproduct of our analysis, we recover classical results of IO
data detection for randomly-spread CDMA. We furthermore provide practical ways
for LAMA to approach the theoretical performance limits in realistic,
finite-dimensional systems at low computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.01980</identifier>
 <datestamp>2019-02-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.01980</id><created>2018-11-05</created><updated>2019-01-31</updated><authors><author><keyname>Alfarraj</keyname><forenames>Motaz</forenames></author><author><keyname>Alaudah</keyname><forenames>Yazeed</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>Content-adaptive non-parametric texture similarity measure</title><categories>eess.IV</categories><comments>7 pages, 7 Figures, 2016 IEEE 18th International Workshop on
  Multimedia Signal Processing (MMSP)</comments><journal-ref>2016 IEEE 18th International Workshop on Multimedia Signal
  Processing (MMSP), Montreal, QC, 2016, pp. 1-6</journal-ref><doi>10.1109/MMSP.2016.7813338</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a non-parametric texture similarity measure based
on the singular value decomposition of the curvelet coefficients followed by a
content-based truncation of the singular values. This measure focuses on images
with repeating structures and directional content such as those found in
natural texture images. Such textural content is critical for image perception
and its similarity plays a vital role in various computer vision applications.
In this paper, we evaluate the effectiveness of the proposed measure using a
retrieval experiment. The proposed measure outperforms the state-of-the-art
texture similarity metrics on CURet and PerTEx texture databases, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02000</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02000</id><created>2018-11-05</created><authors><author><keyname>Agarwal</keyname><forenames>Aayushya</forenames></author><author><keyname>Pandey</keyname><forenames>Amritanshu</forenames></author><author><keyname>Jereminov</keyname><forenames>Marko</forenames></author><author><keyname>Pileggi</keyname><forenames>Larry</forenames></author></authors><title>Continuously Differentiable Analytical Models for Implicit Control
  within Power Flow</title><categories>eess.SP</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving robust and scalable convergence for simulation of realistic power
flow cases can be challenging. One specific issue relates to the disconnected
solution space that is created by the use of piecewise-discontinuous models of
power grid devices that perform control mechanisms. These models are generally
resolved by outer iteration loops around power flow, which can result in
solution oscillations, increased iteration count, divergence or even
convergence to a solution in an unstable operational region. This paper
introduces a continuously differentiable model for device control mechanisms
that is incorporated within the power flow formulation. To ensure robust power
flow convergence properties, recently introduced homotopy methods are extended
to include these continuous models. The scalability and efficacy of the
proposed formulation is demonstrated on several large-scale test cases that
represent the US Eastern Interconnect network, the Synthetic USA, and the
Nigerian grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02013</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02013</id><created>2018-11-05</created><authors><author><keyname>Zhang</keyname><forenames>Shuang</forenames></author><author><keyname>Stevenson</keyname><forenames>Robert L.</forenames></author></authors><title>Inertia Sensor Aided Alignment for Burst Pipeline in Low Light
  Conditions</title><categories>eess.IV</categories><comments>5 pages, 2 figures, 2018 25th IEEE International Conference on Image
  Processing (ICIP)</comments><journal-ref>S. Zhang and R. L. Stevenson, &quot;Inertia Sensor Aided Alignment for
  Burst Pipeline in Low Light Conditions,&quot; 2018 25th IEEE International
  Conference on Image Processing (ICIP), Athens, Greece, 2018, pp. 3953-3957</journal-ref><doi>10.1109/ICIP.2018.8451134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Merging short-exposure frames can provide an image with reduced noise in low
light conditions. However, how best to align images before merging is an open
problem. To improve the performance of alignment, we propose an inertia-sensor
aided algorithm for smartphone burst photography, which takes rotation and
out-plane relative movement into account. To calculate homography between
frames, a three by three rotation matrix is calculated from gyro data recorded
by smartphone inertia sensor and three-dimensional translation vector are
estimated by matched feature points detected from two frames. The rotation
matrix and translations are combined to form the initial guess of homography.
An unscented Kalman filter is utilized to provide a more accurate homography
estimation. We have tested the algorithm on a variety of different scenes with
different camera relative motions. We compare the proposed method to benchmark
single-image and multi-image denoising methods with favorable results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02020</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02020</id><created>2018-10-31</created><authors><author><keyname>Servin</keyname><forenames>Manuel</forenames></author><author><keyname>Padilla</keyname><forenames>Moises</forenames></author><author><keyname>Garnica</keyname><forenames>Guillermo</forenames></author><author><keyname>Paez</keyname><forenames>Gonzalo</forenames></author></authors><title>Design of non-uniformly spaced phase-stepped algorithms using their
  frequency transfer function</title><categories>eess.SP physics.ins-det physics.optics</categories><comments>5 pages, 6 figures</comments><doi>10.1364/AO.58.001134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we show how to design phase-shifting algorithms (PSAs) for nonuniform
phase-shifted fringe patterns using their frequency transfer function (FTF).
Assuming that the nonuniform/nonlinear (NL) phase-steps are known, we introduce
the desired zeroes in the FTF to obtain the specific NL-PSA formula. The
advantage of designing NL-PSAs based on their FTF is that one can reject many
distorting harmonics of the fringes. We can also estimate the signal-to-noise
ratio (SNR) for interferograms corrupted by additive white Gaussian noise
(AWGN). Finally, for non-distorted noiseless fringes, the proposed NL-PSA
retrieves the modulating phase error-free, just as standard/linear PSAs do.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02050</identifier>
 <datestamp>2019-02-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02050</id><created>2018-11-05</created><updated>2019-02-10</updated><authors><author><keyname>Jia</keyname><forenames>Ye</forenames></author><author><keyname>Johnson</keyname><forenames>Melvin</forenames></author><author><keyname>Macherey</keyname><forenames>Wolfgang</forenames></author><author><keyname>Weiss</keyname><forenames>Ron J.</forenames></author><author><keyname>Cao</keyname><forenames>Yuan</forenames></author><author><keyname>Chiu</keyname><forenames>Chung-Cheng</forenames></author><author><keyname>Ari</keyname><forenames>Naveen</forenames></author><author><keyname>Laurenzo</keyname><forenames>Stella</forenames></author><author><keyname>Wu</keyname><forenames>Yonghui</forenames></author></authors><title>Leveraging Weakly Supervised Data to Improve End-to-End Speech-to-Text
  Translation</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  End-to-end Speech Translation (ST) models have many potential advantages when
compared to the cascade of Automatic Speech Recognition (ASR) and text Machine
Translation (MT) models, including lowered inference latency and the avoidance
of error compounding. However, the quality of end-to-end ST is often limited by
a paucity of training data, since it is difficult to collect large parallel
corpora of speech and translated transcript pairs. Previous studies have
proposed the use of pre-trained components and multi-task learning in order to
benefit from weakly supervised training data, such as speech-to-transcript or
text-to-foreign-text pairs. In this paper, we demonstrate that using
pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly
supervised data into speech-to-translation pairs for ST training can be more
effective than multi-task learning. Furthermore, we demonstrate that a high
quality end-to-end ST model can be trained using only weakly supervised
datasets, and that synthetic data sourced from unlabeled monolingual text or
speech can be used to improve performance. Finally, we discuss methods for
avoiding overfitting to synthetic speech with a quantitative ablation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02062</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02062</id><created>2018-11-05</created><authors><author><keyname>Chang</keyname><forenames>Xuankai</forenames></author><author><keyname>Qian</keyname><forenames>Yanmin</forenames></author><author><keyname>Yu</keyname><forenames>Kai</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author></authors><title>End-to-End Monaural Multi-speaker ASR System without Pretraining</title><categories>cs.CL cs.SD eess.AS</categories><comments>submitted to ICASSP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, end-to-end models have become a popular approach as an alternative
to traditional hybrid models in automatic speech recognition (ASR). The
multi-speaker speech separation and recognition task is a central task in
cocktail party problem. In this paper, we present a state-of-the-art monaural
multi-speaker end-to-end automatic speech recognition model. In contrast to
previous studies on the monaural multi-speaker speech recognition, this
end-to-end framework is trained to recognize multiple label sequences
completely from scratch. The system only requires the speech mixture and
corresponding label sequences, without needing any indeterminate supervisions
obtained from non-mixture speech or corresponding labels/alignments. Moreover,
we exploited using the individual attention module for each separated speaker
and the scheduled sampling to further improve the performance. Finally, we
evaluate the proposed model on the 2-speaker mixed speech generated from the
WSJ corpus and the wsj0-2mix dataset, which is a speech separation and
recognition benchmark. The experiments demonstrate that the proposed methods
can improve the performance of the end-to-end model in separating the
overlapping speech and recognizing the separated streams. From the results, the
proposed model leads to ~10.0% relative performance gains in terms of CER and
WER respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02063</identifier>
 <datestamp>2019-02-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02063</id><created>2018-11-05</created><updated>2019-02-26</updated><authors><author><keyname>He</keyname><forenames>Di</forenames></author><author><keyname>Yang</keyname><forenames>Xuesong</forenames></author><author><keyname>Lim</keyname><forenames>Boon Pang</forenames></author><author><keyname>Liang</keyname><forenames>Yi</forenames></author><author><keyname>Hasegawa-Johnson</keyname><forenames>Mark</forenames></author><author><keyname>Chen</keyname><forenames>Deming</forenames></author></authors><title>When CTC Training Meets Acoustic Landmarks</title><categories>eess.AS cs.AI cs.SD</categories><comments>To Appear in ICASSP 2019; The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connectionist temporal classification (CTC) provides an end-to-end acoustic
model (AM) training strategy. CTC learns accurate AMs without time-aligned
phonetic transcription, but sometimes fails to converge, especially in
resource-constrained scenarios. In this paper, the convergence properties of
CTC are improved by incorporating acoustic landmarks. We tailored a new set of
acoustic landmarks to help CTC training converge more rapidly and smoothly
while also reducing recognition error rates. We leveraged new target label
sequences mixed with both phone and manner changes to guide CTC training.
Experiments on TIMIT demonstrated that CTC based acoustic models converge
significantly faster and smoother when they are augmented by acoustic
landmarks. The models pretrained with mixed target labels can be further
finetuned, resulting in phone error rates 8.72% below baseline on TIMIT.
Consistent performance gain is also observed on WSJ (a larger corpus) and
reduced TIMIT (smaller). With WSJ, we are the first to succeed in verifying the
effectiveness of acoustic landmark theory on a mid-sized ASR task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02066</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02066</id><created>2018-11-05</created><authors><author><keyname>Zeinali</keyname><forenames>Hossein</forenames></author><author><keyname>Burget</keyname><forenames>Lukas</forenames></author><author><keyname>Rohdin</keyname><forenames>Johan</forenames></author><author><keyname>Stafylakis</keyname><forenames>Themos</forenames></author><author><keyname>Cernocky</keyname><forenames>Jan</forenames></author></authors><title>How to Improve Your Speaker Embeddings Extractor in Generic Toolkits</title><categories>cs.SD cs.CL eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, speaker embeddings extracted with deep neural networks became the
state-of-the-art method for speaker verification. In this paper we aim to
facilitate its implementation on a more generic toolkit than Kaldi, which we
anticipate to enable further improvements on the method. We examine several
tricks in training, such as the effects of normalizing input features and
pooled statistics, different methods for preventing overfitting as well as
alternative non-linearities that can be used instead of Rectifier Linear Units.
In addition, we investigate the difference in performance between TDNN and CNN,
and between two types of attention mechanism. Experimental results on Speaker
in the Wild, SRE 2016 and SRE 2018 datasets demonstrate the effectiveness of
the proposed implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02081</identifier>
 <datestamp>2019-06-10</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02081</id><created>2018-11-05</created><updated>2019-02-14</updated><authors><author><keyname>Chang</keyname><forenames>Huibin</forenames></author><author><keyname>Enfedaque</keyname><forenames>Pablo</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Reinhardt</keyname><forenames>Juliane</forenames></author><author><keyname>Enders</keyname><forenames>Bjoern</forenames></author><author><keyname>Yu</keyname><forenames>Young-Sang</forenames></author><author><keyname>Shapiro</keyname><forenames>David</forenames></author><author><keyname>Schroer</keyname><forenames>Christian G.</forenames></author><author><keyname>Zeng</keyname><forenames>Tieyong</forenames></author><author><keyname>Marchesini</keyname><forenames>Stefano</forenames></author></authors><title>Advanced Denoising for X-ray Ptychography</title><categories>eess.IV math.OC physics.optics</categories><comments>24 pages, 9 figures</comments><journal-ref>Optics express 27 (8), 10395-10418 (2019)</journal-ref><doi>10.1364/OE.27.010395</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of ptychographic imaging experiments strongly depends on
achieving high signal-to-noise ratio. This is particularly important in
nanoscale imaging experiments when diffraction signals are very weak and the
experiments are accompanied by significant parasitic scattering (background),
outliers or correlated noise sources. It is also critical when rare events such
as cosmic rays, or bad frames caused by electronic glitches or shutter timing
malfunction take place.
  In this paper, we propose a novel iterative algorithm with rigorous analysis
that exploits the direct forward model for parasitic noise and sample
smoothness to achieve a thorough characterization and removal of structured and
random noise. We present a formal description of the proposed algorithm and
prove its convergence under mild conditions. Numerical experiments from
simulations and real data (both soft and hard X-ray beamlines) demonstrate that
the proposed algorithms produce better results when compared to
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02095</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02095</id><created>2018-11-05</created><authors><author><keyname>Hui</keyname><forenames>Like</forenames></author><author><keyname>Ma</keyname><forenames>Siyuan</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author></authors><title>Kernel Machines Beat Deep Neural Networks on Mask-based Single-channel
  Speech Enhancement</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply a fast kernel method for mask-based single-channel speech
enhancement. Specifically, our method solves a kernel regression problem
associated to a non-smooth kernel function (exponential power kernel) with a
highly efficient iterative method (EigenPro). Due to the simplicity of this
method, its hyper-parameters such as kernel bandwidth can be automatically and
efficiently selected using line search with subsamples of training data. We
observe an empirical correlation between the regression loss (mean square
error) and regular metrics for speech enhancement. This observation justifies
our training target and motivates us to achieve lower regression loss by
training separate kernel model per frequency subband. We compare our method
with the state-of-the-art deep neural networks on mask-based HINT and TIMIT.
Experimental results show that our kernel method consistently outperforms deep
neural networks while requiring less training time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02098</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02098</id><created>2018-11-05</created><authors><author><keyname>Yan</keyname><forenames>Han</forenames></author><author><keyname>Hanna</keyname><forenames>Samer</forenames></author><author><keyname>Balke</keyname><forenames>Kevin</forenames></author><author><keyname>Gupta</keyname><forenames>Riten</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Software Defined Radio Implementation of Carrier and Timing
  Synchronization for Distributed Arrays</title><categories>eess.SP</categories><comments>Submitted to 2019 IEEE Aerospace Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communication range of wireless networks can be greatly improved by using
distributed beamforming from a set of independent radio nodes. One of the key
challenges in establishing a beamformed communication link from separate radios
is achieving carrier frequency and sample timing synchronization. This paper
describes an implementation that addresses both carrier frequency and sample
timing synchronization simultaneously using RF signaling between designated
master and slave nodes. By using a pilot signal transmitted by the master node,
each slave estimates and tracks the frequency and timing offset and digitally
compensates for them. A real-time implementation of the proposed system was
developed in GNU Radio and tested with Ettus USRP N210 software defined radios.
The measurements show that the distributed array can reach a residual frequency
error of 5 Hz and a residual timing offset of 1/16 the sample duration for 70
percent of the time. This performance enables distributed beamforming for range
extension applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02122</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02122</id><created>2018-11-05</created><updated>2019-02-18</updated><authors><author><keyname>Lee</keyname><forenames>Younggun</forenames></author><author><keyname>Kim</keyname><forenames>Taesu</forenames></author></authors><title>Robust and fine-grained prosody control of end-to-end speech synthesis</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>ICASSP 2019, best viewed in color</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose prosody embeddings for emotional and expressive speech synthesis
networks. The proposed methods introduce temporal structures in the embedding
networks, thus enabling fine-grained control of the speaking style of the
synthesized speech. The temporal structures can be designed either on the
speech side or the text side, leading to different control resolutions in time.
The prosody embedding networks are plugged into end-to-end speech synthesis
networks and trained without any other supervision except for the target speech
for synthesizing. It is demonstrated that the prosody embedding networks
learned to extract prosodic features. By adjusting the learned prosody
features, we could change the pitch and amplitude of the synthesized speech
both at the frame level and the phoneme level. We also introduce the temporal
normalization of prosody embeddings, which shows better robustness against
speaker perturbations during prosody transfer tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02130</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02130</id><created>2018-11-05</created><authors><author><keyname>Seetharaman</keyname><forenames>Prem</forenames></author><author><keyname>Wichern</keyname><forenames>Gordon</forenames></author><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author><author><keyname>Pardo</keyname><forenames>Bryan</forenames></author></authors><title>Bootstrapping single-channel source separation via unsupervised spatial
  clustering on stereo mixtures</title><categories>cs.SD cs.AI cs.LG eess.AS stat.ML</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separating an audio scene into isolated sources is a fundamental problem in
computer audition, analogous to image segmentation in visual scene analysis.
Source separation systems based on deep learning are currently the most
successful approaches for solving the underdetermined separation problem, where
there are more sources than channels. Traditionally, such systems are trained
on sound mixtures where the ground truth decomposition is already known. Since
most real-world recordings do not have such a decomposition available, this
limits the range of mixtures one can train on, and the range of mixtures the
learned models may successfully separate. In this work, we use a simple blind
spatial source separation algorithm to generate estimated decompositions of
stereo mixtures. These estimates, together with a weighting scheme in the
time-frequency domain, based on confidence in the separation quality, are used
to train a deep learning model that can be used for single-channel separation,
where no source direction information is available. This demonstrates how a
simple cue such as the direction of origin of source can be used to bootstrap a
model for source separation that can be used in situations where that cue is
not available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02136</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02136</id><created>2018-11-05</created><authors><author><keyname>Hanna</keyname><forenames>Samer</forenames></author><author><keyname>Yan</keyname><forenames>Han</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Distributed UAV Placement Optimization for Cooperative Line-of-Sight
  MIMO Communications</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative communication using unmanned aerial vehicles (UAVs) is a
promising technology for infrastructureless wireless networks. One of the key
challenges in UAV based communications is the backhaul throughput. In this
paper, we propose optimization of the UAV swarm positions to achieve a high
mulitplexing gain in line-of-sight (LoS) MIMO backhaul. We develop two
distributed algorithms to position the UAVs such that each UAV moves a minimal
distance to realize the highest capacity LoS MIMO channel. The first approach
uses iterative gradient descent (GD) and the second uses iterative brute force
(BF). Simulations show that both algorithms can achieve up to 6 times higher
capacity compared to the approach relying on random UAV placement, earlier
proposed in the literature. BF has the advantage of not requiring any location
information, while GD is less sensitive to errors in motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02155</identifier>
 <datestamp>2019-05-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02155</id><created>2018-11-05</created><updated>2019-05-20</updated><authors><author><keyname>Kim</keyname><forenames>Sungwon</forenames></author><author><keyname>Lee</keyname><forenames>Sang-gil</forenames></author><author><keyname>Song</keyname><forenames>Jongyoon</forenames></author><author><keyname>Kim</keyname><forenames>Jaehyeon</forenames></author><author><keyname>Yoon</keyname><forenames>Sungroh</forenames></author></authors><title>FloWaveNet : A Generative Flow for Raw Audio</title><categories>cs.SD eess.AS</categories><comments>9 pages, ICML'2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most modern text-to-speech architectures use a WaveNet vocoder for
synthesizing high-fidelity waveform audio, but there have been limitations,
such as high inference time, in its practical application due to its ancestral
sampling scheme. The recently suggested Parallel WaveNet and ClariNet have
achieved real-time audio synthesis capability by incorporating inverse
autoregressive flow for parallel sampling. However, these approaches require a
two-stage training pipeline with a well-trained teacher network and can only
produce natural sound by using probability distillation along with auxiliary
loss terms. We propose FloWaveNet, a flow-based generative model for raw audio
synthesis. FloWaveNet requires only a single-stage training procedure and a
single maximum likelihood loss, without any additional auxiliary terms, and it
is inherently parallel due to the characteristics of generative flow. The model
can efficiently sample raw audio in real-time, with clarity comparable to
previous two-stage parallel models. The code and samples for all models,
including our FloWaveNet, are publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02162</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02162</id><created>2018-11-05</created><authors><author><keyname>Cho</keyname><forenames>Jaejin</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author><author><keyname>Baskar</keyname><forenames>Murali Karthick</forenames></author><author><keyname>Inaguma</keyname><forenames>Hirofumi</forenames></author><author><keyname>Villalba</keyname><forenames>Jesus</forenames></author><author><keyname>Dehak</keyname><forenames>Najim</forenames></author></authors><title>Language model integration based on memory control for sequence to
  sequence speech recognition</title><categories>eess.AS cs.SD</categories><comments>4 pages, 1 figure, 5 tables, submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore several new schemes to train a seq2seq model to
integrate a pre-trained LM. Our proposed fusion methods focus on the memory
cell state and the hidden state in the seq2seq decoder long short-term memory
(LSTM), and the memory cell state is updated by the LM unlike the prior
studies. This means the memory retained by the main seq2seq would be adjusted
by the external LM. These fusion methods have several variants depending on the
architecture of this memory cell update and the use of memory cell and hidden
states which directly affects the final label inference. We performed the
experiments to show the effectiveness of the proposed methods in a mono-lingual
ASR setup on the Librispeech corpus and in a transfer learning setup from a
multilingual ASR (MLASR) base model to a low-resourced language. In
Librispeech, our best model improved WER by 3.7%, 2.4% for test clean, test
other relatively to the shallow fusion baseline, with multi-level decoding. In
transfer learning from an MLASR base model to the IARPA Babel Swahili model,
the best scheme improved the transferred model on eval set by 9.9%, 9.8% in
CER, WER relatively to the 2-stage transfer baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02168</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02168</id><created>2018-11-06</created><authors><author><keyname>Ghosh</keyname><forenames>Sanjay</forenames></author><author><keyname>Nair</keyname><forenames>Pravin</forenames></author><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author></authors><title>Optimized Fourier Bilateral Filtering</title><categories>eess.IV</categories><comments>5 pages, 6 figures, IEEE Signal Processing Letters</comments><journal-ref>S. Ghosh, P. Nair, and K. N. Chaudhury, &quot;Optimized Fourier
  bilateral filtering,&quot; IEEE Signal Processing Letters, vol. 25, no. 10, pp.
  1555-1559, 2018</journal-ref><doi>10.1109/LSP.2018.2866949</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating a truncated Gaussian kernel using
Fourier (trigonometric) functions. The computation-intensive bilateral filter
can be expressed using fast convolutions by applying such an approximation to
its range kernel, where the truncation in question is the dynamic range of the
input image. The error from such an approximation depends on the period, the
number of sinusoids, and the coefficient of each sinusoid. For a fixed period,
we recently proposed a model for optimizing the coefficients using
least-squares fitting. Following the Compressive Bilateral Filter (CBF), we
demonstrate that the approximation can be improved by taking the period into
account during the optimization. The accuracy of the resulting filtering is
found to be at least as good as CBF, but significantly better for certain
cases. The proposed approximation can also be used for non-Gaussian kernels,
and it comes with guarantees on the filtering accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02182</identifier>
 <datestamp>2018-12-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02182</id><created>2018-11-06</created><authors><author><keyname>Kim</keyname><forenames>Geonmin</forenames></author><author><keyname>Lee</keyname><forenames>Hwaran</forenames></author><author><keyname>Kim</keyname><forenames>Bo-Kyeong</forenames></author><author><keyname>Oh</keyname><forenames>Sang-Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Soo-Young</forenames></author></authors><title>Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for
  Speech Recognition</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>will be published in IEEE Signal Processing Letter</comments><doi>10.1109/LSP.2018.2880285</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many speech enhancement methods try to learn the relationship between noisy
and clean speech, obtained using an acoustic room simulator. We point out
several limitations of enhancement methods relying on clean speech targets; the
goal of this work is proposing an alternative learning algorithm, called
acoustic and adversarial supervision (AAS). AAS makes the enhanced output both
maximizing the likelihood of transcription on the pre-trained acoustic model
and having general characteristics of clean speech, which improve
generalization on unseen noisy speeches. We employ the connectionist temporal
classification and the unpaired conditional boundary equilibrium generative
adversarial network as the loss function of AAS. AAS is tested on two datasets
including additive noise without and with reverberation, Librispeech + DEMAND
and CHiME-4. By visualizing the enhanced speech with different loss
combinations, we demonstrate the role of each supervision. AAS achieves a lower
word error rate than other state-of-the-art methods using the clean speech
target in both datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02214</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02214</id><created>2018-11-06</created><authors><author><keyname>Tanveer</keyname><forenames>Md. Sayed</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Kamrul</forenames></author></authors><title>Cuffless Blood Pressure Estimation from Electrocardiogram and
  Photoplethysmogram Using Waveform Based ANN-LSTM Network</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goal: Although photoplethysmogram (PPG) and electrocardiogram (ECG) signals
can be used to estimate blood pressure (BP) by extracting various features, the
changes in morphological contours of both PPG and ECG signals due to various
diseases of circulatory system and interaction of other physiological systems
make the extraction of such features very difficult. Methods: In this work, we
propose a waveform-based hierarchical Artificial Neural Network - Long Short
Term Memory (ANN-LSTM) model for BP estimation. The model consists of two
hierarchy levels, where the lower hierarchy level uses ANNs to extract
necessary morphological features from ECG and PPG waveforms and the upper
hierarchy level uses LSTM layers to account for the time domain variation of
the features extracted by lower hierarchy level. Results: The proposed model is
evaluated on 39 subjects using the Association for the Advancement of Medical
Instrumentations (AAMI) standard and the British Hypertension Society (BHS)
standard. The method satisfies both the standards in the estimation of systolic
blood pressure (SBP) and diastolic blood pressure (DBP). For the proposed
network, the mean absolute error (MAE) and the root mean square error (RMSE)
for SBP estimation are 1.10 and 1.56 mmHg, respectively, and for DBP estimation
are 0.58 and 0.85 mmHg, respectively. Conclusion: The performance of the
proposed hierarchical ANN-LSTM model is found to be better than the other
feature engineering-based networks. It is shown that the proposed model is able
to automatically extract the necessary features and their time domain
variations to estimate BP reliably in a noninvasive continuous manner.
Significance: The method is expected to greatly facilitate the presently
available mobile health-care gadgets in continuous BP estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02219</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02219</id><created>2018-11-06</created><authors><author><keyname>Bai</keyname><forenames>Zixuan</forenames></author><author><keyname>Hu</keyname><forenames>Zhiwen</forenames></author><author><keyname>Bian</keyname><forenames>Kaigui</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author></authors><title>Real-Time Prediction for Fine-Grained Air Quality Monitoring System with
  Asynchronous Sensing</title><categories>eess.SP</categories><comments>5 pages, 3 figures, submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the significant air pollution problem, monitoring and prediction for
air quality have become increasingly necessary. To provide real-time
fine-grained air quality monitoring and prediction in urban areas, we have
established our own Internet-of-Things-based sensing system in Peking
University. Due to the energy constraint of the sensors, it is preferred that
the sensors wake up alternatively in an asynchronous pattern, which leads to a
sparse sensing dataset. In this paper, we propose a novel approach to predict
the real-time fine-grained air quality based on asynchronous sensing. The
sparse dataset and the spatial-temporal-meteorological relations are modeled
into the correlation graph, in which way the prediction procedures are
carefully designed. The advantage of the proposed solution over existing ones
is evaluated over the dataset collected by our air quality monitoring system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02260</identifier>
 <datestamp>2019-03-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02260</id><created>2018-11-06</created><updated>2019-03-07</updated><authors><author><keyname>Mohammad</keyname><forenames>Umar</forenames><affiliation>Student Member Ieee</affiliation></author></authors><title>A Novel Square Wave Generator Based on the Translinear Circuit Scheme of
  Second Generation Current Controlled Current Conveyor-CCCII</title><categories>eess.SP</categories><comments>10 pages. arXiv admin note: substantial text overlap with
  arXiv:1811.02402</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust square wave generator employing a sole capacitor and two resistors
has been presented in this study. Low power and popular translinear circuit
scheme of Second generation current controlled current conveyor-CCCII has been
taken as an active element to implement the proposed square wave generator.
CCCII inhibits promising features like availability of three mutually
independently and electronically adjustable parameters corresponding
transconductance (gm), intrinsic resistance (r) of the current input terminal
and current gain between two terminals) that are very prevalent for control
applications accepted currently. The operating frequency of the proposed model
has been analyzed with respect to the passive components present there, with no
exposure of output signals to the thermal voltage (VT). Electrical/Device
properties (like Noise, Threshold, area etc.) of the proposed circuit have also
been discussed in this work. The simulation work was carried out on Synopsis
Hspice tool (v-2008.03) from Avant. Satisfying results with anticipation of
theoretical and simulated results, including precision (consistency assessment)
with Pareto analysis (Ist order Best Test flavored by Decision making analysis)
were observed during the study. The 45nm BSIM CMOS modelling parameters were
adopted to prove the theory. The elementary purpose of using such parameters is
to maximize the circuit drive and lowering the leakage current. Another purpose
of using these modelling parameters is to enhance the realization of the
proposed circuit in the custom Integrated circuit(IC) form, from a Standard
local foundry. Maximum power consumption of the circuit is 600 micro Watt, with
2V rail to rail operating voltages
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02275</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02275</id><created>2018-11-06</created><updated>2018-11-14</updated><authors><author><keyname>Morfi</keyname><forenames>Veronica</forenames></author><author><keyname>Bas</keyname><forenames>Yves</forenames></author><author><keyname>Pamu&#x142;a</keyname><forenames>Hanna</forenames></author><author><keyname>Glotin</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Stowell</keyname><forenames>Dan</forenames></author></authors><title>NIPS4Bplus: a richly annotated birdsong audio dataset</title><categories>cs.SD cs.DL eess.AS</categories><comments>5 pages, 5 figures, submitted to ICASSP 2019</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in birdsong detection and classification have approached a
limit due to the lack of fully annotated recordings. In this paper, we present
NIPS4Bplus, the first richly annotated birdsong audio dataset, that is
comprised of recordings containing bird vocalisations along with their active
species tags plus the temporal annotations acquired for them. Statistical
information about the recordings, their species specific tags and their
temporal annotations are presented along with example uses. NIPS4Bplus could be
used in various ecoacoustic tasks, such as training models for bird population
monitoring, species classification, birdsong vocalisation detection and
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02317</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02317</id><created>2018-11-06</created><authors><author><keyname>Chobineh</keyname><forenames>Amirreza</forenames></author><author><keyname>Huang</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Mazloum</keyname><forenames>Taghrid</forenames></author><author><keyname>Conil</keyname><forenames>Emmanuelle</forenames></author><author><keyname>Wiart</keyname><forenames>Joe</forenames></author></authors><title>Statistical model of the human RF exposure in Small cells environment</title><categories>eess.SP</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cells are one of the solutions to face the imperative demand on
increasing mobile data traffic. They are low-powered base stations installed
close to the users to offer better network services and to deal with increased
data traffic. In this paper, the global exposure induced in such networks as a
whole from user equipment and base stations has been investigated. As the small
cell is close to the user, the propagation channel becomes highly variable and
strongly susceptible by environmental factors such as the road traffic. An
innovative statistical path loss model is constructed based on measurements on
two French commercial LTE small cells, operating at LTE 1800 MHz and 2600 MHz .
This statistical path loss model is then used to assess global exposure of the
adult proportion of a population in a scenario composed of a street lined with
buildings, indoor and outdoor data users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02331</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02331</id><created>2018-11-06</created><authors><author><keyname>Rohdin</keyname><forenames>Johan</forenames></author><author><keyname>Stafylakis</keyname><forenames>Themos</forenames></author><author><keyname>Silnova</keyname><forenames>Anna</forenames></author><author><keyname>Zeinali</keyname><forenames>Hossein</forenames></author><author><keyname>Burget</keyname><forenames>Lukas</forenames></author><author><keyname>Plchot</keyname><forenames>Oldrich</forenames></author></authors><title>Speaker verification using end-to-end adversarial language adaptation</title><categories>eess.AS cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the use of adversarial domain adaptation for
addressing the problem of language mismatch between speaker recognition
corpora. In the context of speaker verification, adversarial domain adaptation
methods aim at minimizing certain divergences between the distribution that the
utterance-level features follow (i.e. speaker embeddings) when drawn from
source and target domains (i.e. languages), while preserving their capacity in
recognizing speakers. Neural architectures for extracting utterance-level
representations enable us to apply adversarial adaptation methods in an
end-to-end fashion and train the network jointly with the standard
cross-entropy loss. We examine several configurations, such as the use of
(pseudo-)labels on the target domain as well as domain labels in the feature
extractor, and we demonstrate the effectiveness of our method on the
challenging NIST SRE16 and SRE18 benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02353</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02353</id><created>2018-11-06</created><authors><author><keyname>Zhang</keyname><forenames>Xian-Rui</forenames></author><author><keyname>Lei</keyname><forenames>Meng-Ying</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author></authors><title>An amplitudes-perturbation data augmentation method in convolutional
  neural networks for EEG decoding</title><categories>eess.SP cs.HC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain-Computer Interface (BCI) system provides a pathway between humans and
the outside world by analyzing brain signals which contain potential neural
information. Electroencephalography (EEG) is one of most commonly used brain
signals and EEG recognition is an important part of BCI system. Recently,
convolutional neural networks (ConvNet) in deep learning are becoming the new
cutting edge tools to tackle the problem of EEG recognition. However, training
an effective deep learning model requires a big number of data, which limits
the application of EEG datasets with a small number of samples. In order to
solve the issue of data insufficiency in deep learning for EEG decoding, we
propose a novel data augmentation method that add perturbations to amplitudes
of EEG signals after transform them to frequency domain. In experiments, we
explore the performance of signal recognition with the state-of-the-art models
before and after data augmentation on BCI Competition IV dataset 2a and our
local dataset. The results show that our data augmentation technique can
improve the accuracy of EEG recognition effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02359</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02359</id><created>2018-11-06</created><authors><author><keyname>Wang</keyname><forenames>Li</forenames></author><author><keyname>Fortunati</keyname><forenames>Stefano</forenames></author><author><keyname>Greco</keyname><forenames>Maria Sabrina</forenames></author><author><keyname>Gini</keyname><forenames>Fulvio</forenames></author></authors><title>Reinforcement learning-based waveform optimization for MIMO multi-target
  detection</title><categories>eess.SP</categories><comments>Presented at ASILOMAR 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cognitive beamforming algorithm for colocated MIMO radars, based on
Reinforcement Learning (RL) framework, is proposed. We analyse an RL-based
optimization protocol that allows the MIMO radar, i.e. the \textit{agent}, to
iteratively sense the unknown environment, i.e. the radar scene involving an
unknown number of targets at unknown angular positions, and consequently, to
synthesize a set of transmitted waveforms whose related beam patter is tailored
on the acquired knowledge. The performance of the proposed RL-based beamforming
algorithm is assessed through numerical simulations in terms of Probability of
Detection ($P_D$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02402</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02402</id><created>2018-11-06</created><authors><author><keyname>Mohammad</keyname><forenames>Umar</forenames></author><author><keyname>Shafi</keyname><forenames>Mir Aamir</forenames></author></authors><title>Realisation of Highly Precise and Low Power Tunable Voltage Amplifier
  Based on the Translinear Circuit Scheme of CCCII+</title><categories>eess.SP</categories><comments>7 pages Under revisions in Indonesian journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, advancements in the field of nano circuit design has
become tougher than the demand. Low power devices have emerged
tremendously.Both voltage mode aswell as current mode devices have proven
alternative to each other for satisfying the demand of the growing market. As
such, current conveyors have equitably established their uniqueness as an
important circuit design element. The literature available to us during the few
years in the field of analog VLSI design, quotes a huge number of application
elements based on current conveyors. Likely, in this paper, a new tunable low
power voltage amplifier based on the translinear circuit scheme of second
generation current controlled current conveyor has been proposed. The modeling
of the circuit presented in this paper employs the minimum number of passive
elements. The magnitude of the tuning or the amplitude of the voltage presented
here, is being controlled by means of two variable resistors. Current conveyor
second generation translinear circuit scheme is taken into consideration to
implement the proposed tunable voltage amplifier. CCCII works on the outlines
of low power and low voltage design. Tunable voltage amplifiers find use in
analog as well as in digital signal processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02406</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02406</id><created>2018-11-06</created><authors><author><keyname>Ramires</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>Penha</keyname><forenames>Rui</forenames></author><author><keyname>Davies</keyname><forenames>Matthew E. P.</forenames></author></authors><title>User Specific Adaptation in Automatic Transcription of Vocalised
  Percussion</title><categories>cs.SD eess.AS</categories><journal-ref>Proc. of RecPad-2017, Amadora, Portugal, pp. 19-20, October, 2017</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to develop an application that enables music
producers to use their voice to create drum patterns when composing in Digital
Audio Workstations (DAWs). An easy-to-use and user-oriented system capable of
automatically transcribing vocalisations of percussion sounds, called LVT -
Live Vocalised Transcription, is presented. LVT is developed as a Max for Live
device which follows the `segment-and-classify' methodology for drum
transcription, and includes three modules: i) an onset detector to segment
events in time; ii) a module that extracts relevant features from the audio
content; and iii) a machine-learning component that implements the k-Nearest
Neighbours (kNN) algorithm for the classification of vocalised drum timbres.
  Due to the wide differences in vocalisations from distinct users for the same
drum sound, a user-specific approach to vocalised transcription is proposed. In
this perspective, a given end-user trains the algorithm with their own
vocalisations for each drum sound before inputting their desired pattern into
the DAW. The user adaption is achieved via a new Max external which implements
Sequential Forward Selection (SFS) for choosing the most relevant features for
a given set of input drum sounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02411</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02411</id><created>2018-11-06</created><authors><author><keyname>Ramires</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>Cocharro</keyname><forenames>Diogo</forenames></author><author><keyname>Davies</keyname><forenames>Matthew E. P.</forenames></author></authors><title>An audio-only method for advertisement detection in broadcast television
  content</title><categories>cs.SD eess.AS</categories><journal-ref>Proc. of RecPad-2017, Amadora, Portugal, pp. 21-22, October, 2017</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the task of advertisement detection in broadcast television
content. While typically approached from a video-only or audio-visual
perspective, we present an audio-only method. Our approach centres on the
detection of short silences which exist at the boundaries between programming
and advertising, as well as between the advertisements themselves. To identify
advertising regions we first locate all points within the broadcast content
with very low signal energy. Next, we use a multiple linear regression model to
reject non-boundary silences based on features extracted from the local context
immediately surrounding the silence. Finally, we determine the advertising
regions based on the long-term grouping of detected boundary silences. When
evaluated over a 26 hour annotated database covering national and commercial
Portuguese television channels we obtain a Matthews correlation coefficient in
excess of 0.87 and outperform a freely available audio-visual approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02423</identifier>
 <datestamp>2019-10-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02423</id><created>2018-11-03</created><updated>2019-10-08</updated><authors><author><keyname>Huang</keyname><forenames>Changcun</forenames></author></authors><title>New Fundamental Formulas of Image Restoration in Spatial and Frequency
  Domains</title><categories>eess.IV</categories><comments>v2:descriptions revised; v3:several details refined; v4:typos
  corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Circular convolutions and the corresponding frequency domain formula are
fundamentally important in image restoration; however, in this paper, we'll
prove that the usual computing method of circular convolutions violates the
physical meaning of blur producing. Especially for the image restoration
algorithms in frequency domain, this violation will affect the restoration
result. Relevant problems are proved rigorously and modified formulas are given
in both spatial and frequency domains. Experiments are done to show the effects
of new formulas. For clarity of proving, the one-dimensional case is dealt with
first, which may be useful in one-dimensional signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02438</identifier>
 <datestamp>2019-02-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02438</id><created>2018-11-05</created><updated>2019-02-19</updated><authors><author><keyname>Koizumi</keyname><forenames>Yuma</forenames></author><author><keyname>Harada</keyname><forenames>Noboru</forenames></author><author><keyname>Haneda</keyname><forenames>Yoichi</forenames></author></authors><title>Trainable Adaptive Window Switching for Speech Enhancement</title><categories>eess.AS cs.LG cs.SD eess.SP stat.ML</categories><comments>accepted to the 44th International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2019)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes a trainable adaptive window switching (AWS) method and
apply it to a deep-neural-network (DNN) for speech enhancement in the modified
discrete cosine transform domain. Time-frequency (T-F) mask processing in the
short-time Fourier transform (STFT)-domain is a typical speech enhancement
method. To recover the target signal precisely, DNN-based short-time frequency
transforms have recently been investigated and used instead of the STFT.
However, since such a fixed-resolution short-time frequency transform method
has a T-F resolution problem based on the uncertainty principle, not only the
short-time frequency transform but also the length of the windowing function
should be optimized. To overcome this problem, we incorporate AWS into the
speech enhancement procedure, and the windowing function of each time-frame is
manipulated using a DNN depending on the input signal. We confirmed that the
proposed method achieved a higher signal-to-distortion ratio than conventional
speech enhancement methods in fixed-resolution frequency domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02477</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02477</id><created>2018-11-05</created><authors><author><keyname>Semper</keyname><forenames>Sebastian</forenames></author><author><keyname>R&#xf6;mer</keyname><forenames>Florian</forenames></author></authors><title>ADMM for ND Line Spectral Estimation using Grid-Free Compressive Sensing
  from Multiple Measurements with Applications to DOA Estimation</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with estimating unknown multi-dimensional frequencies
from linear compressive measurements. This is accomplished by employing the
recently proposed atomic norm minimization framework to recover these
frequencies under a sparsity prior without imposing any grid restriction on
these frequencies. To this end, we give a rigorous derivation of an iterative
scheme called alternating direction of multipliers method, which is able to
incorporate multiple compressive snapshots from a multi-dimensional
superposition of complex harmonics. The key result here is how to formulate the
objective function minimized by this scheme and its partial derivatives, which
become hard to manage if the dimensionality of the frequencies is larger than
1. Moreover we demonstrate the performance of this approach in case of 3D line
spectral estimation and 2D DOA estimation with a synthetic antenna array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02480</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02480</id><created>2018-11-06</created><updated>2019-05-02</updated><authors><author><keyname>Morrone</keyname><forenames>Giovanni</forenames></author><author><keyname>Pasa</keyname><forenames>Luca</forenames></author><author><keyname>Tikhanoff</keyname><forenames>Vadim</forenames></author><author><keyname>Bergamaschi</keyname><forenames>Sonia</forenames></author><author><keyname>Fadiga</keyname><forenames>Luciano</forenames></author><author><keyname>Badino</keyname><forenames>Leonardo</forenames></author></authors><title>Face Landmark-based Speaker-Independent Audio-Visual Speech Enhancement
  in Multi-Talker Environments</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><comments>Proceedings of 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of enhancing the speech of a speaker of
interest in a cocktail party scenario when visual information of the speaker of
interest is available. Contrary to most previous studies, we do not learn
visual features on the typically small audio-visual datasets, but use an
already available face landmark detector (trained on a separate image dataset).
The landmarks are used by LSTM-based models to generate time-frequency masks
which are applied to the acoustic mixed-speech spectrogram. Results show that:
(i) landmark motion features are very effective features for this task, (ii)
similarly to previous work, reconstruction of the target speaker's spectrogram
mediated by masking is significantly more accurate than direct spectrogram
reconstruction, and (iii) the best masks depend on both motion landmark
features and the input mixed-speech spectrogram. To the best of our knowledge,
our proposed models are the first models trained and evaluated on the limited
size GRID and TCD-TIMIT datasets, that achieve speaker-independent speech
enhancement in a multi-talker setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02485</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02485</id><created>2018-11-06</created><authors><author><keyname>Ha</keyname><forenames>Vu N.</forenames></author></authors><title>Radio resource management for high-speed wireless cellular networks</title><categories>eess.SP</categories><comments>PhD Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fifth-generation (5G) wireless cellular system, which would be deployed
by 2020, is expected to deliver significantly higher capacity and better
network performance compared to those of the current fourth-generation (4G)
system. Specifically, it is predicted that tens of billions of wireless devices
will be connected to the wireless network over the next few years, which
results in an exponential explosion of mobile data traffic. Therefore, more
advanced wireless architecture, as well as radical and innovative access
technologies, must be proposed to meet this urgent increasing growth of mobile
data and connectivity requirements in the coming years. Toward this end, two
important wireless cellular architectures, namely wireless heterogeneous
networks (HetNets) based on the dense deployment of small cells and the cloud
radio access networks (C-RANs) have been proposed and actively studied by both
academic and industry communities. Besides enabling a lot of advantages in
increasing network coverage as well as end-to-end system throughput, these two
novel network architectures have also raised some novel technical challenges
and opened exciting research areas for further research. Motivated by the
aforementioned technical challenges, the general objective of this Ph.D.
research is to develop efficient radio resource allocation and interference
management algorithms for the future high-speed wireless cellular networks. In
particular, we have developed various efficient resource allocation algorithms
for reducing the transmission power and increasing the end-to-end network
throughput for both HetNets and C-RANs. Furthermore, extensive numerical
results are presented to gain further insights and to evaluate the performance
of our resource allocation designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02489</identifier>
 <datestamp>2019-02-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02489</id><created>2018-11-06</created><updated>2019-02-12</updated><authors><author><keyname>Wilkinson</keyname><forenames>William J.</forenames></author><author><keyname>Andersen</keyname><forenames>Michael Riis</forenames></author><author><keyname>Reiss</keyname><forenames>Joshua D.</forenames></author><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Solin</keyname><forenames>Arno</forenames></author></authors><title>Unifying Probabilistic Models for Time-Frequency Analysis</title><categories>eess.SP cs.LG cs.SD eess.AS stat.ML</categories><comments>Accepted to International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In audio signal processing, probabilistic time-frequency models have many
benefits over their non-probabilistic counterparts. They adapt to the incoming
signal, quantify uncertainty, and measure correlation between the signal's
amplitude and phase information, making time domain resynthesis
straightforward. However, these models are still not widely used since they
come at a high computational cost, and because they are formulated in such a
way that it can be difficult to interpret all the modelling assumptions. By
showing their equivalence to Spectral Mixture Gaussian processes, we illuminate
the underlying model assumptions and provide a general framework for
constructing more complex models that better approximate real-world signals.
Our interpretation makes it intuitive to inspect, compare, and alter the models
since all prior knowledge is encoded in the Gaussian process kernel functions.
We utilise a state space representation to perform efficient inference via
Kalman smoothing, and we demonstrate how our interpretation allows for
efficient parameter learning in the frequency domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02500</identifier>
 <datestamp>2019-01-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02500</id><created>2018-11-06</created><updated>2018-11-09</updated><authors><author><keyname>Nimr</keyname><forenames>Ahmad</forenames></author><author><keyname>Chafii</keyname><forenames>Marwa</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Unified Low Complexity Radix-2 Architectures for Time and
  Frequency-domain GFDM Modem</title><categories>eess.SP</categories><comments>Accepted in IEEE CASM 2018/Q4 special issue</comments><doi>10.1109/MCAS.2018.2872662</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the conventional multicarrier waveforms explicitly or implicitly
involve a generalized frequency division multiplexing (GFDM)-based modem as a
core part of the baseband processing. Some are based on GFDM with a single
prototype filter, e.g. orthogonal frequency division multiplexing (OFDM) and
others employ multiple filters such as filter bank multicarrier (FBMC).
Moreover, the GFDM degrees of freedom combined with multiple prototype filters
design allow the development and optimization of new waveforms. Nevertheless,
GFDM has been widely considered as a complex modulation because of the
requirements of odd number of subcarriers or subsymbols. Accordingly, the
current state of the art implementations consume high resources. One solution
to reduce the complexity is utilizing radix-2 parameters. Due to the
advancement in GFDM filter design, the constraint of using odd parameters has
been overcome and radix-2 realization is now possible. In this paper, we
propose a unified low complexity architecture that can be reconfigured to
provide both time-domain and frequency-domain modulation/demodulation. The
design consists of several radix-2 fast Fourier transform (FFT) and memory
blocks, in addition to one complex multiplier. Moreover, we provide a unified
architecture for the state of the art implementations, which is designed based
on direct computation of circular convolution using parallel multiplier chains.
As we demonstrate in this work, the FFTbased architecture is computationally
more efficient, provides more flexibility, significantly reduces the resource
consumption, and achieves similar latency for larger block size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02508</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02508</id><created>2018-11-06</created><authors><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author><author><keyname>Wisdom</keyname><forenames>Scott</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author><author><keyname>Hershey</keyname><forenames>John R.</forenames></author></authors><title>SDR - half-baked or well done?</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In speech enhancement and source separation, signal-to-noise ratio is a
ubiquitous objective measure of denoising/separation quality. A decade ago, the
BSS_eval toolkit was developed to give researchers worldwide a way to evaluate
the quality of their algorithms in a simple, fair, and hopefully insightful
way: it attempted to account for channel variations, and to not only evaluate
the total distortion in the estimated signal but also split it in terms of
various factors such as remaining interference, newly added artifacts, and
channel errors. In recent years, hundreds of papers have been relying on this
toolkit to evaluate their proposed methods and compare them to previous works,
often arguing that differences on the order of 0.1 dB proved the effectiveness
of a method over others. We argue here that the signal-to-distortion ratio
(SDR) implemented in the BSS_eval toolkit has generally been improperly used
and abused, especially in the case of single-channel separation, resulting in
misleading results. We propose to use a slightly modified definition, resulting
in a simpler, more robust measure, called scale-invariant SDR (SI-SDR). We
present various examples of critical failure of the original SDR that SI-SDR
overcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02512</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02512</id><created>2018-10-25</created><authors><author><keyname>Shi</keyname><forenames>Junjie</forenames></author><author><keyname>Liu</keyname><forenames>Guangyi</forenames></author><author><keyname>Dai</keyname><forenames>Renchang</forenames></author><author><keyname>Wu</keyname><forenames>Jingjin</forenames></author><author><keyname>Yuan</keyname><forenames>Chen</forenames></author><author><keyname>Wang</keyname><forenames>Zhiwei</forenames></author></authors><title>Graph Based Power Flow Calculation for Energy Management System</title><categories>eess.SP cs.DC cs.SY</categories><comments>5 pages, 4 figures, 3 tables, Proc. of 2018 IEEE Power and Energy
  Society General Meeting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power flow calculation in EMS is required to accommodate a large and complex
power system. To achieve a faster than real-time calculation, a graph based
power flow calculation is proposed in this paper. Graph database and graph
computing advantages in power system calculations are presented. A linear
solver for power flow application is formulated and decomposed in nodal
parallelism and hierarchical parallelism to fully utilize graph parallel
computing capability. Comparison of the algorithm with traditional sequential
programs shows significant benefits on computation efficiency. Case studies on
practical large-scale systems provide supporting evidence that the new
algorithm is promising for online computing for EMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02513</identifier>
 <datestamp>2018-11-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02513</id><created>2018-11-02</created><authors><author><keyname>Trevlakis</keyname><forenames>Stylianos E.</forenames></author><author><keyname>Boulogeorgos</keyname><forenames>Alexandros-Apostolos A.</forenames></author><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Optical Wireless Cochlear Implants</title><categories>eess.SP physics.app-ph physics.med-ph</categories><comments>24 pages, 24 figures, accepted for publication in Biomedical Optics
  Express</comments><journal-ref>Biomedical Optics Express 2018</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present contribution, we introduce a wireless optical
communication-based system architecture which is shown to significantly improve
the reliability and the spectral and power efficiency of the transcutaneous
link in cochlear implants (CIs). We refer to the proposed system as optical
wireless cochlear implant (OWCI).In order to provide a quantified understanding
of its design parameters, we establish a theoretical framework that takes into
account the channel particularities, the integration area of the internal unit,
the transceivers misalignment, and the characteristics of the optical units. To
this end, we derive explicit expressions for the corresponding average
signal-to-noise-ratio, outage probability, ergodic spectral efficiency and
capacity of the transcutaneous optical link (TOL). These expressions are
subsequently used to assess the dependence of the TOL's communication quality
on the transceivers design parameters and the corresponding channels
characteristics. The offered analytic results are corroborated with respective
results from Monte Carlo simulations. Our findings reveal that OWCI is a
particularly promising architecture that drastically increases the reliability
and effectiveness of the CI TOL, whilst it requires considerably lower transmit
power compared to the corresponding widely-used radio frequency (RF) solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02514</identifier>
 <datestamp>2019-09-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02514</id><created>2018-11-04</created><updated>2019-09-05</updated><authors><author><keyname>Cai</keyname><forenames>Xiaohao</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author></authors><title>Quantifying Uncertainty in High Dimensional Inverse Problems by Convex
  Optimisation</title><categories>eess.SP astro-ph.IM cs.NA math.NA</categories><comments>5 pages, 5 figures</comments><journal-ref>EUSIPCO 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverse problems play a key role in modern image/signal processing methods.
However, since they are generally ill-conditioned or ill-posed due to lack of
observations, their solutions may have significant intrinsic uncertainty.
Analysing and quantifying this uncertainty is very challenging, particularly in
high-dimensional problems and problems with non-smooth objective functionals
(e.g. sparsity-promoting priors). In this article, a series of strategies to
visualise this uncertainty are presented, e.g. highest posterior density
credible regions, and local credible intervals (cf. error bars) for individual
pixels and superpixels. Our methods support non-smooth priors for inverse
problems and can be scaled to high-dimensional settings. Moreover, we present
strategies to automatically set regularisation parameters so that the proposed
uncertainty quantification (UQ) strategies become much easier to use. Also,
different kinds of dictionaries (complete and over-complete) are used to
represent the image/signal and their performance in the proposed UQ methodology
is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02566</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02566</id><created>2018-11-06</created><authors><author><keyname>Parcollet</keyname><forenames>Titouan</forenames></author><author><keyname>Morchid</keyname><forenames>Mohamed</forenames></author><author><keyname>Linar&#xe8;s</keyname><forenames>Georges</forenames></author><author><keyname>De Mori</keyname><forenames>Renato</forenames></author></authors><title>Bidirectional Quaternion Long-Short Term Memory Recurrent Neural
  Networks for Speech Recognition</title><categories>eess.AS cs.LG cs.SD eess.SP stat.ML</categories><comments>Submitted at ICASSP 2019. arXiv admin note: text overlap with
  arXiv:1806.04418</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks (RNN) are at the core of modern automatic speech
recognition (ASR) systems. In particular, long-short term memory (LSTM)
recurrent neural networks have achieved state-of-the-art results in many speech
recognition tasks, due to their efficient representation of long and short term
dependencies in sequences of inter-dependent features. Nonetheless, internal
dependencies within the element composing multidimensional features are weakly
considered by traditional real-valued representations. We propose a novel
quaternion long-short term memory (QLSTM) recurrent neural network that takes
into account both the external relations between the features composing a
sequence, and these internal latent structural dependencies with the quaternion
algebra. QLSTMs are compared to LSTMs during a memory copy-task and a realistic
application of speech recognition on the Wall Street Journal (WSJ) dataset.
QLSTM reaches better performances during the two experiments with up to $2.8$
times less learning parameters, leading to a more expressive representation of
the information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02655</identifier>
 <datestamp>2020-01-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02655</id><created>2018-11-06</created><updated>2020-01-20</updated><authors><author><keyname>Atamturk</keyname><forenames>Alper</forenames></author><author><keyname>Gomez</keyname><forenames>Andres</forenames></author><author><keyname>Han</keyname><forenames>Shaoning</forenames></author></authors><title>Sparse and Smooth Signal Estimation: Convexification of L0 Formulations</title><categories>stat.ML cs.LG eess.SP</categories><comments>BCOL Research Report 18.05, IEOR, UC Berkeley</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal estimation problems with smoothness and sparsity priors can be
naturally modeled as quadratic optimization with L0-&quot;norm&quot; constraints. Since
such problems are non-convex and hard-to-solve, the standard approach is,
instead, to tackle their convex surrogates based on L1-norm relaxations. In
this paper, we propose new iterative conic quadratic relaxations that exploit
not only the L0-&quot;norm&quot; terms but also the fitness and smoothness functions. The
iterative convexification approach substantially closes the gap between the
L0-&quot;norm&quot; and its L1 surrogate. Experiments using an off-the-shelf conic
quadratic solver on synthetic as well as real datasets indicate that the
proposed iterative convex relaxations lead to significantly better estimators
than L1-norm while preserving the computational efficiency. In addition, the
parameters of the model and the resulting estimators are easily interpretable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02694</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02694</id><created>2018-11-06</created><updated>2018-11-07</updated><authors><author><keyname>Wang</keyname><forenames>Ran</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Flinker</keyname><forenames>Adeen</forenames></author></authors><title>Reconstructing Speech Stimuli From Human Auditory Cortex Activity Using
  a WaveNet Approach</title><categories>cs.SD cs.LG eess.AS q-bio.NC stat.ML</categories><comments>6 pages, 3 figures. Conference of 2018 IEEE Signal Processing in
  Medicine and Biology Symposium (SPMB 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The superior temporal gyrus (STG) region of cortex critically contributes to
speech recognition. In this work, we show that a proposed WaveNet, with limited
available data, is able to reconstruct speech stimuli from STG intracranial
recordings. We further investigate the impulse response of the fitted model for
each recording electrode and observe phoneme level temporospectral tuning
properties for the recorded area of cortex. This discovery is consistent with
previous studies implicating the posterior STG (pSTG) in a phonetic
representation of speech and provides detailed acoustic features that certain
electrode sites possibly extract during speech recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02695</identifier>
 <datestamp>2020-01-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02695</id><created>2018-11-06</created><authors><author><keyname>Zhu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Mousavi</keyname><forenames>S. Mostafa</forenames></author><author><keyname>Beroza</keyname><forenames>Gregory C.</forenames></author></authors><title>Seismic Signal Denoising and Decomposition Using Deep Neural Networks</title><categories>physics.geo-ph eess.SP</categories><doi>10.1109/TGRS.2019.2926772</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denoising and filtering are widely used in routine seismic-data-processing to
improve the signal-to-noise ratio (SNR) of recorded signals and by doing so to
improve subsequent analyses. In this paper we develop a new
denoising/decomposition method, DeepDenoiser, based on a deep neural network.
This network is able to learn simultaneously a sparse representation of data in
the time-frequency domain and a non-linear function that maps this
representation into masks that decompose input data into a signal of interest
and noise (defined as any non-seismic signal). We show that DeepDenoiser
achieves impressive denoising of seismic signals even when the signal and noise
share a common frequency band. Our method properly handles a variety of colored
noise and non-earthquake signals. DeepDenoiser can significantly improve the
SNR with minimal changes in the waveform shape of interest, even in presence of
high noise levels. We demonstrate the effect of our method on improving
earthquake detection. There are clear applications of DeepDenoiser to seismic
imaging, micro-seismic monitoring, and preprocessing of ambient noise data. We
also note that potential applications of our approach are not limited to these
applications or even to earthquake data, and that our approach can be adapted
to diverse signals and applications in other settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02735</identifier>
 <datestamp>2019-06-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02735</id><created>2018-11-06</created><updated>2019-06-20</updated><authors><author><keyname>Yalta</keyname><forenames>Nelson</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author><author><keyname>Nakadai</keyname><forenames>Kazuhiro</forenames></author><author><keyname>Ogata</keyname><forenames>Tetsuya</forenames></author></authors><title>CNN-based MultiChannel End-to-End Speech Recognition for everyday home
  environments</title><categories>eess.AS cs.CL cs.SD</categories><comments>5 pages, 1 figure, EUSIPCO 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Casual conversations involving multiple speakers and noises from surrounding
devices are common in everyday environments, which degrades the performances of
automatic speech recognition systems. These challenging characteristics of
environments are the target of the CHiME-5 challenge. By employing a
convolutional neural network (CNN)-based multichannel end-to-end speech
recognition system, this study attempts to overcome the presents difficulties
in everyday environments. The system comprises of an attention-based
encoder-decoder neural network that directly generates a text as an output from
a sound input. The multichannel CNN encoder, which uses residual connections
and batch renormalization, is trained with augmented data, including white
noise injection. The experimental results show that the word error rate is
reduced by 8.5% and 0.6% absolute from a single channel end-to-end and the best
baseline (LF-MMI TDNN) on the CHiME-5 corpus, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02736</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02736</id><created>2018-11-06</created><updated>2018-11-27</updated><authors><author><keyname>Lim</keyname><forenames>Hyungjun</forenames></author><author><keyname>Kim</keyname><forenames>Younggwan</forenames></author><author><keyname>Jung</keyname><forenames>Youngmoon</forenames></author><author><keyname>Jung</keyname><forenames>Myunghun</forenames></author><author><keyname>Kim</keyname><forenames>Hoirin</forenames></author></authors><title>Learning acoustic word embeddings with phonetically associated triplet
  network</title><categories>eess.AS cs.AI cs.CL cs.SD eess.SP</categories><comments>5 pages, 4 figures, submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous researches on acoustic word embeddings used in query-by-example
spoken term detection have shown remarkable performance improvements when using
a triplet network. However, the triplet network is trained using only a limited
information about acoustic similarity between words. In this paper, we propose
a novel architecture, phonetically associated triplet network (PATN), which
aims at increasing discriminative power of acoustic word embeddings by
utilizing phonetic information as well as word identity. The proposed model is
learned to minimize a combined loss function that was made by introducing a
cross entropy loss to the lower layer of LSTM-based triplet network. We
observed that the proposed method performs significantly better than the
baseline triplet network on a word discrimination task with the WSJ dataset
resulting in over 20% relative improvement in recall rate at 1.0 false alarm
per hour. Finally, we examined the generalization ability by conducting the
out-of-domain test on the RM dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02764</identifier>
 <datestamp>2019-05-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02764</id><created>2018-11-07</created><updated>2019-05-01</updated><authors><author><keyname>Song</keyname><forenames>Peiyang</forenames></author><author><keyname>Gong</keyname><forenames>Fengkui</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Zhai</keyname><forenames>Shenghua</forenames></author><author><keyname>Ding</keyname><forenames>Haiyang</forenames></author></authors><title>Deep-learning-based Signal Detection for Channel Coded
  Faster-than-Nyquist Transmission</title><categories>eess.SP</categories><comments>The new version has considered the channel coded FTN scenario and are
  significantly different from the previous manuscripts</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Faster-than-Nyquist (FTN) is a promising paradigm to improve bandwidth
utilization at the expense of additional intersymbol interference (ISI). In
this letter, we develop a signal detection architecture based on deep learning
(DL) for FTN system, which employs the sliding window and works without any
iteration. Furthermore, to better fit the channel coded scenarios, we develop a
signal reconstruction method based on hard decision and FTN mapping. To the
best of our knowledge, this is the first attempt to apply DL to channel coded
FTN signal detection. As demonstrated by simulation results, in the uncoded
scenario, our proposed DL-based detection can achieve a near-optimal bit error
rate (BER) performance and shows the potential in high order modulations. Also,
the proposed detection can achieve great performance gains from
start-of-the-art channel coding scheme. Finally, the proposed DL-based
detection has the robustness to signal to noise ratio (SNR). In a nutshell, DL
has been proved to be a powerful tool for FTN signal detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02770</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02770</id><created>2018-11-07</created><authors><author><keyname>Baskar</keyname><forenames>Murali Karthick</forenames></author><author><keyname>Burget</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Karafi&#xe1;t</keyname><forenames>Martin</forenames></author><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author><author><keyname>&#x10c;ernock&#xfd;</keyname><forenames>Jan Honza</forenames></author></authors><title>Promising Accurate Prefix Boosting for sequence-to-sequence ASR</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present promising accurate prefix boosting (PAPB), a
discriminative training technique for attention based sequence-to-sequence
(seq2seq) ASR. PAPB is devised to unify the training and testing scheme in an
effective manner. The training procedure involves maximizing the score of each
partial correct sequence obtained during beam search compared to other
hypotheses. The training objective also includes minimization of token
(character) error rate. PAPB shows its efficacy by achieving 10.8\% and 3.8\%
WER with and without RNNLM respectively on Wall Street Journal dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02784</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02784</id><created>2018-11-07</created><authors><author><keyname>Sheen</keyname><forenames>Spencer</forenames></author><author><keyname>Lyu</keyname><forenames>Jiancheng</forenames></author></authors><title>Median Binary-Connect Method and a Binary Convolutional Neural Nework
  for Word Recognition</title><categories>cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and study a new projection formula for training binary weight
convolutional neural networks. The projection formula measures the error in
approximating a full precision (32 bit) vector by a 1-bit vector in the l_1
norm instead of the standard l_2 norm. The l_1 projector is in closed
analytical form and involves a median computation instead of an arithmatic
average in the l_2 projector. Experiments on 10 keywords classification show
that the l_1 (median) BinaryConnect (BC) method outperforms the regular BC,
regardless of cold or warm start. The binary network trained by median BC and a
recent blending technique reaches test accuracy 92.4%, which is 1.1% lower than
the full-precision network accuracy 93.5%. On Android phone app, the trained
binary network doubles the speed of full-precision network in spoken keywords
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02893</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02893</id><created>2018-11-07</created><authors><author><keyname>M&#xe9;riaux</keyname><forenames>Bruno</forenames><affiliation>SONDRA/CentraleSup&#xe9;lec, FRANCE</affiliation></author><author><keyname>Zhang</keyname><forenames>Xin</forenames><affiliation>AI Department, Echiev Autonomous Driving Technology, CHINA</affiliation></author><author><keyname>Korso</keyname><forenames>Mohammed Nabil El</forenames><affiliation>LEME/Paris Nanterre University, FRANCE</affiliation></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames><affiliation>Communication Systems Group, Technische Universit&#xe4;t Darmstadt, GERMANY</affiliation></author></authors><title>Iterative Marginal Maximum Likelihood DOD and DOA Estimation for MIMO
  Radar in the Presence of SIRP Clutter</title><categories>eess.SP stat.AP</categories><journal-ref>Signal Processing 155 (2019) 384-390</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spherically invariant random process (SIRP) clutter model is commonly
used in scenarios where the radar clutter cannot be correctly modeled as a
Gaussian process. In this short communication, we devise a novel
Maximum-Likelihood (ML)-based iterative estimator for direction-of-departure
and direction-of-arrival estimation in the Multiple-input multiple-output
(MIMO) radar context in the presence of SIRP clutter. The proposed estimator
employs a stepwise numerical concentration approach w.r.t. the objective
function related to the marginal likelihood of the observation data. Our
estimator leads to superior performance, as our simulations show, w.r.t. to the
existing likelihood based methods, namely, the conventional, the conditional
and the joint likelihood based estimators, and w.r.t. the robust subspace
decomposition based methods. Finally, interconnections and comparison between
the Iterative Marginal ML Estimator (IMMLE), Iterative Joint ML Estimator
(IJMLE) and Iterative Conditional ML Estimator (ICdMLE) are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02915</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02915</id><created>2018-11-07</created><authors><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Ozolins</keyname><forenames>Oskars</forenames></author><author><keyname>Lin</keyname><forenames>Rui</forenames></author><author><keyname>Udalcovs</keyname><forenames>Aleksejs</forenames></author><author><keyname>Pang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Gan</keyname><forenames>Lin</forenames></author><author><keyname>Schatz</keyname><forenames>Richard</forenames></author><author><keyname>Djupsj&#xf6;backa</keyname><forenames>Anders</forenames></author><author><keyname>M&#xe5;rtensson</keyname><forenames>Jonas</forenames></author><author><keyname>Westergren</keyname><forenames>Urban</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Fu</keyname><forenames>Sonnian</forenames></author><author><keyname>Liu</keyname><forenames>Deming</forenames></author><author><keyname>Tong</keyname><forenames>Weijun</forenames></author><author><keyname>Popov</keyname><forenames>Sergei</forenames></author><author><keyname>Jacobsen</keyname><forenames>Gunnar</forenames></author><author><keyname>Hu</keyname><forenames>Weisheng</forenames></author><author><keyname>Xiao</keyname><forenames>Shilin</forenames></author><author><keyname>Chen</keyname><forenames>Jiajia</forenames></author></authors><title>Kernel Adaptive Filtering for Nonlinearity-Tolerant Optical Direct
  Detection Systems</title><categories>eess.SP</categories><comments>3 pages, 44th European Conference on Optical Communication (ECOC
  2018), Rome, Italy, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel adaptive filtering (KAF) is proposed for nonlinearity-tolerant optical
direct detection. For 7x128Gbit/s PAM4 transmission over 33.6km 7-core-fiber,
KAF only needs 10 equalizer taps to reach KP4-FEC limit (BER@2.2e-4), whereas
decision-feedback-equalizer needs 43 equalizer taps to reach HD-FEC limit
(BER@3.8e-3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02938</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02938</id><created>2018-11-07</created><authors><author><keyname>Novotny</keyname><forenames>Ondrej</forenames></author><author><keyname>Plchot</keyname><forenames>Oldrich</forenames></author><author><keyname>Matejka</keyname><forenames>Pavel</forenames></author><author><keyname>Glembek</keyname><forenames>Ondrej</forenames></author></authors><title>On the use of DNN Autoencoder for Robust Speaker Recognition</title><categories>eess.AS cs.SD</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an analysis of a DNN-based autoencoder for speech
enhancement, dereverberation and denoising. The target application is a robust
speaker recognition system. We started with augmenting the Fisher database with
artificially noised and reverberated data and we trained the autoencoder to map
noisy and reverberated speech to its clean version. We use the autoencoder as a
preprocessing step for a state-of-the-art text-independent speaker recognition
system. We compare results achieved with pure autoencoder enhancement,
multi-condition PLDA training and their simultaneous use. We present a detailed
analysis with various conditions of NIST SRE 2010, PRISM and artificially
corrupted NIST SRE 2010 telephone condition. We conclude that the proposed
preprocessing significantly outperforms the baseline and that this technique
can be used to build a robust speaker recognition system for reverberated and
noisy data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.02973</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.02973</id><created>2018-11-07</created><authors><author><keyname>Poloskei</keyname><forenames>Peter Zsolt</forenames></author><author><keyname>Papp</keyname><forenames>Gergely</forenames></author><author><keyname>Por</keyname><forenames>Gabor</forenames></author><author><keyname>Horvath</keyname><forenames>Laszlo</forenames></author><author><keyname>Pokol</keyname><forenames>Gergo I.</forenames></author></authors><title>Bicoherence analysis of nonstationary and nonlinear processes</title><categories>eess.SP stat.AP</categories><comments>8 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bicoherence analysis is a well established method for identifying the
quadratic nonlinearity of stationary processes. However, it is often applied
without checking the basic assumptions of stationarity and convergence. The
classic bicoherence, unfortunately, tends to give false positives -- high
bicoherence values without actual nonlinear coupling of different frequency
components -- for signals exhibiting rapidly changing amplitudes and limited
length. The effect of false positive values can lead to misinterpretation of
results, therefore a more prudent analysis is necessary in such cases. This
paper analyses the properties of bispectrum and bicoherence in detail,
generalizing these quantities to nonstationary processes. A step-by-step method
is proposed to filter out false positives at a given confidence level for the
case of nonstationary signals. We present a number of test cases, where the
method is demonstrated on simple physics-based numerical systems. The approach
and methodology introduced in the paper can be generalized to lower and higher
order coherence calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03021</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03021</id><created>2018-11-07</created><authors><author><keyname>Klejsa</keyname><forenames>Janusz</forenames></author><author><keyname>Hedelin</keyname><forenames>Per</forenames></author><author><keyname>Zhou</keyname><forenames>Cong</forenames></author><author><keyname>Fejgin</keyname><forenames>Roy</forenames></author><author><keyname>Villemoes</keyname><forenames>Lars</forenames></author></authors><title>High-quality speech coding with SampleRNN</title><categories>eess.AS cs.SD</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a speech coding scheme employing a generative model based on
SampleRNN that, while operating at significantly lower bitrates, matches or
surpasses the perceptual quality of state-of-the-art classic wide-band codecs.
Moreover, it is demonstrated that the proposed scheme can provide a meaningful
rate-distortion trade-off without retraining. We evaluate the proposed scheme
in a series of listening tests and discuss limitations of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03055</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03055</id><created>2018-11-07</created><authors><author><keyname>Bhattacharya</keyname><forenames>Gautam</forenames></author><author><keyname>Alam</keyname><forenames>Jahangir</forenames></author><author><keyname>Kenny</keyname><forenames>Patrick</forenames></author></authors><title>Adapting End-to-End Neural Speaker Verification to New Languages and
  Recording Conditions with Adversarial Training</title><categories>eess.AS cs.CV cs.SD</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we propose a novel approach for adapting speaker embeddings
to new domains based on adversarial training of neural networks. We apply our
embeddings to the task of text-independent speaker verification, a challenging,
real-world problem in biometric security. We further the development of
end-to-end speaker embedding models by combing a novel 1-dimensional,
self-attentive residual network, an angular margin loss function and
adversarial training strategy. Our model is able to learn extremely compact,
64-dimensional speaker embeddings that deliver competitive performance on a
number of popular datasets using simple cosine distance scoring. One the
NIST-SRE 2016 task we are able to beat a strong i-vector baseline, while on the
Speakers in the Wild task our model was able to outperform both i-vector and
x-vector baselines, showing an absolute improvement of 2.19% over the latter.
Additionally, we show that the integration of adversarial training consistently
leads to a significant improvement over an unadapted model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03063</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03063</id><created>2018-11-07</created><authors><author><keyname>Bhattacharya</keyname><forenames>Gautam</forenames></author><author><keyname>Monteiro</keyname><forenames>Joao</forenames></author><author><keyname>Alam</keyname><forenames>Jahangir</forenames></author><author><keyname>Kenny</keyname><forenames>Patrick</forenames></author></authors><title>Generative Adversarial Speaker Embedding Networks for Domain Robust
  End-to-End Speaker Verification</title><categories>eess.AS cs.CV cs.SD</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a novel approach for learning domain-invariant speaker
embeddings using Generative Adversarial Networks. The main idea is to confuse a
domain discriminator so that is can't tell if embeddings are from the source or
target domains. We train several GAN variants using our proposed framework and
apply them to the speaker verification task. On the challenging NIST-SRE 2016
dataset, we are able to match the performance of a strong baseline x-vector
system. In contrast to the the baseline systems which are dependent on
dimensionality reduction (LDA) and an external classifier (PLDA), our proposed
speaker embeddings can be scored using simple cosine distance. This is achieved
by optimizing our models end-to-end, using an angular margin loss function.
Furthermore, we are able to significantly boost verification performance by
averaging our different GAN models at the score level, achieving a relative
improvement of 7.2% over the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03076</identifier>
 <datestamp>2018-11-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03076</id><created>2018-11-07</created><authors><author><keyname>Seetharaman</keyname><forenames>Prem</forenames></author><author><keyname>Wichern</keyname><forenames>Gordon</forenames></author><author><keyname>Venkataramani</keyname><forenames>Shrikant</forenames></author><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author></authors><title>Class-conditional embeddings for music source separation</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Isolating individual instruments in a musical mixture has a myriad of
potential applications, and seems imminently achievable given the levels of
performance reached by recent deep learning methods. While most musical source
separation techniques learn an independent model for each instrument, we
propose using a common embedding space for the time-frequency bins of all
instruments in a mixture inspired by deep clustering and deep attractor
networks. Additionally, an auxiliary network is used to generate parameters of
a Gaussian mixture model (GMM) where the posterior distribution over GMM
components in the embedding space can be used to create a mask that separates
individual sources from a mixture. In addition to outperforming a
mask-inference baseline on the MUSDB-18 dataset, our embedding space is easily
interpretable and can be used for query-based separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03131</identifier>
 <datestamp>2019-02-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03131</id><created>2018-11-07</created><updated>2019-02-11</updated><authors><author><keyname>Tindemans</keyname><forenames>Simon H.</forenames></author><author><keyname>Woolf</keyname><forenames>Matthew</forenames></author><author><keyname>Strbac</keyname><forenames>Goran</forenames></author></authors><title>Capacity Value of Interconnection Between Two Systems</title><categories>cs.CE eess.SP</categories><comments>Copyright 2019 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concerns about system adequacy have led to the establishment of capacity
mechanisms in a number of regulatory areas. Against this background, it is
essential to accurately quantify the contribution to security of supply that
results from interconnectors to neighbouring systems. This paper introduces a
definition of capacity value for interconnection between two systems in the
form of a capacity allocation curve. Four power flow policies are proposed to
encompass the full range of possible market outcomes that may affect the
capacity value. A convolution-based method is presented to efficiently compute
and compare capacity allocation curves, and it is applied to a model system
that is inspired by Great Britain's interconnection with the continental
Europe. The results indicate areas of interest for the coordination of capacity
mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03193</identifier>
 <datestamp>2019-03-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03193</id><created>2018-11-07</created><updated>2019-03-15</updated><authors><author><keyname>Tabaghi</keyname><forenames>Puoya</forenames></author><author><keyname>Dokmani&#x107;</keyname><forenames>Ivan</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Kinetic Euclidean Distance Matrices</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Euclidean distance matrices (EDMs) are a major tool for localization from
distances, with applications ranging from protein structure determination to
global positioning and manifold learning. They are, however, static objects
which serve to localize points from a snapshot of distances. If the objects
move, one expects to do better by modeling the motion. In this paper, we
introduce Kinetic Euclidean Distance Matrices (KEDMs)---a new kind of
time-dependent distance matrices that incorporate motion. The entries of KEDMs
become functions of time, the squared time-varying distances. We study two
smooth trajectory models---polynomial and bandlimited trajectories---and show
that these trajectories can be reconstructed from incomplete, noisy distance
observations, scattered over multiple time instants. Our main contribution is a
semidefinite relaxation (SDR), inspired by SDRs for static EDMs. Similarly to
the static case, the SDR is followed by a spectral factorization step; however,
because spectral factorization of polynomial matrices is more challenging than
for constant matrices, we propose a new factorization method that uses anchor
measurements. Extensive numerical experiments show that KEDMs and the new
semidefinite relaxation accurately reconstruct trajectories from noisy,
incomplete distance data and that, in fact, motion improves rather than
degrades localization if properly modeled. This makes KEDMs a promising tool
for problems in geometry of dynamic points sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03206</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03206</id><created>2018-11-07</created><updated>2019-02-16</updated><authors><author><keyname>Bai</keyname><forenames>Yuanchao</forenames></author><author><keyname>Cheung</keyname><forenames>Gene</forenames></author><author><keyname>Wang</keyname><forenames>Fen</forenames></author><author><keyname>Liu</keyname><forenames>Xianming</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author></authors><title>Reconstruction-Cognizant Graph Sampling using Gershgorin Disc Alignment</title><categories>eess.SP</categories><comments>accepted to International Conference on Acoustics, Speech, and Signal
  Processing 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph sampling with noise is a fundamental problem in graph signal processing
(GSP). Previous works assume an unbiased least square (LS) signal
reconstruction scheme and select samples greedily via expensive extreme
eigenvector computation. A popular biased scheme using graph Laplacian
regularization (GLR) solves a system of linear equations for its
reconstruction. Assuming this GLR-based scheme, we propose a
reconstruction-cognizant sampling strategy to maximize the numerical stability
of the linear system---\textit{i.e.}, minimize the condition number of the
coefficient matrix. Specifically, we maximize the eigenvalue lower bounds of
the matrix, represented by left-ends of Gershgorin discs of the coefficient
matrix. To accomplish this efficiently, we propose an iterative algorithm to
traverse the graph nodes via Breadth First Search (BFS) and align the left-ends
of all corresponding Gershgorin discs at lower-bound threshold $T$ using two
basic operations: disc shifting and scaling. We then perform binary search to
maximize $T$ given a sample budget $K$. Experiments on real graph data show
that the proposed algorithm can effectively promote large eigenvalue lower
bounds, and the reconstruction MSE is the same or smaller than existing
sampling methods for different budget $K$ at much lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03255</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03255</id><created>2018-11-07</created><authors><author><keyname>Li</keyname><forenames>Lantian</forenames></author><author><keyname>Tang</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Shi</keyname><forenames>Ying</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author></authors><title>Phonetic-attention scoring for deep speaker features in speaker
  verification</title><categories>eess.AS cs.CL cs.SD</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have shown that frame-level deep speaker features can be
derived from a deep neural network with the training target set to discriminate
speakers by a short speech segment. By pooling the frame-level features,
utterance-level representations, called d-vectors, can be derived and used in
the automatic speaker verification (ASV) task. This simple average pooling,
however, is inherently sensitive to the phonetic content of the utterance. An
interesting idea borrowed from machine translation is the attention-based
mechanism, where the contribution of an input word to the translation at a
particular time is weighted by an attention score. This score reflects the
relevance of the input word and the present translation. We can use the same
idea to align utterances with different phonetic contents. This paper proposes
a phonetic-attention scoring approach for d-vector systems. By this approach,
an attention score is computed for each frame pair. This score reflects the
similarity of the two frames in phonetic content, and is used to weigh the
contribution of this frame pair in the utterance-based scoring. This new
scoring approach emphasizes the frame pairs with similar phonetic contents,
which essentially provides a soft alignment for utterances with any phonetic
contents. Experimental results show that compared with the naive average
pooling, this phonetic-attention scoring approach can deliver consistent
performance improvement in ASV tasks of both text-dependent and
text-independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03258</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03258</id><created>2018-11-07</created><updated>2019-02-17</updated><authors><author><keyname>Li</keyname><forenames>Lantian</forenames></author><author><keyname>Tang</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Shi</keyname><forenames>Ying</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author></authors><title>Gaussian-Constrained training for speaker verification</title><categories>eess.AS cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural models, in particular the d-vector and x-vector architectures, have
produced state-of-the-art performance on many speaker verification tasks.
However, two potential problems of these neural models deserve more
investigation. Firstly, both models suffer from `information leak', which means
that some parameters participating in model training will be discarded during
inference, i.e, the layers that are used as the classifier. Secondly, these
models do not regulate the distribution of the derived speaker vectors. This
`unconstrained distribution' may degrade the performance of the subsequent
scoring component, e.g., PLDA. This paper proposes a Gaussian-constrained
training approach that (1) discards the parametric classifier, and (2) enforces
the distribution of the derived speaker vectors to be Gaussian. Our experiments
on the VoxCeleb and SITW databases demonstrated that this new training approach
produced more representative and regular speaker embeddings, leading to
consistent performance improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03271</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03271</id><created>2018-11-08</created><authors><author><keyname>Hung</keyname><forenames>Yun-Ning</forenames></author><author><keyname>Chen</keyname><forenames>Yi-An</forenames></author><author><keyname>Yang</keyname><forenames>Yi-Hsuan</forenames></author></authors><title>Learning Disentangled Representations for Timber and Pitch in Music
  Audio</title><categories>cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Timbre and pitch are the two main perceptual properties of musical sounds.
Depending on the target applications, we sometimes prefer to focus on one of
them, while reducing the effect of the other. Researchers have managed to
hand-craft such timbre-invariant or pitch-invariant features using domain
knowledge and signal processing techniques, but it remains difficult to
disentangle them in the resulting feature representations. Drawing upon
state-of-the-art techniques in representation learning, we propose in this
paper two deep convolutional neural network models for learning disentangled
representation of musical timbre and pitch. Both models use encoders/decoders
and adversarial training to learn music representations, but the second model
additionally uses skip connections to deal with the pitch information. As music
is an art of time, the two models are supervised by frame-level instrument and
pitch labels using a new dataset collected from MuseScore. We compare the
result of the two disentangling models with a new evaluation protocol called
&quot;timbre crossover&quot;, which leads to interesting applications in audio-domain
music editing. Via various objective evaluations, we show that the second model
can better change the instrumentation of a multi-instrument music piece without
much affecting the pitch structure. By disentangling timbre and pitch, we
envision that the model can contribute to generating more realistic music audio
as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03289</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03289</id><created>2018-11-08</created><authors><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Interference Exploitation Precoding for Multi-Level Modulations:
  Closed-Form Solutions</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study closed-form interference-exploitation precoding for
multi-level modulations in the downlink of multi-user multiple-input
single-output (MU-MISO) systems. We consider two distinct cases: first, for the
case where the number of served users is not larger than the number of transmit
antennas at the base station (BS), we mathematically derive the optimal
precoding structure based on the Karush-Kuhn-Tucker (KKT) conditions. By
formulating the dual problem, the precoding problem for multi-level modulations
is transformed into a pre-scaling operation using quadratic programming (QP)
optimization. We further consider the case where the number of served users is
larger than the number of transmit antennas at the BS. By employing the pseudo
inverse, we show that the optimal solution of the pre-scaling vector is
equivalent to a linear combination of the right singular vectors corresponding
to zero singular values, and derive the equivalent QP formulation. We also
present the condition under which multiplexing more streams than the number of
transmit antennas is achievable. For both considered scenarios, we propose a
modified iterative algorithm to obtain the optimal precoding matrix, as well as
a sub-optimal closed-form precoder. Numerical results validate our derivations
on the optimal precoding structures for multi-level modulations, and
demonstrate the superiority of interference-exploitation precoding for both
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03293</identifier>
 <datestamp>2019-02-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03293</id><created>2018-11-08</created><updated>2019-02-10</updated><authors><author><keyname>Vestman</keyname><forenames>Ville</forenames></author><author><keyname>Soomro</keyname><forenames>Bilal</forenames></author><author><keyname>Kanervisto</keyname><forenames>Anssi</forenames></author><author><keyname>Hautam&#xe4;ki</keyname><forenames>Ville</forenames></author><author><keyname>Kinnunen</keyname><forenames>Tomi</forenames></author></authors><title>Who Do I Sound Like? Showcasing Speaker Recognition Technology by
  YouTube Voice Search</title><categories>eess.AS cs.SD</categories><comments>Accepted for presentation in ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularization of science can often be disregarded by scientists as it
may be challenging to put highly sophisticated research into words that general
public can understand. This work aims to help presenting speaker recognition
research to public by proposing a publicly appealing concept for showcasing
recognition systems. We leverage data from YouTube and use it in a large-scale
voice search web application that finds the celebrity voices that best match to
the user's voice. The concept was tested in a public event as well as &quot;in the
wild&quot; and the received feedback was mostly positive. The i-vector based speaker
identification back end was found to be fast (665 ms per request) and had a
high identification accuracy (93 %) for the YouTube target speakers. To help
other researchers to develop the idea further, we share the source codes of the
web platform used for the demo at
https://github.com/bilalsoomro/speech-demo-platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03311</identifier>
 <datestamp>2019-07-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03311</id><created>2018-11-08</created><updated>2019-06-28</updated><authors><author><keyname>Song</keyname><forenames>Eunwoo</forenames></author><author><keyname>Kim</keyname><forenames>Jinseob</forenames></author><author><keyname>Byun</keyname><forenames>Kyungguen</forenames></author><author><keyname>Kang</keyname><forenames>Hong-Goo</forenames></author></authors><title>Speaker-adaptive neural vocoders for parametric speech synthesis systems</title><categories>eess.AS cs.LG cs.SD</categories><comments>5 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes speaker-adaptive neural vocoders for statistical
parametric speech synthesis (SPSS) systems. Recently proposed WaveNet-based
neural vocoding systems successfully generate a time sequence of speech signal
with an autoregressive framework. However, it remains a challenge to build
high-quality speech synthesis systems when the amount of a target speaker's
training data is insufficient. To generate more natural speech signals with the
constraint of limited training data, we propose a speaker adaptation task with
an effective variation of neural vocoding models. In the proposed method, a
speaker-independent training method is applied to capture universal attributes
embedded in multiple speakers, and the trained model is then optimized to
represent the specific characteristics of the target speaker. Experimental
results verify that the proposed SPSS systems with speaker-adaptive neural
vocoders outperform those with traditional source-filter model-based vocoders
and those with WaveNet vocoders, trained either speaker-dependently or
speaker-independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03324</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03324</id><created>2018-11-08</created><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Bjornson</keyname><forenames>Emil</forenames></author><author><keyname>Hoydis</keyname><forenames>Jakob</forenames></author></authors><title>Fundamental Asymptotic Behavior of (Two-User) Distributed Massive MIMO</title><categories>cs.IT eess.SP math.IT</categories><comments>6 pages, 2 figures, to be presented at GLOBECOM 2018, Abu Dhabi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the uplink of a distributed Massive MIMO network where
$N$ base stations (BSs), each equipped with $M$ antennas, receive data from
$K=2$ users. We study the asymptotic spectral efficiency (as $M\to \infty$)
with spatial correlated channels, pilot contamination, and different degrees of
channel state information (CSI) and statistical knowledge at the BSs. By
considering a two-user setup, we can simply derive fundamental asymptotic
behaviors and provide novel insights into the structure of the optimal
combining schemes. In line with [1], when global CSI is available at all BSs,
the optimal minimum-mean squared error combining has an unbounded capacity as
$M\to \infty$, if the global channel covariance matrices of the users are
asymptotically linearly independent. This result is instrumental to derive a
suboptimal combining scheme that provides unbounded capacity as $M\to \infty$
using only local CSI and global channel statistics. The latter scheme is shown
to outperform a generalized matched filter scheme, which also achieves
asymptotic unbounded capacity by using only local CSI and global channel
statistics, but is derived following [2] on the basis of a more conservative
capacity bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03343</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03343</id><created>2018-11-08</created><authors><author><keyname>Li</keyname><forenames>Xiaoxiao</forenames></author><author><keyname>Singh</keyname><forenames>Vivek</forenames></author><author><keyname>Wu</keyname><forenames>Yifan</forenames></author><author><keyname>Kirchberg</keyname><forenames>Klaus</forenames></author><author><keyname>Duncan</keyname><forenames>James</forenames></author><author><keyname>Kapoor</keyname><forenames>Ankur</forenames></author></authors><title>Repetitive Motion Estimation Network: Recover cardiac and respiratory
  signal from thoracic imaging</title><categories>cs.CV eess.IV</categories><comments>Accepted by NIPS workshop MED-NIPS 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking organ motion is important in image-guided interventions, but motion
annotations are not always easily available. Thus, we propose Repetitive Motion
Estimation Network (RMEN) to recover cardiac and respiratory signals. It learns
the spatio-temporal repetition patterns, embedding high dimensional motion
manifolds to 1D vectors with partial motion phase boundary annotations.
Compared with the best alternative models, our proposed RMEN significantly
decreased the QRS peaks detection offsets by 59.3%. Results showed that RMEN
could handle the irregular cardiac and respiratory motion cases. Repetitive
motion patterns learned by RMEN were visualized and indicated in the feature
maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03352</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03352</id><created>2018-11-08</created><authors><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Udalcovs</keyname><forenames>Aleksejs</forenames></author><author><keyname>Lin</keyname><forenames>Rui</forenames></author><author><keyname>Ozolins</keyname><forenames>Oskars</forenames></author><author><keyname>Pang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Gan</keyname><forenames>Lin</forenames></author><author><keyname>Schatz</keyname><forenames>Richard</forenames></author><author><keyname>Djupsj&#xf6;backa</keyname><forenames>Anders</forenames></author><author><keyname>M&#xe5;rtensson</keyname><forenames>Jonas</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Fu</keyname><forenames>Songnian</forenames></author><author><keyname>Liu</keyname><forenames>Deming</forenames></author><author><keyname>Tong</keyname><forenames>Weijun</forenames></author><author><keyname>Popov</keyname><forenames>Sergei</forenames></author><author><keyname>Jacobsen</keyname><forenames>Gunnar</forenames></author><author><keyname>Hu</keyname><forenames>Weisheng</forenames></author><author><keyname>Xiao</keyname><forenames>Shilin</forenames></author><author><keyname>Chen</keyname><forenames>Jiajia</forenames></author></authors><title>Digital Radio-over-Multicore-Fiber System with Self-Homodyne Coherent
  Detection and Entropy Coding for Mobile Fronthaul</title><categories>eess.SP</categories><comments>3 pages, 44th European Conference on Optical Communication (ECOC
  2018), Rome, Italy, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We experimentally demonstrate a 28-Gbaud 16-QAM self-homodyne digital
radio-over- 33.6km-7-core-fiber system with entropy coding for mobile
fronthaul, achieving error-free carrier aggregation of 330 100-MHz 4096-QAM
5G-new-radio channels and 921 100-MHz QPSK 5G-new-radio channels with
CPRI-equivalent data rate up to 3.73-Tbit/s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03438</identifier>
 <datestamp>2019-10-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03438</id><created>2018-11-08</created><updated>2019-10-07</updated><authors><author><keyname>He</keyname><forenames>Xingkang</forenames></author><author><keyname>Xue</keyname><forenames>Wenchao</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaocheng</forenames></author><author><keyname>Fang</keyname><forenames>Haitao</forenames></author></authors><title>Distributed Filtering for Uncertain Systems Under Switching Sensor
  Networks and Quantized Communications</title><categories>eess.SP cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the distributed filtering problem for a class of
stochastic uncertain systems under quantized data flowing over switching sensor
networks. Employing the biased noisy observations of the local sensor and
interval-quantized messages from neighboring sensors successively, an extended
state based distributed Kalman filter (DKF) is proposed for simultaneously
estimating both system state and uncertain dynamics. To alleviate the effect of
observation biases, an event-triggered update based DKF is presented with a
tighter mean square error bound than that of the time-driven one by designing a
proper threshold. Both the two DKFs are shown to provide the upper bounds of
mean square errors online for each sensor. Under mild conditions on systems and
networks, the mean square error boundedness and asymptotic unbiasedness for the
proposed two DKFs are proved. Finally, the numerical simulations demonstrate
the effectiveness of the developed filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03451</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03451</id><created>2018-11-07</created><authors><author><keyname>Karafi&#xe1;t</keyname><forenames>Martin</forenames></author><author><keyname>Baskar</keyname><forenames>Murali Karthick</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author><author><keyname>Wiesner</keyname><forenames>Matthew</forenames></author><author><keyname>&#x10c;ernock&#xfd;</keyname><forenames>Jan &quot;Honza''</forenames></author></authors><title>Analysis of Multilingual Sequence-to-Sequence speech recognition systems</title><categories>eess.AS cs.CL cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1810.03459</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the applications of various multilingual approaches
developed in conventional hidden Markov model (HMM) systems to
sequence-to-sequence (seq2seq) automatic speech recognition (ASR). On a set
composed of Babel data, we first show the effectiveness of multi-lingual
training with stacked bottle-neck (SBN) features. Then we explore various
architectures and training strategies of multi-lingual seq2seq models based on
CTC-attention networks including combinations of output layer, CTC and/or
attention component re-training. We also investigate the effectiveness of
language-transfer learning in a very low resource scenario when the target
language is not included in the original multi-lingual training data.
Interestingly, we found multilingual features superior to multilingual models,
and this finding suggests that we can efficiently combine the benefits of the
HMM system with the seq2seq system through these multilingual feature
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03455</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03455</id><created>2018-11-07</created><authors><author><keyname>Deng</keyname><forenames>Chao</forenames></author><author><keyname>Hu</keyname><forenames>Xuemei</forenames></author><author><keyname>Li</keyname><forenames>Xiaoxu</forenames></author><author><keyname>Suo</keyname><forenames>Jinli</forenames></author><author><keyname>Zhang</keyname><forenames>Zhili</forenames></author><author><keyname>Dai</keyname><forenames>Qionghai</forenames></author></authors><title>High fidelity single-pixel imaging</title><categories>eess.IV</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single-pixel imaging (SPI) is an emerging technique which has attracts wide
attention in various research fields. However, restricted by the low
reconstruction quality and large amount of measurements, the practical
application is still in its infancy. Inspired by the fact that natural scenes
exhibit unique degenerate structures in the low dimensional subspace, we
propose to take advantage of the local prior in convolutional sparse coding to
implement high fidelity single-pixel imaging. Specifically, by statistically
learning strategy, the target scene can be sparse represented on an
overcomplete dictionary. The dictionary is composed of various basis learned
from a natural image database. We introduce the above local prior into
conventional SPI framework to promote the final reconstruction quality.
Experiments both on synthetic data and real captured data demonstrate that our
method can achieve better reconstruction from the same measurements, and thus
consequently reduce the number of required measurements for same reconstruction
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03458</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03458</id><created>2018-11-07</created><authors><author><keyname>Cariow</keyname><forenames>Aleksandr</forenames></author><author><keyname>Cariowa</keyname><forenames>Galina</forenames></author></authors><title>Hardware-Efficient Structure of the Accelerating Module for
  Implementation of Convolutional Neural Network Basic Operation</title><categories>eess.SP cs.AR</categories><comments>3 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a structural design of the hardware-efficient module for
implementation of convolution neural network (CNN) basic operation with reduced
implementation complexity. For this purpose we utilize some modification of the
Winograd minimal filtering method as well as computation vectorization
principles. This module calculate inner products of two consecutive segments of
the original data sequence, formed by a sliding window of length 3, with the
elements of a filter impulse response. The fully parallel structure of the
module for calculating these two inner products, based on the implementation of
a naive method of calculation, requires 6 binary multipliers and 4 binary
adders. The use of the Winograd minimal filtering method allows to construct a
module structure that requires only 4 binary multipliers and 8 binary adders.
Since a high-performance convolutional neural network can contain tens or even
hundreds of such modules, such a reduction can have a significant effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03486</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03486</id><created>2018-11-08</created><authors><author><keyname>Lee</keyname><forenames>Shih-kuang</forenames></author><author><keyname>Wang</keyname><forenames>Syu-Siang</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Hung</keyname><forenames>Jeih-weih</forenames></author></authors><title>Speech Enhancement Based on Reducing the Detail Portion of Speech
  Spectrograms in Modulation Domain via Discrete Wavelet Transform</title><categories>eess.AS cs.SD</categories><comments>4 pages, 4 figures, to appear in ISCSLP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel speech enhancement (SE) method by
exploiting the discrete wavelet transform (DWT). This new method reduces the
amount of fast time-varying portion, viz. the DWT-wise detail component, in the
spectrogram of speech signals so as to highlight the speech-dominant component
and achieves better speech quality. A particularity of this new method is that
it is completely unsupervised and requires no prior information about the clean
speech and noise in the processed utterance. The presented DWT-based SE method
with various scaling factors for the detail part is evaluated with a subset of
Aurora-2 database, and the PESQ metric is used to indicate the quality of
processed speech signals. The preliminary results show that the processed
speech signals reveal a higher PESQ score in comparison with the original
counterparts. Furthermore, we show that this method can still enhance the
signal by totally discarding the detail part (setting the respective scaling
factor to zero), revealing that the spectrogram can be down-sampled and thus
compressed without the cost of lowered quality. In addition, we integrate this
new method with conventional speech enhancement algorithms, including spectral
subtraction, Wiener filtering, and spectral MMSE estimation, and show that the
resulting integration behaves better than the respective component method. As a
result, this new method is quite effective in improving the speech quality and
well additive to the other SE methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03601</identifier>
 <datestamp>2018-11-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03601</id><created>2018-11-05</created><authors><author><keyname>Qiu</keyname><forenames>Ziming</forenames></author><author><keyname>Langerman</keyname><forenames>Jack</forenames></author><author><keyname>Nair</keyname><forenames>Nitin</forenames></author><author><keyname>Aristizabal</keyname><forenames>Orlando</forenames></author><author><keyname>Mamou</keyname><forenames>Jonathan</forenames></author><author><keyname>Turnbull</keyname><forenames>Daniel H.</forenames></author><author><keyname>Ketterling</keyname><forenames>Jeffrey</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Deep BV: A Fully Automated System for Brain Ventricle Localization and
  Segmentation in 3D Ultrasound Images of Embryonic Mice</title><categories>eess.IV cs.CV cs.LG q-bio.QM stat.ML</categories><comments>IEEE Signal Processing in Medicine and Biology Symposium - 2018, 6
  pages, 5 figures</comments><acm-class>I.2.6; I.4.6; I.5.1; I.5.4; I.5.5; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volumetric analysis of brain ventricle (BV) structure is a key tool in the
study of central nervous system development in embryonic mice. High-frequency
ultrasound (HFU) is the only non-invasive, real-time modality available for
rapid volumetric imaging of embryos in utero. However, manual segmentation of
the BV from HFU volumes is tedious, time-consuming, and requires specialized
expertise. In this paper, we propose a novel deep learning based BV
segmentation system for whole-body HFU images of mouse embryos. Our fully
automated system consists of two modules: localization and segmentation. It
first applies a volumetric convolutional neural network on a 3D sliding window
over the entire volume to identify a 3D bounding box containing the entire BV.
It then employs a fully convolutional network to segment the detected bounding
box into BV and background. The system achieves a Dice Similarity Coefficient
(DSC) of 0.8956 for BV segmentation on an unseen 111 HFU volume test set
surpassing the previous state-of-the-art method (DSC of 0.7119) by a margin of
25%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03612</identifier>
 <datestamp>2019-04-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03612</id><created>2018-11-08</created><authors><author><keyname>Awan</keyname><forenames>Hamdan</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj S.</forenames></author><author><keyname>Wallbridge</keyname><forenames>Nigel</forenames></author><author><keyname>Plummer</keyname><forenames>Carrol</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew W.</forenames></author></authors><title>Communication and Information Theory of Single Action Potential Signals
  in Plants</title><categories>q-bio.NC cs.IT eess.SP math.IT</categories><comments>13 Pages, 15 Figures, Accepted for Publication in IEEE Transactions
  on NanoBioscience</comments><doi>10.1109/TNB.2018.2880924</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many plants, such as Mimosa pudica (the sensitive plant), employ
electrochemical signals known as action potentials (APs) for rapid
intercellular communication. In this paper, we consider a reaction diffusion
model of individual AP signals to analyze APs from a communication and
information theoretic perspective. We use concepts from molecular communication
to explain the underlying process of information transfer in a plant for a
single AP pulse that is shared with one or more receiver cells. We also use the
chemical Langevin equation to accommodate the deterministic as well as
stochastic component of the system. Finally we present an information theoretic
analysis of single action potentials, obtaining achievable information rates
for these signals. We show that, in general, the presence of an AP signal can
increase the mutual information and information propagation speed among
neighboring cells with receivers in different settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03659</identifier>
 <datestamp>2018-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03659</id><created>2018-11-08</created><authors><author><keyname>Sun</keyname><forenames>Yu</forenames></author><author><keyname>Wohlberg</keyname><forenames>Brendt</forenames></author><author><keyname>Kamilov</keyname><forenames>Ulugbek S.</forenames></author></authors><title>Plug-In Stochastic Gradient Method</title><categories>eess.SP cs.LG</categories><comments>To be presented at International Biomedical and Astronomical Signal
  Processing (BASP) Frontiers workshop 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plug-and-play priors (PnP) is a popular framework for regularized signal
reconstruction by using advanced denoisers within an iterative algorithm. In
this paper, we discuss our recent online variant of PnP that uses only a subset
of measurements at every iteration, which makes it scalable to very large
datasets. We additionally present novel convergence results for both batch and
online PnP algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03700</identifier>
 <datestamp>2018-12-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03700</id><created>2018-11-08</created><updated>2018-11-17</updated><authors><author><keyname>Weng</keyname><forenames>Chao</forenames></author><author><keyname>Yu</keyname><forenames>Dong</forenames></author></authors><title>A Comparison of Lattice-free Discriminative Training Criteria for Purely
  Sequence-Trained Neural Network Acoustic Models</title><categories>cs.LG cs.AI cs.CL eess.AS stat.ML</categories><comments>under review ICASSP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, three lattice-free (LF) discriminative training criteria for
purely sequence-trained neural network acoustic models are compared on LVCSR
tasks, namely maximum mutual information (MMI), boosted maximum mutual
information (bMMI) and state-level minimum Bayes risk (sMBR). We demonstrate
that, analogous to LF-MMI, a neural network acoustic model can also be trained
from scratch using LF-bMMI or LF-sMBR criteria respectively without the need of
cross-entropy pre-training. Furthermore, experimental results on
Switchboard-300hrs and Switchboard+Fisher-2100hrs datasets show that models
trained with LF-bMMI consistently outperform those trained with plain LF-MMI
and achieve a relative word error rate (WER) reduction of 5% over competitive
temporal convolution projected LSTM (TDNN-LSTMP) LF-MMI baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03755</identifier>
 <datestamp>2019-01-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03755</id><created>2018-11-08</created><updated>2019-01-17</updated><authors><author><keyname>Hu</keyname><forenames>Meiling</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Xue</keyname><forenames>Jinpei</forenames></author><author><keyname>Lu</keyname><forenames>Jing</forenames></author></authors><title>A new insight into the secondary path modeling problem in active noise
  control</title><categories>eess.SP</categories><comments>17 pages 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The close relationship between the feedforward ANC system and the stereo
acoustic echo cancellation system is revealed in this paper. Accordingly, the
convergence behavior of the ANC system can be analyzed by investigating the
joint auto-correlation matrix of the reference and the filtered reference
signal. It is proved that the straightforward secondary path modeling can be
carried out without the injection of any additive noise as long as the control
filter is of a sufficiently long length. Furthermore, by taking advantage of
the time-varying characteristic of the control filter, effective modeling of
the secondary path can be even achieved without any restriction on the control
filter length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03777</identifier>
 <datestamp>2018-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03777</id><created>2018-11-09</created><authors><author><keyname>Lai</keyname><forenames>Ke</forenames></author><author><keyname>Wen</keyname><forenames>Lei</forenames></author><author><keyname>Lei</keyname><forenames>Jing</forenames></author><author><keyname>Chen</keyname><forenames>Gaojie</forenames></author><author><keyname>Xiao</keyname><forenames>Pei</forenames></author><author><keyname>Maaref</keyname><forenames>Amine</forenames></author></authors><title>Codeword Position Index based Sparse Code Multiple Access System</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, a novel variation of sparse code multiple access (SCMA),
called codeword position index based SCMA (CPI-SCMA), is proposed. In this
scheme, the information is transmitted not only by the codewords in M point
SCMA codebook, but also by the indices of the codeword positions in a data
block. As such, both the power and transmission efficiency (TE) can be
improved, moreover, CPI-SCMA can achieve a better error rate performance
compare to conventional SCMA (C-SCMA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03790</identifier>
 <datestamp>2018-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03790</id><created>2018-11-09</created><authors><author><keyname>Kinnunen</keyname><forenames>Tomi</forenames></author><author><keyname>Hautam&#xe4;ki</keyname><forenames>Rosa Gonz&#xe1;lez</forenames></author><author><keyname>Vestman</keyname><forenames>Ville</forenames></author><author><keyname>Sahidullah</keyname><forenames>Md</forenames></author></authors><title>Can We Use Speaker Recognition Technology to Attack Itself? Enhancing
  Mimicry Attacks Using Automatic Target Speaker Selection</title><categories>eess.AS cs.CL stat.ML</categories><comments>(A slightly shorter version) has been submitted to IEEE ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider technology-assisted mimicry attacks in the context of automatic
speaker verification (ASV). We use ASV itself to select targeted speakers to be
attacked by human-based mimicry. We recorded 6 naive mimics for whom we select
target celebrities from VoxCeleb1 and VoxCeleb2 corpora (7,365 potential
targets) using an i-vector system. The attacker attempts to mimic the selected
target, with the utterances subjected to ASV tests using an independently
developed x-vector system. Our main finding is negative: even if some of the
attacker scores against the target speakers were slightly increased, our mimics
did not succeed in spoofing the x-vector system. Interestingly, however, the
relative ordering of the selected targets (closest, furthest, median) are
consistent between the systems, which suggests some level of transferability
between the systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03795</identifier>
 <datestamp>2018-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03795</id><created>2018-11-09</created><authors><author><keyname>Turlapaty</keyname><forenames>Anish C.</forenames></author><author><keyname>Gokaraju</keyname><forenames>Balakrishna</forenames></author></authors><title>Feature Analysis for Classification of Physical Actions using surface
  EMG Data</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on recent health statistics, there are several thousands of people with
limb disability and gait disorders that require a medical assistance. A robot
assisted rehabilitation therapy can help them recover and return to a normal
life. In this scenario, a successful methodology is to use the EMG signal based
information to control the support robotics. For this mechanism to function
properly, the EMG signal from the muscles has to be sensed and then the
biological motor intention has to be decoded and finally the resulting
information has to be communicated to the controller of the robot. An accurate
detection of the motor intention requires a pattern recognition based
categorical identification. Hence in this paper, we propose an improved
classification framework by identification of the relevant features that drive
the pattern recognition algorithm. Major contributions include a set of
modified spectral moment based features and another relevant inter-channel
correlation feature that contribute to an improved classification performance.
Next, we conducted a sensitivity analysis of the classification algorithm to
different EMG channels. Finally, the classifier performance is compared to that
of the other state-of the art algorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03875</identifier>
 <datestamp>2019-04-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03875</id><created>2018-11-09</created><updated>2019-04-15</updated><authors><author><keyname>Eloff</keyname><forenames>Ryan</forenames></author><author><keyname>Engelbrecht</keyname><forenames>Herman A.</forenames></author><author><keyname>Kamper</keyname><forenames>Herman</forenames></author></authors><title>Multimodal One-Shot Learning of Speech and Images</title><categories>cs.CL cs.CV cs.LG eess.AS</categories><comments>5 pages, 1 figure, 3 tables; accepted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imagine a robot is shown new concepts visually together with spoken tags,
e.g. &quot;milk&quot;, &quot;eggs&quot;, &quot;butter&quot;. After seeing one paired audio-visual example per
class, it is shown a new set of unseen instances of these objects, and asked to
pick the &quot;milk&quot;. Without receiving any hard labels, could it learn to match the
new continuous speech input to the correct visual instance? Although unimodal
one-shot learning has been studied, where one labelled example in a single
modality is given per class, this example motivates multimodal one-shot
learning. Our main contribution is to formally define this task, and to propose
several baseline and advanced models. We use a dataset of paired spoken and
visual digits to specifically investigate recent advances in Siamese
convolutional neural networks. Our best Siamese model achieves twice the
accuracy of a nearest neighbour model using pixel-distance over images and
dynamic time warping over speech in 11-way cross-modal matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.03911</identifier>
 <datestamp>2018-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.03911</id><created>2018-11-09</created><authors><author><keyname>Zhao</keyname><forenames>Ji</forenames></author><author><keyname>Chen</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author><author><keyname>Jin</keyname><forenames>Xin</forenames></author></authors><title>Unsupervised Learnable Sinogram Inpainting Network (SIN) for Limited
  Angle CT reconstruction</title><categories>physics.med-ph eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a sinogram inpainting network (SIN) to solve
limited-angle CT reconstruction problem, which is a very challenging ill-posed
issue and of great interest for several clinical applications. A common
approach to the problem is an iterative reconstruction algorithm with
regularization term, which can suppress artifacts and improve image quality,
but requires high computational cost.
  The starting point of this paper is the proof of inpainting function for
limited-angle sinogram is continuous, which can be approached by neural
networks in a data-driven method, granted by the universal approximation
theorem. Based on this, we propose SIN as the fitting function -- a
convolutional neural network trained to generate missing sinogram data
conditioned on scanned data. Besides CNN module, we design two differentiable
and rapid modules, Radon and Inverse Radon Transformer network, to encapsulate
the physical model in the training procedure. They enable new joint loss
functions to optimize both sinogram and reconstructed image in sync, which
improved the image quality significantly. To tackle the labeled data bottleneck
in clinical research, we form a sinogram-image-sinogram closed loop, and the
difference between sinograms can be used as training loss. In this way, the
proposed network can be self-trained, with only limited-angle data for
unsupervised domain adaptation.
  We demonstrate the performance of our proposed network on parallel beam X-ray
CT in lung CT datasets from Data Science Bowl 2017 and the ability of
unsupervised transfer learning in Zubal's phantom. The proposed method performs
better than state-of-art method SART-TV in PSNR and SSIM metrics, with
noticeable visual improvements in reconstructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04033</identifier>
 <datestamp>2018-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04033</id><created>2018-11-09</created><authors><author><keyname>Seifert</keyname><forenames>Bastian</forenames></author><author><keyname>H&#xfc;per</keyname><forenames>Knut</forenames></author></authors><title>The discrete cosine transform on triangles</title><categories>math.NA cs.NA eess.SP</categories><comments>4 pages, 2 figures, submitted conference paper,</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discrete cosine transform is a valuable tool in analysis of data on
undirected rectangular grids, like images. In this paper it is shown how one
can define an analogue of the discrete cosine transform on triangles. This is
done by combining algebraic signal processing theory with a specific kind of
multivariate Chebyshev polynomials. Using a multivariate Christoffel-Darboux
formula it is shown how to derive an orthogonal version of the transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04048</identifier>
 <datestamp>2018-11-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04048</id><created>2018-11-09</created><authors><author><keyname>Kothinti</keyname><forenames>Sandeep</forenames></author><author><keyname>Imoto</keyname><forenames>Keisuke</forenames></author><author><keyname>Chakrabarty</keyname><forenames>Debmalya</forenames></author><author><keyname>Sell</keyname><forenames>Gregory</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Elhilali</keyname><forenames>Mounya</forenames></author></authors><title>Joint Acoustic and Class Inference for Weakly Supervised Sound Event
  Detection</title><categories>eess.AS cs.SD</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sound event detection is a challenging task, especially for scenes with
multiple simultaneous events. While event classification methods tend to be
fairly accurate, event localization presents additional challenges, especially
when large amounts of labeled data are not available. Task4 of the 2018 DCASE
challenge presents an event detection task that requires accuracy in both
segmentation and recognition of events while providing only weakly labeled
training data. Supervised methods can produce accurate event labels but are
limited in event segmentation when training data lacks event timestamps. On the
other hand, unsupervised methods that model the acoustic properties of the
audio can produce accurate event boundaries but are not guided by the
characteristics of event classes and sound categories. We present a hybrid
approach that combines an acoustic-driven event boundary detection and a
supervised label inference using a deep neural network. This framework
leverages benefits of both unsupervised and supervised methodologies and takes
advantage of large amounts of unlabeled data, making it ideal for large-scale
weakly labeled event detection. Compared to a baseline system, the proposed
approach delivers a 15% absolute improvement in F-score, demonstrating the
benefits of the hybrid bottom-up, top-down approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04076</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04076</id><created>2018-11-09</created><authors><author><keyname>Tanaka</keyname><forenames>Kou</forenames></author><author><keyname>Kameoka</keyname><forenames>Hirokazu</forenames></author><author><keyname>Kaneko</keyname><forenames>Takuhiro</forenames></author><author><keyname>Hojo</keyname><forenames>Nobukatsu</forenames></author></authors><title>AttS2S-VC: Sequence-to-Sequence Voice Conversion with Attention and
  Context Preservation Mechanisms</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>Submitted to ICASSP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method based on a sequence-to-sequence learning
(Seq2Seq) with attention and context preservation mechanism for voice
conversion (VC) tasks. Seq2Seq has been outstanding at numerous tasks involving
sequence modeling such as speech synthesis and recognition, machine
translation, and image captioning. In contrast to current VC techniques, our
method 1) stabilizes and accelerates the training procedure by considering
guided attention and proposed context preservation losses, 2) allows not only
spectral envelopes but also fundamental frequency contours and durations of
speech to be converted, 3) requires no context information such as phoneme
labels, and 4) requires no time-aligned source and target speech data in
advance. In our experiment, the proposed VC framework can be trained in only
one day, using only one GPU of an NVIDIA Tesla K80, while the quality of the
synthesized speech is higher than that of speech converted by Gaussian mixture
model-based VC and is comparable to that of speech generated by recurrent
neural network-based text-to-speech synthesis, which can be regarded as an
upper limit on VC performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04133</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04133</id><created>2018-11-09</created><authors><author><keyname>Tzinis</keyname><forenames>Efthymios</forenames></author><author><keyname>Paraskevopoulos</keyname><forenames>Georgios</forenames></author><author><keyname>Baziotis</keyname><forenames>Christos</forenames></author><author><keyname>Potamianos</keyname><forenames>Alexandros</forenames></author></authors><title>Integrating Recurrence Dynamics for Speech Emotion Recognition</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><journal-ref>Proc. Interspeech 2018, pp. 927-931</journal-ref><doi>10.21437/Interspeech.2018-1377</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the performance of features that can capture nonlinear
recurrence dynamics embedded in the speech signal for the task of Speech
Emotion Recognition (SER). Reconstruction of the phase space of each speech
frame and the computation of its respective Recurrence Plot (RP) reveals
complex structures which can be measured by performing Recurrence
Quantification Analysis (RQA). These measures are aggregated by using
statistical functionals over segment and utterance periods. We report SER
results for the proposed feature set on three databases using different
classification methods. When fusing the proposed features with traditional
feature sets, we show an improvement in unweighted accuracy of up to 5.7% and
10.7% on Speaker-Dependent (SD) and Speaker-Independent (SI) SER tasks,
respectively, over the baseline. Following a segment-based approach we
demonstrate state-of-the-art performance on IEMOCAP using a Bidirectional
Recurrent Neural Network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04137</identifier>
 <datestamp>2019-11-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04137</id><created>2018-11-09</created><updated>2019-02-26</updated><authors><author><keyname>Krishnan</keyname><forenames>Joshin P.</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9; M.</forenames></author></authors><title>SURE-fuse WFF: A Multi-resolution Windowed Fourier Analysis for
  Interferometric Phase Denoising</title><categories>eess.SP</categories><doi>10.1109/ACCESS.2019.2936991</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interferometric phase (InPhase) imaging is an important part of many
present-day coherent imaging technologies. Often in such imaging techniques,
the acquired images, known as interferograms, suffer from two major
degradations: 1) phase wrapping caused by the fact that the sensing mechanism
can only measure sinusoidal $2\pi$-periodic functions of the actual phase, and
2) noise introduced by the acquisition process or the system. This work
focusses on InPhase denoising which is a fundamental restoration step to many
posterior applications of InPhase, namely to phase unwrapping. The presence of
sharp fringes that arises from phase wrapping makes InPhase denoising a
hard-inverse problem. Motivated by the fact that the InPhase images are often
locally sparse in Fourier domain, we propose a multi-resolution windowed
Fourier filtering (WFF) analysis that fuses WFF estimates with different
resolutions, thus overcoming the WFF fixed resolution limitation. The proposed
fusion relies on an unbiased estimate of the mean square error derived using
the Stein's lemma adapted to complex-valued signals. This estimate, known as
SURE, is minimized using an optimization framework to obtain the fusion
weights. Strong experimental evidence, using synthetic and real (InSAR &amp; MRI)
data, that the developed algorithm, termed as SURE-fuse WFF, outperforms the
best hand-tuned fixed resolution WFF as well as other state-of-the-art InPhase
denoising algorithms, is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04139</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04139</id><created>2018-11-09</created><authors><author><keyname>Orife</keyname><forenames>Iroro</forenames></author><author><keyname>Walker</keyname><forenames>Shane</forenames></author><author><keyname>Flaks</keyname><forenames>Jason</forenames></author></authors><title>Audio Spectrogram Factorization for Classification of Telephony Signals
  below the Auditory Threshold</title><categories>cs.SD eess.AS</categories><comments>7 pages, 4 figures. Marchex Technical Report on VoIP SPAM
  classification</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic Pumping attacks are a form of high-volume SPAM that target telephone
networks, defraud customers and squander telephony resources. One type of call
in these attacks is characterized by very low-amplitude signal levels, notably
below the auditory threshold. We propose a technique to classify so-called
&quot;dead air&quot; or &quot;silent&quot; SPAM calls based on features derived from factorizing
the caller audio spectrogram. We describe the algorithms for feature extraction
and classification as well as our data collection methods and production
performance on millions of calls per week.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04205</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04205</id><created>2018-11-10</created><authors><author><keyname>Hamzi</keyname><forenames>Boumediene</forenames></author><author><keyname>Abed</keyname><forenames>Eyad</forenames></author></authors><title>A Note on Local Mode-in-State Participation Factors for Nonlinear
  Systems</title><categories>cs.SY eess.SP math.DS math.OC</categories><msc-class>93-XX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies an extension to nonlinear systems of a recently proposed
approach to the concept of modal participation factors. First, a definition is
given for local mode-in-state participation factors for smooth nonlinear
autonomous systems. The definition is general, and, unlike in the more
traditional approach, the resulting participation measures depend on the
assumed uncertainty law governing the system initial condition. The work
follows Hashlamoun, Hassouneh and Abed (2009) in taking a mathematical
expectation (or set-theoretic average) of a modal contribution measure with
respect to an assumed uncertain initial state. As in the linear case, it is
found that a symmetry assumption on the distribution of the initial state
results in a tractable calculation and an explicit and simple formula for
mode-in-state participation factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04224</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04224</id><created>2018-11-10</created><authors><author><keyname>Shen</keyname><forenames>Yih-Liang</forenames></author><author><keyname>Huang</keyname><forenames>Chao-Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Syu-Siang</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Hsin-Min</forenames></author><author><keyname>Chi</keyname><forenames>Tai-Shih</forenames></author></authors><title>Reinforcement Learning Based Speech Enhancement for Robust Speech
  Recognition</title><categories>eess.AS cs.SD</categories><comments>Conference paper with 4 pages, reinforcement learning, automatic
  speech recognition, speech enhancement, deep neural network, character error
  rate</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional deep neural network (DNN)-based speech enhancement (SE)
approaches aim to minimize the mean square error (MSE) between enhanced speech
and clean reference. The MSE-optimized model may not directly improve the
performance of an automatic speech recognition (ASR) system. If the target is
to minimize the recognition error, the recognition results should be used to
design the objective function for optimizing the SE model. However, the
structure of an ASR system, which consists of multiple units, such as acoustic
and language models, is usually complex and not differentiable. In this study,
we proposed to adopt the reinforcement learning algorithm to optimize the SE
model based on the recognition results. We evaluated the propsoed SE system on
the Mandarin Chinese broadcast news corpus (MATBN). Experimental results
demonstrate that the proposed method can effectively improve the ASR results
with a notable 12.40% and 19.23% error rate reductions for signal to noise
ratio at 0 dB and 5 dB conditions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04230</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04230</id><created>2018-11-10</created><authors><author><keyname>Pratiher</keyname><forenames>Sawon</forenames></author><author><keyname>Chattoraj</keyname><forenames>Subhankar</forenames></author><author><keyname>Mukherjee</keyname><forenames>Rajdeep</forenames></author></authors><title>StationPlot: A New Non-stationarity Quantification Tool for Detection of
  Epileptic Seizures</title><categories>eess.SP cs.CV q-bio.NC</categories><comments>This paper is accepted for presentation at IEEE Global Conference on
  Signal and Information Processing (IEEE GlobalSIP), California, USA, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel non-stationarity visualization tool known as StationPlot is developed
for deciphering the chaotic behavior of a dynamical time series. A family of
analytic measures enumerating geometrical aspects of the non-stationarity &amp;
degree of variability is formulated by convex hull geometry (CHG) on
StationPlot. In the Euclidean space, both trend-stationary (TS) &amp;
difference-stationary (DS) perturbations are comprehended by the asymmetric
structure of StationPlot's region of interest (ROI). The proposed method is
experimentally validated using EEG signals, where it comprehend the relative
temporal evolution of neural dynamics &amp; its non-stationary morphology, thereby
exemplifying its diagnostic competence for seizure activity (SA) detection.
Experimental results &amp; analysis-of-Variance (ANOVA) on the extracted CHG
features demonstrates better classification performances as compared to the
existing shallow feature based state-of-the-art &amp; validates its efficacy as
geometry-rich discriminative descriptors for signal processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04258</identifier>
 <datestamp>2019-07-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04258</id><created>2018-11-10</created><updated>2019-07-06</updated><authors><author><keyname>Faradonbeh</keyname><forenames>Mohamad Kazem Shirani</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>Input Perturbations for Adaptive Control and Learning</title><categories>eess.SY cs.LG cs.RO cs.SY math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies adaptive algorithms for simultaneous regulation and
estimation of MIMO linear dynamical systems. Efficient practical control
policies that utilize input signals perturbations are designed and analyzed. We
show that a perturbed greedy algorithm guarantees non-asymptotic regret bounds
of (nearly) square-root magnitude with respect to time. More generally, we
establish high probability finite time bounds on both the regret and the
learning accuracy under arbitrary input perturbations. The settings where
greedy policies attain the information theoretic lower bound of logarithmic
regret are also discussed. To obtain the results, state-of-the-art tools from
martingale theory together with the recently introduced method of policy
decomposition are leveraged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04347</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04347</id><created>2018-11-10</created><authors><author><keyname>Khan</keyname><forenames>Irfan</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Vikram</forenames></author><author><keyname>Xu</keyname><forenames>Yinliang</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Chow</keyname><forenames>Mo-Yuen</forenames></author></authors><title>Compressive Sensing and Morphology Singular Entropy-Based Real-time
  Secondary Voltage Control of Multi-area Power Systems</title><categories>eess.SP</categories><comments>10 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an improved secondary voltage control (SVC) methodology
incorporating compressive sensing (CS) for a multi-area power system. SVC
minimizes the voltage deviation of the load buses while CS deals with the
problem of the limited bandwidth capacity of the communication channel by
reducing the size of massive data output from phasor measurement unit (PMU)
based monitoring system. The proposed strategy further incorporates the
application of a Morphological Median Filter (MMF) to reduce noise from the
output of the PMUs. To keep the control area secure and protected locally,
Mathematical Singular Entropy (MSE) based fault identification approach is
utilized for fast discovery of faults in the control area. Simulation results
with 27-bus and 486-bus power systems show that CS can reduce the data size up
to 1/10th while the MSE based fault identification technique can accurately
distinguish between fault and steady state conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04356</identifier>
 <datestamp>2019-02-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04356</id><created>2018-11-11</created><updated>2019-02-24</updated><authors><author><keyname>Lan</keyname><forenames>Xinjie</forenames></author><author><keyname>Guo</keyname><forenames>Xin</forenames></author><author><keyname>Barner</keyname><forenames>Kenneth E.</forenames></author></authors><title>Bayesian Convolutional Neural Networks for Compressed Sensing
  Restoration</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Networks (DNNs) have aroused great attention in Compressed
Sensing (CS) restoration. However, the working mechanism of DNNs is not
explainable, thereby it is unclear that how to design an optimal DNNs for CS
restoration. In this paper, we propose a novel statistical framework to explain
DNNs, which proves that the hidden layers of DNNs are equivalent to Gibbs
distributions and interprets DNNs as a Bayesian hierarchical model. The
framework provides a Bayesian perspective to explain the working mechanism of
DNNs, namely some hidden layers learn a prior distribution and other layers
learn a likelihood distribution. Moreover, the framework provides insights into
DNNs and reveals two inherent limitations of DNNs for CS restoration. In
contrast to most previous works designing an end-to-end DNNs for CS
restoration, we propose a novel DNNs to model a prior distribution only, which
can circumvent the limitations of DNNs. Given the prior distribution generated
from the DNNs, we design a Bayesian inference algorithm to realize CS
restoration in the framework of Bayesian Compressed Sensing. Finally, extensive
simulations validate the proposed theory of DNNs and demonstrate that the
proposed algorithm outperforms the state-of-the-art CS restoration methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04357</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04357</id><created>2018-11-11</created><authors><author><keyname>Wang</keyname><forenames>Bryan</forenames></author><author><keyname>Yang</keyname><forenames>Yi-Hsuan</forenames></author></authors><title>PerformanceNet: Score-to-Audio Music Generation with Multi-Band
  Convolutional Residual Network</title><categories>cs.SD cs.MM eess.AS</categories><comments>8 pages, 6 figures, AAAI 2019 camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Music creation is typically composed of two parts: composing the musical
score, and then performing the score with instruments to make sounds. While
recent work has made much progress in automatic music generation in the
symbolic domain, few attempts have been made to build an AI model that can
render realistic music audio from musical scores. Directly synthesizing audio
with sound sample libraries often leads to mechanical and deadpan results,
since musical scores do not contain performance-level information, such as
subtle changes in timing and dynamics. Moreover, while the task may sound like
a text-to-speech synthesis problem, there are fundamental differences since
music audio has rich polyphonic sounds. To build such an AI performer, we
propose in this paper a deep convolutional model that learns in an end-to-end
manner the score-to-audio mapping between a symbolic representation of music
called the piano rolls and an audio representation of music called the
spectrograms. The model consists of two subnets: the ContourNet, which uses a
U-Net structure to learn the correspondence between piano rolls and
spectrograms and to give an initial result; and the TextureNet, which further
uses a multi-band residual network to refine the result by adding the spectral
texture of overtones and timbre. We train the model to generate music clips of
the violin, cello, and flute, with a dataset of moderate size. We also present
the result of a user study that shows our model achieves higher mean opinion
score (MOS) in naturalness and emotional expressivity than a WaveNet-based
model and two commercial sound libraries. We open our source code at
https://github.com/bwang514/PerformanceNet
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04419</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04419</id><created>2018-11-11</created><authors><author><keyname>Schindler</keyname><forenames>Alexander</forenames></author><author><keyname>Lidy</keyname><forenames>Thomas</forenames></author><author><keyname>Rauber</keyname><forenames>Andreas</forenames></author></authors><title>Multi-Temporal Resolution Convolutional Neural Networks for Acoustic
  Scene Classification</title><categories>cs.SD cs.MM eess.AS</categories><comments>In Proceedings of the Detection and Classification of Acoustic Scenes
  and Events 2017 Workshop (DCASE2017), November 2017</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a Deep Neural Network architecture for the task of
acoustic scene classification which harnesses information from increasing
temporal resolutions of Mel-Spectrogram segments. This architecture is composed
of separated parallel Convolutional Neural Networks which learn spectral and
temporal representations for each input resolution. The resolutions are chosen
to cover fine-grained characteristics of a scene's spectral texture as well as
its distribution of acoustic events. The proposed model shows a 3.56% absolute
improvement of the best performing single resolution model and 12.49% of the
DCASE 2017 Acoustic Scenes Classification task baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04448</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04448</id><created>2018-11-11</created><authors><author><keyname>Fazeka</keyname><forenames>Botond</forenames></author><author><keyname>Schindler</keyname><forenames>Alexander</forenames></author><author><keyname>Lidy</keyname><forenames>Thomas</forenames></author><author><keyname>Rauber</keyname><forenames>Andreas</forenames></author></authors><title>A Multi-modal Deep Neural Network approach to Bird-song identification</title><categories>cs.SD eess.AS</categories><comments>LifeCLEF 2017 working notes, Dublin, Ireland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a multi-modal Deep Neural Network (DNN) approach for bird song
identification. The presented approach takes both audio samples and metadata as
input. The audio is fed into a Convolutional Neural Network (CNN) using four
convolutional layers. The additionally provided metadata is processed using
fully connected layers. The flattened convolutional layers and the fully
connected layer of the metadata are joined and fed into a fully connected
layer. The resulting architecture achieved 2., 3. and 4. rank in the
BirdCLEF2017 task in various training configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04460</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04460</id><created>2018-11-11</created><authors><author><keyname>Kotzagiannidis</keyname><forenames>Madeleine S.</forenames></author><author><keyname>Davies</keyname><forenames>Mike E.</forenames></author></authors><title>Analysis vs Synthesis - An Investigation of (Co)sparse Signal Models on
  Graphs</title><categories>eess.SP cs.DM</categories><comments>IEEE GlobalSIP 2018. An extended version of this work can be found at
  arXiv:1811.04493</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a theoretical study of signals with sparse
representations in the vertex domain of a graph, which is primarily motivated
by the discrepancy arising from respectively adopting a synthesis and analysis
view of the graph Laplacian matrix. Sparsity on graphs and, in particular, the
characterization of the subspaces of signals which are sparse with respect to
the connectivity of the graph, as induced by analysis with a suitable graph
operator, remains in general an opaque concept which we aim to elucidate. By
leveraging the theory of cosparsity, we present a novel (co)sparse graph
Laplacian-based signal model and characterize the underlying (structured)
(co)sparsity, smoothness and localization of its solution subspaces on
undirected graphs, while providing more refined statements for special cases
such as circulant graphs. Ultimately, we substantiate fundamental discrepancies
between the cosparse analysis and sparse synthesis models in this structured
setting, by demonstrating that the former constitutes a special, constrained
instance of the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04463</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04463</id><created>2018-11-11</created><authors><author><keyname>Hamid</keyname><forenames>Kanza</forenames></author><author><keyname>Asif</keyname><forenames>Amina</forenames></author><author><keyname>Abbasi</keyname><forenames>Wajid</forenames></author><author><keyname>Sabih</keyname><forenames>Durre</forenames></author><author><keyname>Minhas</keyname><forenames>Fayyaz</forenames></author></authors><title>Machine Learning with Abstention for Automated Liver Disease Diagnosis</title><categories>cs.LG cs.CV eess.IV stat.ML</categories><comments>Preprint version before submission for publication. complete version
  published in proc. 15th International Conference on Frontiers of Information
  Technology (FIT 2017), December 18-20, 2017, Islamabad, Pakistan.
  http://ieeexplore.ieee.org/document/8261064/</comments><journal-ref>15th IEEE International Conference on Frontiers of Information
  Technology (FIT 2017), December 18-20, 2017, Islamabad, Pakistan</journal-ref><doi>10.1109/FIT.2017.00070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach for detection of liver abnormalities in
an automated manner using ultrasound images. For this purpose, we have
implemented a machine learning model that can not only generate labels (normal
and abnormal) for a given ultrasound image but it can also detect when its
prediction is likely to be incorrect. The proposed model abstains from
generating the label of a test example if it is not confident about its
prediction. Such behavior is commonly practiced by medical doctors who, when
given insufficient information or a difficult case, can chose to carry out
further clinical or diagnostic tests before generating a diagnosis. However,
existing machine learning models are designed in a way to always generate a
label for a given example even when the confidence of their prediction is low.
We have proposed a novel stochastic gradient based solver for the learning with
abstention paradigm and use it to make a practical, state of the art method for
liver disease classification. The proposed method has been benchmarked on a
data set of approximately 100 patients from MINAR, Multan, Pakistan and our
results show that the proposed scheme offers state of the art classification
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04521</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04521</id><created>2018-11-11</created><authors><author><keyname>Hanna</keyname><forenames>Samer S.</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Deep Learning Based Transmitter Identification using Power Amplifier
  Nonlinearity</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The imperfections in the RF frontend of different transmitters can be used to
distinguish them. This process is called transmitter identification using RF
fingerprints. The nonlinearity in the power amplifier of the RF frontend is a
significant cause of the discrepancy in RF fingerprints, which enables
transmitter identification. In this work, we use deep learning to identify
different transmitters using their nonlinear characteristics. By developing a
nonlinear model generator based on extensive measurements, we were able to
extend the evaluation of transmitter identification to include a larger number
of transmitters beyond what exists in the literature. We were also able to
study the impact of transmitter variability on identification accuracy.
Additionally, many other factors were considered including modulation type,
length of data used for identification, and type of data being transmitted
whether identical or random under a realistic channel model. Simulation results
were compared with experiments which confirmed similar trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04568</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04568</id><created>2018-11-12</created><authors><author><keyname>Seki</keyname><forenames>Hiroshi</forenames></author><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author></authors><title>Vectorization of hypotheses and speech for faster beam search in encoder
  decoder-based speech recognition</title><categories>cs.SD cs.CL eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention-based encoder decoder network uses a left-to-right beam search
algorithm in the inference step. The current beam search expands hypotheses and
traverses the expanded hypotheses at the next time step. This traversal is
implemented using a for-loop program in general, and it leads to speed down of
the recognition process. In this paper, we propose a parallelism technique for
beam search, which accelerates the search process by vectorizing multiple
hypotheses to eliminate the for-loop program. We also propose a technique to
batch multiple speech utterances for off-line recognition use, which reduces
the for-loop program with regard to the traverse of multiple utterances. This
extension is not trivial during beam search unlike during training due to
several pruning and thresholding techniques for efficient decoding. In
addition, our method can combine scores of external modules, RNNLM and CTC, in
a batch as shallow fusion. We achieved 3.7 x speedup compared with the original
beam search algorithm by vectoring hypotheses, and achieved 10.5 x speedup by
further changing processing unit to GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04610</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04610</id><created>2018-11-12</created><authors><author><keyname>Ozolins</keyname><forenames>Oskars</forenames></author><author><keyname>Estaran</keyname><forenames>Jos&#xe9; Manuel</forenames></author><author><keyname>Udalcovs</keyname><forenames>Aleksejs</forenames></author><author><keyname>Jorge</keyname><forenames>Filipe</forenames></author><author><keyname>Mardoyan</keyname><forenames>Haik</forenames></author><author><keyname>Konczykowska</keyname><forenames>Agnieszka</forenames></author><author><keyname>Riet</keyname><forenames>Muriel</forenames></author><author><keyname>Duval</keyname><forenames>Bernadette</forenames></author><author><keyname>Nodjiadjim</keyname><forenames>Virginie</forenames></author><author><keyname>Dupuy</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Pang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Westergren</keyname><forenames>Urban</forenames></author><author><keyname>Chen</keyname><forenames>Jiajia</forenames></author><author><keyname>Popov</keyname><forenames>Sergei</forenames></author><author><keyname>Bigo</keyname><forenames>S&#xe9;bastien</forenames></author></authors><title>140 Gbaud On-Off Keying Links in C-Band for Short-Reach Optical
  Interconnects</title><categories>eess.SP</categories><comments>3 pages, 44th European Conference on Optical Communication (ECOC
  2018), Rome, Italy, 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate 140 Gbaud intensity modulated direct detection
dispersion-uncompensated links with Mach Zehnder modulator and distributed
feedback travelling-wave electro-absorption modulator over 5500 and 960 meters
of standard single mode fibre, respectively, enabled by compact packaged
ultra-high speed InP-based 2:1-Selector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04615</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04615</id><created>2018-11-12</created><authors><author><keyname>Zhao</keyname><forenames>Hui</forenames></author><author><keyname>Liu</keyname><forenames>Zhedong</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Different Power Adaption Methods on Fluctuating Two-Ray Fading Channels</title><categories>eess.SP</categories><comments>4 pages, 1 figure</comments><doi>10.1109/LWC.2018.2881158</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we consider a typical scenario where the transmitter employs
different power adaption methods, including the optimal rate and power
algorithm, optimal rate adaption, channel inversion and truncated channel
inversion, to enhance the ergodic capacity (EC) with an average transmit power
constraint over fluctuating two-way fading channels. In particular, we derive
exact closed-form expressions for the EC under different power adaption
methods, as well as corresponding asymptotic formulas for the EC valid in the
high signal-to-noise ratio (SNR) region. Finally, we compare the performance of
the EC under different power adaption methods, and this also validates the
accuracy of our derived expressions for the exact and asymptotic EC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04626</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04626</id><created>2018-11-12</created><authors><author><keyname>Lim</keyname><forenames>Jonathan</forenames></author><author><keyname>Stanley-Marbell</keyname><forenames>Phillip</forenames></author></authors><title>Newton: A Language for Describing Physics</title><categories>cs.PL eess.SP physics.ins-det</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces Newton, a specification language for notating the
analytic form, units of measure, and sensor signal properties for
physical-object-specific invariants and general physical laws. We designed
Newton to provide a means for hardware designers (e.g., sensor integrated
circuit manufacturers, computing hardware architects, or mechanical engineers)
to specify properties of the physical environments in which embedded computing
systems will be deployed (e.g., a sensing platform deployed on a bridge versus
worn by a human). Compilers and other program analysis tools for embedded
systems can use a library interface to the Newton compiler to obtain
information about the sensors, sensor signals, and inter-signal relationships
imposed by the structure and materials properties of a given physical system.
The information encoded within Newton specifications could enable new
compile-time transformations that exploit information about the physical world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04634</identifier>
 <datestamp>2019-05-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04634</id><created>2018-11-12</created><updated>2019-05-17</updated><authors><author><keyname>Ozdemir</keyname><forenames>Firat</forenames></author><author><keyname>Goksel</keyname><forenames>Orcun</forenames></author></authors><title>Extending Pretrained Segmentation Networks with Additional Anatomical
  Structures</title><categories>eess.IV cs.LG stat.ML</categories><comments>Published in IJCARS. 8 pages, 4 figures, contains supplementary
  material</comments><journal-ref>International Journal of Computer Assisted Radiology and Surgery,
  2 May 2019, issn 1861-6429</journal-ref><doi>10.1007/s11548-019-01984-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comprehensive surgical planning require complex patient-specific anatomical
models. For instance, functional muskuloskeletal simulations necessitate all
relevant structures to be segmented, which could be performed in real-time
using deep neural networks given sufficient annotated samples. Such large
datasets of multiple structure annotations are costly to procure and are often
unavailable in practice. Nevertheless, annotations from different studies and
centers can be readily available, or become available in the future in an
incremental fashion. We propose a class-incremental segmentation framework for
extending a deep network trained for some anatomical structure to yet another
structure using a small incremental annotation set. Through distilling
knowledge from the current state of the framework, we bypass the need for a
full retraining. This is a meta-method to extend any choice of desired deep
segmentation network with only a minor addition per structure, which makes it
suitable for lifelong class-incremental learning and applicable also for future
deep neural network architectures. We evaluated our methods on a public knee
dataset of 100 MR volumes. Through varying amount of incremental annotation
ratios, we show how our proposed method can retain the previous anatomical
structure segmentation performance superior to the conventional finetuning
approach. In addition, our framework inherently exploits transferable knowledge
from previously trained structures to incremental tasks, demonstrated by
superior results compared to non-incremental training. With the presented
method, new anatomical structures can be learned without catastrophic
forgetting of older structures and without extensive increase of memory and
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04635</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04635</id><created>2018-11-12</created><authors><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Tataria</keyname><forenames>Harsh</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author></authors><title>Massive MIMO with a Generalized Channel Model: Fundamental Aspects</title><categories>eess.SP</categories><comments>IEEE ICC 2019, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) is becoming a mature
technology, and has been approved for standardization in the 5G ecosystem.
Although there is a large body of papers on the theoretical analysis of massive
MIMO, the majority of relevant work assumes the simplified, yet overly
idealistic, Kronecker-type model for spatial correlation. Motivated by the
deficiencies of the Kronecker model, we invoke a naturally generalized spatial
correlation model, that is the Weichselberger model. For this model, we pursue
a comprehensive analysis of massive MIMO performance in terms of channel
hardening and favorable propagation (FP). We identify a number of scenarios
under which massive MIMO may fail, in terms of channel hardening and FP, and
discuss their relevance from a practical perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04662</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04662</id><created>2018-11-12</created><authors><author><keyname>Cooray</keyname><forenames>Navin</forenames><affiliation>University of Oxford, Institute of Biomedical Engineering, Dept. Engineering Sciences, Oxford, UK</affiliation></author><author><keyname>Andreotti</keyname><forenames>Fernando</forenames><affiliation>University of Oxford, Institute of Biomedical Engineering, Dept. Engineering Sciences, Oxford, UK</affiliation></author><author><keyname>Lo</keyname><forenames>Christine</forenames><affiliation>Nuffield Department of Clinical Neurosciences, Oxford Parkinson's Disease Centre</affiliation></author><author><keyname>Symmonds</keyname><forenames>Mkael</forenames><affiliation>Department of Clinical Neurophysiology, Oxford University Hospitals, John Radcliffe Hospital, University of Oxford, UK</affiliation></author><author><keyname>Hu</keyname><forenames>Michele T. M.</forenames><affiliation>Nuffield Department of Clinical Neurosciences, Oxford Parkinson's Disease Centre</affiliation></author><author><keyname>De Vos</keyname><forenames>Maarten</forenames><affiliation>University of Oxford, Institute of Biomedical Engineering, Dept. Engineering Sciences, Oxford, UK</affiliation></author></authors><title>Detection of REM Sleep Behaviour Disorder by Automated Polysomnography
  Analysis</title><categories>cs.LG eess.SP q-bio.NC stat.ML</categories><comments>20 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Evidence suggests Rapid-Eye-Movement (REM) Sleep Behaviour Disorder (RBD) is
an early predictor of Parkinson's disease. This study proposes a
fully-automated framework for RBD detection consisting of automated sleep
staging followed by RBD identification. Analysis was assessed using a limited
polysomnography montage from 53 participants with RBD and 53 age-matched
healthy controls. Sleep stage classification was achieved using a Random Forest
(RF) classifier and 156 features extracted from electroencephalogram (EEG),
electrooculogram (EOG) and electromyogram (EMG) channels. For RBD detection, a
RF classifier was trained combining established techniques to quantify muscle
atonia with additional features that incorporate sleep architecture and the EMG
fractal exponent. Automated multi-state sleep staging achieved a 0.62 Cohen's
Kappa score. RBD detection accuracy improved by 10% to 96% (compared to
individual established metrics) when using manually annotated sleep staging.
Accuracy remained high (92%) when using automated sleep staging. This study
outperforms established metrics and demonstrates that incorporating sleep
architecture and sleep stage transitions can benefit RBD detection. This study
also achieved automated sleep staging with a level of accuracy comparable to
manual annotation. This study validates a tractable, fully-automated, and
sensitive pipeline for RBD identification that could be translated to wearable
take-home technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04761</identifier>
 <datestamp>2019-05-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04761</id><created>2018-11-05</created><updated>2019-05-20</updated><authors><author><keyname>Liu</keyname><forenames>Hong</forenames></author><author><keyname>Ye</keyname><forenames>Hanrong</forenames></author><author><keyname>Li</keyname><forenames>Xia</forenames></author><author><keyname>Shi</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Mengyuan</forenames></author><author><keyname>Sun</keyname><forenames>Qianru</forenames></author></authors><title>Self-Refining Deep Symmetry Enhanced Network for Rain Removal</title><categories>eess.IV</categories><comments>Accepted by ICIP 19. Corresponding and contact author: Hanrong Ye</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Rain removal aims to remove the rain streaks on rain images. The
state-of-the-art methods are mostly based on Convolutional Neural
Network~(CNN). However, as CNN is not equivariant to object rotation, these
methods are unsuitable for dealing with the tilted rain streaks. To tackle this
problem, we propose Deep Symmetry Enhanced Network~(DSEN) that is able to
explicitly extract the rotation equivariant features from rain images. In
addition, we design a self-refining mechanism to remove the accumulated rain
streaks in a coarse-to-fine manner. This mechanism reuses DSEN with a novel
information link which passes the gradient flow to the higher stages. Extensive
experiments on both synthetic and real-world rain images show that our
self-refining DSEN yields the top performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04769</identifier>
 <datestamp>2019-08-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04769</id><created>2018-11-09</created><updated>2019-08-21</updated><authors><author><keyname>Song</keyname><forenames>Eunwoo</forenames></author><author><keyname>Byun</keyname><forenames>Kyungguen</forenames></author><author><keyname>Kang</keyname><forenames>Hong-Goo</forenames></author></authors><title>ExcitNet vocoder: A neural excitation model for parametric speech
  synthesis systems</title><categories>eess.AS cs.LG cs.SD</categories><comments>Accepted to the conference of EUSIPCO 2019. arXiv admin note: text
  overlap with arXiv:1811.03311</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a WaveNet-based neural excitation model (ExcitNet) for
statistical parametric speech synthesis systems. Conventional WaveNet-based
neural vocoding systems significantly improve the perceptual quality of
synthesized speech by statistically generating a time sequence of speech
waveforms through an auto-regressive framework. However, they often suffer from
noisy outputs because of the difficulties in capturing the complicated
time-varying nature of speech signals. To improve modeling efficiency, the
proposed ExcitNet vocoder employs an adaptive inverse filter to decouple
spectral components from the speech signal. The residual component, i.e.
excitation signal, is then trained and generated within the WaveNet framework.
In this way, the quality of the synthesized speech signal can be further
improved since the spectral component is well represented by a deep learning
framework and, moreover, the residual component is efficiently generated by the
WaveNet framework. Experimental results show that the proposed ExcitNet
vocoder, trained both speaker-dependently and speaker-independently,
outperforms traditional linear prediction vocoders and similarly configured
conventional WaveNet vocoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04772</identifier>
 <datestamp>2018-12-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04772</id><created>2018-11-12</created><updated>2018-11-30</updated><authors><author><keyname>Manerikar</keyname><forenames>Ankit</forenames></author><author><keyname>Prakash</keyname><forenames>Tanmay</forenames></author><author><keyname>Kak</keyname><forenames>Avinash C.</forenames></author></authors><title>Adaptive Target Recognition: A Case Study Involving Airport Baggage
  Screening</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses the question whether it is possible to design a
computer-vision based automatic threat recognition (ATR) system so that it can
adapt to changing specifications of a threat without having to create a new ATR
each time. The changes in threat specifications, which may be warranted by
intelligence reports and world events, are typically regarding the physical
characteristics of what constitutes a threat: its material composition, its
shape, its method of concealment, etc. Here we present our design of an AATR
system (Adaptive ATR) that can adapt to changing specifications in materials
characterization (meaning density, as measured by its x-ray attenuation
coefficient), its mass, and its thickness. Our design uses a two-stage cascaded
approach, in which the first stage is characterized by a high recall rate over
the entire range of possibilities for the threat parameters that are allowed to
change. The purpose of the second stage is to then fine-tune the performance of
the overall system for the current threat specifications. The computational
effort for this fine-tuning for achieving a desired PD/PFA rate is far less
than what it would take to create a new classifier with the same overall
performance for the new set of threat specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04791</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04791</id><created>2018-11-09</created><authors><author><keyname>Hermann</keyname><forenames>Enno</forenames></author><author><keyname>Kamper</keyname><forenames>Herman</forenames></author><author><keyname>Goldwater</keyname><forenames>Sharon</forenames></author></authors><title>Multilingual and Unsupervised Subword Modeling for Zero-Resource
  Languages</title><categories>eess.AS cs.CL</categories><comments>11 pages, 5 figures, 7 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible. arXiv admin note:
  substantial text overlap with arXiv:1803.08863</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised subword modeling aims to learn low-level representations of
speech audio in &quot;zero-resource&quot; settings: that is, without using transcriptions
or other resources from the target language (such as text corpora or
pronunciation dictionaries). A good representation should capture phonetic
content and abstract away from other types of variability, such as speaker
differences and channel noise. Previous work in this area has primarily focused
on learning from target language data only, and has been evaluated only
intrinsically. Here we directly compare multiple methods, including some that
use only target language speech data and some that use transcribed speech from
other (non-target) languages, and we evaluate using two intrinsic measures as
well as on a downstream unsupervised word segmentation and clustering task. We
find that combining two existing target-language-only methods yields better
features than either method alone. Nevertheless, even better results are
obtained by extracting target language bottleneck features using a model
trained on other languages. Cross-lingual training using just one other
language is enough to provide this benefit, but multilingual training helps
even more. In addition to these results, which hold across both intrinsic
measures and the extrinsic task, we discuss the qualitative differences between
the different types of learned features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04876</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04876</id><created>2018-11-12</created><authors><author><keyname>Chaudhry</keyname><forenames>Ritwick</forenames></author><author><keyname>Ghosh</keyname><forenames>Arunabh</forenames></author><author><keyname>Rajwade</keyname><forenames>Ajit</forenames></author></authors><title>Noise- and Outlier-Resistant Tomographic Reconstruction under Unknown
  Viewing Parameters</title><categories>eess.IV</categories><comments>8 pages, Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an algorithm for effectively reconstructing an
object from a set of its tomographic projections without any knowledge of the
viewing directions or any prior structural information, in the presence of
pathological amounts of noise, unknown shifts in the projections, and outliers
among the projections. The outliers are mainly in the form of a number of
projections of a completely different object, as compared to the object of
interest. We introduce a novel approach of first processing the projections,
then obtaining an initial estimate for the orientations and the shifts, and
then define a refinement procedure to obtain the final reconstruction. Even in
the presence of high noise variance (up to $50\%$ of the average value of the
(noiseless) projections) and presence of outliers, we are able to successfully
reconstruct the object. We also provide interesting empirical comparisons of
our method with the sparsity based optimization procedures that have been used
earlier for image reconstruction tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04903</identifier>
 <datestamp>2019-02-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04903</id><created>2018-11-12</created><updated>2019-02-18</updated><authors><author><keyname>Wang</keyname><forenames>Xiaofei</forenames></author><author><keyname>Li</keyname><forenames>Ruizhi</forenames></author><author><keyname>Mallid</keyname><forenames>Sri Harish</forenames></author><author><keyname>Hori</keyname><forenames>Takaaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Shinji</forenames></author><author><keyname>Hermansky</keyname><forenames>Hynek</forenames></author></authors><title>Stream attention-based multi-array end-to-end speech recognition</title><categories>cs.CL cs.SD eess.AS</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic Speech Recognition (ASR) using multiple microphone arrays has
achieved great success in the far-field robustness. Taking advantage of all the
information that each array shares and contributes is crucial in this task.
Motivated by the advances of joint Connectionist Temporal Classification
(CTC)/attention mechanism in the End-to-End (E2E) ASR, a stream attention-based
multi-array framework is proposed in this work. Microphone arrays, acting as
information streams, are activated by separate encoders and decoded under the
instruction of both CTC and attention networks. In terms of attention, a
hierarchical structure is adopted. On top of the regular attention networks,
stream attention is introduced to steer the decoder toward the most informative
encoders. Experiments have been conducted on AMI and DIRHA multi-array corpora
using the encoder-decoder architecture. Compared with the best single-array
results, the proposed framework has achieved relative Word Error Rates (WERs)
reduction of 3.7% and 9.7% in the two datasets, respectively, which is better
than conventional strategies as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04912</identifier>
 <datestamp>2018-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04912</id><created>2018-11-12</created><authors><author><keyname>Arafa</keyname><forenames>Ahmed</forenames></author><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Online Timely Status Updates with Erasures for Energy Harvesting Sensors</title><categories>cs.IT cs.NI eess.SP math.IT</categories><comments>Appeared at Allerton 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An energy harvesting sensor that is sending status updates to a destination
through an erasure channel is considered, in which transmissions are prone to
being erased with some probability $q$, independently from other transmissions.
The sensor, however, is unaware of erasure events due to lack of feedback from
the destination. Energy expenditure is normalized in the sense that one
transmission consumes one unit of energy. The sensor is equipped with a
unit-sized battery to save its incoming energy, which arrives according to a
Poisson process of unit rate. The setting is online, in which energy arrival
times are only revealed causally after being harvested, and the goal is to
design transmission times such that the long term average age of information
(AoI), defined as the time elapsed since the latest update has reached the
destination successfully, is minimized. The optimal status update policy is
first shown to have a renewal structure, in which the time instants at which
the destination receives an update successfully constitute a renewal process.
Then, for $q\leq\frac{1}{2}$, the optimal renewal policy is shown to have a
threshold structure, in which a new status update is transmitted only if the
AoI grows above a certain threshold, that is shown to be a decreasing function
of $q$. While for $q&gt;\frac{1}{2}$, the optimal renewal policy is shown to be
greedy, in which a new status update is transmitted whenever energy is
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.04962</identifier>
 <datestamp>2019-01-31</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.04962</id><created>2018-11-12</created><updated>2019-01-30</updated><authors><author><keyname>Laury</keyname><forenames>John</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Lars</forenames></author><author><keyname>Bollen</keyname><forenames>Math H. J</forenames></author></authors><title>A simplified static frequency converter model for electromechanical
  transient stability studies of 16$\frac{2}{3}$ Hz railways</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increased share of Static Frequency Converters (SFCs) in 16$\frac{2}{3}$
Hz railway grids concerns about stability have increased. Stability studies for
such low-frequency railway grids are few, and models that describe SFC dynamics
are especially few. This paper presents an open SFC model for electromechanical
stability studies in the phasor domain, suited for 16$\frac{2}{3}$ Hz
synchronous railway grids. The developed and proposed SFC model is implemented
in MatLab Simulink, together with grid and loads. Numerical studies are made,
in which the proposed SFC model is validated against both measured RMS-phasor
amplitude of voltage and current at the railway grid side of an SFC. The SFC
model developed is able to reproduce the measured RMS voltage and current with
an acceptable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05058</identifier>
 <datestamp>2018-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05058</id><created>2018-11-12</created><authors><author><keyname>Cabrera</keyname><forenames>Maria E.</forenames></author><author><keyname>Novak</keyname><forenames>Keisha</forenames></author><author><keyname>Foti</keyname><forenames>Dan</forenames></author><author><keyname>Voyles</keyname><forenames>Richard</forenames></author><author><keyname>Wachs</keyname><forenames>Juan P.</forenames></author></authors><title>Electrophysiological indicators of gesture perception</title><categories>q-bio.NC eess.SP</categories><comments>29 pages, 8 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: While there has been abundant research concerning neurological
responses to gesture generation, the time course of gesture processing is not
well understood. Specifically, it is not clear if or how particular
characteristics within the kinematic execution of gestures capture attention
and aid in the classification of gestures with communicative intent. If indeed
key features of gestures with perceptual saliency exist, such features could
help form the basis of a compact representation of the gestures in memory.
Methods: This study used a set of available gesture videos as stimuli. The
timing for salient features of performed gestures was determined by isolating
inflection points in the hands' motion trajectories. Participants passively
viewed the gesture videos while continuous EEG data was collected. We focused
on mu oscillations (10 Hz) and used linear regression to test for associations
between the timing of mu oscillations and inflection points in motion
trajectories. Results: Peaks in the EEG signals at central and occipital
electrodes were used to isolate the salient events within each gesture. EEG
power oscillations were detected 343 and 400ms on average after inflection
points at occipital and central electrodes, respectively. A regression model
showed that inflection points in the motion trajectories strongly predicted
subsequent mu oscillations (R^2=0.961, p&lt;.01). Conclusion: The results suggest
that coordinated activity in the visual and motor cortices are highly
correlated with key motion components within gesture trajectories. These points
may be associated with neural signatures used to encode gestures in memory for
later identification and even recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05086</identifier>
 <datestamp>2019-12-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05086</id><created>2018-11-12</created><updated>2019-12-11</updated><authors><author><keyname>Rezaie</keyname><forenames>Reza</forenames></author><author><keyname>Li</keyname><forenames>X. Rong</forenames></author></authors><title>Nonsingular Gaussian Conditionally Markov Sequences</title><categories>math.PR cs.SY eess.SP math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov processes are widely used in modeling random phenomena/problems.
However, they may not be adequate in some cases where more general processes
are needed. The conditionally Markov (CM) process is a generalization of the
Markov process based on conditioning. There are several classes of CM processes
(one of them is the class of reciprocal processes), which provide more
capability (than Markov) for modeling random phenomena. Reciprocal processes
have been used in many different applications (e.g., image processing, intent
inference, intelligent systems). In this paper, nonsingular Gaussian (NG) CM
sequences are studied, characterized, and their dynamic models are presented.
The presented results provide effective tools for studying reciprocal sequences
from the CM viewpoint, which is different from that of the literature. Also,
the presented models and characterizations serve as a basis for application of
CM sequences, e.g., in motion trajectory modeling with destination information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05097</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05097</id><created>2018-11-12</created><updated>2019-04-22</updated><authors><author><keyname>Wang</keyname><forenames>Senmao</forenames></author><author><keyname>Zhou</keyname><forenames>Pan</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Jia</keyname><forenames>Jia</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author></authors><title>Exploring RNN-Transducer for Chinese Speech Recognition</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  End-to-end approaches have drawn much attention recently for significantly
simplifying the construction of an automatic speech recognition (ASR) system.
RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous
studies have shown that RNN-T is difficult to train and a very complex training
process is needed for a reasonable performance. In this paper, we explore RNN-T
for a Chinese large vocabulary continuous speech recognition (LVCSR) task and
aim to simplify the training process while maintaining performance. First, a
new strategy of learning rate decay is proposed to accelerate the model
convergence. Second, we find that adding convolutional layers at the beginning
of the network and using ordered data can discard the pre-training process of
the encoder without loss of performance. Besides, we design experiments to find
a balance among the usage of GPU memory, training circle and model performance.
Finally, we achieve 16.9% character error rate (CER) on our test set which is
2% absolute improvement from a strong BLSTM CE system with language model
trained on the same text corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05150</identifier>
 <datestamp>2018-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05150</id><created>2018-11-13</created><authors><author><keyname>Barthelme</keyname><forenames>Andreas</forenames></author><author><keyname>Joham</keyname><forenames>Michael</forenames></author><author><keyname>Strobel</keyname><forenames>Rainer</forenames></author><author><keyname>Utschick</keyname><forenames>Wolfgang</forenames></author></authors><title>User Demand Based Precoding for G.fast DSL Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It can be observed that the achievable rate region of a G.fast DSL system is
no longer rectangular, as it is the case for vectored VDSL systems, due to
stronger crosstalk couplings at high frequencies. Therefore, alternative
operating points that are not optimal in a sum-rate sense may be utilized to
adapt the system performance to the users' actual demands. To this end, we
propose a new precoding scheme based on defining a subset of prioritized users,
where we optimize the sum-rate of the prioritized users under a minimum rate
guarantee for the remaining users. We present a solution based on Lagrangian
duality theory and propose a well-performing one-step heuristic solution. By
means of simulations, we show that significant rate gains for the prioritized
users can be obtained by the proposed precoding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05185</identifier>
 <datestamp>2019-05-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05185</id><created>2018-11-13</created><authors><author><keyname>Rossi</keyname><forenames>Silvia</forenames></author><author><keyname>De Simone</keyname><forenames>Francesca</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author><author><keyname>Toni</keyname><forenames>Laura</forenames></author></authors><title>Spherical clustering of users navigating 360$^\circ$ content</title><categories>cs.MM eess.IV</categories><comments>5 pages, submitted to conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Virtual Reality (VR) applications, understanding how users explore the
visual content is important in order to optimize content creation and
distribution, develop user-centric services, or even to detect disorders in
medical applications. In this paper, we propose a graph-based method to
identify clusters of users who are attending the same portion of spherical
content, within one frame or a series of frames. With respect to other
clustering methods, the proposed solution takes into account the spherical
geometry of the content and correctly identifies clusters that group viewers
who actually display the same portion of spherical content. Results, carried
out by using a set of publicly available VR user navigation patterns, show that
the proposed method identifies more meaningful clusters, i.e., clusters of
users who are consistently attending the same portion of spherical content,
with respect to other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05214</identifier>
 <datestamp>2018-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05214</id><created>2018-11-13</created><authors><author><keyname>Mangal</keyname><forenames>Jyoti</forenames></author><author><keyname>Monga</keyname><forenames>Rashi</forenames></author><author><keyname>Mathur</keyname><forenames>Sandeep R.</forenames></author><author><keyname>Dinda</keyname><forenames>Amit K.</forenames></author><author><keyname>Joseph</keyname><forenames>Joby</forenames></author><author><keyname>Ahlawat</keyname><forenames>Sarita</forenames></author><author><keyname>Khare</keyname><forenames>Kedar</forenames></author></authors><title>Unsupervised organization of cervical cells using high resolution
  digital holographic microscopy</title><categories>eess.IV physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report results on unsupervised organization of cervical cells using
microscopy of Pap-smear samples in brightfield (3-channel colour) as well as
high resolution quantitative phase imaging modalities. A number of
morphological parameters are measured for each of the 1450 cell nuclei (from
$10$ woman subjects) imaged in this study. The Principal Component Analysis
(PCA) methodology applied to this data shows that the cell image clustering
performance improves significantly when brightfield as well as phase
information is utilized for PCA as compared to when brightfield-only
information is used. The results point to the feasibility of an image-based
tool that will be able to mark suspicious cells for further examination by the
pathologist. More importantly our results suggest that the information in
quantitative phase images of cells that is typically not used in clinical
practice is valuable for automated cell classification applications in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05247</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05247</id><created>2018-11-13</created><updated>2019-04-25</updated><authors><author><keyname>Fan</keyname><forenames>Ruchao</forenames></author><author><keyname>Zhou</keyname><forenames>Pan</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Jia</keyname><forenames>Jia</forenames></author><author><keyname>Liu</keyname><forenames>Gang</forenames></author></authors><title>An Online Attention-based Model for Speech Recognition</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention-based end-to-end models such as Listen, Attend and Spell (LAS),
simplify the whole pipeline of traditional automatic speech recognition (ASR)
systems and become popular in the field of speech recognition. In previous
work, researchers have shown that such architectures can acquire comparable
results to state-of-the-art ASR systems, especially when using a bidirectional
encoder and global soft attention (GSA) mechanism. However, bidirectional
encoder and GSA are two obstacles for real-time speech recognition. In this
work, we aim to stream LAS baseline by removing the above two obstacles. On the
encoder side, we use a latency-controlled (LC) bidirectional structure to
reduce the delay of forward computation. Meanwhile, an adaptive monotonic
chunk-wise attention (AMoChA) mechanism is proposed to replace GSA for the
calculation of attention weight distribution. Furthermore, we propose two
methods to alleviate the huge performance degradation when combining LC and
AMoChA. Finally, we successfully acquire an online LAS model, LC-AMoChA, which
has only 3.5% relative performance reduction to LAS baseline on our internal
Mandarin corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05250</identifier>
 <datestamp>2019-04-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05250</id><created>2018-11-13</created><updated>2019-04-23</updated><authors><author><keyname>Zhou</keyname><forenames>Pan</forenames></author><author><keyname>Yang</keyname><forenames>Wenwen</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Yanfeng</forenames></author><author><keyname>Jia</keyname><forenames>Jia</forenames></author></authors><title>Modality Attention for End-to-End Audio-visual Speech Recognition</title><categories>cs.CL cs.CV cs.SD eess.AS</categories><comments>accepted by ICASSP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio-visual speech recognition (AVSR) system is thought to be one of the
most promising solutions for robust speech recognition, especially in noisy
environment. In this paper, we propose a novel multimodal attention based
method for audio-visual speech recognition which could automatically learn the
fused representation from both modalities based on their importance. Our method
is realized using state-of-the-art sequence-to-sequence (Seq2seq)
architectures. Experimental results show that relative improvements from 2% up
to 36% over the auditory modality alone are obtained depending on the different
signal-to-noise-ratio (SNR). Compared to the traditional feature concatenation
methods, our proposed approach can achieve better recognition performance under
both clean and noisy conditions. We believe modality attention based end-to-end
method can be easily generalized to other multimodal tasks with correlated
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05473</identifier>
 <datestamp>2019-08-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05473</id><created>2018-11-13</created><updated>2019-03-26</updated><authors><author><keyname>Liao</keyname><forenames>Congyu</forenames></author><author><keyname>Stockmann</keyname><forenames>Jason</forenames></author><author><keyname>Tian</keyname><forenames>Qiyuan</forenames></author><author><keyname>Bilgic</keyname><forenames>Berkin</forenames></author><author><keyname>Arango</keyname><forenames>Nicolas S.</forenames></author><author><keyname>Manhard</keyname><forenames>Mary Kate</forenames></author><author><keyname>Grissom</keyname><forenames>William A.</forenames></author><author><keyname>Wald</keyname><forenames>Lawrence L.</forenames></author><author><keyname>Setsompop</keyname><forenames>Kawin</forenames></author></authors><title>High-fidelity, high-isotropic resolution diffusion imaging through
  gSlider acquisition with B1+ &amp; T1 corrections and integrated {\Delta}B0/Rx
  shim array</title><categories>physics.med-ph eess.IV</categories><comments>7 figures</comments><journal-ref>Magnetic Resonance in Medicine (2019)</journal-ref><doi>10.1002/mrm.27899</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: B1+ and T1 corrections and dynamic multi-coil shimming approaches
were proposed to improve the fidelity of high isotropic resolution Generalized
slice dithered enhanced resolution (gSlider) diffusion imaging. Methods: An
extended reconstruction incorporating B1+ inhomogeneity and T1 recovery
information was developed to mitigate slab-boundary artifacts in short-TR
gSlider acquisitions. Slab-by-slab dynamic B0 shimming using a multi-coil
integrated {\Delta}B0/Rx shim-array, and high in-plane acceleration
(Rinplane=4) achieved with virtual-coil GRAPPA were also incorporated into a 1
mm isotropic resolution gSlider acquisition/reconstruction framework to achieve
an 8-11 fold reduction in geometric distortion compared to single-shot EPI.
Results: The slab-boundary artifacts were alleviated by the proposed B1+ and T1
corrections compared to the standard gSlider reconstruction pipeline for
short-TR acquisitions. Dynamic shimming provided &gt;50% reduction in geometric
distortion compared to conventional global 2nd order shimming. 1 mm isotropic
resolution diffusion data show that the typically problematic temporal and
frontal lobes of the brain can be imaged with high geometric fidelity using
dynamic shimming. Conclusions: The proposed B1+ and T1 corrections and
local-field control substantially improved the fidelity of high isotropic
resolution diffusion imaging, with reduced slab-boundary artifacts and
geometric distortion compared to conventional gSlider acquisition and
reconstruction. This enabled high-fidelity whole-brain 1 mm isotropic diffusion
imaging with 64 diffusion-directions in 20 minutes using a 3T clinical scanner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05517</identifier>
 <datestamp>2019-06-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05517</id><created>2018-11-13</created><updated>2019-06-10</updated><authors><author><keyname>Rebollo-Neira</keyname><forenames>Laura</forenames></author><author><keyname>Cerna</keyname><forenames>Dana</forenames></author></authors><title>Wavelet Based Dictionaries for Piecewise Dimensionality Reduction of ECG
  Signals</title><categories>eess.SP</categories><comments>Software for constructing the dictionaries, as well as for
  reproducing results, has been made available on
  http://www.nonlinear-approx.info/examples/node011.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimensionality reduction of ECG signals is considered within the framework of
sparse representation. The approach constructs the signal model by selecting
elementary components from a redundant dictionary via a greedy strategy. The
proposed wavelet dictionaries are built from the multiresolution scheme, but
translating the prototypes within a shorter step than that corresponding to the
wavelet basis. The reduced representation of the signal is shown to be suitable
for compression at low level distortion. In that regard, compression results
are superior to previously reported benchmarks on the MIT-BIH Arrhythmia data
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05540</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05540</id><created>2018-11-09</created><authors><author><keyname>Uddin</keyname><forenames>Ahmed Nazim</forenames></author><author><keyname>Rahman</keyname><forenames>Md Ashequr</forenames></author><author><keyname>Islam</keyname><forenames>Md. Rafidul</forenames></author><author><keyname>Haque</keyname><forenames>Mohammad Ariful</forenames></author></authors><title>Native Language Identification using i-vector</title><categories>cs.CL cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of determining a speaker's native language based only on his
speeches in a second language is known as Native Language Identification or
NLI. Due to its increasing applications in various domains of speech signal
processing, this has emerged as an important research area in recent times. In
this paper we have proposed an i-vector based approach to develop an automatic
NLI system using MFCC and GFCC features. For evaluation of our approach, we
have tested our framework on the 2016 ComParE Native language sub-challenge
dataset which has English language speakers from 11 different native language
backgrounds. Our proposed method outperforms the baseline system with an
improvement in accuracy by 21.95% for the MFCC feature based i-vector framework
and 22.81% for the GFCC feature based i-vector framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05550</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05550</id><created>2018-11-13</created><updated>2018-11-16</updated><authors><author><keyname>Hantrakul</keyname><forenames>Lamtharn</forenames></author><author><keyname>Yang</keyname><forenames>Li-Chia</forenames></author></authors><title>Neural Wavetable: a playable wavetable synthesizer using neural networks</title><categories>cs.SD cs.LG cs.MM eess.AS</categories><comments>2 pages, Accepted by Conference on Neural Information Processing
  Systems (NIPS), Workshop on Machine Learning for Creativity and Design</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Neural Wavetable, a proof-of-concept wavetable synthesizer that
uses neural networks to generate playable wavetables. The system can produce
new, distinct waveforms through the interpolation of traditional wavetables in
an autoencoder's latent space. It is available as a VST/AU plugin for use in a
Digital Audio Workstation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05571</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05571</id><created>2018-11-13</created><authors><author><keyname>Heredia-Juesas</keyname><forenames>Juan</forenames></author><author><keyname>Molaei</keyname><forenames>Ali</forenames></author><author><keyname>Tirado</keyname><forenames>Luis</forenames></author><author><keyname>Martinez-Lorenzo</keyname><forenames>Jose A.</forenames></author></authors><title>Consensus and Sectioning-based ADMM with Norm-1 Regularization for
  Imaging with a Compressive Reflector Antenna</title><categories>cs.CE eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents three distributed techniques to find a sparse solution of
the underdetermined linear problem $\textbf{g}=\textbf{Hu}$ with a norm-1
regularization, based on the Alternating Direction Method of Multipliers
(ADMM). These techniques divide the matrix $\textbf{H}$ in submatrices by rows,
columns, or both rows and columns, leading to the so-called consensus-based
ADMM, sectioning-based ADMM, and consensus and sectioning-based ADMM,
respectively. These techniques are applied particularly for millimeter-wave
imaging through the use of a Compressive Reflector Antenna (CRA). The CRA is a
hardware designed to increase the sensing capacity of an imaging system and
reduce the mutual information among measurements, allowing an effective imaging
of sparse targets with the use of Compressive Sensing (CS) techniques.
Consensus-based ADMM has been proved to accelerate the imaging process and
sectioning-based ADMM has shown to highly reduce the amount of information to
be exchange among the computational nodes. In this paper, the mathematical
formulation and graphical interpretation of these two techniques, together with
the consensus and sectioning-based ADMM approach, are presented. The imaging
quality, the imaging time, the convergence, and the communication efficiency
among the nodes are analyzed and compared. The distributed capabitities of the
ADMM-based approaches, together with the high sensing capacity of the CRA,
allow the imaging of metallic targets in a 3D domain in quasi-real time with a
reduced amount of information exchanged among the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05639</identifier>
 <datestamp>2019-12-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05639</id><created>2018-11-13</created><updated>2019-12-12</updated><authors><author><keyname>Rezaie</keyname><forenames>Reza</forenames></author><author><keyname>Li</keyname><forenames>X. Rong</forenames></author></authors><title>Gaussian Reciprocal Sequences from the Viewpoint of Conditionally Markov
  Sequences</title><categories>math.PR cs.SY eess.SP math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conditionally Markov (CM) sequence contains several classes, including
the reciprocal sequence. Reciprocal sequences have been widely used in many
areas of engineering, including image processing, acausal systems, intelligent
systems, and intent inference. In this paper, the reciprocal sequence is
studied from the CM sequence point of view, which is different from the
viewpoint of the literature and leads to more insight into the reciprocal
sequence. Based on this viewpoint, new results, properties, and easily
applicable tools are obtained for the reciprocal sequence. The nonsingular
Gaussian (NG) reciprocal sequence is modeled and characterized from the CM
viewpoint. It is shown that an NG sequence is reciprocal if and only if it is
both $CM_L$ and $CM_F$ (two special classes of CM sequences). New dynamic
models are presented for the NG reciprocal sequence. These models (unlike the
existing one, which is driven by colored noise) are driven by white noise and
are easily applicable. As a special reciprocal sequence, the Markov sequence is
also discussed. Finally, it can be seen how all CM sequences, including Markov
and reciprocal, are unified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05720</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05720</id><created>2018-11-14</created><authors><author><keyname>B&#xe4;ckstr&#xf6;m</keyname><forenames>Tom</forenames></author></authors><title>Speech Coding, Speech Interfaces and IoT - Opportunities and Challenges</title><categories>eess.AS</categories><comments>accepted and presented at Asilomar Conference on Signals, Systems,
  and Computers 2018 (submitted version)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent speech and audio coding standards such as 3GPP Enhanced Voice Services
match the foreseeable needs and requirements in transmission of speech and
audio, when using current transmission infrastructure and applications. Trends
in Internet-of-Things technology and development in personal digital assistants
(PDAs) however begs us to consider future requirements for speech and audio
codecs. The opportunities and challenges are here summarized in three concepts:
collaboration, unification and privacy. First, an increasing number of devices
will in the future be speech-operated, whereby the ability to focus voice
commands to a specific devices becomes essential. We therefore need methods
which allows collaboration between devices, such that ambiguities can be
resolved. Second, such collaboration can be achieved with a unified and
standardized communication protocol between voice-operated devices. To achieve
such collaboration protocols, we need to develop distributed speech coding
technology for ad-hoc IoT networks. Finally however, collaboration will
increase the demand for privacy protection in speech interfaces and it is
therefore likely that technologies for supporting privacy and generating trust
will be in high demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05760</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05760</id><created>2018-10-10</created><authors><author><keyname>Bhattacharya</keyname><forenames>Aniruddha</forenames></author><author><keyname>Kadambari</keyname><forenames>K. V.</forenames></author></authors><title>A Multimodal Approach towards Emotion Recognition of Music using Audio
  and Lyrical Content</title><categories>eess.AS cs.CL cs.CV cs.MM cs.SD</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose MoodNet - A Deep Convolutional Neural Network based architecture
to effectively predict the emotion associated with a piece of music given its
audio and lyrical content.We evaluate different architectures consisting of
varying number of two-dimensional convolutional and subsampling layers,followed
by dense layers.We use Mel-Spectrograms to represent the audio content and word
embeddings-specifically 100 dimensional word vectors, to represent the textual
content represented by the lyrics.We feed input data from both modalities to
our MoodNet architecture.The output from both the modalities are then fused as
a fully connected layer and softmax classfier is used to predict the category
of emotion.Using F1-score as our metric,our results show excellent performance
of MoodNet over the two datasets we experimented on-The MIREX Multimodal
dataset and the Million Song Dataset.Our experiments reflect the hypothesis
that more complex models perform better with more training data.We also observe
that lyrics outperform audio as a better expressed modality and conclude that
combining and using features from multiple modalities for prediction tasks
result in superior performance in comparison to using a single modality as
input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05784</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05784</id><created>2018-11-13</created><authors><author><keyname>Aussal</keyname><forenames>Matthieu</forenames></author><author><keyname>Gueguen</keyname><forenames>Robin</forenames></author></authors><title>Open-source platforms for fast room acoustic simulations in complex
  structures</title><categories>eess.AS cs.CE cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents new numerical simulation tools, respectively developed
in Matlab and Blender softwares. Available in open-source under the GPL 3.0
license, it uses a ray-tracing/image-sources hybrid method to calculate the
room acoustics for large meshes. Performances are optimized to solve problems
of significant size (typically more than 100,000 surface elements and about a
million of rays). For this purpose, a Divide and Conquer approach with a
recursive binary tree structure has been implemented to reduce the quadratic
complexity of the ray/element interactions to near-linear. Thus, execution
times are less sensitive to the mesh density, which allows simulations of
complex geometry. After ray propagation, a hybrid method leads to
image-sources, which can be visually analyzed to localize sound map. Finally,
impulse responses are constructed from the image-sources and FIR filters are
proposed natively over 8 octave bands, taking into account material absorption
properties and propagation medium. This algorithm is validated by comparisons
with theoretical test cases. Furthermore, an example on a quite complex case,
namely the ancient theater of Orange is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05844</identifier>
 <datestamp>2019-06-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05844</id><created>2018-11-14</created><updated>2019-05-30</updated><authors><author><keyname>Izacard</keyname><forenames>Gautier</forenames></author><author><keyname>Bernstein</keyname><forenames>Brett</forenames></author><author><keyname>Fernandez-Granda</keyname><forenames>Carlos</forenames></author></authors><title>A Learning-Based Framework for Line-Spectra Super-resolution</title><categories>cs.LG eess.SP stat.ML</categories><comments>Accepted at ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a learning-based approach for estimating the spectrum of a
multisinusoidal signal from a finite number of samples. A neural-network is
trained to approximate the spectra of such signals on simulated data. The
proposed methodology is very flexible: adapting to different signal and noise
models only requires modifying the training data accordingly. Numerical
experiments show that the approach performs competitively with classical
methods designed for additive Gaussian noise at a range of noise levels, and is
also effective in the presence of impulsive noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05933</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05933</id><created>2018-11-14</created><authors><author><keyname>Mehrjou</keyname><forenames>Arash</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Deep Nonlinear Non-Gaussian Filtering for Dynamical Systems</title><categories>eess.SP cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Filtering is a general name for inferring the states of a dynamical system
given observations. The most common filtering approach is Gaussian Filtering
(GF) where the distribution of the inferred states is a Gaussian whose mean is
an affine function of the observations. There are two restrictions in this
model: Gaussianity and Affinity. We propose a model to relax both these
assumptions based on recent advances in implicit generative models. Empirical
results show that the proposed method gives a significant advantage over GF and
nonlinear methods based on fixed nonlinear kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05958</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05958</id><created>2018-11-14</created><authors><author><keyname>Tudose</keyname><forenames>Mihai</forenames></author><author><keyname>Anghel</keyname><forenames>Andrei</forenames></author><author><keyname>Cacoveanu</keyname><forenames>Remus</forenames></author><author><keyname>Datcu</keyname><forenames>Mihai</forenames></author></authors><title>Pulse radar with FPGA range compression for real time displacement and
  vibration monitoring</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at presenting the basic functionality of a radar platform for
real-time monitoring of displacement and vibration. The real time capabilities
make the radar platform useful when live monitoring of targets is required. The
system is based on the RF analog front-end of an USRP, and the range
compression (time-domain cross-correlation) is implemented on the FPGA included
in the USRP. Further processing is performed on the host computer to plot real
time range profiles, displacements, vibration frequencies spectra and
spectrograms (waterfall plots) for long term monitoring. The system is
currently in experimental form and the present paper aims at proving its
functionality. The precision of this system is estimated at 0.6 mm for
displacement measurements and 1.8 mm for vibration amplitude measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.05961</identifier>
 <datestamp>2018-11-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.05961</id><created>2018-11-14</created><authors><author><keyname>Buyukates</keyname><forenames>Baturalp</forenames></author><author><keyname>Soysal</keyname><forenames>Alkan</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Age of Information Scaling in Large Networks</title><categories>cs.IT cs.NI eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study age of information in a multiple source-multiple destination setting
with a focus on its scaling in large wireless networks. There are $n$ nodes
that are randomly paired with each other on a fixed area to form $n$
source-destination (S-D) pairs. We propose a three-phase transmission scheme
which utilizes local cooperation between the nodes by forming what we call mega
update packets to serve multiple S-D pairs at once. We show that under the
proposed scheme average age of an S-D pair scales as $O(n^{\frac{1}{4}})$ as
the number of users, $n$, in the network grows. To the best of our knowledge,
this is the best age scaling result for a multiple source-multiple destination
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06016</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06016</id><created>2018-11-14</created><authors><author><keyname>Nolasco</keyname><forenames>In&#xea;s</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author></authors><title>To bee or not to bee: Investigating machine learning approaches for
  beehive sound recognition</title><categories>cs.SD eess.AS</categories><comments>To be presented at Detection and Classification of Acoustic Scenes
  and Events (DCASE) workshop 2018</comments><journal-ref>Proceedings of the Detection and Classification of Acoustic Scenes
  and Events 2018 Workshop (DCASE2018)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we aim to explore the potential of machine learning methods to
the problem of beehive sound recognition. A major contribution of this work is
the creation and release of annotations for a selection of beehive recordings.
By experimenting with both support vector machines and convolutional neural
networks, we explore important aspects to be considered in the development of
beehive sound recognition systems using machine learning approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06030</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06030</id><created>2018-11-14</created><authors><author><keyname>Zhang</keyname><forenames>Xuejing</forenames></author><author><keyname>He</keyname><forenames>Zishu</forenames></author><author><keyname>Liao</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Xuepan</forenames></author><author><keyname>Yang</keyname><forenames>Yue</forenames></author></authors><title>A Fast Method for Array Response Adjustment with Phase-Only Constraint</title><categories>eess.SP</categories><comments>This paper is submitted to IEEE Radar Conference 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a fast method for array response adjustment with
phase-only constraint. This method can precisely and rapidly adjust the array
response of a given point by only varying the entry phases of a pre-assigned
weight vector. We show that phase-only array response adjustment can be
formulated as a polygon construction problem, which can be solved by edge
rotation in complex plain. Unlike the existing approaches, the proposed
algorithm provides an analytical solution and guarantees a precise phase-only
adjustment without pattern distortion. Moreover, the proposed method is
suitable for an arbitrarily given weight vector and has a low computational
complexity. Representative examples are presented to demonstrate the
effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06038</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06038</id><created>2018-11-14</created><authors><author><keyname>Hosseini</keyname><forenames>Mahdi S.</forenames></author><author><keyname>Zhang</keyname><forenames>Yueyang</forenames></author><author><keyname>Chan</keyname><forenames>Lyndon</forenames></author><author><keyname>Plataniotis</keyname><forenames>Konstantinos N.</forenames></author><author><keyname>Brawley-Hayes</keyname><forenames>Jasper A. Z.</forenames></author><author><keyname>Damaskinos</keyname><forenames>Savvas</forenames></author></authors><title>Focus Quality Assessment of High-Throughput Whole Slide Imaging in
  Digital Pathology</title><categories>eess.IV cs.CV</categories><comments>10 pages, This work has been submitted to the IEEE for possible
  publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the challenges facing the adoption of digital pathology workflows for
clinical use is the need for automated quality control. As the scanners
sometimes determine focus inaccurately, the resultant image blur deteriorates
the scanned slide to the point of being unusable. Also, the scanned slide
images tend to be extremely large when scanned at greater or equal 20X image
resolution. Hence, for digital pathology to be clinically useful, it is
necessary to use computational tools to quickly and accurately quantify the
image focus quality and determine whether an image needs to be re-scanned. We
propose a no-reference focus quality assessment metric specifically for digital
pathology images, that operates by using a sum of even-derivative filter bases
to synthesize a human visual system-like kernel, which is modeled as the
inverse of the lens' point spread function. This kernel is then applied to a
digital pathology image to modify high-frequency image information deteriorated
by the scanner's optics and quantify the focus quality at the patch level. We
show in several experiments that our method correlates better with ground-truth
$z$-level data than other methods, and is more computationally efficient. We
also extend our method to generate a local slide-level focus quality heatmap,
which can be used for automated slide quality control, and demonstrate the
utility of our method for clinical scan quality control by comparison with
subjective slide quality scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06039</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06039</id><created>2018-11-14</created><authors><author><keyname>Zadi</keyname><forenames>Armin Soltan</forenames></author><author><keyname>Alex</keyname><forenames>Raichel</forenames></author><author><keyname>Zhang</keyname><forenames>Rong</forenames></author><author><keyname>Watenpaugh</keyname><forenames>Donald E.</forenames></author><author><keyname>Behbehani</keyname><forenames>Khosrow</forenames></author></authors><title>Arterial Blood Pressure Feature Estimation Using Photoplethysmography</title><categories>eess.SP cs.CE</categories><comments>12 pages, published in Computers in Biology and Medicine journal</comments><journal-ref>Armin Soltan Zadi, Raichel Alex, Rong Zhang, Donald E. Watenpaugh,
  Khosrow Behbehani, &quot;Arterial Blood Pressure Feature Estimation Using
  Photoplethysmography&quot;,Computers in Biology and Medicine,Volume 102, 1
  November 2018, Pages 104-111</journal-ref><doi>10.1016/j.compbiomed.2018.09.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous and noninvasive monitoring of blood pressure has numerous clinical
and fitness applications. Current methods of continuous measurement of blood
pressure are either invasive and/or require expensive equipment. Therefore, we
investigated a new method for the continuous estimation of two main features of
blood pressure waveform: systolic and diastolic pressures. The estimates were
obtained from a photoplethysmography signal as input to the fifth order
autoregressive moving average models. The performance of the method was
evaluated using beat-to-beat full-wave blood pressure measurements from 15
young subjects, with no known cardiovascular disorder, in supine position as
they breathed normally and also while they performed a breath-hold maneuver.
The level of error in the estimates, as measured by the root mean square of the
model residuals, was less than 5 mmHg during normal breathing and less than 8
mmHg during the breath-hold maneuver. The mean of model residuals both during
normal breathing and breath-hold maneuvers was considered to be less than 3.2
mmHg. The dependency of the accuracy of the estimates on the subject data was
assessed by comparing the modeling errors for the 15 subjects. Less than 1% of
the models showed significant differences (p &lt; 0.05) from the other models,
which indicates a high level of consistency among the models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06071</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06071</id><created>2018-11-14</created><authors><author><keyname>Klatt</keyname><forenames>Matthias</forenames></author><author><keyname>Meyer</keyname><forenames>Jan</forenames></author><author><keyname>Schegner</keyname><forenames>Peter</forenames></author></authors><title>Derivation of an aggregated band pseudo phasor for single phase pulse
  width modulation voltage waveforms</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the equation for the waveform of single phase PWM voltages is
transformed from its Fourier series form to a variation in which the voltage in
each submission band is represented as a single frequency modulated voltage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06090</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06090</id><created>2018-11-14</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>ReSIFT: Reliability-Weighted SIFT-based Image Quality Assessment</title><categories>eess.IV cs.CV</categories><comments>5 pages, 3 figures, 4 tables</comments><acm-class>I.4</acm-class><journal-ref>D. Temel and G. AlRegib, &quot;ReSIFT: Reliability-weighted sift-based
  image quality assessment,&quot; 2016 IEEE International Conference on Image
  Processing (ICIP), Phoenix, AZ, 2016, pp. 2047-2051</journal-ref><doi>10.1109/ICIP.2016.7532718</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a full-reference image quality estimator based on SIFT
descriptor matching over reliability-weighted feature maps. Reliability
assignment includes a smoothing operation, a transformation to perceptual color
domain, a local normalization stage, and a spectral residual computation with
global normalization. The proposed method ReSIFT is tested on the LIVE and the
LIVE Multiply Distorted databases and compared with 11 state-of-the-art
full-reference quality estimators. In terms of the Pearson and the Spearman
correlation, ReSIFT is the best performing quality estimator in the overall
databases. Moreover, ReSIFT is the best performing quality estimator in at
least one distortion group in compression, noise, and blur category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06096</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06096</id><created>2018-11-14</created><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Lalitha</keyname><forenames>Anusha</forenames></author><author><keyname>Lee</keyname><forenames>Jinwon</forenames></author><author><keyname>Lott</keyname><forenames>Chris</forenames></author></authors><title>Automatic Grammar Augmentation for Robust Voice Command Recognition</title><categories>cs.CL cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel pipeline for automatic grammar augmentation that
provides a significant improvement in the voice command recognition accuracy
for systems with small footprint acoustic model (AM). The improvement is
achieved by augmenting the user-defined voice command set, also called grammar
set, with alternate grammar expressions. For a given grammar set, a set of
potential grammar expressions (candidate set) for augmentation is constructed
from an AM-specific statistical pronunciation dictionary that captures the
consistent patterns and errors in the decoding of AM induced by variations in
pronunciation, pitch, tempo, accent, ambiguous spellings, and noise conditions.
Using this candidate set, greedy optimization based and cross-entropy-method
(CEM) based algorithms are considered to search for an augmented grammar set
with improved recognition accuracy utilizing a command-specific dataset. Our
experiments show that the proposed pipeline along with algorithms considered in
this paper significantly reduce the mis-detection and mis-classification rate
without increasing the false-alarm rate. Experiments also demonstrate the
consistent superior performance of CEM method over greedy-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06103</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06103</id><created>2018-11-14</created><authors><author><keyname>Ahmed</keyname><forenames>S. Asim</forenames></author><author><keyname>Chakravarty</keyname><forenames>Subhashish</forenames></author><author><keyname>Newhouse</keyname><forenames>Michael</forenames></author></authors><title>Deep Neural Networks based Modrec: Some Results with Inter-Symbol
  Interference and Adversarial Examples</title><categories>cs.LG eess.SP stat.ML</categories><comments>4 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent successes and advances in Deep Neural Networks (DNN) in machine vision
and Natural Language Processing (NLP) have motivated their use in traditional
signal processing and communications systems. In this paper, we present results
of such applications to the problem of automatic modulation recognition.
Variations in wireless communication channels are represented by statistical
channel models and their parameterization will increase with the advent of 5G.
In this paper, we report effect of simple two path channel model on our naive
deep neural network based implementation. We also report impact of adversarial
perturbation to the input signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06181</identifier>
 <datestamp>2019-05-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06181</id><created>2018-11-15</created><authors><author><keyname>Maddali</keyname><forenames>Siddharth</forenames></author><author><keyname>Allain</keyname><forenames>Marc</forenames></author><author><keyname>Cha</keyname><forenames>Wonsuk</forenames></author><author><keyname>Harder</keyname><forenames>Ross</forenames></author><author><keyname>Park</keyname><forenames>Jun-Sang</forenames></author><author><keyname>Kenesei</keyname><forenames>Peter</forenames></author><author><keyname>Almer</keyname><forenames>Jonathan</forenames></author><author><keyname>Nashed</keyname><forenames>Youssef</forenames></author><author><keyname>Hruszkewycz</keyname><forenames>Stephan O.</forenames></author></authors><title>Phase retrieval for Bragg coherent diffraction imaging at high X-ray
  energies</title><categories>cond-mat.mtrl-sci eess.SP physics.optics</categories><comments>10 pages, 8 figures</comments><journal-ref>Phys. Rev. A 99, 053838 (2019)</journal-ref><doi>10.1103/PhysRevA.99.053838</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coherent X-ray beams with energies $\geq 50$ keV can potentially enable
three-dimensional imaging of atomic lattice distortion fields within individual
crystallites in bulk polycrystalline materials through Bragg coherent
diffraction imaging (BCDI). However, the undersampling of the diffraction
signal due to Fourier space compression at high X-ray energies renders
conventional phase retrieval algorithms unsuitable for three-dimensional
reconstruction. To address this problem we utilize a phase retrieval method
with a Fourier constraint specifically tailored for undersampled diffraction
data measured with coarse-pitched detector pixels that bin the underlying
signal. With our approach, we show that it is possible to reconstruct
three-dimensional strained crystallites from an undersampled Bragg diffraction
data set subject to pixel-area integration without having to physically
upsample the diffraction signal. Using simulations and experimental results, we
demonstrate that explicit modeling of Fourier space compression during phase
retrieval provides a viable means by which to invert high-energy BCDI data,
which is otherwise intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06234</identifier>
 <datestamp>2019-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06234</id><created>2018-11-15</created><authors><author><keyname>Michelsanti</keyname><forenames>Daniel</forenames></author><author><keyname>Tan</keyname><forenames>Zheng-Hua</forenames></author><author><keyname>Sigurdsson</keyname><forenames>Sigurdur</forenames></author><author><keyname>Jensen</keyname><forenames>Jesper</forenames></author></authors><title>On Training Targets and Objective Functions for Deep-Learning-Based
  Audio-Visual Speech Enhancement</title><categories>eess.AS cs.LG cs.SD eess.IV</categories><doi>10.1109/ICASSP.2019.8682790</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio-visual speech enhancement (AV-SE) is the task of improving speech
quality and intelligibility in a noisy environment using audio and visual
information from a talker. Recently, deep learning techniques have been adopted
to solve the AV-SE task in a supervised manner. In this context, the choice of
the target, i.e. the quantity to be estimated, and the objective function,
which quantifies the quality of this estimate, to be used for training is
critical for the performance. This work is the first that presents an
experimental study of a range of different targets and objective functions used
to train a deep-learning-based AV-SE system. The results show that the
approaches that directly estimate a mask perform the best overall in terms of
estimated speech quality and intelligibility, although the model that directly
estimates the log magnitude spectrum performs as good in terms of estimated
speech quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06250</identifier>
 <datestamp>2019-11-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06250</id><created>2018-11-15</created><authors><author><keyname>Michelsanti</keyname><forenames>Daniel</forenames></author><author><keyname>Tan</keyname><forenames>Zheng-Hua</forenames></author><author><keyname>Sigurdsson</keyname><forenames>Sigurdur</forenames></author><author><keyname>Jensen</keyname><forenames>Jesper</forenames></author></authors><title>Effects of Lombard Reflex on the Performance of Deep-Learning-Based
  Audio-Visual Speech Enhancement Systems</title><categories>eess.AS cs.LG cs.SD eess.IV</categories><doi>10.1109/ICASSP.2019.8682713</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans tend to change their way of speaking when they are immersed in a noisy
environment, a reflex known as Lombard effect. Current speech enhancement
systems based on deep learning do not usually take into account this change in
the speaking style, because they are trained with neutral (non-Lombard) speech
utterances recorded under quiet conditions to which noise is artificially
added. In this paper, we investigate the effects that the Lombard reflex has on
the performance of audio-visual speech enhancement systems based on deep
learning. The results show that a gap in the performance of as much as
approximately 5 dB between the systems trained on neutral speech and the ones
trained on Lombard speech exists. This indicates the benefit of taking into
account the mismatch between neutral and Lombard speech in the design of
audio-visual speech enhancement systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06292</identifier>
 <datestamp>2019-07-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06292</id><created>2018-11-15</created><updated>2019-07-04</updated><authors><author><keyname>Lorenzo-Trueba</keyname><forenames>Jaime</forenames></author><author><keyname>Drugman</keyname><forenames>Thomas</forenames></author><author><keyname>Latorre</keyname><forenames>Javier</forenames></author><author><keyname>Merritt</keyname><forenames>Thomas</forenames></author><author><keyname>Putrycz</keyname><forenames>Bartosz</forenames></author><author><keyname>Barra-Chicote</keyname><forenames>Roberto</forenames></author><author><keyname>Moinet</keyname><forenames>Alexis</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vatsal</forenames></author></authors><title>Towards achieving robust universal neural vocoding</title><categories>eess.AS cs.SD</categories><comments>4 pages, 1 extra for references. Accepted on Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the potential universality of neural vocoders. We train a
WaveRNN-based vocoder on 74 speakers coming from 17 languages. This vocoder is
shown to be capable of generating speech of consistently good quality (98%
relative mean MUSHRA when compared to natural speech) regardless of whether the
input spectrogram comes from a speaker or style seen during training or from an
out-of-domain scenario when the recording conditions are studio-quality. When
the recordings show significant changes in quality, or when moving towards
non-speech vocalizations or singing, the vocoder still significantly
outperforms speaker-dependent vocoders, but operates at a lower average
relative MUSHRA of 75%. These results are shown to be consistent across
languages, regardless of them being seen during training (e.g. English or
Japanese) or unseen (e.g. Wolof, Swahili, Ahmaric).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06296</identifier>
 <datestamp>2018-12-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06296</id><created>2018-11-15</created><updated>2018-12-11</updated><authors><author><keyname>Merritt</keyname><forenames>Thomas</forenames></author><author><keyname>Putrycz</keyname><forenames>Bartosz</forenames></author><author><keyname>Nadolski</keyname><forenames>Adam</forenames></author><author><keyname>Ye</keyname><forenames>Tianjun</forenames></author><author><keyname>Korzekwa</keyname><forenames>Daniel</forenames></author><author><keyname>Dolecki</keyname><forenames>Wiktor</forenames></author><author><keyname>Drugman</keyname><forenames>Thomas</forenames></author><author><keyname>Klimkov</keyname><forenames>Viacheslav</forenames></author><author><keyname>Moinet</keyname><forenames>Alexis</forenames></author><author><keyname>Breen</keyname><forenames>Andrew</forenames></author><author><keyname>Kuklinski</keyname><forenames>Rafal</forenames></author><author><keyname>Strom</keyname><forenames>Nikko</forenames></author><author><keyname>Barra-Chicote</keyname><forenames>Roberto</forenames></author></authors><title>Comprehensive evaluation of statistical speech waveform synthesis</title><categories>eess.AS cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical TTS systems that directly predict the speech waveform have
recently reported improvements in synthesis quality. This investigation
evaluates Amazon's statistical speech waveform synthesis (SSWS) system. An
in-depth evaluation of SSWS is conducted across a number of domains to better
understand the consistency in quality. The results of this evaluation are
validated by repeating the procedure on a separate group of testers. Finally,
an analysis of the nature of speech errors of SSWS compared to hybrid unit
selection synthesis is conducted to identify the strengths and weaknesses of
SSWS. Having a deeper insight into SSWS allows us to better define the focus of
future work to improve this new technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06315</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06315</id><created>2018-11-15</created><updated>2018-11-23</updated><authors><author><keyname>Latorre</keyname><forenames>Javier</forenames></author><author><keyname>Lachowicz</keyname><forenames>Jakub</forenames></author><author><keyname>Lorenzo-Trueba</keyname><forenames>Jaime</forenames></author><author><keyname>Merritt</keyname><forenames>Thomas</forenames></author><author><keyname>Drugman</keyname><forenames>Thomas</forenames></author><author><keyname>Ronanki</keyname><forenames>Srikanth</forenames></author><author><keyname>Viacheslav</keyname><forenames>Klimkov</forenames></author></authors><title>Effect of data reduction on sequence-to-sequence neural TTS</title><categories>cs.CL eess.AS</categories><comments>4 pages, 1 extra for references. Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent speech synthesis systems based on sampling from autoregressive neural
networks models can generate speech almost undistinguishable from human
recordings. However, these models require large amounts of data. This paper
shows that the lack of data from one speaker can be compensated with data from
other speakers. The naturalness of Tacotron2-like models trained on a blend of
5k utterances from 7 speakers is better than that of speaker dependent models
trained on 15k utterances, but in terms of stability multi-speaker models are
always more stable. We also demonstrate that models mixing only 1250 utterances
from a target speaker with 5k utterances from another 6 speakers can produce
significantly better quality than state-of-the-art DNN-guided unit selection
systems trained on more than 10 times the data from the target speaker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06321</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06321</id><created>2018-11-15</created><authors><author><keyname>Yuan</keyname><forenames>Baichuan</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author><author><keyname>Bertozzi</keyname><forenames>Andrea L.</forenames></author><author><keyname>Brantingham</keyname><forenames>P. Jeffrey</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Multivariate Spatiotemporal Hawkes Processes and Network Reconstruction</title><categories>cs.SI eess.SP nlin.AO physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is often latent network structure in spatial and temporal data and the
tools of network analysis can yield fascinating insights into such data. In
this paper, we develop a nonparametric method for network reconstruction from
spatiotemporal data sets using multivariate Hawkes processes. In contrast to
prior work on network reconstruction with point-process models, which has often
focused on exclusively temporal information, our approach uses both temporal
and spatial information and does not assume a specific parametric form of
network dynamics. This leads to an effective way of recovering an underlying
network. We illustrate our approach using both synthetic networks and networks
constructed from real-world data sets (a location-based social media network, a
narrative of crime events, and violent gang crimes). Our results demonstrate
that, in comparison to using only temporal data, our spatiotemporal approach
yields improved network reconstruction, providing a basis for meaningful
subsequent analysis --- such as community structure and motif analysis --- of
the reconstructed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06330</identifier>
 <datestamp>2019-02-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06330</id><created>2018-11-15</created><updated>2019-02-15</updated><authors><author><keyname>Nolasco</keyname><forenames>In&#xea;s</forenames></author><author><keyname>Terenzi</keyname><forenames>Alessandro</forenames></author><author><keyname>Cecchi</keyname><forenames>Stefania</forenames></author><author><keyname>Orcioni</keyname><forenames>Simone</forenames></author><author><keyname>Bear</keyname><forenames>Helen L.</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author></authors><title>Audio-based identification of beehive states</title><categories>cs.SD eess.AS</categories><comments>Accepted for ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The absence of the queen in a beehive is a very strong indicator of the need
for beekeeper intervention. Manually searching for the queen is an arduous
recurrent task for beekeepers that disrupts the normal life cycle of the
beehive and can be a source of stress for bees. Sound is an indicator for
signalling different states of the beehive, including the absence of the queen
bee. In this work, we apply machine learning methods to automatically recognise
different states in a beehive using audio as input. % The system is built on
top of a method for beehive sound recognition in order to detect bee sounds
from other external sounds. We investigate both support vector machines and
convolutional neural networks for beehive state recognition, using audio data
of beehives collected from the NU-Hive project. Results indicate the potential
of machine learning methods as well as the challenges of generalizing the
system to new hives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06349</identifier>
 <datestamp>2018-11-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06349</id><created>2018-11-15</created><authors><author><keyname>Epstein</keyname><forenames>Benjamin</forenames><affiliation>ECS Federal</affiliation></author><author><keyname>Olsson</keyname><forenames>Roy H.</forenames><suffix>III</suffix><affiliation>Defense Advanced Research Projects Agency</affiliation></author></authors><title>Physical Signal Classification Via Deep Neural Networks</title><categories>cs.LG eess.SP</categories><comments>4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Deep Neural Network is applied to classify physical signatures obtained
from physical sensor measurements of running gasoline and diesel-powered
vehicles and other devices. The classification provides information on the
target identities as to vehicle type and even vehicle model. The physical
measurements include acoustic, acceleration (vibration), geophonic, and
magnetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06378</identifier>
 <datestamp>2019-03-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06378</id><created>2018-11-14</created><authors><author><keyname>Aliev</keyname><forenames>M.</forenames></author><author><keyname>Ershov</keyname><forenames>E. I.</forenames></author><author><keyname>Nikolaev</keyname><forenames>D. P.</forenames></author></authors><title>On the use of FHT, its modification for practical applications and the
  structure of Hough image</title><categories>eess.IV cs.CV</categories><comments>8 pages, 8 figures. Submitted and presented at ICMV 2018</comments><doi>10.1117/12.2522803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on the Fast Hough Transform (FHT) algorithm proposed by
M.L. Brady. We propose how to modify the standard FHT to calculate sums along
lines within any given range of their inclination angles. We also describe a
new way to visualise Hough-image based on regrouping of accumulator space
around its center. Finally, we prove that using Brady parameterization
transforms any line into a figure of type &quot;angle&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06439</identifier>
 <datestamp>2019-11-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06439</id><created>2018-11-15</created><updated>2019-11-12</updated><authors><author><keyname>Ananthabhotla</keyname><forenames>Ishwarya</forenames></author><author><keyname>Ramsay</keyname><forenames>David B.</forenames></author><author><keyname>Paradiso</keyname><forenames>Joseph A.</forenames></author></authors><title>HCU400: An Annotated Dataset for Exploring Aural Phenomenology Through
  Causal Uncertainty</title><categories>eess.AS cs.CL cs.SD</categories><journal-ref>IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP). IEEE, 2019</journal-ref><doi>10.1109/ICASSP.2019.8683147</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The way we perceive a sound depends on many aspects-- its ecological
frequency, acoustic features, typicality, and most notably, its identified
source. In this paper, we present the HCU400: a dataset of 402 sounds ranging
from easily identifiable everyday sounds to intentionally obscured artificial
ones. It aims to lower the barrier for the study of aural phenomenology as the
largest available audio dataset to include an analysis of causal attribution.
Each sample has been annotated with crowd-sourced descriptions, as well as
familiarity, imageability, arousal, and valence ratings. We extend existing
calculations of causal uncertainty, automating and generalizing them with word
embeddings. Upon analysis we find that individuals will provide less polarized
emotion ratings as a sound's source becomes increasingly ambiguous; individual
ratings of familiarity and imageability, on the other hand, diverge as
uncertainty increases despite a clear negative trend on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06452</identifier>
 <datestamp>2019-03-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06452</id><created>2018-11-15</created><authors><author><keyname>Ahmad</keyname><forenames>Farhan Ammar</forenames></author></authors><title>Review of isolation enhancement with the help of Theory of
  characteristic modes</title><categories>eess.SP</categories><report-no>2617-9709 (Online) 2617-9695 (Print)</report-no><journal-ref>https://pisrt.org/psr-press/journals/easl-vol-2-issue-1-2019/review-of-isolation-enhancement-with-the-help-of-theory-of-characteristic-modes/</journal-ref><doi>10.30538/psrp-easl2019.0012</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multiple-input-multiple-output (MIMO) antennas performance can be degraded
due to the poor isolation between the MIMO antenna elements. In this paper a
review of the different isolation enhancement schemes available in the
literature is presented. Empirically the isolation between the antennas can be
improved by placing the antenna as far as possible and it can be enhanced
further by introducing different isolation enhancement schemes. Theory of
characteristic modes (TCM) was recently proposed that had useful benefits. TCM
was also used to enhance the isolation. This papers will also focus on the
different approaches of TCM, to enhance the isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06541</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06541</id><created>2018-11-14</created><authors><author><keyname>Zadi</keyname><forenames>Armin Soltan</forenames></author><author><keyname>Alex</keyname><forenames>Raichel M.</forenames></author><author><keyname>Zhang</keyname><forenames>Rong</forenames></author><author><keyname>Watenpaugh</keyname><forenames>Donald E.</forenames></author><author><keyname>Behbehani</keyname><forenames>Khosrow</forenames></author></authors><title>Mathematical Modeling of Arterial Blood Pressure Using
  Photo-Plethysmography Signal in Breath-hold Maneuver</title><categories>eess.SP cs.CE</categories><comments>4 pages, published in 2018 40th Annual International Conference of
  the IEEE Engineering in Medicine and Biology Society (EMBC)</comments><journal-ref>pages 2711-2714 (publisher:IEEE), 40th Annual International
  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),
  October 2018</journal-ref><doi>10.1109/EMBC.2018.8512776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has shown that each apnea episode results in a significant
rise in the beat-to-beat blood pressure and by a drop to the pre-episode levels
when patient resumes normal breathing. While the physiological implications of
these repetitive and significant oscillations are still unknown, it is of
interest to quantify them. Since current array of instruments deployed for
polysomnography studies does not include beat-to-beat measurement of blood
pressure, but includes oximetry, it is both of clinical interest to estimate
the magnitude of BP oscillations from the photoplethysmography (PPG) signal
that is readily available from sleep lab oximeters. We have investigated a new
method for continuous estimation of systolic (SBP), diastolic (DBP), and mean
(MBP) blood pressure waveforms from PPG. Peaks and troughs of PPG waveform are
used as input to a 5th order autoregressive moving average model to construct
estimates of SBP, DBP, and MBP waveforms. Since breath hold maneuvers are shown
to simulate apnea episodes faithfully, we evaluated the performance of the
proposed method in 7 subjects (4 F; 32+-4 yrs., BMI 24.57+-3.87 kg/m2) in
supine position doing 5 breath maneuvers with 90s of normal breathing between
them. The modeling error ranges were (all units are in mmHg) -0.88+-4.87 to
-2.19+-5.73 (SBP); 0.29+-2.39 to -0.97+-3.83 (DBP); and -0.42+-2.64 to
-1.17+-3.82 (MBP). The cross validation error ranges were 0.28+-6.45 to
-1.74+-6.55 (SBP); 0.09+-3.37 to -0.97+-3.67 (DBP); and 0.33+-4.34 to
-0.87+-4.42 (MBP). The level of estimation error in, as measured by the root
mean squared of the model residuals, was less than 7 mmHg
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06633</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06633</id><created>2018-11-15</created><authors><author><keyname>Carr</keyname><forenames>CJ</forenames></author><author><keyname>Zukowski</keyname><forenames>Zack</forenames></author></authors><title>Generating Albums with SampleRNN to Imitate Metal, Rock, and Punk Bands</title><categories>cs.SD eess.AS</categories><comments>3 pages</comments><journal-ref>Proceedings of the 6th International Workshop on Musical
  Metacreation (MUME 2018)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This early example of neural synthesis is a proof-of-concept for how machine
learning can drive new types of music software. Creating music can be as simple
as specifying a set of music influences on which a model trains. We demonstrate
a method for generating albums that imitate bands in experimental music genres
previously unrealized by traditional synthesis techniques (e.g. additive,
subtractive, FM, granular, concatenative). Raw audio is generated
autoregressively in the time-domain using an unconditional SampleRNN. We create
six albums this way. Artwork and song titles are also generated using materials
from the original artists' back catalog as training data. We try a
fully-automated method and a human-curated method. We discuss its potential for
machine-assisted production.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06639</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06639</id><created>2018-11-15</created><authors><author><keyname>Zukowski</keyname><forenames>Zack</forenames></author><author><keyname>Carr</keyname><forenames>CJ</forenames></author></authors><title>Generating Black Metal and Math Rock: Beyond Bach, Beethoven, and
  Beatles</title><categories>cs.SD eess.AS</categories><comments>3 pages</comments><journal-ref>NIPS Workshop on Machine Learning for Creativity and Design (2017)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We use a modified SampleRNN architecture to generate music in modern genres
such as black metal and math rock. Unlike MIDI and symbolic models, SampleRNN
generates raw audio in the time domain. This requirement becomes increasingly
important in modern music styles where timbre and space are used
compositionally. Long developmental compositions with rapid transitions between
sections are possible by increasing the depth of the network beyond the number
used for speech datasets. We are delighted by the unique characteristic
artifacts of neural synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06704</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06704</id><created>2018-11-16</created><authors><author><keyname>Ganji</keyname><forenames>Jabbar</forenames></author></authors><title>Concept of round non-flat thin film solar cells and their power
  conversion efficiency calculation</title><categories>physics.app-ph eess.SP</categories><comments>19 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thin-film solar cells that are considered as the second generation of solar
cells are known for their low cost and acceptable efficiency. In this
technology, semiconductor layers with a thickness of micrometer are deposited
on thick enough substrates to maintain physical consistency. The relatively low
processing temperature helps use substrates of different materials. Compared
with crystalline solar cells, which are mainly made up of rigid flat plates, it
is also possible to make thin-film cells on flexible or non-flat substrates. In
this study, a method was first proposed to calculate the efficiency of such
cells without the need for 3D simulation, and then it is investigated using
non-flat conical and paraboloid substrates as a novel method to enhance the
light trapping. As a result, a significant increase in the efficiency of the
studied non-flat cells was observed and reported in comparison with the flat
cells. In addition, the paraboloid shape shows a better performance than that
of the conical, to use as the cell's substrate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06712</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06712</id><created>2018-11-16</created><authors><author><keyname>Dharmawansa</keyname><forenames>Prathapasinghe</forenames></author><author><keyname>Kahatapitiya</keyname><forenames>Kumara</forenames></author><author><keyname>Atapattu</keyname><forenames>Saman</forenames></author><author><keyname>Tellambura</keyname><forenames>Chintha</forenames></author></authors><title>Outage Analysis of $2\times2 $ MIMO-MRC in Correlated Rician Fading</title><categories>eess.SP cs.IT math.IT</categories><comments>7 pages</comments><msc-class>62H10, 15B52,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses one of the classical problems in random matrix theory--
finding the distribution of the maximum eigenvalue of the correlated Wishart
unitary ensemble. In particular, we derive a new exact expression for the
cumulative distribution function (c.d.f.) of the maximum eigenvalue of a
$2\times 2$ correlated non-central Wishart matrix with rank-$1$ mean. By using
this new result, we derive an exact analytical expression for the outage
probability of $2\times 2$ multiple-input multiple-output
maximum-ratio-combining (MIMO-MRC) in Rician fading with transmit correlation
and a strong line-of-sight (LoS) component (rank-$1$ channel mean). We also
show that the outage performance is affected by the relative alignment of the
eigen-spaces of the mean and correlation matrices. In general, when the LoS
path aligns with the least eigenvector of the correlation matrix, in the {\it
high} transmit signal-to-noise ratio (SNR) regime, the outage gradually
improves with the increasing correlation. Moreover, we show that as $K$ (Rician
factor) grows large, the outage event can be approximately characterized by the
c.d.f. of a certain Gaussian random variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06713</identifier>
 <datestamp>2019-05-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06713</id><created>2018-11-16</created><updated>2019-04-30</updated><authors><author><keyname>Leglaive</keyname><forenames>Simon</forenames></author><author><keyname>Girin</keyname><forenames>Laurent</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>Semi-supervised multichannel speech enhancement with variational
  autoencoders and non-negative matrix factorization</title><categories>cs.SD eess.AS stat.ML</categories><comments>5 pages, 2 figures, audio examples and code available online at
  https://team.inria.fr/perception/icassp-2019-mvae/</comments><report-no>hal-02005102</report-no><journal-ref>IEEE International Conference on Acoustics Speech and Signal
  Processing (ICASSP), Brighton, UK, May 2019, pp. 101-105</journal-ref><doi>10.1109/ICASSP.2019.8683704</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address speaker-independent multichannel speech enhancement
in unknown noisy environments. Our work is based on a well-established
multichannel local Gaussian modeling framework. We propose to use a neural
network for modeling the speech spectro-temporal content. The parameters of
this supervised model are learned using the framework of variational
autoencoders. The noisy recording environment is supposed to be unknown, so the
noise spectro-temporal modeling remains unsupervised and is based on
non-negative matrix factorization (NMF). We develop a Monte Carlo
expectation-maximization algorithm and we experimentally show that the proposed
approach outperforms its NMF-based counterpart, where speech is modeled using
supervised NMF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06715</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06715</id><created>2018-11-16</created><authors><author><keyname>Kim</keyname><forenames>Junghoon</forenames></author><author><keyname>Chun</keyname><forenames>Joohwan</forenames></author><author><keyname>Song</keyname><forenames>Sungchan</forenames></author></authors><title>Joint Range and Angle Estimation for FMCW MIMO Radar and Its Application</title><categories>eess.SP</categories><comments>10 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, frequency-modulated continuous wave (FMCW) radars with array
antennas are gaining in popularity on a wide variety of commercial
applications. A usual approach of the range and angle estimation of a target
with an array FMCW radar is to form a range-angle matrix with deramped receive
signal, and then apply the two-dimensional fast Fourier transformation (2D-FFT)
on the range-angle matrix. However, such frequency estimation approaches give
bias error because two frequencies on the range-angle matrix are not
independent to each other, unlike the 2D angle estimation with a passive planar
antenna array. We propose a new maximum-likelihood based algorithm for the
joint range and angle estimation of multiple targets with an array FMCW radar,
and show that the proposed algorithm achieves the Cramer-Rao bounds (CRBs) both
for the range and angle estimation. The proposed algorithm is also compared
with other algorithms for a simultaneous localization and mapping (SLAM)
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06729</identifier>
 <datestamp>2019-04-08</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06729</id><created>2018-11-16</created><updated>2019-04-05</updated><authors><author><keyname>Brighente</keyname><forenames>Alessandro</forenames></author><author><keyname>Formaggio</keyname><forenames>Francesco</forenames></author><author><keyname>Centenaro</keyname><forenames>Marco</forenames></author><author><keyname>Di Nunzio</keyname><forenames>Giorgio Maria</forenames></author><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author></authors><title>Location-Verification and Network Planning via Machine Learning
  Approaches</title><categories>eess.SP</categories><comments>Accepted for Workshop on Machine Learning for Communications, June 07
  2019, Avignon, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-region location verification (IRLV) in wireless networks is the problem of
deciding if user equipment (UE) is transmitting from inside or outside a
specific physical region (e.g., a safe room). The decision process exploits the
features of the channel between the UE and a set of network access points
(APs). We propose a solution based on machine learning (ML) implemented by a
neural network (NN) trained with the channel features (in particular, noisy
attenuation values) collected by the APs for various positions both inside and
outside the specific region. The output is a decision on the UE position
(inside or outside the region). By seeing IRLV as an hypothesis testing
problem, we address the optimal positioning of the APs for minimizing either
the area under the curve (AUC) of the receiver operating characteristic (ROC)
or the cross entropy (CE) between the NN output and ground truth (available
during the training). In order to solve the minimization problem we propose a
twostage particle swarm optimization (PSO) algorithm. We show that for a long
training and a NN with enough neurons the proposed solution achieves the
performance of the Neyman-Pearson (N-P) lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06753</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06753</id><created>2018-11-16</created><authors><author><keyname>V&#xe9;niat</keyname><forenames>Tom</forenames></author><author><keyname>Schwander</keyname><forenames>Olivier</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author></authors><title>Stochastic Adaptive Neural Architecture Search for Keyword Spotting</title><categories>cs.LG eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of keyword spotting i.e. identifying keywords in a real-time
audio stream is mainly solved by applying a neural network over successive
sliding windows. Due to the difficulty of the task, baseline models are usually
large, resulting in a high computational cost and energy consumption level. We
propose a new method called SANAS (Stochastic Adaptive Neural Architecture
Search) which is able to adapt the architecture of the neural network
on-the-fly at inference time such that small architectures will be used when
the stream is easy to process (silence, low noise, ...) and bigger networks
will be used when the task becomes more difficult. We show that this adaptive
model can be learned end-to-end by optimizing a trade-off between the
prediction performance and the average computational cost per unit of time.
Experiments on the Speech Commands dataset show that this approach leads to a
high recognition level while being much faster (and/or energy saving) than
classical approaches where the network architecture is static.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06756</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06756</id><created>2018-11-16</created><authors><author><keyname>Byker</keyname><forenames>Rudolf</forenames></author><author><keyname>Niesler</keyname><forenames>Thomas</forenames></author></authors><title>Direction of Arrival Estimation of Wide-band Signals with Planar
  Microphone Arrays</title><categories>cs.SD eess.AS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach to the estimation of the Direction of Arrival (DOA) of wide-band
signals with a planar microphone array is presented. Our algorithm estimates an
unambiguous DOA using a single planar array in which the microphones are placed
fairly close together and the sound source is expected to be in the far field.
The algorithm uses the ambiguous DOA estimates obtained from microphone pairs
in the array to determine an unambiguous DOA estimate for the array as a whole.
The required pair-wise DOAs may be calculated using Time Delay Estimations
(TDEs), which may in turn be calculated using cross-correlation, making the
algorithm suitable for wide-band signals. No a priori knowledge of the true
Sound Source Location (SSL) is required. Simulations show that the algorithm is
robust against noise in the input data. An average ratio of approximately 3:1
exists between the input DOA errors and the output DOA error. Field tests with
a moving sound source provided DOA estimates with standard deviations between
20.4 and 15.2 degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06795</identifier>
 <datestamp>2019-09-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06795</id><created>2018-11-16</created><updated>2019-05-30</updated><authors><author><keyname>Szoke</keyname><forenames>Igor</forenames></author><author><keyname>Skacel</keyname><forenames>Miroslav</forenames></author><author><keyname>Mosner</keyname><forenames>Ladislav</forenames></author><author><keyname>Paliesek</keyname><forenames>Jakub</forenames></author><author><keyname>Cernocky</keyname><forenames>Jan &quot;Honza&quot;</forenames></author></authors><title>Building and Evaluation of a Real Room Impulse Response Dataset</title><categories>eess.AS</categories><comments>Submitted to Journal of Selected Topics in Signal Processing,
  November 2018</comments><doi>10.1109/JSTSP.2019.2917582</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents BUT ReverbDB - a dataset of real room impulse responses
(RIR), background noises and re-transmitted speech data. The retransmitted data
includes LibriSpeech test-clean, 2000 HUB5 English evaluation and part of 2010
NIST Speaker Recognition Evaluation datasets. We provide a detailed description
of RIR collection (hardware, software, post-processing) that can serve as a
&quot;cook-book&quot; for similar efforts. We also validate BUT ReverbDB in two sets of
automatic speech recognition (ASR) experiments and draw conclusions for
augmenting ASR training data with real and artificially generated RIRs. We show
that a limited number of real RIRs, carefully selected to match the target
environment, provide results comparable to a large number of artificially
generated RIRs, and that both sets can be combined to achieve the best ASR
results. The dataset is distributed for free under a non-restrictive license
and it currently contains data from 8 rooms, which is growing. The distribution
package also contains a Kaldi-based recipe for augmenting publicly available
AMI close-talk meeting data and test the results on an AMI single distant
microphone set, allowing it to reproduce our experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06805</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06805</id><created>2018-11-16</created><authors><author><keyname>Grzywalski</keyname><forenames>Tomasz</forenames></author><author><keyname>Drgas</keyname><forenames>Szymon</forenames></author></authors><title>Using recurrences in time and frequency within U-net architecture for
  speech enhancement</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When designing fully-convolutional neural network, there is a trade-off
between receptive field size, number of parameters and spatial resolution of
features in deeper layers of the network. In this work we present a novel
network design based on combination of many convolutional and recurrent layers
that solves these dilemmas. We compare our solution with U-nets based models
known from the literature and other baseline models on speech enhancement task.
We test our solution on TIMIT speech utterances combined with noise segments
extracted from NOISEX-92 database and show clear advantage of proposed solution
in terms of SDR (signal-to-distortion ratio), SIR (signal-to-interference
ratio) and STOI (spectro-temporal objective intelligibility) metrics compared
to the current state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06856</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06856</id><created>2018-11-16</created><updated>2019-02-22</updated><authors><author><keyname>Rapp</keyname><forenames>Joshua</forenames></author><author><keyname>Dawson</keyname><forenames>Robin M. A.</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author></authors><title>Estimation from Quantized Gaussian Measurements: When and How to Use
  Dither</title><categories>stat.AP eess.SP</categories><comments>Revision with added references, figures, and appendices</comments><doi>10.1109/TSP.2019.2916046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subtractive dither is a powerful method for removing the signal dependence of
quantization noise for coarsely-quantized signals. However, estimation from
dithered measurements often naively applies the sample mean or midrange, even
when the total noise is not well described with a Gaussian or uniform
distribution. We show that the generalized Gaussian distribution approximately
describes subtractively-dithered, quantized samples of a Gaussian signal.
Furthermore, a generalized Gaussian fit leads to simple estimators based on
order statistics that match the performance of more complicated maximum
likelihood estimators requiring iterative solvers. The order statistics-based
estimators outperform both the sample mean and midrange for nontrivial sums of
Gaussian and uniform noise. Additional analysis of the generalized Gaussian
approximation yields rules of thumb for determining when and how to apply
dither to quantized measurements. Specifically, we find subtractive dither to
be beneficial when the ratio between the Gaussian standard deviation and
quantization interval length is roughly less than 1/3. If that ratio is also
greater than 0.822/$K^{0.930}$ for the number of measurements $K&gt;20$, we
present estimators more efficient than the midrange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06858</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06858</id><created>2018-11-16</created><authors><author><keyname>Goudard</keyname><forenames>Vincent</forenames><affiliation>STMS</affiliation></author></authors><title>John, the semi-conductor : a tool for comprovisation</title><categories>cs.HC cs.SD eess.AS</categories><proxy>ccsd</proxy><journal-ref>Sandeep Bhagwati; Jean Bresson. International Conference on
  Technologies for Music Notation and Representation (TENOR'18), May 2018,
  Montr{\'e}al, Canada. 2018, Proceedings of the 4th International Conference
  on Technologies for Music Notation and Representation.
  http://tenor-conference.org/</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents &quot;John&quot;, an open-source software designed to help
collective free improvisation. It provides generated screen-scores running on
distributed, reactive web-browsers. The musicians can then concurrently edit
the scores in their own browser. John is used by ONE, a septet playing
improvised electro-acoustic music with digital musical instruments (DMI). One
of the original features of John is that its design takes care of leaving the
musician's attention as free as possible. Firstly, a quick review of the
context of screen-based scores will help situate this research in the history
of contemporary music notation. Then I will trace back how improvisation
sessions led to John's particular &quot;notational perspective&quot;. A brief description
of the software will precede a discussion about the various aspects guiding its
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06859</identifier>
 <datestamp>2018-11-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06859</id><created>2018-11-16</created><authors><author><keyname>Ananthabhotla</keyname><forenames>Ishwarya</forenames></author><author><keyname>Paradiso</keyname><forenames>Joseph A.</forenames></author></authors><title>SoundSignaling: Realtime, Stylistic Modification of a Personal Music
  Corpus for Information Delivery</title><categories>eess.AS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing inspiration from the notion of cognitive incongruence associated with
Stroop's famous experiment, from musical principles, and from the observation
that music consumption on an individual basis is becoming increasingly
ubiquitous, we present the SoundSignaling system -- a software platform
designed to make real-time, stylistically relevant modifications to a personal
corpus of music as a means of conveying information or notifications. In this
work, we discuss in detail the system's technical implementation and its
motivation from a musical perspective, and validate these design choices
through a crowd-sourced signal identification experiment consisting of 200
independent tasks performed by 50 online participants. We then qualitatively
discuss the potential implications of such a system from the standpoint of
switch cost, cognitive load, and listening behavior by considering the
anecdotal outcomes of a small-scale, in-the-wild experiment consisting of over
180 hours of usage from 6 participants. Through this work, we suggest a
re-evaluation of the age-old paradigm of binary audio notifications in favor of
a system designed to operate upon the relatively unexplored medium of a user's
musical preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06950</identifier>
 <datestamp>2019-03-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06950</id><created>2018-11-16</created><authors><author><keyname>Choi</keyname><forenames>Gunho</forenames></author><author><keyname>Ryu</keyname><forenames>Donghun</forenames></author><author><keyname>Jo</keyname><forenames>Youngju</forenames></author><author><keyname>Kim</keyname><forenames>Youngseo</forenames></author><author><keyname>Park</keyname><forenames>Weisun</forenames></author><author><keyname>Min</keyname><forenames>Hyun-Seok</forenames></author><author><keyname>Park</keyname><forenames>Yongkeun</forenames></author></authors><title>Deep learning approach to coherent noise reduction in optical
  diffraction tomography</title><categories>physics.optics eess.IV</categories><doi>10.1364/OE.27.004927</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a deep neural network to reduce coherent noise in
three-dimensional quantitative phase imaging. Inspired by the cycle generative
adversarial network, the denoising network was trained to learn a transform
between two image domains: clean and noisy refractive index tomograms. The
unique feature of this network, distinct from previous machine learning
approaches employed in the optical imaging problem, is that it uses unpaired
images. The learned network quantitatively demonstrated its performance and
generalization capability through denoising experiments of various samples. We
concluded by applying our technique to reduce the temporally changing noise
emerging from focal drift in time-lapse imaging of biological cells. This
reduction cannot be performed using other optical methods for denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.06981</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.06981</id><created>2018-11-16</created><authors><author><keyname>Rippel</keyname><forenames>Oren</forenames></author><author><keyname>Nair</keyname><forenames>Sanjay</forenames></author><author><keyname>Lew</keyname><forenames>Carissa</forenames></author><author><keyname>Branson</keyname><forenames>Steve</forenames></author><author><keyname>Anderson</keyname><forenames>Alexander G.</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author></authors><title>Learned Video Compression</title><categories>eess.IV cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for video coding, learned end-to-end for the
low-latency mode. In this setting, our approach outperforms all existing video
codecs across nearly the entire bitrate range. To our knowledge, this is the
first ML-based method to do so.
  We evaluate our approach on standard video compression test sets of varying
resolutions, and benchmark against all mainstream commercial codecs, in the
low-latency mode. On standard-definition videos, relative to our algorithm,
HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On
high-definition 1080p videos, H.265 and VP9 typically produce codes up to 20%
larger, and H.264 up to 35% larger. Furthermore, our approach does not suffer
from blocking artifacts and pixelation, and thus produces videos that are more
visually pleasing.
  We propose two main contributions. The first is a novel architecture for
video compression, which (1) generalizes motion estimation to perform any
learned compensation beyond simple translations, (2) rather than strictly
relying on previously transmitted reference frames, maintains a state of
arbitrary information learned by the model, and (3) enables jointly compressing
all transmitted signals (such as optical flow and residual).
  Secondly, we present a framework for ML-based spatial rate control: namely, a
mechanism for assigning variable bitrates across space for each frame. This is
a critical component for video coding, which to our knowledge had not been
developed within a machine learning setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07018</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07018</id><created>2018-11-16</created><authors><author><keyname>Gong</keyname><forenames>Yuan</forenames></author><author><keyname>Poellabauer</keyname><forenames>Christian</forenames></author></authors><title>Protecting Voice Controlled Systems Using Sound Source Identification
  Based on Acoustic Cues</title><categories>cs.CR cs.SD eess.AS</categories><comments>Proceedings of the 27th International Conference on Computer
  Communications and Networks (ICCCN), Hangzhou, China, July-August 2018. arXiv
  admin note: text overlap with arXiv:1803.09156</comments><doi>10.1109/ICCCN.2018.8487334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, a rapidly increasing number of Internet-of-Things
(IoT) systems that adopt voice as the primary user input have emerged. These
systems have been shown to be vulnerable to various types of voice spoofing
attacks. Existing defense techniques can usually only protect from a specific
type of attack or require an additional authentication step that involves
another device. Such defense strategies are either not strong enough or lower
the usability of the system. Based on the fact that legitimate voice commands
should only come from humans rather than a playback device, we propose a novel
defense strategy that is able to detect the sound source of a voice command
based on its acoustic features. The proposed defense strategy does not require
any information other than the voice command itself and can protect a system
from multiple types of spoofing attacks. Our proof-of-concept experiments
verify the feasibility and effectiveness of this defense strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07021</identifier>
 <datestamp>2019-04-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07021</id><created>2018-11-16</created><updated>2019-04-24</updated><authors><author><keyname>Voleti</keyname><forenames>Rohit</forenames></author><author><keyname>Liss</keyname><forenames>Julie M.</forenames></author><author><keyname>Berisha</keyname><forenames>Visar</forenames></author></authors><title>Investigating the Effects of Word Substitution Errors on Sentence
  Embeddings</title><categories>cs.CL cs.SD eess.AS</categories><comments>4 Pages, 2 figures. Copyright IEEE 2019. Accepted and to appear in
  the Proceedings of the 44th International Conference on Acoustics, Speech,
  and Signal Processing 2019 (IEEE-ICASSP-2019), May 12-17 in Brighton, U.K.
  Personal use of this material is permitted. However, permission to
  reprint/republish this material must be obtained from the IEEE</comments><doi>10.1109/ICASSP.2019.8683367</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key initial step in several natural language processing (NLP) tasks
involves embedding phrases of text to vectors of real numbers that preserve
semantic meaning. To that end, several methods have been recently proposed with
impressive results on semantic similarity tasks. However, all of these
approaches assume that perfect transcripts are available when generating the
embeddings. While this is a reasonable assumption for analysis of written text,
it is limiting for analysis of transcribed text. In this paper we investigate
the effects of word substitution errors, such as those coming from automatic
speech recognition errors (ASR), on several state-of-the-art sentence embedding
methods. To do this, we propose a new simulator that allows the experimenter to
induce ASR-plausible word substitution errors in a corpus at a desired word
error rate. We use this simulator to evaluate the robustness of several
sentence embedding methods. Our results show that pre-trained neural sentence
encoders are both robust to ASR errors and perform well on textual similarity
tasks after errors are introduced. Meanwhile, unweighted averages of word
vectors perform well with perfect transcriptions, but their performance
degrades rapidly on textual similarity tasks for text with word substitution
errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07030</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07030</id><created>2018-11-16</created><authors><author><keyname>Wilson</keyname><forenames>Kevin</forenames></author><author><keyname>Chinen</keyname><forenames>Michael</forenames></author><author><keyname>Thorpe</keyname><forenames>Jeremy</forenames></author><author><keyname>Patton</keyname><forenames>Brian</forenames></author><author><keyname>Hershey</keyname><forenames>John</forenames></author><author><keyname>Saurous</keyname><forenames>Rif A.</forenames></author><author><keyname>Skoglund</keyname><forenames>Jan</forenames></author><author><keyname>Lyon</keyname><forenames>Richard F.</forenames></author></authors><title>Exploring Tradeoffs in Models for Low-latency Speech Enhancement</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a variety of neural networks configurations for one- and
two-channel spectrogram-mask-based speech enhancement. Our best model improves
on previous state-of-the-art performance on the CHiME2 speech enhancement task
by 0.4 decibels in signal-to-distortion ratio (SDR). We examine trade-offs such
as non-causal look-ahead, computation, and parameter count versus enhancement
performance and find that zero-look-ahead models can achieve, on average,
within 0.03 dB SDR of our best bidirectional model. Further, we find that 200
milliseconds of look-ahead is sufficient to achieve equivalent performance to
our best bidirectional model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07044</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07044</id><created>2018-11-16</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>BLeSS: Bio-inspired Low-level Spatiochromatic Similarity Assisted Image
  Quality Assessment</title><categories>eess.IV cs.CV eess.SP</categories><comments>7 pages, 3 figures, 3 tables</comments><acm-class>I.4</acm-class><journal-ref>2016 IEEE International Conference on Multimedia and Expo (ICME),
  Seattle, WA, 2016, pp. 1-6</journal-ref><doi>10.1109/ICME.2016.7552874</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a biologically-inspired low-level
spatiochromatic-model-based similarity method (BLeSS) to assist full-reference
image-quality estimators that originally oversimplify color perception
processes. More specifically, the spatiochromatic model is based on spatial
frequency, spatial orientation, and surround contrast effects. The assistant
similarity method is used to complement image-quality estimators based on phase
congruency, gradient magnitude, and spectral residual. The effectiveness of
BLeSS is validated using FSIM, FSIMc and SR-SIM methods on LIVE, Multiply
Distorted LIVE, and TID 2013 databases. In terms of Spearman correlation, BLeSS
enhances the performance of all quality estimators in color-based degradations
and the enhancement is at 100% for both feature- and spectral residual-based
similarity methods. Moreover, BleSS significantly enhances the performance of
SR-SIM and FSIM in the full TID 2013 database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07065</identifier>
 <datestamp>2019-03-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07065</id><created>2018-11-16</created><updated>2019-03-13</updated><authors><author><keyname>Chaman</keyname><forenames>Anadi</forenames></author><author><keyname>Liu</keyname><forenames>Yu-Jeh</forenames></author><author><keyname>Casebeer</keyname><forenames>Jonah</forenames></author><author><keyname>Dokmani&#x107;</keyname><forenames>Ivan</forenames></author></authors><title>Multipath-enabled private audio with noise</title><categories>eess.AS cs.SD eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of privately communicating audio messages to multiple
listeners in a reverberant room using a set of loudspeakers. We propose two
methods based on emitting noise. In the first method, the loudspeakers emit
noise signals that are appropriately filtered so that after echoing along
multiple paths in the room, they sum up and descramble to yield distinct
meaningful audio messages only at specific focusing spots, while being
incoherent everywhere else. In the second method, adapted from wireless
communications, we project noise signals onto the nullspace of the MIMO channel
matrix between the loudspeakers and listeners. Loudspeakers reproduce a sum of
the projected noise signals and intended messages. Again because of echoes, the
MIMO nullspace changes across different locations in the room. Thus, the
listeners at focusing spots hear intended messages, while the acoustic channel
of an eavesdropper at any other location is jammed. We show, using both
numerical and real experiments, that with a small number of speakers and a few
impulse response measurements, audio messages can indeed be communicated to a
set of listeners while ensuring negligible intelligibility elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07072</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07072</id><created>2018-11-16</created><authors><author><keyname>Hou</keyname><forenames>Yuanbo</forenames></author><author><keyname>Kong</keyname><forenames>Qiuqiang</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Shengchen</forenames></author></authors><title>Polyphonic audio tagging with sequentially labelled data using CRNN with
  learnable gated linear units</title><categories>cs.SD eess.AS</categories><comments>DCASE2018 Workshop. arXiv admin note: text overlap with
  arXiv:1808.01935</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio tagging aims to detect the types of sound events occurring in an audio
recording. To tag the polyphonic audio recordings, we propose to use
Connectionist Temporal Classification (CTC) loss function on the top of
Convolutional Recurrent Neural Network (CRNN) with learnable Gated Linear Units
(GLU-CTC), based on a new type of audio label data: Sequentially Labelled Data
(SLD). In GLU-CTC, CTC objective function maps the frame-level probability of
labels to clip-level probability of labels. To compare the mapping ability of
GLU-CTC for sound events, we train a CRNN with GLU based on Global Max Pooling
(GLU-GMP) and a CRNN with GLU based on Global Average Pooling (GLU-GAP). And we
also compare the proposed GLU-CTC system with the baseline system, which is a
CRNN trained using CTC loss function without GLU. The experiments show that the
GLU-CTC achieves an Area Under Curve (AUC) score of 0.882 in audio tagging,
outperforming the GLU-GMP of 0.803, GLU-GAP of 0.766 and baseline system of
0.837. That means based on the same CRNN model with GLU, the performance of CTC
mapping is better than the GMP and GAP mapping. Given both based on the CTC
mapping, the CRNN with GLU outperforms the CRNN without GLU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07082</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07082</id><created>2018-11-16</created><authors><author><keyname>Ramsay</keyname><forenames>David B.</forenames></author><author><keyname>Ananthabhotla</keyname><forenames>Ishwarya</forenames></author><author><keyname>Paradiso</keyname><forenames>Joseph A.</forenames></author></authors><title>The Intrinsic Memorability of Everyday Sounds</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our aural experience plays an integral role in the perception and memory of
the events in our lives. Some of the sounds we encounter throughout the day
stay lodged in our minds more easily than others; these, in turn, may serve as
powerful triggers of our memories. In this paper, we measure the memorability
of everyday sounds across 20,000 crowd-sourced aural memory games, and assess
the degree to which a sound's memorability is constant across subjects. We then
use this data to analyze the relationship between memorability and acoustic
features like harmonicity, spectral skew, and models of cognitive salience; we
also assess the relationship between memorability and high-level features with
a dependence on the sound source itself, such as its familiarity, valence,
arousal, source type, causal certainty, and verbalizability. We find that (1)
our crowd-sourced measures of memorability and confusability are reliable and
robust across participants; (2) that the authors' measure of collective causal
uncertainty detailed in our previous work, coupled with measures of
visualizability and valence, are the strongest individual predictors of
memorability; (3) that acoustic and salience features play a heightened role in
determining &quot;confusability&quot; (the false positive selection rate associated with
a sound) relative to memorability, and that (4), within the framework of our
assessment, memorability is an intrinsic property of the sounds from the
dataset, shown to be independent of surrounding context. We suggest that
modeling these cognitive processes opens the door for human-inspired
compression of sound environments, automatic curation of large-scale
environmental recording datasets, and real-time modification of aural events to
alter their likelihood of memorability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07086</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07086</id><created>2018-11-16</created><authors><author><keyname>Ebied</keyname><forenames>Ahmed</forenames></author><author><keyname>Kinney-Lang</keyname><forenames>Eli</forenames></author><author><keyname>Escudero</keyname><forenames>Javier</forenames></author></authors><title>Use of muscle synergies extracted via higher-order tensor decomposition
  for proportional myoelectric control</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, muscle synergies have been utilised to provide
simultaneous and proportional myoelectric control systems. All of the proposed
synergy-based systems relies on matrix factorisation methods to extract the
muscle synergies which is limited in terms of task-dimensionality. Here, we
seek to demonstrate and discuss the potential of higher-order tensor
decompositions as a framework to estimate muscle synergies for proportional
myoelectric control. We proposed synergy-based myoelectric control model by
utilising muscle synergies extracted by a novel \ac{ctd} technique. Our
approach is compared with \ac{NMF} \ac{SNMF}, the current state-of-the-art
matrix factorisation models for synergy-based myoelectric control systems.
Synergies extracted from three techniques where used to estimate control
signals for wrist's \ac{dof} through regression. The reconstructed control
signals where evaluated by real glove data that capture the wrist's kinematics.
The proposed \ac{ctd} model results was slightly better than matrix
factorisation methods. The three models where compared against random generated
synergies and all of them were able to reject the null hypothesis. This study
provides demonstrate the use of higher-order tensor decomposition in
proportional myoelectric control and highlight the potential applications and
advantages of using higher-order tensor decomposition in muscle synergy
extraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07107</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07107</id><created>2018-11-17</created><authors><author><keyname>Shen</keyname><forenames>Yifei</forenames></author><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Transfer Learning for Mixed-Integer Resource Allocation Problems in
  Wireless Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective resource allocation plays a pivotal role for performance
optimization in wireless networks. Unfortunately, typical resource allocation
problems are mixed-integer nonlinear programming (MINLP) problems, which are
NP-hard. Machine learning based methods recently emerge as a disruptive way to
obtain near-optimal performance for MINLP problems with affordable
computational complexity. However, they suffer from severe performance
deterioration when the network parameters change, which commonly happens in
practice and can be characterized as the task mismatch issue. In this paper, we
propose a transfer learning method via self-imitation, to address this issue
for effective resource allocation in wireless networks. It is based on a
general &quot;learning to optimize&quot; framework for solving MINLP problems. A unique
advantage of the proposed method is that it can tackle the task mismatch issue
with a few additional unlabeled training samples, which is especially important
when transferring to large-size problems. Numerical experiments demonstrate
that with much less training time, the proposed method achieves comparable
performance with the model trained from scratch with sufficient amount of
labeled samples. To the best of our knowledge, this is the first work that
applies transfer learning for resource allocation in wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07110</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07110</id><created>2018-11-17</created><authors><author><keyname>Borijindargoon</keyname><forenames>Narong</forenames></author><author><keyname>Ng</keyname><forenames>Boon Poh</forenames></author></authors><title>Directional Adaptive MUSIC-like Algorithm under {\alpha}-Stable
  Distributed Noise</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm called MUSIC-like algorithm was originally proposed as an
alternative method to the MUltiple SIgnal Classification (MUSIC) algorithm for
direction-of-arrival (DOA) estimation. Without requiring explicit model order
estimation, it was shown to have robust performance particularly in low
signal-to-noise ratio (SNR) scenarios. In this letter, the working principle of
a relaxation parameter {\beta}, a parameter which was introduced into the
formulation of the MUSIC-like algorithm, is provided based on geometrical
interpretation. To illustrate its robustness, the algorithm will be examined
under symmetric {\alpha}-stable distributed noise environment. An adaptive
framework is then developed and proposed in this letter to further optimize the
algorithm. The proposed adaptive framework is compared with the original
MUSIC-like, MUSIC, FLOM-MUSIC, and SSCM-MUSIC algorithms. A notable improvement
in terms of targets resolvability of the proposed method is observed under
different impulse noise scenarios as well as different SNR levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07131</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07131</id><created>2018-11-17</created><authors><author><keyname>Kallummil</keyname><forenames>Sreejith</forenames></author><author><keyname>Kalyani</keyname><forenames>Sheetal</forenames></author></authors><title>High SNR Consistent Compressive Sensing Without Signal and Noise
  Statistics</title><categories>eess.SP cs.LG stat.ML</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovering the support of sparse vectors in underdetermined linear regression
models, \textit{aka}, compressive sensing is important in many signal
processing applications. High SNR consistency (HSC), i.e., the ability of a
support recovery technique to correctly identify the support with increasing
signal to noise ratio (SNR) is an increasingly popular criterion to qualify the
high SNR optimality of support recovery techniques. The HSC results available
in literature for support recovery techniques applicable to underdetermined
linear regression models like least absolute shrinkage and selection operator
(LASSO), orthogonal matching pursuit (OMP) etc. assume \textit{a priori}
knowledge of noise variance or signal sparsity. However, both these parameters
are unavailable in most practical applications. Further, it is extremely
difficult to estimate noise variance or signal sparsity in underdetermined
regression models. This limits the utility of existing HSC results. In this
article, we propose two techniques, \textit{viz.}, residual ratio minimization
(RRM) and residual ratio thresholding with adaptation (RRTA) to operate OMP
algorithm without the \textit{a priroi} knowledge of noise variance and signal
sparsity and establish their HSC analytically and numerically. To the best of
our knowledge, these are the first and only noise statistics oblivious
algorithms to report HSC in underdetermined regression models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07139</identifier>
 <datestamp>2018-12-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07139</id><created>2018-11-17</created><updated>2018-12-17</updated><authors><author><keyname>Escobar</keyname><forenames>Mauro</forenames></author><author><keyname>Bienstock</keyname><forenames>Daniel</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author></authors><title>Learning from power system data stream: phasor-detective approach</title><categories>physics.data-an eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuming access to synchronized stream of Phasor Measurement Unit (PMU) data
over a significant portion of a power system interconnect, say controlled by an
Independent System Operator (ISO), what can you extract about past, current and
future state of the system? We have focused on answering this practical
questions pragmatically - empowered with nothing but standard tools of data
analysis, such as PCA, filtering and cross-correlation analysis. Quite
surprisingly we have found that even during the quiet &quot;no significant events&quot;
period this standard set of statistical tools allows the &quot;phasor-detective&quot; to
extract from the data important hidden anomalies, such as problematic control
loops at loads and wind farms, and mildly malfunctioning assets, such as
transformers and generators. We also discuss and sketch future challenges a
mature phasor-detective can possibly tackle by adding machine learning and
physics modeling sophistication to the basic approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07144</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07144</id><created>2018-11-17</created><authors><author><keyname>Markoulakis</keyname><forenames>Emmanouil</forenames></author><author><keyname>Antonidakis</keyname><forenames>Emmanuel</forenames></author><author><keyname>Stavrakakis</keyname><forenames>George S.</forenames></author></authors><title>A simulink circuit model for measurement of consumption of electric
  energy using frequency method</title><categories>eess.SP</categories><comments>8 pages, 8 figures, 3 tables, IASTED Power and Energy Systems
  International Conference EuroPES 2011, Crete, Greece, June 22 to 24, 2011</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The following analysis and presentation summarizes the implementation in
Matlab / Simulink environment of a prototype model digital energy circuit for
measurement of the consumption of electric energy of an electrification network
in the installations of customer of electric energy that can be applied as part
of any consumption smart meter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07240</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07240</id><created>2018-11-17</created><updated>2018-11-24</updated><authors><author><keyname>Kastner</keyname><forenames>Kyle</forenames></author><author><keyname>Santos</keyname><forenames>Jo&#xe3;o Felipe</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author></authors><title>Representation Mixing for TTS Synthesis</title><categories>cs.LG cs.CL cs.SD eess.AS stat.ML</categories><comments>5 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent character and phoneme-based parametric TTS systems using deep learning
have shown strong performance in natural speech generation. However, the choice
between character or phoneme input can create serious limitations for practical
deployment, as direct control of pronunciation is crucial in certain cases. We
demonstrate a simple method for combining multiple types of linguistic
information in a single encoder, named representation mixing, enabling flexible
choice between character, phoneme, or mixed representations during inference.
Experiments and user studies on a public audiobook corpus show the efficacy of
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07281</identifier>
 <datestamp>2019-02-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07281</id><created>2018-11-18</created><updated>2019-02-19</updated><authors><author><keyname>Kaneko</keyname><forenames>Yuhei</forenames></author><author><keyname>Muramatsu</keyname><forenames>Shogo</forenames></author><author><keyname>Yasuda</keyname><forenames>Hiroyasu</forenames></author><author><keyname>Hayasaka</keyname><forenames>Kiyoshi</forenames></author><author><keyname>Otake</keyname><forenames>Yu</forenames></author><author><keyname>Ono</keyname><forenames>Shunsuke</forenames></author><author><keyname>Yukawa</keyname><forenames>Masahiro</forenames></author></authors><title>Convolutional-Sparse-Coded Dynamic Mode Decomposition and Its
  Application to River State Estimation</title><categories>eess.SP math.DS</categories><comments>To appear at IEEE ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes convolutional-sparse-coded dynamic mode decomposition
(CSC-DMD) by unifying extended dynamic mode decomposition (EDMD) and
convolutional sparse coding. EDMD is a data driven analysis method for
describing a nonlinear dynamical system with a linear time-evolution equation.
Compared with existing EDMD methods, CSC-DMD has an advantage of reflecting
spatial structure of the target. As an example, the proposed method is applied
to river bed shape estimation from the water surface observation. The
estimation problem is reduced to sparsity-aware restoration with a hard
constraint, which is given by the CSC-DMD prediction, where the algorithm is
derived by the primal-dual splitting method. A time series set of water surface
and bed shape measured through an experimental river setup is used to train and
test the system. From the result, the significance of the proposed method is
verified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07330</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07330</id><created>2018-11-18</created><authors><author><keyname>Siddique</keyname><forenames>Ayesha</forenames></author><author><keyname>Hasan</keyname><forenames>Osman</forenames></author><author><keyname>Khalid</keyname><forenames>Faiq</forenames></author><author><keyname>Shafique</keyname><forenames>Muhammad</forenames></author></authors><title>ApproxCS: Near-Sensor Approximate Compressed Sensing for IoT-Healthcare
  Systems</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoTs) is an emerging trend that has enabled an upgrade in
the design of wearable healthcare monitoring systems through the (integrated)
edge, fog, and cloud computing paradigm. Energy efficiency is one of the most
important design metrics in such IoT-healthcare systems especially, for the
edge and fog nodes. Due to the sensing noise and inherent redundancy in the
input data, even the most safety-critical biomedical applications can sometimes
afford a slight degradation in the output quality. Hence, such inherent error
tolerance in the bio-signals can be exploited to achieve high energy savings
through the emerging trends like, the Approximate Computing which is applicable
at both software and hardware levels. In this paper, we propose to leverage the
approximate computing in digital Compressed Sensing (CS), through low-power
approximate adders (LPAA) in an accurate Bernoulli sensing-based CS acquisition
(BCS). We demonstrate that approximations can indeed be safely employed in IoT
healthcare without affecting the detection of critical events in the biomedical
signals. Towards this, we explored the trade-of between energy efficiency and
output quality using the state-of-the-art lp2d RLS reconstruction algorithm.
The proposed framework is validated with the MIT-BIH Arrhythmia database. Our
results demonstrated approximately 59% energy savings as compared to the
accurate design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07334</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07334</id><created>2018-11-18</created><authors><author><keyname>Ren</keyname><forenames>Hai-Peng</forenames></author><author><keyname>Zheng</keyname><forenames>Wu-Yun</forenames></author><author><keyname>Grebogi</keyname><forenames>Celso</forenames></author></authors><title>Radio-wave communication with chaos</title><categories>eess.SP nlin.CD</categories><comments>6 pages 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The invariance of the Lyapunov exponent of a chaotic signal as it propagates
along a wireless transmission channel provides a theoretical base for the
application of chaos in wireless communication. In additive Gaussian channel,
the chaotic signal is proved to be the optimal coherent communication waveform
in the sense of using the very simple matched filter to obtain the maximum
signal-to-noise ratio. The properties of chaos can be used to reduce simply and
effectively the Inter-Symbol Interference (ISI) and to achieve low bit error
rate in the wireless communication system. However, chaotic signals need very
wide bandwidth to be transmitted in the practical channel, which is difficult
for the practical transducer or antenna to convert such a broad band signal. To
solve this problem, in this work, the chaotic signal is applied to a radio-wave
communication system, and the corresponding coding and decoding algorithms are
proposed. A hybrid chaotic system is used as the pulse-shaping filter to obtain
the baseband signal, and the corresponding matched filter is used at the
receiver, instead of the conventional low-pass filter, to maximize the
signal-to-noise ratio. At the same time, the symbol judgment threshold
determined by the chaos property is used to reduce the Inter-Symbol
Interference (ISI) effect. Simulations and virtual channel experiments show
that the radio-wave communication system using chaos obtains lower bit error
rate in the multi-path transmission channel compared with the traditional
radio-wave communication system using Binary Phase Shift Keying (BPSK)
modulation technology and channel equalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07400</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07400</id><created>2018-11-18</created><authors><author><keyname>Khawaja</keyname><forenames>Wahab Ali Gulzar</forenames></author><author><keyname>Ozdemir</keyname><forenames>Ozgur</forenames></author><author><keyname>Erden</keyname><forenames>Fatih</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>Ezuma</keyname><forenames>Martins</forenames></author><author><keyname>Kakishima</keyname><forenames>Yuichi</forenames></author></authors><title>Effect of Passive Reflectors for Enhancing Coverage of 28 GHz mmWave
  Systems in an Outdoor Setting</title><categories>eess.SP</categories><comments>This research article is accepted for publication in IEEE Radio &amp;
  Wireless Week Conference (RWW), 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of large unused spectrum at millimeter wave (mmWave)
frequency bands has steered the future 5G research towards these bands.
However, mmWave signals are attenuated severely in the non-lineof-sight (NLOS)
scenarios, thereby leaving the strong link quality by a large margin to
line-of-sight (LOS) links. In this paper, a passive metallic reflector is used
to enhance the coverage for mmWave signals in an outdoor, NLOS propagation
scenarios. The received power from different azimuth and elevation angles are
measured at 28 GHz in a parking lot setting. Our results show that using a 33
inch by 33 inch metallic reflector, the received power can be enhanced by 19 dB
compared to no reflector case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07417</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07417</id><created>2018-11-18</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>PerSIM: Multi-resolution Image Quality Assessment in the Perceptually
  Uniform Color Domain</title><categories>eess.IV cs.CV cs.MM eess.SP</categories><comments>5 pages, 1 figure, 3 tables</comments><acm-class>I.4</acm-class><journal-ref>2015 IEEE International Conference on Image Processing (ICIP),
  Quebec City, QC, 2015, pp. 1682-1686</journal-ref><doi>10.1109/ICIP.2015.7351087</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An average observer perceives the world in color instead of black and white.
Moreover, the visual system focuses on structures and segments instead of
individual pixels. Based on these observations, we propose a full reference
objective image quality metric modeling visual system characteristics and
chroma similarity in the perceptually uniform color domain (Lab). Laplacian of
Gaussian features are obtained in the L channel to model the retinal ganglion
cells in human visual system and color similarity is calculated over the a and
b channels. In the proposed perceptual similarity index (PerSIM), a
multi-resolution approach is followed to mimic the hierarchical nature of human
visual system. LIVE and TID2013 databases are used in the validation and PerSIM
outperforms all the compared metrics in the overall databases in terms of
ranking, monotonic behavior and linearity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07426</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07426</id><created>2018-11-18</created><authors><author><keyname>Kastner</keyname><forenames>Kyle</forenames></author><author><keyname>Kumar</keyname><forenames>Rithesh</forenames></author><author><keyname>Cooijmans</keyname><forenames>Tim</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author></authors><title>Harmonic Recomposition using Conditional Autoregressive Modeling</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>3 pages, 2 figures. In Proceedings of The Joint Workshop on Machine
  Learning for Music, ICML 2018</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We demonstrate a conditional autoregressive pipeline for efficient music
recomposition, based on methods presented in van den Oord et al.(2017).
Recomposition (Casal &amp; Casey, 2010) focuses on reworking existing musical
pieces, adhering to structure at a high level while also re-imagining other
aspects of the work. This can involve reuse of pre-existing themes or parts of
the original piece, while also requiring the flexibility to generate new
content at different levels of granularity. Applying the aforementioned
modeling pipeline to recomposition, we show diverse and structured generation
conditioned on chord sequence annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07428</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07428</id><created>2018-11-18</created><authors><author><keyname>Tsitsikas</keyname><forenames>Georgios</forenames></author><author><keyname>Papalexakis</keyname><forenames>Evangelos E.</forenames></author></authors><title>The core consistency of a compressed tensor</title><categories>cs.LG eess.SP stat.ML</categories><comments>5 pages, 4 figures, submitted to International Conference on
  Acoustics, Speech, and Signal Processing ( IEEE ICASSP 2019 )</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor decomposition on big data has attracted significant attention
recently. Among the most popular methods is a class of algorithms that
leverages compression in order to reduce the size of the tensor and potentially
parallelize computations. A fundamental requirement for such methods to work
properly is that the low-rank tensor structure is retained upon compression. In
lieu of efficient and realistic means of computing and studying the effects of
compression on the low rank of a tensor, we study the effects of compression on
the core consistency; a widely used heuristic that has been used as a proxy for
estimating that low rank. We provide theoretical analysis, where we identify
sufficient conditions for the compression such that the core consistency is
preserved, and we conduct extensive experiments that validate our analysis.
Further, we explore popular compression schemes and how they affect the core
consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07435</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07435</id><created>2018-11-18</created><authors><author><keyname>Mohapatra</keyname><forenames>Debasish Ray</forenames></author><author><keyname>Fels</keyname><forenames>Sidney</forenames></author></authors><title>Limitations of Source-Filter Coupling In Phonation</title><categories>cs.SD cs.CL eess.AS</categories><comments>2 pages, 2 figures</comments><doi>10.1121/1.5068357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coupling of vocal fold (source) and vocal tract (filter) is one of the
most critical factors in source-filter articulation theory. The traditional
linear source-filter theory has been challenged by current research which
clearly shows the impact of acoustic loading on the dynamic behavior of the
vocal fold vibration as well as the variations in the glottal flow pulses
shape. This paper outlines the underlying mechanism of source-filter
interactions; demonstrates the design and working principles of coupling for
the various existing vocal cord and vocal tract biomechanical models. For our
study, we have considered self-oscillating lumped-element models of the
acoustic source and computational models of the vocal tract as articulators. To
understand the limitations of source-filter interactions which are associated
with each of those models, we compare them concerning their mechanical design,
acoustic and physiological characteristics and aerodynamic simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07445</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07445</id><created>2018-11-18</created><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Mei</keyname><forenames>Kai</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaochen</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaoying</forenames></author><author><keyname>Ma</keyname><forenames>Dongtang</forenames></author><author><keyname>Wei</keyname><forenames>Jibo</forenames></author></authors><title>High-precision timing and frequency synchronization method for MIMO-OFDM
  systems in double-selective channels</title><categories>eess.SP cs.IT math.IT</categories><comments>2 pages letter with 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, a novel synchronization method for MIMO-OFDM systems is
proposed. The new approach has an accurate estimate of both symbol timing and
large frequency offest. Simulation results show the excellent robustness of our
method in double-selective channel even if the strongest multipath component
arrives behind the first path.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07453</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07453</id><created>2018-11-18</created><updated>2019-02-15</updated><authors><author><keyname>Ravanelli</keyname><forenames>Mirco</forenames></author><author><keyname>Parcollet</keyname><forenames>Titouan</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>The PyTorch-Kaldi Speech Recognition Toolkit</title><categories>eess.AS cs.CL cs.LG cs.NE</categories><comments>Accepted at ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of open-source software is playing a remarkable role in the
popularization of speech recognition and deep learning. Kaldi, for instance, is
nowadays an established framework used to develop state-of-the-art speech
recognizers. PyTorch is used to build neural networks with the Python language
and has recently spawn tremendous interest within the machine learning
community thanks to its simplicity and flexibility.
  The PyTorch-Kaldi project aims to bridge the gap between these popular
toolkits, trying to inherit the efficiency of Kaldi and the flexibility of
PyTorch. PyTorch-Kaldi is not only a simple interface between these software,
but it embeds several useful features for developing modern speech recognizers.
For instance, the code is specifically designed to naturally plug-in
user-defined acoustic models. As an alternative, users can exploit several
pre-implemented neural networks that can be customized using intuitive
configuration files. PyTorch-Kaldi supports multiple feature and label streams
as well as combinations of neural networks, enabling the use of complex neural
architectures. The toolkit is publicly-released along with a rich documentation
and is designed to properly work locally or on HPC clusters.
  Experiments, that are conducted on several datasets and tasks, show that
PyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech
recognizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07505</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07505</id><created>2018-11-19</created><authors><author><keyname>Feng</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Menghan</forenames></author><author><keyname>Wang</keyname><forenames>Dongming</forenames></author><author><keyname>You</keyname><forenames>Xiaohu</forenames></author></authors><title>Low Complexity Iterative Detection for a Large-scale Distributed MIMO
  Prototyping System</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the low-complexity iterative soft-input soft-output
(SISO) detection algorithm in a large-scale distributed multiple-input
multiple-output (MIMO) system. The uplink interference suppression matrix is
designed to decompose the received multi-user signal into independent
single-user receptions. An improved minimum-mean-square-error iterative soft
decision interference cancellation (MMSE-ISDIC) based on eigenvalue
decomposition (EVD-MMSE-ISDIC) is proposed to perform low-complexity detection
of the decomposed signals. Furthermore, two iteration schemes are given to
improve receiving performance, which are iterative detection and decoding (IDD)
scheme and iterative detection (ID) scheme. While IDD utilizes the external
information generated by the decoder for iterative detection, the output
information of the detector is directly exploited with ID. In particular, a
large-scale distributed MIMO prototyping system is introduced and a 32$ \times
$32 (4 user equipments (UEs) and 4 remote antenna units (RAUs), each equipped
with 8 antennas) experimental tests at 3.5 GHz was performed. The experimental
results show that the proposed iterative receiver greatly outperforms the
linear MMSE receiver, since it reduces the average number of error blocks of
the system significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07564</identifier>
 <datestamp>2019-02-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07564</id><created>2018-11-19</created><updated>2019-02-18</updated><authors><author><keyname>Stolk</keyname><forenames>Christiaan C.</forenames></author><author><keyname>Sbrizzi</keyname><forenames>Alessandro</forenames></author></authors><title>Understanding the combined effect of $k$-space undersampling and
  transient states excitation in MR Fingerprinting reconstructions</title><categories>physics.med-ph eess.SP math.NA</categories><comments>Main document: 11 pages, 5 figures, 2 tables. Supplementary material:
  7 pages, 5 figures. Total 18 pages</comments><msc-class>65R32, 92C55</msc-class><journal-ref>IEEE Transactions on Medical Imaging, 2019 (Early Access, see DOI)</journal-ref><doi>10.1109/TMI.2019.2900585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonance fingerprinting (MRF) is able to estimate multiple
quantitative tissue parameters from a relatively short acquisition. The main
characteristic of an MRF sequence is the simultaneous application of (a)
transient states excitation and (b) highly undersampled $k$-space. Despite the
promising empirical results obtained with MRF, no work has appeared that
formally describes the combined impact of these two aspects on the
reconstruction accuracy. In this paper, a mathematical model is derived that
directly relates the time varying RF excitation and the $k$-space sampling to
the spatially dependent reconstruction errors. A subsequent in-depth analysis
identifies the mechanisms by which MRF sequence properties affect accuracy,
providing a formal explanation of several empirically observed or intuitively
understood facts. New insights are obtained which show how this analytical
framework could be used to improve the MRF protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07629</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07629</id><created>2018-11-19</created><authors><author><keyname>Novotny</keyname><forenames>Ondrej</forenames></author><author><keyname>Plchot</keyname><forenames>Oldrich</forenames></author><author><keyname>Glembek</keyname><forenames>Ondrej</forenames></author><author><keyname>Cernocky</keyname><forenames>Jan &quot;Honza&quot;</forenames></author><author><keyname>Burget</keyname><forenames>Lukas</forenames></author></authors><title>Analysis of DNN Speech Signal Enhancement for Robust Speaker Recognition</title><categories>eess.AS cs.SD</categories><comments>16 pages, 7 figures, Submission to Computer Speech and Language,
  special issue on Speaker and language characterization and recognition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present an analysis of a DNN-based autoencoder for speech
enhancement, dereverberation and denoising. The target application is a robust
speaker verification (SV) system. We start our approach by carefully designing
a data augmentation process to cover wide range of acoustic conditions and
obtain rich training data for various components of our SV system. We augment
several well-known databases used in SV with artificially noised and
reverberated data and we use them to train a denoising autoencoder (mapping
noisy and reverberated speech to its clean version) as well as an x-vector
extractor which is currently considered as state-of-the-art in SV. Later, we
use the autoencoder as a preprocessing step for text-independent SV system. We
compare results achieved with autoencoder enhancement, multi-condition PLDA
training and their simultaneous use. We present a detailed analysis with
various conditions of NIST SRE 2010, 2016, PRISM and with re-transmitted data.
We conclude that the proposed preprocessing can significantly improve both
i-vector and x-vector baselines and that this technique can be used to build a
robust SV system for various target domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07637</identifier>
 <datestamp>2018-11-20</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07637</id><created>2018-11-19</created><authors><author><keyname>Jiang</keyname><forenames>Zouyi</forenames></author><author><keyname>Zhao</keyname><forenames>Lei</forenames></author><author><keyname>Gao</keyname><forenames>Xingshun</forenames></author><author><keyname>Dong</keyname><forenames>Ruoshi</forenames></author><author><keyname>Liu</keyname><forenames>Jinxin</forenames></author><author><keyname>An</keyname><forenames>Qi</forenames></author></authors><title>Mismatch error correction for time interleaved analog-to-digital
  converter over a wide frequency range</title><categories>eess.SP hep-ex physics.ins-det</categories><journal-ref>Review of Scientific Instruments 89, 084709 (2018)</journal-ref><doi>10.1063/1.5030782</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-speed high-resolution Analog-to-Digital Conversion is the key part for
waveform digitization in physics experiments and many other domains. This paper
presents a new fully digital correction of mismatch errors among the channels
in Time Interleaved Analog-to-Digital Converter (TIADC) systems. We focus on
correction with wide-band input signal, which means that we can correct the
mismatch errors for any frequency point in a broad band with only one set of
filter coefficients. Studies were also made to show how to apply the correction
algorithm beyond the base band, i.e. other Nyquist zones in the under-sampling
situation. Structure of the correction algorithm is presented in this paper, as
well as simulation results. To evaluate the correction performance, we actually
conducted a series of tests with two TIADC systems. The results indicate that
the performance of both two TIADC systems can be greatly improved by
correction, and the Effective Number Of Bits (ENOB) is successfully improved to
be better than 9.5 bits and 5.5 bits for an input signal up to the bandwidth
(-3dB) range in the 1.6-Gsps 14-bit and the 10-Gsps 8-bit TIADC systems,
respectively. Tests were also conducted for input signal frequencies in the
second Nyquist zone, which shows that the correction algorithms also work well
as expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07684</identifier>
 <datestamp>2019-02-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07684</id><created>2018-11-19</created><updated>2019-02-18</updated><authors><author><keyname>Coucke</keyname><forenames>Alice</forenames></author><author><keyname>Chlieh</keyname><forenames>Mohammed</forenames></author><author><keyname>Gisselbrecht</keyname><forenames>Thibault</forenames></author><author><keyname>Leroy</keyname><forenames>David</forenames></author><author><keyname>Poumeyrol</keyname><forenames>Mathieu</forenames></author><author><keyname>Lavril</keyname><forenames>Thibaut</forenames></author></authors><title>Efficient keyword spotting using dilated convolutions and gating</title><categories>cs.LG cs.CL cs.SD eess.AS stat.ML</categories><comments>Accepted for publication to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the application of end-to-end stateless temporal modeling to
small-footprint keyword spotting as opposed to recurrent networks that model
long-term temporal dependencies using internal states. We propose a model
inspired by the recent success of dilated convolutions in sequence modeling
applications, allowing to train deeper architectures in resource-constrained
configurations. Gated activations and residual connections are also added,
following a similar configuration to WaveNet. In addition, we apply a custom
target labeling that back-propagates loss from specific frames of interest,
therefore yielding higher accuracy and only requiring to detect the end of the
keyword. Our experimental results show that our model outperforms a max-pooling
loss trained recurrent neural network using LSTM cells, with a significant
decrease in false rejection rate. The underlying dataset - &quot;Hey Snips&quot;
utterances recorded by over 2.2K different speakers - has been made publicly
available to establish an open reference for wake-word detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07804</identifier>
 <datestamp>2019-12-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07804</id><created>2018-11-19</created><updated>2019-12-12</updated><authors><author><keyname>Rezaie</keyname><forenames>Reza</forenames></author><author><keyname>Li</keyname><forenames>X. Rong</forenames></author></authors><title>Explicitly Sample-Equivalent Dynamic Models for Gaussian Markov,
  Reciprocal, and Conditionally Markov Sequences</title><categories>math.PR cs.SY eess.SP math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conditionally Markov (CM) sequence contains different classes including
Markov, reciprocal, and so-called $CM_L$ and $CM_F$ (two special classes of CM
sequences). Each class has its own forward and backward dynamic models. The
evolution of a CM sequence can be described by different models. For example, a
Markov sequence can be described by a Markov model, as well as by reciprocal,
$CM_L$, and $CM_F$ models. Also, sometimes a forward model is available, but it
is desirable to have a backward model for the same sequence (e.g., in
smoothing). Therefore, it is important to study relationships between different
dynamic models of a CM sequence. This paper discusses such relationships
between models of nonsingular Gaussian (NG) $CM_L$, $CM_F$, reciprocal, and
Markov sequences. Two models are said to be explicitly sample-equivalent if not
only they govern the same sequence, but also a one-one correspondence between
their sample paths is made explicitly. A unified approach is presented, such
that given a forward/backward $CM_L$/$CM_F$/reciprocal/Markov model, any
explicitly equivalent model can be obtained. As a special case, a backward
Markov model explicitly equivalent to a given forward Markov model can be
obtained regardless of the singularity/nonsingularity of the state transition
matrix of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07875</identifier>
 <datestamp>2020-01-07</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07875</id><created>2018-10-25</created><updated>2019-01-05</updated><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Lin</keyname><forenames>Youzuo</forenames></author></authors><title>InversionNet: A Real-Time and Accurate Full Waveform Inversion with CNNs
  and continuous CRFs</title><categories>eess.SP</categories><doi>10.1109/TCI.2019.2956866</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-waveform inversion problems are usually formulated as optimization
problems, where the forward-wave propagation operator $f$ maps the subsurface
velocity structures to seismic signals. The existing computational methods for
solving full-waveform inversion are not only computationally expensive, but
also yields low-resolution results because of the ill-posedness and cycle
skipping issues of full-waveform inversion. To resolve those issues, we employ
machine-learning techniques to solve the full-waveform inversion. Specifically,
we focus on applying the convolutional neural network~(CNN) to directly derive
the inversion operator $f^{-1}$ so that the velocity structure can be obtained
without knowing the forward operator $f$. We build a convolutional neural
network with an encoder-decoder structure to model the correspondence from
seismic data to subsurface velocity structures. Furthermore, we employ the
conditional random field~(CRF) on top of the CNN to generate structural
predictions by modeling the interactions between different locations on the
velocity model. Our numerical examples using synthetic seismic reflection data
show that the propose CNN-CRF model significantly improve the accuracy of the
velocity inversion while the computational time is reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07988</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07988</id><created>2018-11-13</created><authors><author><keyname>Chen</keyname><forenames>Zhanli</forenames></author><author><keyname>Ansari</keyname><forenames>Rashid</forenames></author><author><keyname>Wilkie</keyname><forenames>Diana</forenames></author></authors><title>Automated Pain Detection from Facial Expressions using FACS: A Review</title><categories>cs.CV cs.LG eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facial pain expression is an important modality for assessing pain,
especially when the patient's verbal ability to communicate is impaired. The
facial muscle-based action units (AUs), which are defined by the Facial Action
Coding System (FACS), have been widely studied and are highly reliable as a
method for detecting facial expressions (FE) including valid detection of pain.
Unfortunately, FACS coding by humans is a very time-consuming task that makes
its clinical use prohibitive. Significant progress on automated facial
expression recognition (AFER) has led to its numerous successful applications
in FACS-based affective computing problems. However, only a handful of studies
have been reported on automated pain detection (APD), and its application in
clinical settings is still far from a reality. In this paper, we review the
progress in research that has contributed to automated pain detection, with
focus on 1) the framework-level similarity between spontaneous AFER and APD
problems; 2) the evolution of system design including the recent development of
deep learning methods; 3) the strategies and considerations in developing a
FACS-based pain detection framework from existing research; and 4) introduction
of the most relevant databases that are available for AFER and APD studies. We
attempt to present key considerations in extending a general AFER framework to
an APD framework in clinical settings. In addition, the performance metrics are
also highlighted in evaluating an AFER or an APD system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.07990</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.07990</id><created>2018-11-19</created><authors><author><keyname>Bashir</keyname><forenames>Muhammad Salman</forenames></author></authors><title>Free-Space Optical Communications with Detector Arrays</title><categories>eess.SP</categories><comments>20 pages and 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detector arrays are commonly used for free-space optical communications in
deep space. Such detector arrays---by virtue of their size---help in the
collection of the optical signal even when there is some misalignment between
the transmitter and receiver systems. In this paper, we argue that for the
common Gaussian beam profile, a detector array receiver is more useful for
minimizing the probability of error than a single detector receiver of the same
dimensions. Furthermore, the improvement in the error probability is more
pronounced for low signal-to-noise ratio conditions, and the probability of
error decreases monotonically as a function of the number of detectors in the
array. However, communication with detector arrays results in a larger
computational complexity at the receiver. Additionally, such detector arrays
are also more advantageous for beam position tracking on the detector array in
order to minimize the pointing loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08012</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08012</id><created>2018-11-19</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>A Comparative Study of Computational Aesthetics</title><categories>eess.IV cs.CV cs.MM</categories><comments>6 pages, 5 figures, 1 table</comments><acm-class>I.4</acm-class><journal-ref>2014 IEEE International Conference on Image Processing (ICIP),
  Paris, 2014, pp. 590-594</journal-ref><doi>10.1109/ICIP.2014.7025118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective metrics model image quality by quantifying image degradations or
estimating perceived image quality. However, image quality metrics do not model
what makes an image more appealing or beautiful. In order to quantify the
aesthetics of an image, we need to take it one step further and model the
perception of aesthetics. In this paper, we examine computational aesthetics
models that use hand-crafted, generic and hybrid descriptors. We show that
generic descriptors can perform as well as state of the art hand-crafted
aesthetics models that use global features. However, neither generic nor
hand-crafted features is sufficient to model aesthetics when we only use global
features without considering spatial composition or distribution. We also
follow a visual dictionary approach similar to state of the art methods and
show that it performs poorly without the spatial pyramid step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08013</identifier>
 <datestamp>2019-12-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08013</id><created>2018-11-19</created><updated>2019-12-13</updated><authors><author><keyname>Rezaie</keyname><forenames>Reza</forenames></author><author><keyname>Li</keyname><forenames>X. Rong</forenames></author></authors><title>Models and Representations of Gaussian Reciprocal and Conditionally
  Markov Sequences</title><categories>math.PR cs.SY eess.SP</categories><comments>arXiv admin note: substantial text overlap with arXiv:1811.05639</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditionally Markov (CM) sequences are powerful mathematical tools for
modeling random phenomena. There are several classes of CM sequences one of
which is the reciprocal sequence. Reciprocal sequences have been widely used in
many areas including image processing, intelligent systems, and acausal
systems. To use them in application, we need not only their applicable dynamic
models, but also some general approaches to designing parameters of dynamic
models. Dynamic models governing two important classes of nonsingular Gaussian
(NG) CM sequences (called $CM_L$ and $CM_F$ models), and a dynamic model
governing the NG reciprocal sequence (called reciprocal $CM_L$ model) were
presented in our previous work. In this paper, these models are studied in more
detail and general approaches are presented for their parameter design. It is
shown that every reciprocal $CM_L$ model can be induced by a Markov model and
parameters of the reciprocal $CM_L$ model can be obtained from those of the
Markov model. Also, it is shown how NG CM sequences can be represented in terms
of an NG Markov sequence and an independent NG vector. This representation
provides a general approach for parameter design of $CM_L$ and $CM_F$ models.
In addition, it leads to a better understanding of CM sequences, including the
reciprocal sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08021</identifier>
 <datestamp>2019-12-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08021</id><created>2018-11-19</created><updated>2019-12-13</updated><authors><author><keyname>Rezaie</keyname><forenames>Reza</forenames></author><author><keyname>Li</keyname><forenames>X. Rong</forenames></author></authors><title>Destination-Directed Trajectory Modeling and Prediction Using
  Conditionally Markov Sequences</title><categories>cs.SY eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some problems there is information about the destination of a moving
object. An example is an airliner flying from an origin to a destination. Such
problems have three main components: an origin, a destination, and motion in
between. To emphasize that the motion trajectories end up at the destination,
we call them \textit{destination-directed trajectories}. The Markov sequence is
not flexible enough to model such trajectories. Given an initial density and an
evolution law, the future of a Markov sequence is determined probabilistically.
One class of conditionally Markov (CM) sequences, called the $CM_L$ sequence
(including the Markov sequence as a special case), has the following main
components: a joint endpoint density (i.e., an initial density and a final
density conditioned on the initial) and a Markov-like evolution law. This paper
proposes using the $CM_L$ sequence for modeling destination-directed
trajectories. It is demonstrated how the $CM_L$ sequence enjoys several
desirable properties for destination-directed trajectory modeling. Some
simulations of trajectory modeling and prediction are presented for
illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08026</identifier>
 <datestamp>2019-05-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08026</id><created>2018-11-19</created><updated>2019-05-27</updated><authors><author><keyname>Tygert</keyname><forenames>Mark</forenames></author><author><keyname>Zbontar</keyname><forenames>Jure</forenames></author></authors><title>Simulating single-coil MRI from the responses of multiple coils</title><categories>eess.IV cs.CE</categories><comments>14 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We convert the information-rich measurements of parallel and phased-array MRI
into noisier data that a corresponding single-coil scanner could have taken.
Specifically, we replace the responses from multiple receivers with a linear
combination that emulates the response from only a single, aggregate receiver,
replete with the low signal-to-noise ratio and phase problems of any single one
of the original receivers (combining several receivers is necessary, however,
since the original receivers usually have limited spatial sensitivity). This
enables experimentation in the simpler context of a single-coil scanner prior
to development of algorithms for the full complexity of multiple receiver
coils.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08029</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08029</id><created>2018-11-19</created><authors><author><keyname>Saha</keyname><forenames>Pramit</forenames></author><author><keyname>Mohapatra</keyname><forenames>Debasish Ray</forenames></author><author><keyname>SV</keyname><forenames>Praneeth</forenames></author><author><keyname>Fels</keyname><forenames>Sidney</forenames></author></authors><title>Sound-Stream II: Towards Real-Time Gesture Controlled Articulatory Sound
  Synthesis</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an interface involving four degrees-of-freedom (DOF) mechanical
control of a two dimensional, mid-sagittal tongue through a biomechanical
toolkit called ArtiSynth and a sound synthesis engine called JASS towards
articulatory sound synthesis. As a demonstration of the project, the user will
learn to produce a range of JASS vocal sounds, by varying the shape and
position of the ArtiSynth tongue in 2D space through a set of four force-based
sensors. In other words, the user will be able to physically play around with
these four sensors, thereby virtually controlling the magnitude of four
selected muscle excitations of the tongue to vary articulatory structure. This
variation is computed in terms of Area Functions in ArtiSynth environment and
communicated to the JASS based audio-synthesizer coupled with two-mass glottal
excitation model to complete this end-to-end gesture-to-sound mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08035</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08035</id><created>2018-11-19</created><authors><author><keyname>Afrin</keyname><forenames>Kahkashan</forenames></author><author><keyname>Verma</keyname><forenames>Parikshit</forenames></author><author><keyname>Srivatsa</keyname><forenames>Sanjay S.</forenames></author><author><keyname>Bukkapatnam</keyname><forenames>Satish T. S.</forenames></author></authors><title>Simultaneous 12-Lead Electrocardiogram Synthesis using a Single-Lead ECG
  Signal: Application to Handheld ECG Devices</title><categories>eess.SP cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent introduction of wearable single-lead ECG devices of diverse
configurations has caught the intrigue of the medical community. While these
devices provide a highly affordable support tool for the caregivers for
continuous monitoring and to detect acute conditions, such as arrhythmia, their
utility for cardiac diagnostics remains limited. This is because clinical
diagnosis of many cardiac pathologies is rooted in gleaning patterns from
synchronous 12-lead ECG. If synchronous 12-lead signals of clinical quality can
be synthesized from these single-lead devices, it can transform cardiac care by
substantially reducing the costs and enhancing access to cardiac diagnostics.
However, prior attempts to synthesize synchronous 12-lead ECG have not been
successful. Vectorcardiography (VCG) analysis suggests that cardiac axis
synthesized from earlier attempts deviates significantly from that estimated
from 12-lead and/or Frank lead measurements. This work is perhaps the first
successful attempt to synthesize clinically equivalent synchronous 12-lead ECG
from single-lead ECG. Our method employs a random forest machine learning model
that uses a subject's historical 12-lead recordings to estimate the morphology
including the actual timing of various ECG events (relative to the measured
single-lead ECG) for all 11 missing leads of the subject. Our method was
validated on two benchmark datasets as well as paper ECG and AliveCor-Kardia
data obtained from the Heart, Artery, and Vein Center of Fresno, California.
Results suggest that this approach can synthesize synchronous ECG with
accuracies (R2) exceeding 90%. Accurate synthesis of 12-lead ECG from a
single-lead device can ultimately enable its wider application and improved
point-of-care (POC) diagnostics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08045</identifier>
 <datestamp>2019-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08045</id><created>2018-11-19</created><updated>2019-11-26</updated><authors><author><keyname>Thickstun</keyname><forenames>John</forenames></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames></author><author><keyname>Foster</keyname><forenames>Dean P.</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author></authors><title>Coupled Recurrent Models for Polyphonic Music Composition</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>13 pages; long version of the paper appearing in ISMIR 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel recurrent model for music composition that is
tailored to the structure of polyphonic music. We propose an efficient new
conditional probabilistic factorization of musical scores, viewing a score as a
collection of concurrent, coupled sequences: i.e. voices. To model the
conditional distributions, we borrow ideas from both convolutional and
recurrent neural models; we argue that these ideas are natural for capturing
music's pitch invariances, temporal structure, and polyphony. We train models
for single-voice and multi-voice composition on 2,300 scores from the
KernScores dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08065</identifier>
 <datestamp>2019-08-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08065</id><created>2018-11-19</created><updated>2019-07-31</updated><authors><author><keyname>Chen</keyname><forenames>Feiyang</forenames></author><author><keyname>Luo</keyname><forenames>Ziqian</forenames></author></authors><title>Learning Robust Heterogeneous Signal Features from Parallel Neural
  Network for Audio Sentiment Analysis</title><categories>eess.AS cs.CL cs.SD</categories><comments>21 pages, PR JOURNAL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio Sentiment Analysis is a popular research area which extends the
conventional text-based sentiment analysis to depend on the effectiveness of
acoustic features extracted from speech. However, current progress on audio
sentiment analysis mainly focuses on extracting homogeneous acoustic features
or doesn't fuse heterogeneous features effectively. In this paper, we propose
an utterance-based deep neural network model, which has a parallel combination
of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based
network, to obtain representative features termed Audio Sentiment Vector (ASV),
that can maximally reflect sentiment information in an audio. Specifically, our
model is trained by utterance-level labels and ASV can be extracted and fused
creatively from two branches. In the CNN model branch, spectrum graphs produced
by signals are fed as inputs while in the LSTM model branch, inputs include
spectral features and cepstrum coefficient extracted from dependent utterances
in audio. Besides, Bidirectional Long Short-Term Memory (BiLSTM) with attention
mechanism is used for feature fusion. Extensive experiments have been conducted
to show our model can recognize audio sentiment precisely and quickly, and
demonstrate our ASV is better than traditional acoustic features or vectors
extracted from other deep learning models. Furthermore, experimental results
indicate that the proposed model outperforms the state-of-the-art approach by
9.33\% on Multimodal Opinion-level Sentiment Intensity dataset (MOSI) dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08111</identifier>
 <datestamp>2020-01-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08111</id><created>2018-11-20</created><authors><author><keyname>Zhang</keyname><forenames>Jing-Xuan</forenames></author><author><keyname>Ling</keyname><forenames>Zhen-Hua</forenames></author><author><keyname>Jiang</keyname><forenames>Yuan</forenames></author><author><keyname>Liu</keyname><forenames>Li-Juan</forenames></author><author><keyname>Liang</keyname><forenames>Chen</forenames></author><author><keyname>Dai</keyname><forenames>Li-Rong</forenames></author></authors><title>Improving Sequence-to-Sequence Acoustic Modeling by Adding
  Text-Supervision</title><categories>cs.SD eess.AS</categories><comments>5 pages, 4 figures, 2 tables. Submitted to IEEE ICASSP 2019</comments><journal-ref>IEEE International Conference on Acoustic, Speech and Signal
  Processing (2019) 6785-6789</journal-ref><doi>10.1109/ICASSP.2019.8682380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents methods of making using of text supervision to improve
the performance of sequence-to-sequence (seq2seq) voice conversion. Compared
with conventional frame-to-frame voice conversion approaches, the seq2seq
acoustic modeling method proposed in our previous work achieved higher
naturalness and similarity. In this paper, we further improve its performance
by utilizing the text transcriptions of parallel training data. First, a
multi-task learning structure is designed which adds auxiliary classifiers to
the middle layers of the seq2seq model and predicts linguistic labels as a
secondary task. Second, a data-augmentation method is proposed which utilizes
text alignment to produce extra parallel sequences for model training.
Experiments are conducted to evaluate our proposed method with training sets at
different sizes. Experimental results show that the multi-task learning with
linguistic labels is effective at reducing the errors of seq2seq voice
conversion. The data-augmentation method can further improve the performance of
seq2seq voice conversion when only 50 or 100 training utterances are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08119</identifier>
 <datestamp>2019-07-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08119</id><created>2018-11-20</created><updated>2018-12-03</updated><authors><author><keyname>Tang</keyname><forenames>Wankai</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Dai</keyname><forenames>Jun Yan</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Cheng</keyname><forenames>Qiang</forenames></author><author><keyname>Cui</keyname><forenames>Tie Jun</forenames></author></authors><title>Wireless Communications with Programmable Metasurface: Transceiver
  Design and Experimental Results</title><categories>eess.SP</categories><journal-ref>China Communications, vol. 16, no. 5, pp. 46-61, May. 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metasurfaces have drawn significant attentions due to their superior
capability in tailoring electromagnetic waves with a wide frequency range, from
microwave to visible light. Recently, programmable metasurfaces have
demonstrated the ability of manipulating the amplitude or phase of
electromagnetic waves in a programmable manner in real time, which renders them
especially appealing in the applications of wireless communications. To
practically demonstrate the feasibility of programmable metasurfaces in future
communication systems, in this paper, we design and realize a novel
metasurface-based wireless communication system. By exploiting the dynamically
controllable property of programmable metasurface, we firstly introduce the
fundamental principle of the metasurface-based wireless communication system
design. We then present the design, implementation and experimental evaluation
of the proposed metasurface-based wireless communication system with a
prototype, which realizes single carrier quadrature phase shift keying (QPSK)
transmission over the air. In the developed prototype, the phase of the
reflected electromagnetic wave of programmable metasurface is directly
manipulated in real time according to the baseband control signal, which
achieves 2.048 Mbps data transfer rate with video streaming transmission over
the air. Experimental result is provided to compare the performance of the
proposed metasurface-based architecture against the conventional one. With the
slight increase of the transmit power by 5 dB, the same bit error rate (BER)
performance can be achieved as the conventional system in the absence of
channel coding. Such a result is encouraging considering that the
metasurface-based system has the advantages of low hardware cost and simple
structure, thus leading to a promising new architecture for wireless
communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08162</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08162</id><created>2018-11-20</created><authors><author><keyname>Goyal</keyname><forenames>Mohit</forenames></author><author><keyname>Tatwawadi</keyname><forenames>Kedar</forenames></author><author><keyname>Chandak</keyname><forenames>Shubham</forenames></author><author><keyname>Ochoa</keyname><forenames>Idoia</forenames></author></authors><title>DeepZip: Lossless Data Compression using Recurrent Neural Networks</title><categories>cs.CL eess.SP q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential data is being generated at an unprecedented pace in various forms,
including text and genomic data. This creates the need for efficient
compression mechanisms to enable better storage, transmission and processing of
such data. To solve this problem, many of the existing compressors attempt to
learn models for the data and perform prediction-based compression. Since
neural networks are known as universal function approximators with the
capability to learn arbitrarily complex mappings, and in practice show
excellent performance in prediction tasks, we explore and devise methods to
compress sequential data using neural network predictors. We combine recurrent
neural network predictors with an arithmetic coder and losslessly compress a
variety of synthetic, text and genomic datasets. The proposed compressor
outperforms Gzip on the real datasets and achieves near-optimal compression for
the synthetic datasets. The results also help understand why and where neural
networks are good alternatives for traditional finite context models
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08252</identifier>
 <datestamp>2019-01-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08252</id><created>2018-11-20</created><authors><author><keyname>Solomon</keyname><forenames>Oren</forenames></author><author><keyname>Cohen</keyname><forenames>Regev</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Qiong</keyname><forenames>He</forenames></author><author><keyname>Luo</keyname><forenames>Jianwen</forenames></author><author><keyname>van Sloun</keyname><forenames>Ruud J. G.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Deep Unfolded Robust PCA with Application to Clutter Suppression in
  Ultrasound</title><categories>cs.LG eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contrast enhanced ultrasound is a radiation-free imaging modality which uses
encapsulated gas microbubbles for improved visualization of the vascular bed
deep within the tissue. It has recently been used to enable imaging with
unprecedented subwavelength spatial resolution by relying on super-resolution
techniques. A typical preprocessing step in super-resolution ultrasound is to
separate the microbubble signal from the cluttering tissue signal. This step
has a crucial impact on the final image quality. Here, we propose a new
approach to clutter removal based on robust principle component analysis (PCA)
and deep learning. We begin by modeling the acquired contrast enhanced
ultrasound signal as a combination of a low rank and sparse components. This
model is used in robust PCA and was previously suggested in the context of
ultrasound Doppler processing and dynamic magnetic resonance imaging. We then
illustrate that an iterative algorithm based on this model exhibits improved
separation of microbubble signal from the tissue signal over commonly practiced
methods. Next, we apply the concept of deep unfolding to suggest a deep network
architecture tailored to our clutter filtering problem which exhibits improved
convergence speed and accuracy with respect to its iterative counterpart. We
compare the performance of the suggested deep network on both simulations and
in-vivo rat brain scans, with a commonly practiced deep-network architecture
and the fast iterative shrinkage algorithm, and show that our architecture
exhibits better image quality and contrast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08284</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08284</id><created>2018-11-14</created><updated>2019-07-12</updated><authors><author><keyname>Menon</keyname><forenames>Raghav</forenames></author><author><keyname>Kamper</keyname><forenames>Herman</forenames></author><author><keyname>van der Westhuizen</keyname><forenames>Ewald</forenames></author><author><keyname>Quinn</keyname><forenames>John</forenames></author><author><keyname>Niesler</keyname><forenames>Thomas</forenames></author></authors><title>Feature exploration for almost zero-resource ASR-free keyword spotting
  using a multilingual bottleneck extractor and correspondence autoencoders</title><categories>eess.AS cs.LG cs.SD stat.ML</categories><comments>5 pages, 2 figures, 2 tables, 38 references, Accepted at Interspeech
  2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare features for dynamic time warping (DTW) when used to bootstrap
keyword spotting (KWS) in an almost zero-resource setting. Such
quickly-deployable systems aim to support United Nations (UN) humanitarian
relief efforts in parts of Africa with severely under-resourced languages. Our
objective is to identify acoustic features that provide acceptable KWS
performance in such environments. As supervised resource, we restrict ourselves
to a small, easily acquired and independently compiled set of isolated
keywords. For feature extraction, a multilingual bottleneck feature (BNF)
extractor, trained on well-resourced out-of-domain languages, is integrated
with a correspondence autoencoder (CAE) trained on extremely sparse in-domain
data. On their own, BNFs and CAE features are shown to achieve a more than 2%
absolute performance improvement over baseline MFCCs. However, by using BNFs as
input to the CAE, even better performance is achieved, with a more than 11%
absolute improvement in ROC AUC over MFCCs and more than twice as many top-10
retrievals for two evaluated languages, English and Luganda. We conclude that
integrating BNFs with the CAE allows both large out-of-domain and sparse
in-domain resources to be exploited for improved ASR-free keyword spotting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08306</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08306</id><created>2018-11-18</created><authors><author><keyname>Pinto</keyname><forenames>S.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Study of Multi-Step Knowledge-Aided Iterative Nested MUSIC for Direction
  Finding</title><categories>eess.SP cs.IT math.IT</categories><comments>9 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1707.00953, arXiv:1805.00169, arXiv:1703.10523</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a subspace-based algorithm for direction-of-arrival
(DOA) estimation applied to the signals impinging on a two-level nested array,
referred to as multi-step knowledge-aided iterative nested MUSIC method
(MS-KAI-Nested-MUSIC), which significantly improves the accuracy of the
original Nested-MUSIC. Differently from existing knowledge-aided methods
applied to uniform linear arrays (ULAs), which make use of available known DOAs
to improve the estimation of the covariance matrix of the input data, the
proposed Multi-Step KAI-Nested-MU employs knowledge of the structure of the
augmented sample covariance matrix, which is obtained by exploiting the
difference co-array structure covariance matrix, and its perturbation terms and
the gradual incorporation of prior knowledge, which is obtained on line. The
effectiveness of the proposed technique can be noticed by simulations focusing
on uncorrelated closely-spaced sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08361</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08361</id><created>2018-11-20</created><authors><author><keyname>Seyfioglu</keyname><forenames>Mehmet Saygin</forenames></author><author><keyname>Erol</keyname><forenames>Baris</forenames></author><author><keyname>Gurbuz</keyname><forenames>Sevgi Zubeyde</forenames></author><author><keyname>Amin</keyname><forenames>Moeness G.</forenames></author></authors><title>DNN Transfer Learning from Diversified Micro-Doppler for Motion
  Classification</title><categories>eess.SP</categories><comments>Accepted November 2018 to IEEE Transactions on Aerospace and
  Electronics Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, deep neural networks (DNNs) have been the subject of intense
research for the classification of radio frequency (RF) signals, such as
synthetic aperture radar (SAR) imagery or micro-Doppler signatures. However, a
fundamental challenge is the typically small amount of data available due to
the high costs and resources required for measurements. Small datasets limit
the depth of DNNs implementable, and limit performance. In this work, a novel
method for generating diversified radar micro-Doppler signatures using
Kinect-based motion capture simulations is proposed as a training database for
transfer learning with DNNs. In particular, it is shown that together with
residual learning, the proposed DivNet approach allows for the construction of
deeper neural networks and offers improved performance in comparison to
transfer learning from optical imagery. Furthermore, it is shown that
initializing the network using diversified synthetic micro-Doppler signatures
enables not only robust performance for previously unseen target profiles, but
also class generalization. Results are presented for 7-class and 11-class human
activity recognition scenarios using a 4-GHz continuous wave (CW)
software-defined radar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08374</identifier>
 <datestamp>2018-11-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08374</id><created>2018-11-20</created><authors><author><keyname>Islam</keyname><forenames>Md Mofijul</forenames></author><author><keyname>Debnath</keyname><forenames>Amar</forenames></author><author><keyname>Sayeed</keyname><forenames>Tahsin Al</forenames></author><author><keyname>Setu</keyname><forenames>Jyotirmay Nag</forenames></author><author><keyname>Rahman</keyname><forenames>Md Mahmudur</forenames></author><author><keyname>Sakib</keyname><forenames>Md Sadman</forenames></author><author><keyname>Razzaque</keyname><forenames>Md Abdur</forenames></author><author><keyname>Khan</keyname><forenames>Md. Mosaddek</forenames></author><author><keyname>Shatabda</keyname><forenames>Swakkhar</forenames></author></authors><title>A Gray Box Interpretable Visual Debugging Approach for Deep Sequence
  Learning Model</title><categories>cs.LG cs.HC cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Learning algorithms are often used as black box type learning and they
are too complex to understand. The widespread usability of Deep Learning
algorithms to solve various machine learning problems demands deep and
transparent understanding of the internal representation as well as decision
making. Moreover, the learning models, trained on sequential data, such as
audio and video data, have intricate internal reasoning process due to their
complex distribution of features. Thus, a visual simulator might be helpful to
trace the internal decision making mechanisms in response to adversarial input
data, and it would help to debug and design appropriate deep learning models.
However, interpreting the internal reasoning of deep learning model is not well
studied in the literature. In this work, we have developed a visual interactive
web application, namely d-DeVIS, which helps to visualize the internal
reasoning of the learning model which is trained on the audio data. The
proposed system allows to perceive the behavior as well as to debug the model
by interactively generating adversarial audio data point. The web application
of d-DeVIS is available at ddevis.herokuapp.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08380</identifier>
 <datestamp>2019-01-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08380</id><created>2018-11-20</created><updated>2019-01-24</updated><authors><author><keyname>Chen</keyname><forenames>Ke</forenames></author><author><keyname>Zhang</keyname><forenames>Weilin</forenames></author><author><keyname>Dubnov</keyname><forenames>Shlomo</forenames></author><author><keyname>Xia</keyname><forenames>Gus</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author></authors><title>The Effect of Explicit Structure Encoding of Deep Neural Networks for
  Symbolic Music Generation</title><categories>cs.SD cs.AI cs.LG eess.AS</categories><comments>8 pages, 13 figures</comments><journal-ref>Multilayer Music Representation and Processing 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent breakthroughs in artificial neural networks, deep generative
models have become one of the leading techniques for computational creativity.
Despite very promising progress on image and short sequence generation,
symbolic music generation remains a challenging problem since the structure of
compositions are usually complicated. In this study, we attempt to solve the
melody generation problem constrained by the given chord progression. This
music meta-creation problem can also be incorporated into a plan recognition
system with user inputs and predictive structural outputs. In particular, we
explore the effect of explicit architectural encoding of musical structure via
comparing two sequential generative models: LSTM (a type of RNN) and WaveNet
(dilated temporal-CNN). As far as we know, this is the first study of applying
WaveNet to symbolic music generation, as well as the first systematic
comparison between temporal-CNN and RNN for music generation. We conduct a
survey for evaluation in our generations and implemented Variable Markov Oracle
in music pattern discovery. Experimental results show that to encode structure
more explicitly using a stack of dilated convolution layers improved the
performance significantly, and a global encoding of underlying chord
progression into the generation procedure gains even more.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08429</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08429</id><created>2018-11-21</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>Boosting in Image Quality Assessment</title><categories>eess.IV cs.CV cs.MM eess.SP</categories><comments>Paper: 6 pages, 5 tables, 1 figure, Presentation: 16 slides
  [Ancillary files]</comments><acm-class>I.4</acm-class><journal-ref>D. Temel and G. AlRegib, &quot;Boosting in image quality assessment,&quot;
  2016 IEEE 18th International Workshop on Multimedia Signal Processing (MMSP),
  Montreal, QC, 2016, pp. 1-6</journal-ref><doi>10.1109/MMSP.2016.7813335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the effect of boosting in image quality assessment
through multi-method fusion. Existing multi-method studies focus on proposing a
single quality estimator. On the contrary, we investigate the generalizability
of multi-method fusion as a framework. In addition to support vector machines
that are commonly used in the multi-method fusion, we propose using neural
networks in the boosting. To span different types of image quality assessment
algorithms, we use quality estimators based on fidelity, perceptually-extended
fidelity, structural similarity, spectral similarity, color, and learning. In
the experiments, we perform k-fold cross validation using the LIVE, the
multiply distorted LIVE, and the TID 2013 databases and the performance of
image quality assessment algorithms are measured via accuracy-, linearity-, and
ranking-based metrics. Based on the experiments, we show that boosting methods
generally improve the performance of image quality assessment and the level of
improvement depends on the type of the boosting algorithm. Our experimental
results also indicate that boosting the worst performing quality estimator with
two or more additional methods leads to statistically significant performance
enhancements independent of the boosting technique and neural network-based
boosting outperforms support vector machine-based boosting when two or more
methods are fused.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08482</identifier>
 <datestamp>2019-08-21</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08482</id><created>2018-11-20</created><updated>2019-08-20</updated><authors><author><keyname>Loellmann</keyname><forenames>Heinrich W.</forenames></author><author><keyname>Evers</keyname><forenames>Christine</forenames></author><author><keyname>Schmidt</keyname><forenames>Alexander</forenames></author><author><keyname>Barfuss</keyname><forenames>Hendrik</forenames></author><author><keyname>Naylor</keyname><forenames>Patrick A.</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>Proceedings of the LOCATA Challenge Workshop -- a satellite event of
  IWAENC 2018</title><categories>eess.AS cs.SD</categories><comments>Workshop Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for acoustic source localization and tracking provide estimates of
the positional information about active sound sources in acoustic environments
and are essential for a wide range of applications such as personal assistants,
smart homes, tele-conferencing systems, hearing aids, or autonomous systems.
The aim of the IEEE-AASP Challenge on sound source localization and tracking
(LOCATA) was to objectively benchmark state-of-the-art localization and
tracking algorithms using an open-access data corpus of recordings for
scenarios typically encountered in audio and acoustic signal processing
applications. The challenge tasks ranged from the localization of a single
source with a static microphone array to the tracking of multiple moving
sources with a moving microphone array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08487</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08487</id><created>2018-11-20</created><authors><author><keyname>Churchill</keyname><forenames>Victor</forenames></author><author><keyname>Archibald</keyname><forenames>Rick</forenames></author><author><keyname>Gelb</keyname><forenames>Anne</forenames></author></authors><title>Edge-adaptive l2 regularization image reconstruction from non-uniform
  Fourier data</title><categories>eess.IV</categories><msc-class>68U10, 65F22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Total variation regularization based on the l1 norm is ubiquitous in image
reconstruction. However, the resulting reconstructions are not always as sparse
in the edge domain as desired. Iteratively reweighted methods provide some
improvement in accuracy, but at the cost of extended runtime. In this paper we
examine these methods for the case of data acquired as non-uniform Fourier
samples. We then develop a non-iterative weighted regularization method that
uses a pre-processing edge detection to find exactly where the sparsity should
be in the edge domain. We show that its performance in terms of both accuracy
and speed has the potential to outperform reweighted TV regularization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08513</identifier>
 <datestamp>2019-12-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08513</id><created>2018-11-20</created><updated>2019-12-03</updated><authors><author><keyname>Tomita</keyname><forenames>Naofumi</forenames></author><author><keyname>Abdollahi</keyname><forenames>Behnaz</forenames></author><author><keyname>Wei</keyname><forenames>Jason</forenames></author><author><keyname>Ren</keyname><forenames>Bing</forenames></author><author><keyname>Suriawinata</keyname><forenames>Arief</forenames></author><author><keyname>Hassanpour</keyname><forenames>Saeed</forenames></author></authors><title>Attention-Based Deep Neural Networks for Detection of Cancerous and
  Precancerous Esophagus Tissue on Histopathological Slides</title><categories>eess.IV cs.CV</categories><comments>Accepted for publication at the Journal of JAMA Network Open</comments><journal-ref>JAMA Netw Open. 2019;2(11):e1914645</journal-ref><doi>10.1001/jamanetworkopen.2019.14645</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning-based methods, such as the sliding window approach for
cropped-image classification and heuristic aggregation for whole-slide
inference, for analyzing histological patterns in high-resolution microscopy
images have shown promising results. These approaches, however, require a
laborious annotation process and are fragmented. This diagnostic study
collected deidentified high-resolution histological images (N = 379) for
training a new model composed of a convolutional neural network and a
grid-based attention network, trainable without region-of-interest annotations.
Histological images of patients who underwent endoscopic esophagus and
gastroesophageal junction mucosal biopsy between January 1, 2016, and December
31, 2018, at Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire) were
collected. The method achieved a mean accuracy of 0.83 in classifying 123 test
images. These results were comparable with or better than the performance from
the current state-of-the-art sliding window approach, which was trained with
regions of interest. Results of this study suggest that the proposed
attention-based deep neural network framework for Barrett esophagus and
esophageal adenocarcinoma detection is important because it is based solely on
tissue-level annotations, unlike existing methods that are based on regions of
interest. This new model is expected to open avenues for applying deep learning
to digital pathology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08521</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08521</id><created>2018-11-20</created><authors><author><keyname>Wisdom</keyname><forenames>Scott</forenames></author><author><keyname>Hershey</keyname><forenames>John R.</forenames></author><author><keyname>Wilson</keyname><forenames>Kevin</forenames></author><author><keyname>Thorpe</keyname><forenames>Jeremy</forenames></author><author><keyname>Chinen</keyname><forenames>Michael</forenames></author><author><keyname>Patton</keyname><forenames>Brian</forenames></author><author><keyname>Saurous</keyname><forenames>Rif A.</forenames></author></authors><title>Differentiable Consistency Constraints for Improved Deep Speech
  Enhancement</title><categories>cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, deep networks have led to dramatic improvements in speech
enhancement by framing it as a data-driven pattern recognition problem. In many
modern enhancement systems, large amounts of data are used to train a deep
network to estimate masks for complex-valued short-time Fourier transforms
(STFTs) to suppress noise and preserve speech. However, current masking
approaches often neglect two important constraints: STFT consistency and
mixture consistency. Without STFT consistency, the system's output is not
necessarily the STFT of a time-domain signal, and without mixture consistency,
the sum of the estimated sources does not necessarily equal the input mixture.
Furthermore, the only previous approaches that apply mixture consistency use
real-valued masks; mixture consistency has been ignored for complex-valued
masks. In this paper, we show that STFT consistency and mixture consistency can
be jointly imposed by adding simple differentiable projection layers to the
enhancement network. These layers are compatible with real or complex-valued
masks. Using both of these constraints with complex-valued masks provides a 0.7
dB increase in scale-invariant signal-to-distortion ratio (SI-SDR) on a large
dataset of speech corrupted by a wide variety of nonstationary noise across a
range of input SNRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08552</identifier>
 <datestamp>2019-12-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08552</id><created>2018-11-20</created><authors><author><keyname>Chakrabarty</keyname><forenames>Soumitro</forenames></author><author><keyname>Habets</keyname><forenames>Emanu&#xeb;l A. P.</forenames></author></authors><title>Multi-scale aggregation of phase information for reducing computational
  cost of CNN based DOA estimation</title><categories>eess.AS cs.LG cs.SD</categories><comments>arXiv admin note: text overlap with arXiv:1807.11722</comments><doi>10.23919/EUSIPCO.2019.8903176</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent work on direction-of-arrival (DOA) estimation of multiple
speakers with convolutional neural networks (CNNs), the phase component of
short-time Fourier transform (STFT) coefficients of the microphone signal is
given as input and small filters are used to learn the phase relations between
neighboring microphones. Due to this chosen filter size, $M-1$ convolution
layers are required to achieve the best performance for a microphone array with
M microphones. For arrays with large number of microphones, this requirement
leads to a high computational cost making the method practically infeasible. In
this work, we propose to use systematic dilations of the convolution filters in
each of the convolution layers of the previously proposed CNN for expansion of
the receptive field of the filters to reduce the computational cost of the
method. Different strategies for expansion of the receptive field of the
filters for a specific microphone array are explored. With experimental
analysis of the different strategies, it is shown that an aggressive expansion
strategy results in a considerable reduction in computational cost while a
relatively gradual expansion of the receptive field exhibits the best DOA
estimation performance along with reduction in the computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08576</identifier>
 <datestamp>2019-12-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08576</id><created>2018-11-20</created><updated>2019-12-13</updated><authors><author><keyname>Rezaie</keyname><forenames>Reza</forenames></author><author><keyname>Li</keyname><forenames>X. Rong</forenames></author></authors><title>Trajectory Modeling and Prediction with Waypoint Information Using a
  Conditionally Markov Sequence</title><categories>eess.SY cs.SY eess.SP</categories><comments>arXiv admin note: text overlap with arXiv:1811.08021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information about the waypoints of a moving object, e.g., an airliner in an
air traffic control (ATC) problem, should be considered in trajectory modeling
and prediction. Due to the ATC regulations, trajectory design criteria, and
restricted motion capability of airliners there are long-range dependencies in
trajectories of airliners. Waypoint information can be used for modeling such
dependencies in trajectories. This paper proposes a conditionally Markov (CM)
sequence for modeling trajectories passing by waypoints. A dynamic model
governing the proposed sequence is obtained. Filtering and trajectory
prediction formulations are presented. The use of the proposed sequence for
modeling trajectories with waypoints is justified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08592</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08592</id><created>2018-11-20</created><updated>2018-11-26</updated><authors><author><keyname>Haque</keyname><forenames>Albert</forenames></author><author><keyname>Guo</keyname><forenames>Michelle</forenames></author><author><keyname>Miner</keyname><forenames>Adam S</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Measuring Depression Symptom Severity from Spoken Language and 3D Facial
  Expressions</title><categories>cs.CV cs.SD eess.AS</categories><comments>Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216</comments><report-no>ML4H/2018/9</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With more than 300 million people depressed worldwide, depression is a global
problem. Due to access barriers such as social stigma, cost, and treatment
availability, 60% of mentally-ill adults do not receive any mental health
services. Effective and efficient diagnosis relies on detecting clinical
symptoms of depression. Automatic detection of depressive symptoms would
potentially improve diagnostic accuracy and availability, leading to faster
intervention. In this work, we present a machine learning method for measuring
the severity of depressive symptoms. Our multi-modal method uses 3D facial
expressions and spoken language, commonly available from modern cell phones. It
demonstrates an average error of 3.67 points (15.3% relative) on the
clinically-validated Patient Health Questionnaire (PHQ) scale. For detecting
major depressive disorder, our model demonstrates 83.3% sensitivity and 82.6%
specificity. Overall, this paper shows how speech recognition, computer vision,
and natural language processing can be combined to assist mental health
patients and practitioners. This technology could be deployed to cell phones
worldwide and facilitate low-cost universal access to mental health care.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08597</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08597</id><created>2018-11-20</created><authors><author><keyname>Kaloorazi</keyname><forenames>Maboud F.</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Randomized Rank-Revealing UZV Decomposition for Low-Rank Approximation
  of Matrices</title><categories>cs.NA eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Low-rank matrix approximation plays an increasingly important role in signal
and image processing applications. This paper presents a new rank-revealing
decomposition method called randomized rank-revealing UZV decomposition
(RRR-UZVD). RRR-UZVD is powered by randomization to approximate a low-rank
input matrix. Given a large and dense matrix ${\bf A} \in \mathbb R^{m \times
n}$ whose numerical rank is $k$, where $k$ is much smaller than $m$ and $n$,
RRR-UZVD constructs an approximation $\hat{\bf A}$ such as $\hat{\bf A}={\bf
UZV}^T$, where ${\bf U}$ and ${\bf V}$ have orthonormal columns, the
leading-diagonal block of ${\bf Z}$ reveals the rank of $\bf A$, and its
off-diagonal blocks have small $\ell_2$-norms. RRR-UZVD is simple, accurate,
and only requires a few passes through $\bf A$ with an arithmetic cost of
$O(mnk)$ floating-point operations. To demonstrate the effectiveness of the
proposed method, we conduct experiments on synthetic data, as well as real data
in applications of image reconstruction and robust principal component
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08609</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08609</id><created>2018-11-21</created><authors><author><keyname>Safavi</keyname><forenames>Seyed Hamid</forenames></author><author><keyname>Khatua</keyname><forenames>Manas</forenames></author><author><keyname>Cheung</keyname><forenames>Ngai-Man</forenames></author><author><keyname>Torkamani-Azar</keyname><forenames>Farah</forenames></author></authors><title>On Sparse Graph Fourier Transform</title><categories>eess.SP</categories><comments>Presented at 3rd Graph Signal Processing Workshop - GSP 18</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new regression-based algorithm to compute Graph
Fourier Transform (GFT). Our algorithm allows different regularizations to be
included when computing the GFT analysis components, so that the resulting
components can be tuned for a specific task. We propose using the lasso penalty
in our proposed framework to obtain analysis components with sparse loadings.
We show that the components from this proposed {\em sparse GFT} can identify
and select correlated signal sources into sub-graphs, and perform frequency
analysis {\em locally} within these sub-graphs of correlated sources. Using
real network traffic datasets, we demonstrate that sparse GFT can achieve
outstanding performance in an anomaly detection task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08633</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08633</id><created>2018-11-21</created><authors><author><keyname>Tachikawa</keyname><forenames>Kazuki</forenames></author><author><keyname>Kawai</keyname><forenames>Yuji</forenames></author><author><keyname>Park</keyname><forenames>Jihoon</forenames></author><author><keyname>Asada</keyname><forenames>Minoru</forenames></author></authors><title>Compensated Integrated Gradients to Reliably Interpret EEG
  Classification</title><categories>cs.LG eess.SP stat.ML</categories><comments>Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216</comments><report-no>ML4H/2018/63</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrated gradients are widely employed to evaluate the contribution of
input features in classification models because it satisfies the axioms for
attribution of prediction. This method, however, requires an appropriate
baseline for reliable determination of the contributions. We propose a
compensated integrated gradients method that does not require a baseline. In
fact, the method compensates the attributions calculated by integrated
gradients at an arbitrary baseline using Shapley sampling. We prove that the
method retrieves reliable attributions if the processes of input features in a
classifier are mutually independent, and they are identical like shared weights
in convolutional neural networks. Using three electroencephalogram datasets, we
experimentally demonstrate that the attributions of the proposed method are
more reliable than those of the original integrated gradients, and its
computational complexity is much lower than that of Shapley sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08783</identifier>
 <datestamp>2019-02-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08783</id><created>2018-11-17</created><updated>2019-02-04</updated><authors><author><keyname>Kusano</keyname><forenames>Tsubasa</forenames></author><author><keyname>Masuyama</keyname><forenames>Yoshiki</forenames></author><author><keyname>Yatabe</keyname><forenames>Kohei</forenames></author><author><keyname>Oikawa</keyname><forenames>Yasuhiro</forenames></author></authors><title>Designing nearly tight window for improving time-frequency masking</title><categories>eess.SP cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many audio signal processing methods are formulated in the time-frequency
(T-F) domain which is obtained by the short-time Fourier transform (STFT). The
properties of the STFT are fully characterized by window function, number of
frequency channels, and time-shift. Thus, designing a better window is
important for improving the performance of the processing especially when a
less redundant T-F representation is desirable. While many window functions
have been proposed in the literature, they are designed to have a good
frequency response for analysis, which may not perform well in terms of signal
processing. The window design must take the effect of the reconstruction (from
the T-F domain into the time domain) into account for improving the
performance. In this paper, an optimization-based design method of a nearly
tight window is proposed to obtain a window performing well for the T-F domain
signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08792</identifier>
 <datestamp>2018-12-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08792</id><created>2018-11-21</created><updated>2018-12-22</updated><authors><author><keyname>Yao</keyname><forenames>Miao</forenames></author><author><keyname>Sohul</keyname><forenames>Munawwar</forenames></author><author><keyname>Marojevic</keyname><forenames>Vuk</forenames></author><author><keyname>Reed</keyname><forenames>Jeffrey H.</forenames></author></authors><title>Artificial Intelligence-Defined 5G Radio Access Networks</title><categories>eess.SP cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output antenna systems, millimeter wave
communications, and ultra-dense networks have been widely perceived as the
three key enablers that facilitate the development and deployment of 5G
systems. This article discusses the intelligent agent in 5G base station which
combines sensing, learning, understanding and optimizing to facilitate these
enablers. We present a flexible, rapidly deployable, and cross-layer artificial
intelligence (AI)-based framework to enable the imminent and future demands on
5G and beyond infrastructure. We present example AI-enabled 5G use cases that
accommodate important 5G-specific capabilities and discuss the value of AI for
enabling beyond 5G network evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08817</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08817</id><created>2018-11-21</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>Effectiveness of 3VQM in Capturing Depth Inconsistencies</title><categories>eess.IV cs.MM eess.SP</categories><comments>Paper: 5 pages, 1 figure, 1 table, Presentation: 15 slides [Ancillary
  files]</comments><acm-class>I.4</acm-class><journal-ref>D. Temel and G. AlRegib, &quot;Effectiveness of 3VQM in capturing depth
  inconsistencies,&quot; IVMSP 2013, Seoul, 2013, pp. 1-4</journal-ref><doi>10.1109/IVMSPW.2013.6611918</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3D video quality metric (3VQM) was proposed to evaluate the temporal and
spatial variation of the depth errors for the depth values that would lead to
inconsistencies between left and right views, fast changing disparities, and
geometric distortions. Previously, we evaluated 3VQM against subjective scores.
In this paper, we show the effectiveness of 3VQM in capturing errors and
inconsistencies that exist in the rendered depth-based 3D videos. We further
investigate how 3VQM could measure excessive disparities, fast changing
disparities, geometric distortions, temporal flickering and/or spatial noise in
the form of depth cues inconsistency. Results show that 3VQM best captures the
depth inconsistencies based on errors in the reference views. However, the
metric is not sensitive to depth map mild errors such as those resulting from
blur. We also performed a subjective quality test and showed that 3VQM performs
better than PSNR, weighted PSNR and SSIM in terms of accuracy, coherency and
consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08821</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08821</id><created>2018-11-21</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>Coding of 3D Videos Based on Visual Discomfort</title><categories>eess.IV cs.MM eess.SP</categories><comments>Paper: 5 pages, 3 figures, 2 tables, Presentation: 20 slides
  [Ancillary files]</comments><acm-class>I.4</acm-class><journal-ref>2013 Asilomar Conference on Signals, Systems and Computers,
  Pacific Grove, CA, 2013, pp. 1356-1360</journal-ref><doi>10.1109/ACSSC.2013.6810515</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a rate-distortion optimization method for 3D videos based on
visual discomfort estimation. We calculate visual discomfort in the encoded
depth maps using two indexes: temporal outliers (TO) and spatial outliers (SO).
These two indexes are used to measure the difference between the processed
depth map and the ground truth depth map. These indexes implicitly depend on
the amount of edge information within a frame and on the amount of motion
between frames. Moreover, we fuse these indexes considering the temporal and
spatial complexities of the content. We test the proposed method on a number of
videos and compare the results with the default rate-distortion algorithms in
the H.264/AVC codec. We evaluate rate-distortion algorithms by comparing
achieved bit-rates, visual degradations in the depth sequences and the fidelity
of the depth videos measured by SSIM and PSNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08839</identifier>
 <datestamp>2019-12-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08839</id><created>2018-11-21</created><updated>2019-12-11</updated><authors><author><keyname>Zbontar</keyname><forenames>Jure</forenames></author><author><keyname>Knoll</keyname><forenames>Florian</forenames></author><author><keyname>Sriram</keyname><forenames>Anuroop</forenames></author><author><keyname>Murrell</keyname><forenames>Tullie</forenames></author><author><keyname>Huang</keyname><forenames>Zhengnan</forenames></author><author><keyname>Muckley</keyname><forenames>Matthew J.</forenames></author><author><keyname>Defazio</keyname><forenames>Aaron</forenames></author><author><keyname>Stern</keyname><forenames>Ruben</forenames></author><author><keyname>Johnson</keyname><forenames>Patricia</forenames></author><author><keyname>Bruno</keyname><forenames>Mary</forenames></author><author><keyname>Parente</keyname><forenames>Marc</forenames></author><author><keyname>Geras</keyname><forenames>Krzysztof J.</forenames></author><author><keyname>Katsnelson</keyname><forenames>Joe</forenames></author><author><keyname>Chandarana</keyname><forenames>Hersh</forenames></author><author><keyname>Zhang</keyname><forenames>Zizhao</forenames></author><author><keyname>Drozdzal</keyname><forenames>Michal</forenames></author><author><keyname>Romero</keyname><forenames>Adriana</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author><author><keyname>Vincent</keyname><forenames>Pascal</forenames></author><author><keyname>Yakubova</keyname><forenames>Nafissa</forenames></author><author><keyname>Pinkerton</keyname><forenames>James</forenames></author><author><keyname>Wang</keyname><forenames>Duo</forenames></author><author><keyname>Owens</keyname><forenames>Erich</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Recht</keyname><forenames>Michael P.</forenames></author><author><keyname>Sodickson</keyname><forenames>Daniel K.</forenames></author><author><keyname>Lui</keyname><forenames>Yvonne W.</forenames></author></authors><title>fastMRI: An Open Dataset and Benchmarks for Accelerated MRI</title><categories>cs.CV cs.LG eess.SP physics.med-ph stat.ML</categories><comments>35 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements
has the potential to reduce medical costs, minimize stress to patients and make
MRI possible in applications where it is currently prohibitively slow or
expensive. We introduce the fastMRI dataset, a large-scale collection of both
raw MR measurements and clinical MR images, that can be used for training and
evaluation of machine-learning approaches to MR image reconstruction. By
introducing standardized evaluation criteria and a freely-accessible dataset,
our goal is to help the community make rapid advances in the state of the art
for MR image reconstruction. We also provide a self-contained introduction to
MRI for machine learning researchers with no medical imaging background.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08857</identifier>
 <datestamp>2018-11-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08857</id><created>2018-11-21</created><authors><author><keyname>Lei</keyname><forenames>Yi</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Chen</keyname><forenames>Bin</forenames></author><author><keyname>Deng</keyname><forenames>Xiong</forenames></author><author><keyname>Cao</keyname><forenames>Zizheng</forenames></author><author><keyname>Li</keyname><forenames>Jianqiang</forenames></author><author><keyname>Xu</keyname><forenames>Kun</forenames></author></authors><title>Decoding Staircase Codes with Marked Bits</title><categories>eess.SP</categories><comments>Accepted by ISTC2018</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Staircase codes (SCCs) are typically decoded using iterative bounded-distance
decoding (BDD) and hard decisions. In this paper, a novel decoding algorithm is
proposed, which partially uses soft information from the channel. The proposed
algorithm is based on marking certain number of highly reliable and highly
unreliable bits. These marked bits are used to improve the
miscorrection-detection capability of the SCC decoder and the error-correcting
capability of BDD. For SCCs with $2$-error-correcting BCH component codes, our
algorithm improves upon standard SCC decoding by up to $0.30$~dB at a bit-error
rate of $10^{-7}$. The proposed algorithm is shown to achieve almost half of
the gain achievable by an idealized decoder with this structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08891</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08891</id><created>2018-11-21</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>A Comparative Study of Quality and Content-Based Spatial Pooling
  Strategies in Image Quality Assessment</title><categories>eess.IV cs.CV cs.MM eess.SP</categories><comments>Paper: 5 pages, 8 figures, Presentation: 21 slides [Ancillary files]</comments><acm-class>I.4</acm-class><journal-ref>2015 IEEE GlobalSIP, Orlando, FL, 2015, pp. 732-736</journal-ref><doi>10.1109/GlobalSIP.2015.7418293</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of quantifying image quality consists of engineering the quality
features and pooling these features to obtain a value or a map. There has been
a significant research interest in designing the quality features but pooling
is usually overlooked compared to feature design. In this work, we compare the
state of the art quality and content-based spatial pooling strategies and show
that although features are the key in any image quality assessment, pooling
also matters. We also propose a quality-based spatial pooling strategy that is
based on linearly weighted percentile pooling (WPP). Pooling strategies are
analyzed for squared error, SSIM and PerSIM in LIVE, multiply distorted LIVE
and TID2013 image databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08919</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08919</id><created>2018-11-21</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Sethi</keyname><forenames>Saratendu</forenames></author></authors><title>Robust Active Learning for Electrocardiographic Signal Classification</title><categories>eess.SP cs.LG stat.ML</categories><comments>Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216 3</comments><report-no>ML4H/2018/5</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classification of electrocardiographic (ECG) signals is a challenging
problem for healthcare industry. Traditional supervised learning methods
require a large number of labeled data which is usually expensive and difficult
to obtain for ECG signals. Active learning is well-suited for ECG signal
classification as it aims at selecting the best set of labeled data in order to
maximize the classification performance. Motivated by the fact that ECG data
are usually heavily unbalanced among different classes and the class labels are
noisy as they are manually labeled, this paper proposes a novel solution based
on robust active learning for addressing these challenges. The key idea is to
first apply the clustering of the data in a low dimensional embedded space and
then select the most information instances within local clusters. By selecting
the most informative instances relying on local average minimal distances, the
algorithm tends to select the data for labelling in a more diversified way.
Finally, the robustness of the model is further enhanced by adding a novel
noisy label reduction scheme after the selection of the labeled data.
Experiments on the ECG signal classification from the MIT-BIH arrhythmia
database demonstrate the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08927</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08927</id><created>2018-11-21</created><authors><author><keyname>Prabhushankar</keyname><forenames>Mohit</forenames></author><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>Generating Adaptive and Robust Filter Sets Using an Unsupervised
  Learning Framework</title><categories>eess.IV cs.CV cs.MM eess.SP</categories><comments>Paper:5 pages, 5 figures, 3 tables and Poster [Ancillary files]</comments><acm-class>I.4</acm-class><journal-ref>2017 IEEE International Conference on Image Processing (ICIP),
  Beijing, 2017, pp. 3041-3045</journal-ref><doi>10.1109/ICIP.2017.8296841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an adaptive unsupervised learning framework,
which utilizes natural images to train filter sets. The applicability of these
filter sets is demonstrated by evaluating their performance in two contrasting
applications - image quality assessment and texture retrieval. While assessing
image quality, the filters need to capture perceptual differences based on
dissimilarities between a reference image and its distorted version. In texture
retrieval, the filters need to assess similarity between texture images to
retrieve closest matching textures. Based on experiments, we show that the
filter responses span a set in which a monotonicity-based metric can measure
both the perceptual dissimilarity of natural images and the similarity of
texture images. In addition, we corrupt the images in the test set and
demonstrate that the proposed method leads to robust and reliable retrieval
performance compared to existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08935</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08935</id><created>2018-11-14</created><authors><author><keyname>Noroozi</keyname><forenames>Fatemeh</forenames></author><author><keyname>Marjanovic</keyname><forenames>Marina</forenames></author><author><keyname>Njegus</keyname><forenames>Angelina</forenames></author><author><keyname>Escalera</keyname><forenames>Sergio</forenames></author><author><keyname>Anbarjafari</keyname><forenames>Gholamreza</forenames></author></authors><title>A Study of Language and Classifier-independent Feature Analysis for
  Vocal Emotion Recognition</title><categories>eess.AS cs.CL cs.SD</categories><comments>24 pages, 4 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every speech signal carries implicit information about the emotions, which
can be extracted by speech processing methods. In this paper, we propose an
algorithm for extracting features that are independent from the spoken language
and the classification method to have comparatively good recognition
performance on different languages independent from the employed classification
methods. The proposed algorithm is composed of three stages. In the first
stage, we propose a feature ranking method analyzing the state-of-the-art voice
quality features. In the second stage, we propose a method for finding the
subset of the common features for each language and classifier. In the third
stage, we compare our approach with the recognition rate of the
state-of-the-art filter methods. We use three databases with different
languages, namely, Polish, Serbian and English. Also three different
classifiers, namely, nearest neighbour, support vector machine and gradient
descent neural network, are employed. It is shown that our method for selecting
the most significant language-independent and method-independent features in
many cases outperforms state-of-the-art filter methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08940</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08940</id><created>2018-11-21</created><authors><author><keyname>Barazideh</keyname><forenames>Reza</forenames></author><author><keyname>Natarajan</keyname><forenames>Balasubramaniam</forenames></author><author><keyname>Nikitin</keyname><forenames>Alexei V.</forenames></author><author><keyname>Niknam</keyname><forenames>Solmaz</forenames></author></authors><title>Performance Analysis of Analog Intermittently Nonlinear Filter in the
  Presence of Impulsive Noise</title><categories>eess.SP</categories><comments>Accepted in IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Adaptive Nonlinear Differential Limiter (ANDL) is proposed in this paper
to efficiently alleviate the impact of impulsive noise (IN) in a communication
system. Unlike existing nonlinear methods, the ANDL is implemented in the
analog domain where the broader acquisition bandwidth makes outliers more
detectable and consequently it is easier to remove them. While the proposed
ANDL behaves like a linear filter when there is no outlier, it exhibits
intermittent nonlinearity in response to IN. Therefore, the structure of the
matched filter in the receiver is modified to compensate the filtering effect
of the ANDL in the linear regime. In this paper, we quantify the performance of
the ANDL by deriving a closed-form analytical bound for the average
signal-to-noise ratio (SNR) at the output of the filter. The calculation is
based on the idea that the ANDL can be perceived as a time-variant linear
filter whose bandwidth is modified based on the intensity of the IN. In
addition, by linearizing the filter time parameter variations, we treat the
ANDL as a set of linear filters where the exact operating filter at a given
time depends upon the magnitude of the outliers. The theoretical average bit
error rate (BER) is validated through simulations and the performance gains
relative to classical methods such as blanking and clipping are quantified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.08947</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.08947</id><created>2018-11-21</created><authors><author><keyname>Prabhushankar</keyname><forenames>Mohit</forenames></author><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>MS-UNIQUE: Multi-model and Sharpness-weighted Unsupervised Image Quality
  Estimation</title><categories>eess.IV cs.CV cs.MM eess.SP</categories><comments>Paper: 6 pages, 6 figures, 2 tables and Presentation: 21 slides
  [Ancillary files]</comments><acm-class>I.4</acm-class><journal-ref>The Electronic Imaging, IQSP XIV, Burlingame, California, USA,
  Jan. 29 Feb. 2, 2017</journal-ref><doi>10.2352/ISSN.2470-1173.2017.12.IQSP-223</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we train independent linear decoder models to estimate the
perceived quality of images. More specifically, we calculate the responses of
individual non-overlapping image patches to each of the decoders and scale
these responses based on the sharpness characteristics of filter set. We use
multiple linear decoders to capture different abstraction levels of the image
patches. Training each model is carried out on 100,000 image patches from the
ImageNet database in an unsupervised fashion. Color space selection and ZCA
Whitening are performed over these patches to enhance the descriptiveness of
the data. The proposed quality estimator is tested on the LIVE and the TID 2013
image quality assessment databases. Performance of the proposed method is
compared against eleven other state of the art methods in terms of accuracy,
consistency, linearity, and monotonic behavior. Based on experimental results,
the proposed method is generally among the top performing quality estimators in
all categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09010</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09010</id><created>2018-11-21</created><authors><author><keyname>Wang</keyname><forenames>Zhong-Qiu</forenames></author><author><keyname>Tan</keyname><forenames>Ke</forenames></author><author><keyname>Wang</keyname><forenames>DeLiang</forenames></author></authors><title>Deep Learning Based Phase Reconstruction for Speaker Separation: A
  Trigonometric Perspective</title><categories>cs.SD cs.CL eess.AS</categories><comments>5 pages, in submission to ICASSP-2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates phase reconstruction for deep learning based monaural
talker-independent speaker separation in the short-time Fourier transform
(STFT) domain. The key observation is that, for a mixture of two sources, with
their magnitudes accurately estimated and under a geometric constraint, the
absolute phase difference between each source and the mixture can be uniquely
determined; in addition, the source phases at each time-frequency (T-F) unit
can be narrowed down to only two candidates. To pick the right candidate, we
propose three algorithms based on iterative phase reconstruction, group delay
estimation, and phase-difference sign prediction. State-of-the-art results are
obtained on the publicly available wsj0-2mix and 3mix corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09021</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09021</id><created>2018-11-21</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Sainath</keyname><forenames>Tara</forenames></author><author><keyname>Wu</keyname><forenames>Yonghui</forenames></author><author><keyname>Chan</keyname><forenames>William</forenames></author></authors><title>Bytes are All You Need: End-to-End Multilingual Speech Recognition and
  Synthesis with Bytes</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two end-to-end models: Audio-to-Byte (A2B) and Byte-to-Audio
(B2A), for multilingual speech recognition and synthesis. Prior work has
predominantly used characters, sub-words or words as the unit of choice to
model text. These units are difficult to scale to languages with large
vocabularies, particularly in the case of multilingual processing. In this
work, we model text via a sequence of Unicode bytes, specifically, the UTF-8
variable length byte sequence for each character. Bytes allow us to avoid large
softmaxes in languages with large vocabularies, and share representations in
multilingual models. We show that bytes are superior to grapheme characters
over a wide variety of languages in monolingual end-to-end speech recognition.
Additionally, our multilingual byte model outperform each respective single
language baseline on average by 4.4% relatively. In Japanese-English
code-switching speech, our multilingual byte model outperform our monolingual
baseline by 38.6% relatively. Finally, we present an end-to-end multilingual
speech synthesis model using byte representations which matches the performance
of our monolingual baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09081</identifier>
 <datestamp>2019-01-18</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09081</id><created>2018-11-22</created><authors><author><keyname>Zambanini</keyname><forenames>Sebastian</forenames></author></authors><title>Feature-based groupwise registration of historical aerial images to
  present-day ortho-photo maps</title><categories>cs.CV eess.IV</categories><comments>Under review at Elsevier Pattern Recognition</comments><msc-class>68U10 (Primary), 65K10 (Secondary)</msc-class><doi>10.1016/j.patcog.2019.01.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the registration of historical WWII images to
present-day ortho-photo maps for the purpose of geolocalization. Due to the
challenging nature of this problem, we propose to register the images jointly
as a group rather than in a step-by-step manner. To this end, we exploit Hough
Voting spaces as pairwise registration estimators and show how they can be
integrated into a probabilistic groupwise registration framework that can be
efficiently optimized. The feature-based nature of our registration framework
allows to register images with a-priori unknown translational and rotational
relations, and is also able to handle scale changes of up to 30% in our test
data due to a final geometrically guided matching step. The superiority of the
proposed method over existing pairwise and groupwise registration methods is
demonstrated on eight highly challenging sets of historical images with
corresponding ortho-photo maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09181</identifier>
 <datestamp>2019-10-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09181</id><created>2018-11-22</created><updated>2019-10-13</updated><authors><author><keyname>Alfredsson</keyname><forenames>Arni F.</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author><author><keyname>Puttnam</keyname><forenames>Benjamin J.</forenames></author><author><keyname>Rademacher</keyname><forenames>Georg</forenames></author><author><keyname>Lu&#xed;s</keyname><forenames>Ruben S.</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author></authors><title>Pilot-Aided Joint-Channel Carrier-Phase Estimation in Space-Division
  Multiplexed Multicore Fiber Transmission</title><categories>eess.SP</categories><journal-ref>Journal of Lightwave Technology 37 (2019) 1133-1142</journal-ref><doi>10.1109/JLT.2018.2886837</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of pilot-aided joint-channel carrier-phase estimation (CPE)
in space-division multiplexed multicore fiber (MCF) transmission with
correlated phase noise is studied. To that end, a system model describing
uncoded MCF transmission where the phase noise comprises a common laser phase
noise, in addition to core- and polarization-specific phase drifts, is
introduced. It is then shown that the system model can be regarded as a special
case of a multidimensional random-walk phase-noise model. A pilot-aided CPE
algorithm developed for this model is used to evaluate two strategies, namely
joint-channel and per-channel CPE. To quantify the performance differences
between the two strategies, their respective phase-noise tolerances are
assessed through Monte Carlo simulations of uncoded transmission for different
modulation formats, pilot overheads, laser linewidths, numbers of spatial
channels, and degrees of phase-noise correlation across the channels. For 20
GBd transmission with 200 kHz combined laser linewidth and 1% pilot overhead,
joint-channel CPE yields up to 3.4 dB improvement in power efficiency or 25.5%
increased information rate. Moreover, through MCF transmission experiments, the
system model is validated and the strategies are compared in terms of
bit-error-rate performance versus transmission distance for uncoded
transmission of different modulation formats. Up to 21% increase in
transmission reach is observed for 1% pilot overhead through the use of
joint-channel CPE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09237</identifier>
 <datestamp>2019-09-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09237</id><created>2018-11-22</created><updated>2019-06-25</updated><authors><author><keyname>Liao</keyname><forenames>Yicheng</forenames></author><author><keyname>Wang</keyname><forenames>Xiongfei</forenames></author></authors><title>Impedance-Based Stability Analysis for Interconnected Converter Systems
  with Open-Loop RHP Poles</title><categories>cs.SY eess.SP</categories><doi>10.1109/TPEL.2019.2939636</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small-signal instability issues of interconnected converter systems can be
addressed by the impedance-based stability analysis method, where the impedance
ratio at the point of common connection of different subsystems can be regarded
as the open-loop gain, and thus the stability of the system can be predicted by
the Nyquist stability criterion. However, the right-half plan (RHP) poles may
be present in the impedance ratio, which then prevents the direct use of
Nyquist curves for defining stability margins or forbidden regions. To tackle
this challenge, this paper proposes a general rule of impedance-based stability
analysis with the aid of Bode plots. The method serves as a sufficient and
necessary stability condition, and it can be readily used to formulate the
impedance specifications graphically for various interconnected converter
systems. Experimental case studies validate the correctness of the proposed
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09271</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09271</id><created>2018-11-22</created><authors><author><keyname>Ozfatura</keyname><forenames>Emre</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author></authors><title>Distributed Gradient Descent with Coded Partial Gradient Computations</title><categories>cs.LG cs.DC cs.IT eess.SP math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coded computation techniques provide robustness against straggling servers in
distributed computing, with the following limitations: First, they increase
decoding complexity. Second, they ignore computations carried out by straggling
servers; and they are typically designed to recover the full gradient, and
thus, cannot provide a balance between the accuracy of the gradient and
per-iteration completion time. Here we introduce a hybrid approach, called
coded partial gradient computation (CPGC), that benefits from the advantages of
both coded and uncoded computation schemes, and reduces both the computation
time and decoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09301</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09301</id><created>2018-11-22</created><authors><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>Image Quality Assessment and Color Difference</title><categories>eess.IV cs.MM eess.SP</categories><comments>Paper: 5 pages, 5 figures, 2 tables, and Presentation [Ancillary
  files]</comments><acm-class>I.4</acm-class><journal-ref>2014 IEEE Global Conference on Signal and Information Processing
  (GlobalSIP), Atlanta, GA, 2014, pp. 970-974</journal-ref><doi>10.1109/GlobalSIP.2014.7032265</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An average healthy person does not perceive the world in just black and
white. Moreover, the perceived world is not composed of pixels and through
vision humans perceive structures. However, the acquisition and display systems
discretize the world. Therefore, we need to consider pixels, structures and
colors to model the quality of experience. Quality assessment methods use the
pixel-wise and structural metrics whereas color science approaches use the
patch-based color differences. In this work, we combine these approaches by
extending CIEDE2000 formula with perceptual color difference to assess image
quality. We examine how perceptual color difference-based metric (PCDM)
performs compared to PSNR, CIEDE2000, SSIM, MS-SSIM and CW-SSIM on the LIVE
database. In terms of linear correlation, PCDM obtains compatible results under
white noise (97.9%), Jpeg (95.9%) and Jp2k (95.6%) with an overall correlation
of 92.7%. We also show that PCDM captures color-based artifacts that can not be
captured by structure-based metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09307</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09307</id><created>2018-11-22</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Temel</keyname><forenames>Dogancan</forenames></author><author><keyname>AlRegib</keyname><forenames>Ghassan</forenames></author></authors><title>Fault Detection Using Color Blending and Color Transformations</title><categories>eess.IV eess.SP</categories><comments>Paper: 5 pages, 6 figures, 1 table and Poster [Ancillary files]</comments><acm-class>I.4</acm-class><journal-ref>2014 IEEE Global Conference on Signal and Information Processing
  (GlobalSIP), Atlanta, GA, 2014, pp. 999-1003</journal-ref><doi>10.1109/GlobalSIP.2014.7032271</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of seismic interpretation, univariate databased maps are
commonly used by interpreters, especially for fault detection. In these maps,
contrast between target regions and the background is one of the main factors
that affect the accuracy of the interpretation. Since univariate data-based
maps are not capable of providing a high contrast representation, to overcome
this issue, we turn these univariate data-based maps into multivariate
data-based representations using color blending. We blend neighboring time
sections, frames that are viewed in the time direction of migrated seismic
volumes, as if they corresponded to the red, green, and blue channels of a
color image. Furthermore, we apply color transformations to extract more
reliable structural information. Experimental results show that the proposed
method improves the accuracy of fault detection by limiting the average
distance between detected fault lines and the ground truth into one pixel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09315</identifier>
 <datestamp>2019-11-13</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09315</id><created>2018-11-22</created><updated>2019-11-11</updated><authors><author><keyname>Kristensen</keyname><forenames>Martin</forenames></author></authors><title>How to find MH370?</title><categories>eess.SP</categories><comments>24 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The disappearance of flight MH370 is possibly the greatest mystery in
aviation history. A large zone in the Southern Indian Ocean was searched
unsuccessfully leaving an open case and an unacceptable situation for the
family members. We discuss the scientific difficulties with locating the plane
through satellite data and develop an improved analysis using least square
curve fitting of analytical non-Euclidean route equations providing robust
topology-optimization with perturbation theory handling satellite movement. We
find four independent solutions with the final part of the flight following a
great circle. Two are located in stable minima for the error-function, and two
unstable ones agree poorly with most data. One stable solution coincides with
the Inmarsat-result, but fails to explain additional data. Our best solution
leads to an entirely different location agreeing with other data from debris,
acoustics, an eyewitness report, the received microwave power, and a contrail,
providing a clear conclusion where to find the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09337</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09337</id><created>2018-11-22</created><authors><author><keyname>Raza</keyname><forenames>Muhammad Qamar</forenames></author><author><keyname>Mithulananthan</keyname><forenames>N.</forenames></author><author><keyname>Li</keyname><forenames>Jiaming</forenames></author><author><keyname>Lee</keyname><forenames>Kwang Y.</forenames></author><author><keyname>Gooi</keyname><forenames>Hoay Beng</forenames></author></authors><title>An Ensemble Framework For Day-Ahead Forecast of PV Output Power in Smart
  Grids</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The uncertainty associated with solar photo-voltaic (PV) power output is a
big challenge to design, manage and implement effective demand response and
management strategies. Therefore, an accurate PV power output forecast is an
utmost importance to allow seamless integration and a higher level of
penetration. In this research, a neural network ensemble (NNE) scheme is
proposed, which is based on particle swarm optimization (PSO) trained
feedforward neural network (FNN). Five different FFN structures with varying
network complexities are used to achieve the diverse and accurate forecast
results. These results are combined using trim aggregation after removing the
upper and lower forecast error extremes. Correlated variables namely wavelet
transformed historical power output of PV, solar irradiance, wind speed,
temperature and humidity are applied as inputs to the multivariate NNE.
Clearness index is used to classify days into clear, cloudy and partial cloudy
days. Test case studies are designed to predict the solar output for these days
selected from all seasons. The performance of the proposed framework is
analyzed by applying training data set of different resolution, length and
quality from seven solar PV sites of the University of Queensland, Australia.
The forecast results demonstrate that the proposed framework improves the
forecast accuracy significantly in comparison with individual and benchmark
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09339</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09339</id><created>2018-11-22</created><authors><author><keyname>Raza</keyname><forenames>Muhammad Qamar</forenames></author><author><keyname>Mithulananthan</keyname><forenames>N.</forenames></author><author><keyname>Li</keyname><forenames>Jiaming</forenames></author><author><keyname>Lee</keyname><forenames>Kwang Y.</forenames></author></authors><title>Multivariate Ensemble Forecast Framework for Demand Prediction of
  Anomalous Days</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accurate load forecast is always important for the power industry and
energy players as it enables stakeholders to make critical decisions. In
addition, its importance is further increased with growing uncertainties in the
generation sector due to the high penetration of renewable energy and the
introduction of demand side management strategies. An incremental improvement
in grid-level demand forecast of anomalous days can potentially save millions
of dollars. However, due to an increasing penetration of renewable energy
resources and their dependency on several meteorological and exogenous
variables, accurate load forecasting of anomalous days has now become very
challenging. To improve the prediction accuracy of the load forecasting, an
ensemble forecast framework (ENFF) is proposed with a systematic combination of
three multiple predictors, namely Elman neural network (ELM), feedforward
neural network (FNN) and radial basis function (RBF) neural network. These
predictors are trained using global particle swarm optimization (GPSO) to
improve their prediction capability in the ENFF. The outputs of individual
predictors are combined using a trim aggregation technique by removing
forecasting anomalies. Real recorded data of New England ISO grid is used for
training and testing of the ENFF for anomalous days. The forecast results of
the proposed ENFF indicate a significant improvement in prediction accuracy in
comparison to autoregressive integrated moving average (ARIMA) and
back-propagation neural networks (BPNN) based benchmark models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09346</identifier>
 <datestamp>2018-11-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09346</id><created>2018-11-22</created><authors><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Mei</keyname><forenames>Kai</forenames></author><author><keyname>Ma</keyname><forenames>Dongtang</forenames></author><author><keyname>Wei</keyname><forenames>Jibo</forenames></author></authors><title>Deep Neural Network Aided Scenario Identification in Wireless Multi-path
  Fading Channels</title><categories>eess.SP cs.LG</categories><comments>Draft of a four-page letter with 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter illustrates our preliminary works in deep nerual network (DNN)
for wireless communication scenario identification in wireless multi-path
fading channels. In this letter, six kinds of channel scenarios referring to
COST 207 channel model have been performed. 100% identification accuracy has
been observed given signal-to-noise (SNR) over 20dB whereas a 88.4% average
accuracy has been obtained where SNR ranged from 0dB to 40dB. The proposed
method has tested under fast time-varying conditions, which were similar with
real world wireless multi-path fading channels, enabling it to work feasibly in
practical scenario identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09355</identifier>
 <datestamp>2019-05-14</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09355</id><created>2018-11-22</created><updated>2019-05-12</updated><authors><author><keyname>Zhou</keyname><forenames>Jianfeng</forenames></author><author><keyname>Jiang</keyname><forenames>Tao</forenames></author><author><keyname>Li</keyname><forenames>Lin</forenames></author><author><keyname>Hong</keyname><forenames>Qingyang</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Xia</keyname><forenames>Bingyin</forenames></author></authors><title>Training Multi-Task Adversarial Network for Extracting Noise-Robust
  Speaker Embedding</title><categories>cs.SD eess.AS</categories><comments>accepted by ICASSP2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under noisy environments, to achieve the robust performance of speaker
recognition is still a challenging task. Motivated by the promising performance
of multi-task training in a variety of image processing tasks, we explore the
potential of multi-task adversarial training for learning a noise-robust
speaker embedding. In this paper we present a novel framework which consists of
three components: an encoder that extracts noise-robust speaker embedding; a
classifier that classifies the speakers; a discriminator that discriminates the
noise type of the speaker embedding. Besides, we propose a training strategy
using the training accuracy as an indicator to stabilize the multi-class
adversarial optimization process. We conduct our experiments on the English and
Mandarin corpus and the experimental results demonstrate that our proposed
multi-task adversarial training method could greatly outperform the other
methods without adversarial training in noisy environments. Furthermore,
experiments indicate that our method is also able to improve the speaker
verification performance the clean condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09364</identifier>
 <datestamp>2019-04-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09364</id><created>2018-11-23</created><updated>2019-04-12</updated><authors><author><keyname>Lee</keyname><forenames>Younggun</forenames></author><author><keyname>Shon</keyname><forenames>Suwon</forenames></author><author><keyname>Kim</keyname><forenames>Taesu</forenames></author></authors><title>Learning pronunciation from a foreign language in speech synthesis
  networks</title><categories>cs.CL cs.LG cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although there are more than 65,000 languages in the world, the
pronunciations of many phonemes sound similar across the languages. When people
learn a foreign language, their pronunciation often reflects their native
language's characteristics. This motivates us to investigate how the speech
synthesis network learns the pronunciation from datasets from different
languages. In this study, we are interested in analyzing and taking advantage
of multilingual speech synthesis network. First, we train the speech synthesis
network bilingually in English and Korean and analyze how the network learns
the relations of phoneme pronunciation between the languages. Our experimental
result shows that the learned phoneme embedding vectors are located closer if
their pronunciations are similar across the languages. Consequently, the
trained networks can synthesize the English speakers' Korean speech and vice
versa. Using this result, we propose a training framework to utilize
information from a different language. To be specific, we pre-train a speech
synthesis network using datasets from both high-resource language and
low-resource language, then we fine-tune the network using the low-resource
language dataset. Finally, we conducted more simulations on 10 different
languages to show it is generally extendable to other languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09508</identifier>
 <datestamp>2019-05-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09508</id><created>2018-11-23</created><authors><author><keyname>Kwak</keyname><forenames>Semin</forenames></author><author><keyname>Chun</keyname><forenames>Joohwan</forenames></author><author><keyname>Ye</keyname><forenames>Sung Hyuck</forenames></author></authors><title>Monopulse beam synthesis using a sparse single-layer of weights</title><categories>eess.SP</categories><doi>10.1109/TAP.2019.2899850</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conventional monopulse radar system uses three beams; sum beam, elevation
difference beam and azimuth difference beam, which require different layers of
weights to synthesize each beam independently. Since the multi-layer structure
increases hardware complexity, many simplified structures based on a single
layer of weights have been suggested. In this work, we introduce a new
technique for finding disjoint and fully covering sets of weight vectors, each
of which constitutes a sparse subarray, forming a single beam. Our algorithm
decomposes the original non-convex optimization problem for finding disjoint
weight vectors into a sequence of convex problems. We demonstrate the
convergence of the algorithm and show that the interleaved array structure is
able to meet difficult beam constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09607</identifier>
 <datestamp>2019-03-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09607</id><created>2018-11-21</created><authors><author><keyname>Gonzalez-Diaz</keyname><forenames>R.</forenames></author><author><keyname>Paluzo-Hidalgo</keyname><forenames>E.</forenames></author><author><keyname>Quesada</keyname><forenames>J. F.</forenames></author></authors><title>Towards Emotion Recognition: A Persistent Entropy Application</title><categories>cs.SD cs.CL eess.AS</categories><doi>10.1007/978-3-030-10828-1_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotion recognition and classification is a very active area of research. In
this paper, we present a first approach to emotion classification using
persistent entropy and support vector machines. A topology-based model is
applied to obtain a single real number from each raw signal. These data are
used as input of a support vector machine to classify signals into 8 different
emotions (calm, happy, sad, angry, fearful, disgust and surprised).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09620</identifier>
 <datestamp>2019-05-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09620</id><created>2018-11-22</created><updated>2019-05-01</updated><authors><author><keyname>Huang</keyname><forenames>Sicong</forenames></author><author><keyname>Li</keyname><forenames>Qiyang</forenames></author><author><keyname>Anil</keyname><forenames>Cem</forenames></author><author><keyname>Bao</keyname><forenames>Xuchan</forenames></author><author><keyname>Oore</keyname><forenames>Sageev</forenames></author><author><keyname>Grosse</keyname><forenames>Roger B.</forenames></author></authors><title>TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre
  Transfer</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>17 pages, published as a conference paper at ICLR 2019</comments><journal-ref>ICLR 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the problem of musical timbre transfer, where the
goal is to manipulate the timbre of a sound sample from one instrument to match
another instrument while preserving other musical content, such as pitch,
rhythm, and loudness. In principle, one could apply image-based style transfer
techniques to a time-frequency representation of an audio signal, but this
depends on having a representation that allows independent manipulation of
timbre as well as high-quality waveform generation. We introduce TimbreTron, a
method for musical timbre transfer which applies &quot;image&quot; domain style transfer
to a time-frequency representation of the audio signal, and then produces a
high-quality waveform using a conditional WaveNet synthesizer. We show that the
Constant Q Transform (CQT) representation is particularly well-suited to
convolutional architectures due to its approximate pitch equivariance. Based on
human perceptual evaluations, we confirmed that TimbreTron recognizably
transferred the timbre while otherwise preserving the musical content, for both
monophonic and polyphonic samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09655</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09655</id><created>2018-11-23</created><authors><author><keyname>Atlason</keyname><forenames>Hans E.</forenames></author><author><keyname>Love</keyname><forenames>Askell</forenames></author><author><keyname>Sigurdsson</keyname><forenames>Sigurdur</forenames></author><author><keyname>Gudnason</keyname><forenames>Vilmundur</forenames></author><author><keyname>Ellingsen</keyname><forenames>Lotta M.</forenames></author></authors><title>Unsupervised brain lesion segmentation from MRI using a convolutional
  autoencoder</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lesions that appear hyperintense in both Fluid Attenuated Inversion Recovery
(FLAIR) and T2-weighted magnetic resonance images (MRIs) of the human brain are
common in the brains of the elderly population and may be caused by ischemia or
demyelination. Lesions are biomarkers for various neurodegenerative diseases,
making accurate quantification of them important for both disease diagnosis and
progression. Automatic lesion detection using supervised learning requires
manually annotated images, which can often be impractical to acquire.
Unsupervised lesion detection, on the other hand, does not require any manual
delineation; however, these methods can be challenging to construct due to the
variability in lesion load, placement of lesions, and voxel intensities. Here
we present a novel approach to address this problem using a convolutional
autoencoder, which learns to segment brain lesions as well as the white matter,
gray matter, and cerebrospinal fluid by reconstructing FLAIR images as conical
combinations of softmax layer outputs generated from the corresponding T1, T2,
and FLAIR images. Some of the advantages of this model are that it accurately
learns to segment lesions regardless of lesion load, and it can be used to
quickly and robustly segment new images that were not in the training set.
Comparisons with state-of-the-art segmentation methods evaluated on ground
truth manual labels indicate that the proposed method works well for generating
accurate lesion segmentations without the need for manual annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09678</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09678</id><created>2018-11-21</created><authors><author><keyname>Parcollet</keyname><forenames>Titouan</forenames></author><author><keyname>Ravanelli</keyname><forenames>Mirco</forenames></author><author><keyname>Morchid</keyname><forenames>Mohamed</forenames></author><author><keyname>Linar&#xe8;s</keyname><forenames>Georges</forenames></author><author><keyname>De Mori</keyname><forenames>Renato</forenames></author></authors><title>Speech recognition with quaternion neural networks</title><categories>eess.AS cs.SD stat.ML</categories><comments>NIPS 2018 (IRASL). arXiv admin note: text overlap with
  arXiv:1806.04418</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network architectures are at the core of powerful automatic speech
recognition systems (ASR). However, while recent researches focus on novel
model architectures, the acoustic input features remain almost unchanged.
Traditional ASR systems rely on multidimensional acoustic features such as the
Mel filter bank energies alongside with the first, and second order derivatives
to characterize time-frames that compose the signal sequence. Considering that
these components describe three different views of the same element, neural
networks have to learn both the internal relations that exist within these
features, and external or global dependencies that exist between the
time-frames. Quaternion-valued neural networks (QNN), recently received an
important interest from researchers to process and learn such relations in
multidimensional spaces. Indeed, quaternion numbers and QNNs have shown their
efficiency to process multidimensional inputs as entities, to encode internal
dependencies, and to solve many tasks with up to four times less learning
parameters than real-valued models. We propose to investigate modern
quaternion-valued models such as convolutional and recurrent quaternion neural
networks in the context of speech recognition with the TIMIT dataset. The
experiments show that QNNs always outperform real-valued equivalent models with
way less free parameters, leading to a more efficient, compact, and expressive
representation of the relevant information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09703</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09703</id><created>2018-11-23</created><authors><author><keyname>Ghalib</keyname><forenames>Asim</forenames></author><author><keyname>Sharawi</keyname><forenames>Mohammad S.</forenames></author></authors><title>MIMO Antenna Elements Effect on Chassis Modes</title><categories>eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, a 4-element printed multiple-input-multiple-output (MIMO) loop
antenna having a bandwidth of 400 MHz is proposed. The effect of the MIMO
antenna on the chassis modes system is analyzed via the theory of
characteristic modes (TCM), and it is shown that chassis CM are not enough for
the full analysis. A defected-ground-structure (DGS) is also proposed to
enhance the isolation between the antenna elements by blocking the coupling
modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09710</identifier>
 <datestamp>2020-01-15</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09710</id><created>2018-11-09</created><updated>2020-01-14</updated><authors><author><keyname>Angelelli</keyname><forenames>Mario</forenames></author><author><keyname>Ciavolino</keyname><forenames>Enrico</forenames></author><author><keyname>Pasca</keyname><forenames>Paola</forenames></author></authors><title>Streaming Generalized Cross Entropy</title><categories>eess.SP cs.IT math.IT</categories><comments>22 pages, 11 figures</comments><journal-ref>Soft Computing (2019)</journal-ref><doi>10.1007/s00500-019-04632-w</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method to combine adaptive processes with a class of entropy
estimators for the case of streams of data. Starting from a first estimation
obtained from a batch of initial data, model parameters are estimated at each
step by combining the prior knowledge with the new observation (or a block of
observations). This allows to extend the maximum entropy technique to a
dynamical setting, also distinguishing between entropic contributions of the
signal and the error. Furthermore, it provides a suitable approximation of
standard GME problems when the exacted solutions are hard to evaluate. We test
this method by performing numerical simulations at various sample sizes and
batch dimensions. Moreover, we extend this analysis exploring intermediate
cases between streaming GCE and standard GCE, i.e. considering blocks of
observations of different sizes to update the estimates, and incorporating
collinearity effects as well. The role of time in the balance between entropic
contributions of signal and errors is further explored considering a variation
of the Streaming GCE algorithm, namely, Weighted Streaming GCE. Finally, we
discuss the results: in particular, we highlight the main characteristics of
this method, the range of application, and future perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09725</identifier>
 <datestamp>2019-08-12</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09725</id><created>2018-11-23</created><updated>2019-08-09</updated><authors><author><keyname>Ravanelli</keyname><forenames>Mirco</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Interpretable Convolutional Filters with SincNet</title><categories>eess.AS cs.CL cs.LG cs.NE</categories><comments>In Proceedings of NIPS@IRASL 2018. arXiv admin note: substantial text
  overlap with arXiv:1808.00158</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning is currently playing a crucial role toward higher levels of
artificial intelligence. This paradigm allows neural networks to learn complex
and abstract representations, that are progressively obtained by combining
simpler ones. Nevertheless, the internal &quot;black-box&quot; representations
automatically discovered by current neural architectures often suffer from a
lack of interpretability, making of primary interest the study of explainable
machine learning techniques. This paper summarizes our recent efforts to
develop a more interpretable neural model for directly processing speech from
the raw waveform. In particular, we propose SincNet, a novel Convolutional
Neural Network (CNN) that encourages the first layer to discover more
meaningful filters by exploiting parametrized sinc functions. In contrast to
standard CNNs, which learn all the elements of each filter, only low and high
cutoff frequencies of band-pass filters are directly learned from data. This
inductive bias offers a very compact way to derive a customized filter-bank
front-end, that only depends on some parameters with a clear physical meaning.
Our experiments, conducted on both speaker and speech recognition, show that
the proposed architecture converges faster, performs better, and is more
interpretable than standard CNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09801</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09801</id><created>2018-11-24</created><authors><author><keyname>Xu</keyname><forenames>Weihong</forenames><affiliation>Lab of Efficient Architectures for Digital-communication and Signal-processing</affiliation><affiliation>National Mobile Communications Research Laboratory</affiliation></author><author><keyname>You</keyname><forenames>Xiaohu</forenames><affiliation>National Mobile Communications Research Laboratory</affiliation></author><author><keyname>Zhang</keyname><forenames>Chuan</forenames><affiliation>Lab of Efficient Architectures for Digital-communication and Signal-processing</affiliation><affiliation>National Mobile Communications Research Laboratory</affiliation></author><author><keyname>Be'ery</keyname><forenames>Yair</forenames><affiliation>School of Electrical Engineering, Tel-Aviv University, Tel-Aviv, Israel</affiliation></author></authors><title>Polar Decoding on Sparse Graphs with Deep Learning</title><categories>eess.SP cs.IT cs.LG math.IT</categories><comments>submitted to Asilomar 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a sparse neural network decoder (SNND) of polar
codes based on belief propagation (BP) and deep learning. At first, the
conventional factor graph of polar BP decoding is converted to the bipartite
Tanner graph similar to low-density parity-check (LDPC) codes. Then the Tanner
graph is unfolded and translated into the graphical representation of deep
neural network (DNN). The complex sum-product algorithm (SPA) is modified to
min-sum (MS) approximation with low complexity. We dramatically reduce the
number of weight by using single weight to parameterize the networks. Optimized
by the training techniques of deep learning, proposed SNND achieves comparative
decoding performance of SPA and obtains about $0.5$ dB gain over MS decoding on
($128,64$) and ($256,128$) codes. Moreover, $60 \%$ complexity reduction is
achieved and the decoding latency is significantly lower than the conventional
polar BP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09861</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09861</id><created>2018-11-24</created><authors><author><keyname>Ng</keyname><forenames>Chris</forenames></author><author><keyname>Banu</keyname><forenames>Mihai</forenames></author></authors><title>Subband Beamforming in Coherent Hybrid Massive MIMO Using Eigenbeams</title><categories>eess.SP</categories><comments>Presented at 2018 Asilomar Conference on Signals, Systems, and
  Computers (4 pages, 5 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid Massive MIMO reduces implementation complexity but only supports
beamforming coefficients that are common across all subbands. However, in macro
cellular where the channel has limited degrees of freedom, the long-term
component of the channel can be decomposed into a set of subband-independent
beamforming basis functions referred to as eigenbeams. A Coherent Hybrid
Massive MIMO system can form arbitrary linear combinations of the eigenbeams at
every subband to mimic Digital Massive MIMO beamforming as observed across all
locations in the cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09918</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09918</id><created>2018-11-24</created><authors><author><keyname>Schilling</keyname><forenames>Benjamin</forenames></author><author><keyname>Bahmani</keyname><forenames>Keivan</forenames></author><author><keyname>Li</keyname><forenames>Boyang</forenames></author><author><keyname>Banerjee</keyname><forenames>Sean</forenames></author><author><keyname>Smith</keyname><forenames>Jessica Scillieri</forenames></author><author><keyname>Moshier</keyname><forenames>Tim</forenames></author><author><keyname>Schuckers</keyname><forenames>Stephanie</forenames></author></authors><title>Validation of Biometric Identification of Dairy Cows based on Udder NIR
  Images</title><categories>eess.IV</categories><comments>The 9th IEEE International Conference on Biometrics: Theory,
  Applications, and Systems (BTAS 2018)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying dairy cows with infections such as mastitis or cows on
medications is an extremely important task and legally required by the FDA$'$s
Pasteurized Milk Ordinance. The milk produced by these dairy cows cannot be
allowed to mix with the milk from healthy cows or it risks contaminating the
entire bulk tank or milk truck. Ear tags, ankle bands, RFID tags and even iris
patterns are some of the identification methods currently used in the dairy
farms. In this work we propose the use of NIR images of cow$'$s mammary glands
as a novel biometric identification modality. Two datasets, containing 302
samples from 151 cows has been collected and various machine learning
techniques applied to demonstrate the viability of the proposed biometric
modality. The results suggest promising identification accuracy for samples
collected over consecutive days
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09919</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09919</id><created>2018-11-24</created><authors><author><keyname>Luz</keyname><forenames>Saturnino</forenames></author><author><keyname>de la Fuente</keyname><forenames>Sofia</forenames></author><author><keyname>Albert</keyname><forenames>Pierre</forenames></author></authors><title>A Method for Analysis of Patient Speech in Dialogue for Dementia
  Detection</title><categories>eess.AS cs.LG cs.SD</categories><comments>8 pages, Resources and ProcessIng of linguistic, paralinguistic and
  extra-linguistic Data from people with various forms of cognitive impairment,
  LREC 2018</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present an approach to automatic detection of Alzheimer's type dementia
based on characteristics of spontaneous spoken language dialogue consisting of
interviews recorded in natural settings. The proposed method employs additive
logistic regression (a machine learning boosting method) on content-free
features extracted from dialogical interaction to build a predictive model. The
model training data consisted of 21 dialogues between patients with Alzheimer's
and interviewers, and 17 dialogues between patients with other health
conditions and interviewers. Features analysed included speech rate,
turn-taking patterns and other speech parameters. Despite relying solely on
content-free features, our method obtains overall accuracy of 86.5\%, a result
comparable to those of state-of-the-art methods that employ more complex
lexical, syntactic and semantic features. While further investigation is
needed, the fact that we were able to obtain promising results using only
features that can be easily extracted from spontaneous dialogues suggests the
possibility of designing non-invasive and low-cost mental health monitoring
tools for use at scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09933</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09933</id><created>2018-11-24</created><authors><author><keyname>Zafzouf</keyname><forenames>Ghassen</forenames></author></authors><title>Channel Capacity for MIMO Spectrally Precoded OFDM with Block Reflectors</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast to the state-of-the-art technology, it is envisaged that 5G
systems will accommodate a broad range of services and a variety of emerging
applications. In fact, the advent of the Internet of Things (IoT) and its
integration with conventional human-initiated transmissions yields the need for
a fundamental system redesign. Hence, the ideal waveform for the upcoming 5G
radio access technology should cope with several requirements, such as low
computational complexity, good time and frequency localization and
straightforward extension to \emph{multiple-input multiple-output} (MIMO)
systems. In the light of reducing the algorithmic complexity of the spectrally
precoded MIMO OFDM, we provide an in-depth analysis of the use of block
reflectors. The channel capacity formula for MIMO spectrally precoded OFDM is
then derived, and the proposed design techniques are compared to conventional
MIMO OFDM from an information-theoretic perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09940</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09940</id><created>2018-11-24</created><authors><author><keyname>Zehni</keyname><forenames>Mona</forenames></author><author><keyname>Huang</keyname><forenames>Shuai</forenames></author><author><keyname>Dokmani&#x107;</keyname><forenames>Ivan</forenames></author><author><keyname>Zhao</keyname><forenames>Zhizhen</forenames></author></authors><title>Geometric Invariants for Sparse Unknown View Tomography</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a 2D tomography problem for point source models with
random unknown view angles. Rather than recovering the projection angles, we
reconstruct the model through a set of rotation-invariant features that are
estimated from the projection data. For a point source model, we show that
these features reveal geometric information about the model such as the radial
and pairwise distances. This establishes a connection between unknown view
tomography and unassigned distance geometry problem (uDGP). We propose new
methods to extract the distances and approximate the pairwise distance
distribution of the underlying points. We then use the recovered distribution
to estimate the locations of the points through constrained non-convex
optimization. Our simulation results verify the robustness of our point source
reconstruction pipeline to noise and error in the estimation of the features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09956</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09956</id><created>2018-11-25</created><authors><author><keyname>M</keyname><forenames>Gurunath Reddy</forenames></author><author><keyname>Mandal</keyname><forenames>Tanumay</forenames></author><author><keyname>Rao</keyname><forenames>Krothapalli Sreenivasa</forenames></author></authors><title>Glottal Closure Instants Detection From Pathological Acoustic Speech
  Signal Using Deep Learning</title><categories>cs.SD cs.LG eess.AS stat.ML</categories><comments>Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216</comments><report-no>ML4H/2018/39</report-no><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a classification based glottal closure instants
(GCI) detection from pathological acoustic speech signal, which finds many
applications in vocal disorder analysis. Till date, GCI for pathological
disorder is extracted from laryngeal (glottal source) signal recorded from
Electroglottograph, a dedicated device designed to measure the vocal folds
vibration around the larynx. We have created a pathological dataset which
consists of simultaneous recordings of glottal source and acoustic speech
signal of six different disorders from vocal disordered patients. The GCI
locations are manually annotated for disorder analysis and supervised learning.
We have proposed convolutional neural network based GCI detection method by
fusing deep acoustic speech and linear prediction residual features for robust
GCI detection. The experimental results showed that the proposed method is
significantly better than the state-of-the-art GCI detection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09967</identifier>
 <datestamp>2019-07-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09967</id><created>2018-11-25</created><updated>2019-07-14</updated><authors><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author><author><keyname>Shah</keyname><forenames>Ankit</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alex</forenames></author></authors><title>Learning Sound Events From Webly Labeled Data</title><categories>cs.SD cs.MM eess.AS</categories><comments>Accepted IJCAI 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last couple of years, weakly labeled learning has turned out to be an
exciting approach for audio event detection. In this work, we introduce webly
labeled learning for sound events which aims to remove human supervision
altogether from the learning process. We first develop a method of obtaining
labeled audio data from the web (albeit noisy), in which no manual labeling is
involved. We then describe methods to efficiently learn from these webly
labeled audio recordings. In our proposed system, WeblyNet, two deep neural
networks co-teach each other to robustly learn from webly labeled data, leading
to around 17% relative improvement over the baseline method. The method also
involves transfer learning to obtain efficient representations
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.09997</identifier>
 <datestamp>2019-02-11</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.09997</id><created>2018-11-25</created><updated>2019-02-07</updated><authors><author><keyname>Singh</keyname><forenames>Kanwar Bharat</forenames></author></authors><title>Virtual Sensor for Vehicle Sideslip Angle Based on Tire Model Adaptation</title><categories>eess.SP</categories><comments>I will upload an extended version of this work in the coming weeks</comments><report-no>27 pages, 27 figures</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information about the vehicle sideslip angle is crucial for the successful
implementation of advanced stability control systems. In production vehicles,
sideslip angle is difficult to measure within the desired accuracy level
because of high costs and other associated impracticalities. This paper
presents a novel framework for estimation of the sideslip angle. The proposed
algorithm utilizes an adaptive tire model in conjunction with a model-based
observer. The developed algorithm is evaluated through experimental tests on a
rear wheel drive vehicle. Detailed experimental results show that the developed
system can reliably estimate the vehicle sideslip angle during both steady
state and transient maneuvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10077</identifier>
 <datestamp>2019-07-24</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10077</id><created>2018-11-25</created><updated>2019-06-06</updated><authors><author><keyname>Shlezinger</keyname><forenames>Nir</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Asymptotic Task-Based Quantization with Application to Massive MIMO</title><categories>eess.SP</categories><doi>10.1109/TSP.2019.2923149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantizers take part in nearly every digital signal processing system which
operates on physical signals. They are commonly designed to accurately
represent the underlying signal, regardless of the specific task to be
performed on the quantized data. In systems working with high-dimensional
signals, such as massive multiple-input multiple-output (MIMO) systems, it is
beneficial to utilize low-resolution quantizers, due to cost, power, and memory
constraints. In this work we study quantization of high-dimensional inputs,
aiming at improving performance under resolution constraints by accounting for
the system task in the quantizers design. We focus on the task of recovering a
desired signal statistically related to the high-dimensional input, and analyze
two quantization approaches: We first consider vector quantization, which is
typically computationally infeasible, and characterize the optimal performance
achievable with this approach. Next, we focus on practical systems which
utilize hardware-limited scalar uniform analog-to-digital converters (ADCs),
and design a task-based quantizer under this model. The resulting system
accounts for the task by linearly combining the observed signal into a lower
dimension prior to quantization. We then apply our proposed technique to
channel estimation in massive MIMO networks. Our results demonstrate that a
system utilizing low-resolution scalar ADCs can approach the optimal channel
estimation performance by properly accounting for the task in the system
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10079</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10079</id><created>2018-11-25</created><authors><author><keyname>Tung</keyname><forenames>Tze-Yang</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author></authors><title>SparseCast: Hybrid Digital-Analog Wireless Image Transmission Exploiting
  Frequency Domain Sparsity</title><categories>eess.IV cs.IT math.IT</categories><comments>This paper is accepted to appear in IEEE Communications Letters</comments><doi>10.1109/LCOMM.2018.2877316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hybrid digital-analog wireless image transmission scheme, called
SparseCast, is introduced, which provides graceful degradation with channel
quality. SparseCast achieves improved end-to-end reconstruction quality while
reducing the bandwidth requirement by exploiting frequency domain sparsity
through compressed sensing. The proposed algorithm produces a linear
relationship between the channel signal-to-noise ratio (CSNR) and peak
signal-to-noise ratio (PSNR), without requiring the channel state knowledge at
the transmitter. This is particularly attractive when transmitting to multiple
receivers or over unknown time-varying channels, as the receiver PSNR depends
on the experienced channel quality, and is not bottlenecked by the worst
channel. SparseCast is benchmarked against two alternative algorithms: SoftCast
and BCS-SPL. Our findings show that the proposed algorithm outperforms SoftCast
by approximately 3.5 dB and BCS-SPL by 15.2 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10111</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10111</id><created>2018-11-25</created><updated>2018-11-27</updated><authors><author><keyname>Koushik</keyname><forenames>Abhay</forenames></author><author><keyname>Amores</keyname><forenames>Judith</forenames></author><author><keyname>Maes</keyname><forenames>Pattie</forenames></author></authors><title>Real-Time Sleep Staging using Deep Learning on a Smartphone for a
  Wearable EEG</title><categories>cs.HC cs.LG eess.SP q-bio.NC</categories><comments>Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216</comments><report-no>ML4H/2018/114</report-no><msc-class>68T05, 68T10</msc-class><acm-class>I.2.6; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first real-time sleep staging system that uses deep learning
without the need for servers in a smartphone application for a wearable EEG. We
employ real-time adaptation of a single channel Electroencephalography (EEG) to
infer from a Time-Distributed 1-D Deep Convolutional Neural Network.
Polysomnography (PSG)-the gold standard for sleep staging, requires a human
scorer and is both complex and resource-intensive. Our work demonstrates an
end-to-end on-smartphone pipeline that can infer sleep stages in just single
30-second epochs, with an overall accuracy of 83.5% on 20-fold cross validation
for five-class classification of sleep stages using the open Sleep-EDF dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10126</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10126</id><created>2018-11-25</created><authors><author><keyname>Wood</keyname><forenames>Richard</forenames></author><author><keyname>McGlashan</keyname><forenames>Alexander</forenames></author><author><keyname>Moon</keyname><forenames>C. B.</forenames></author><author><keyname>Kim</keyname><forenames>W. Y.</forenames></author></authors><title>Artificial Retina Using A Hybrid Neural Network With Spatial Transform
  Capability</title><categories>cs.CV cs.ET eess.IV</categories><comments>16 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper covers the design and programming of a hybrid (digital/analog)
neural network to function as an artificial retina with the ability to perform
a spatial discrete cosine transform. We describe the structure of the circuit,
which uses an analog cell that is interlinked using a programmable digital
array. The paper is broken into three main parts. First, we present the results
of a Matlab simulation. Then we show the circuit simulation in Spice. This is
followed by a demonstration of the practical device. This system has
intentionally separated components with the specialty analog circuits being
separated from the readily available digital field programmable gate array
(FPGA) components. Further development includes the use of rapid
manufacture-able organic electronics used for the analog components. The
planned uses for this platform include crowd development of software that uses
the underlying pulse based processing. The development package will include
simulators in the form of Matlab and Spice type software platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10129</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10129</id><created>2018-11-25</created><authors><author><keyname>Abrar</keyname><forenames>Alemayehu Solomon</forenames></author><author><keyname>Luong</keyname><forenames>Anh</forenames></author><author><keyname>Hillyard</keyname><forenames>Peter</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author></authors><title>Save Our Spectrum: Contact-Free Human Sensing Using Single Carrier Radio</title><categories>eess.SP</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has demonstrated new capabilities in radio frequency (RF)
sensing that apply to health care, smart home, and security applications.
However, previous work in RF sensing requires heavy utilization of the radio
spectrum, for example, transmitting thousands of WiFi packets per second. In
this paper, we present a device-free human sensing system based on received
signal strength (RSS) measurements from a low-cost single carrier narrowband
radio transceiver. We test and validate its performance in three different
applications: real-time heart rate monitoring, gesture recognition, and human
speed estimation. The challenges in these applications stem from the very low
signal-to-noise ratio and the use of a single-dimensional measurement of the
channel. We apply a combination of linear and non-linear filtering, and
time-frequency analysis, and develop new estimators to address the challenges
in the particular applications. Our experimental results indicate that RF
sensing based on single-carrier magnitude measurements performs nearly as well
as the state-of-the-art while utilizing three orders of magnitude less
bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10132</identifier>
 <datestamp>2019-04-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10132</id><created>2018-11-21</created><updated>2019-04-18</updated><authors><author><keyname>Das</keyname><forenames>Sandip</forenames></author><author><keyname>Ruffini</keyname><forenames>Marco</forenames></author></authors><title>A Variable Rate Fronthaul Scheme for Cloud Radio Access Networks (C-RAN)</title><categories>eess.SP</categories><comments>11 pages, 13 figures</comments><doi>10.1109/JLT.2019.2912127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Radio Access Network (C-RAN) is considered as one of the most promising
candidate for Fifth Generation (5G) wireless communication system. However,
C-RAN imposes stringent requirements on fronthaul in terms of capacity which in
turn becomes a bottleneck for deployment of cloud-RAN in dense networks.
Compressed CPRI and functional split are two primary solutions to overcome
this. However, this makes Remote Radio Unit (RRU) more complex and the
computational resources in RRU are localized and thus cannot be shared with
other RRUs when not in use. In this paper, we introduce the concept of Variable
Rate Fronthaul (VRF) for C-RAN. This scheme operates on a CPRI type of
interface (e.g., one that transmits I/Q data samples) with the novelty of
dynamically changing the cell bandwidth, and consequently the fronthaul data
rates, depending on the cell load, with the support of a Software Defined
Network (SDN) controller. This allows for a more efficient transport of C-RAN
cells data over a shared backhaul. We first propose a mathematical analysis of
the VRF performance using a queuing theory approach based on the Markov model.
We then provide the results of our simulation framework both for validation and
in support of the mathematical analysis. Our results show that the proposed VRF
scheme provides significantly lower blocking probability over a shared backhaul
than standard CPRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10133</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10133</id><created>2018-11-21</created><authors><author><keyname>Zang</keyname><forenames>Guangda</forenames></author><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Cheng</keyname><forenames>Hei Victor</forenames></author><author><keyname>Yang</keyname><forenames>Feng</forenames></author><author><keyname>Ding</keyname><forenames>Lianghui</forenames></author><author><keyname>Liu</keyname><forenames>Hui</forenames></author></authors><title>Optimal Hybrid Beamforming for Multiuser Massive MIMO Systems With
  Individual SINR Constraints</title><categories>eess.SP</categories><comments>4 pages, 3 figures, to be published in IEEE Wireless Communications
  Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we consider optimal hybrid beamforming design to minimize the
transmission power under individual signal-to-interference-plus-noise ratio
(SINR) constraints in a multiuser massive multiple-input-multiple-output (MIMO)
system. This results in a challenging non-convex optimization problem. We
consider two cases. In the case where the number of users is smaller than or
equal to that of radio frequency (RF) chains, we propose a low-complexity
method to obtain a globally optimal solution and show that it achieves the same
transmission power as an optimal fully-digital beamformer. In the case where
the number of users is larger than that of RF chains, we propose a
low-complexity globally convergent alternating algorithm to obtain a stationary
point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10134</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10134</id><created>2018-11-22</created><authors><author><keyname>Hou</keyname><forenames>Jiancao</forenames></author><author><keyname>Yang</keyname><forenames>Zhaohui</forenames></author><author><keyname>Shikh-Bahaei</keyname><forenames>Mohammad</forenames></author></authors><title>Energy-Efficient Data Collection and Wireless Power Transfer Using A
  MIMO Full-Duplex UAV</title><categories>eess.SP</categories><comments>7 pages, 3 figures, submitted conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel energy-efficient data collection and
wireless power transfer (WPT) framework for internet of things (IoT)
applications, via a multiple-input multiple-output (MIMO) full-duplex (FD)
unmanned aerial vehicle (UAV). To exploit the benefits of UAV-enabled WPT and
MIMO FD communications, we allow the MIMO FD UAV charge low-power IoT devices
while at the same time collect data from them. With the aim of saving the total
energy consumed at the UAV, we formulate an energy minimization problem by
taking the FD hardware impairments, the number of uploaded data bits, and the
energy harvesting causality into account. Due to the non-convexity of the
problem in terms of UAV trajectory and transmit beamforming for WPT, tracking
the global optimality is quite challenge. Alternatively, we find a local
optimal point by implementing the proposed iterative search algorithm combining
with successive convex approximation techniques. Numerical results show that
the proposed approach can lead to superior performance compared with other
benchmark schemes with low computational complexity and fast convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10135</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10135</id><created>2018-11-01</created><authors><author><keyname>Movahednasab</keyname><forenames>Mohammad</forenames></author><author><keyname>Omidvar</keyname><forenames>Naeimeh</forenames></author><author><keyname>Pakravan</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author></authors><title>Joint Data Routing and Power Scheduling for Wireless Powered
  Communication Networks</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a wireless powered communication network (WPCN), an energy access point
supplies the energy needs of the network nodes through radio frequency wave
transmission, and the nodes store the received energy in their batteries for
their future data transmission. In this paper, we propose an online stochastic
policy that jointly controls energy transmission from the EAP to the nodes and
data transfer among the nodes. For this purpose, we first introduce a novel
perturbed Lyapunov function to address the limitations on the energy
consumption of the nodes imposed by their batteries. Then, using Lyapunov
optimization method, we propose a policy which is adaptive to any arbitrary
channel statistics in the network. Finally, we provide theoretical analysis for
the performance of the proposed policy and show that it stabilizes the network,
and the average power consumption of the network under this policy is within a
bounded gap of the minimum power level required for stabilizing the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10169</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10169</id><created>2018-11-25</created><authors><author><keyname>Li</keyname><forenames>Jie</forenames></author><author><keyname>Shan</keyname><forenames>Yahui</forenames></author><author><keyname>Wang</keyname><forenames>Xiaorui</forenames></author><author><keyname>Li</keyname><forenames>Yan</forenames></author></authors><title>Improving Gated Recurrent Unit Based Acoustic Modeling with Batch
  Normalization and Enlarged Context</title><categories>cs.CL eess.AS</categories><comments>ISCSLP 2018</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of future contextual information is typically shown to be helpful for
acoustic modeling. Recently, we proposed a RNN model called minimal gated
recurrent unit with input projection (mGRUIP), in which a context module namely
temporal convolution, is specifically designed to model the future context.
This model, mGRUIP with context module (mGRUIP-Ctx), has been shown to be able
of utilizing the future context effectively, meanwhile with quite low model
latency and computation cost. In this paper, we continue to improve mGRUIP-Ctx
with two revisions: applying BN methods and enlarging model context.
Experimental results on two Mandarin ASR tasks (8400 hours and 60K hours) show
that, the revised mGRUIP-Ctx outperform LSTM with a large margin (11% to 38%).
It even performs slightly better than a superior BLSTM on the 8400h task, with
33M less parameters and just 290ms model latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10348</identifier>
 <datestamp>2019-04-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10348</id><created>2018-11-26</created><updated>2019-04-24</updated><authors><author><keyname>Czajkowski</keyname><forenames>Krzysztof M.</forenames></author><author><keyname>Pastuszczak</keyname><forenames>Anna</forenames></author><author><keyname>Kotynski</keyname><forenames>Rafal</forenames></author></authors><title>Single-pixel imaging with sampling distributed over simplex vertices</title><categories>eess.IV</categories><journal-ref>Optics Letters, Vol. 44(5), pp. 1241-1244 (2019)</journal-ref><doi>10.1364/OL.44.001241</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method of reduction of experimental noise in single-pixel
imaging by expressing the subsets of sampling patterns as linear combinations
of vertices of a multidimensional regular simplex. This method may be also
directly extended to complementary sampling. The modified sampling consists
only of non-negative patterns. The measurement becomes theoretically
independent of the ambient illumination, and in practice becomes more robust to
the varying conditions of the experiment. We show how the optimal
dimensionality of the simplex depends on the level of measurement noise. We
present experimental results of single-pixel imaging using binarized sampling
and a real-time reconstruction with the Fourier domain regularized inversion
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10371</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10371</id><created>2018-11-26</created><authors><author><keyname>Asgharimoghaddam</keyname><forenames>Hossein</forenames></author><author><keyname>T&#xf6;lli</keyname><forenames>Antti</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Decentralizing Multicell Beamforming via Deterministic Equivalents</title><categories>eess.SP</categories><comments>IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on developing a decentralized framework for coordinated
minimum power beamforming wherein $L$ base stations (BSs), each equipped with
$N$ antennas, serve $K$ single-antenna users with specific rate constraints.
This is realized by considering user specific intercell interference (ICI)
strength as the principal coupling parameter among BSs. First, explicit
deterministic expressions for transmit powers are derived for spatially
correlated channels in the asymptotic regime in which $N$ and $K$ grow large
with a non-trivial ratio $K/N$. These asymptotic expressions are then used to
compute approximations of the optimal ICI values that depend only on the
channel statistics. By relying on the approximate ICI values as coordination
parameters, a distributed non-iterative coordination algorithm, suitable for
large networks with limited backhaul, is proposed. A heuristic algorithm is
also proposed relaxing coordination requirements even further as it only needs
pathloss values for non-local channels. The proposed algorithms satisfy the
target rates for all users even when $N$ and $K$ are relatively small. Finally,
the potential benefits of grouping users with similar statistics are
investigated to further reduce the overhead and computational effort of the
proposed solutions. Simulation results show that the proposed methods yield
near-optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10376</identifier>
 <datestamp>2018-12-04</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10376</id><created>2018-11-26</created><updated>2018-12-02</updated><authors><author><keyname>Hsu</keyname><forenames>Yi-Te</forenames></author><author><keyname>Zhu</keyname><forenames>Zining</forenames></author><author><keyname>Wang</keyname><forenames>Chi-Te</forenames></author><author><keyname>Fang</keyname><forenames>Shih-Hau</forenames></author><author><keyname>Rudzicz</keyname><forenames>Frank</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author></authors><title>Robustness against the channel effect in pathological voice detection</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216</comments><report-no>ML4H/2018/200</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many people are suffering from voice disorders, which can adversely affect
the quality of their lives. In response, some researchers have proposed
algorithms for automatic assessment of these disorders, based on voice signals.
However, these signals can be sensitive to the recording devices. Indeed, the
channel effect is a pervasive problem in machine learning for healthcare. In
this study, we propose a detection system for pathological voice, which is
robust against the channel effect. This system is based on a bidirectional LSTM
network. To increase the performance robustness against channel mismatch, we
integrate domain adversarial training (DAT) to eliminate the differences
between the devices. When we train on data recorded on a high-quality
microphone and evaluate on smartphone data without labels, our robust detection
system increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target
sample labels). To the best of our knowledge, this is the first study applying
unsupervised domain adaptation to pathological voice detection. Notably, our
system does not need target device sample labels, which allows for
generalization to many new devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10382</identifier>
 <datestamp>2019-10-03</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10382</id><created>2018-10-11</created><updated>2019-10-01</updated><authors><author><keyname>Ma</keyname><forenames>Chao</forenames></author><author><keyname>Bendory</keyname><forenames>Tamir</forenames></author><author><keyname>Boumal</keyname><forenames>Nicolas</forenames></author><author><keyname>Sigworth</keyname><forenames>Fred</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author></authors><title>Heterogeneous multireference alignment for images with application to
  2-D classification in single particle reconstruction</title><categories>eess.IV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the task of 2-D classification in single particle reconstruction
by cryo-electron microscopy (cryo-EM), we consider the problem of heterogeneous
multireference alignment of images. In this problem, the goal is to estimate a
(typically small) set of target images from a (typically large) collection of
observations. Each observation is a rotated, noisy version of one of the target
images. For each individual observation, neither the rotation nor which target
image has been rotated are known. As the noise level in cryo-EM data is high,
clustering the observations and estimating individual rotations is challenging.
We propose a framework to estimate the target images directly from the
observations, completely bypassing the need to cluster or register the images.
The framework consists of two steps. First, we estimate rotation-invariant
features of the images, such as the bispectrum. These features can be estimated
to any desired accuracy, at any noise level, provided sufficiently many
observations are collected. Then, we estimate the images from the invariant
features. Numerical experiments on synthetic cryo-EM datasets demonstrate the
effectiveness of the method. Ultimately, we outline future developments
required to apply this method to experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10559</identifier>
 <datestamp>2020-01-17</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10559</id><created>2018-11-26</created><updated>2020-01-15</updated><authors><author><keyname>Singh</keyname><forenames>Pravendra</forenames></author><author><keyname>Verma</keyname><forenames>Vinay Kumar</forenames></author><author><keyname>Rai</keyname><forenames>Piyush</forenames></author><author><keyname>Namboodiri</keyname><forenames>Vinay P.</forenames></author></authors><title>Leveraging Filter Correlations for Deep Model Compression</title><categories>cs.CV cs.LG eess.IV stat.ML</categories><comments>IEEE Winter Conference on Applications of Computer Vision (WACV),
  2020</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a filter correlation based model compression approach for deep
convolutional neural networks. Our approach iteratively identifies pairs of
filters with the largest pairwise correlations and drops one of the filters
from each such pair. However, instead of discarding one of the filters from
each such pair na\&quot;{i}vely, the model is re-optimized to make the filters in
these pairs maximally correlated, so that discarding one of the filters from
the pair results in minimal information loss. Moreover, after discarding the
filters in each round, we further finetune the model to recover from the
potential small loss incurred by the compression. We evaluate our proposed
approach using a comprehensive set of experiments and ablation studies. Our
compression method yields state-of-the-art FLOPs compression rates on various
benchmarks, such as LeNet-5, VGG-16, and ResNet-50,56, while still achieving
excellent predictive performance for tasks such as object detection on
benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10561</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10561</id><created>2018-11-26</created><authors><author><keyname>Abdelnour</keyname><forenames>Jerome</forenames></author><author><keyname>Salvi</keyname><forenames>Giampiero</forenames></author><author><keyname>Rouat</keyname><forenames>Jean</forenames></author></authors><title>CLEAR: A Dataset for Compositional Language and Elementary Acoustic
  Reasoning</title><categories>cs.CL cs.LG cs.SD eess.AS stat.ML</categories><comments>NeurIPS 2018 Visually Grounded Interaction and Language (ViGIL)
  Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the task of acoustic question answering (AQA) in the area of
acoustic reasoning. In this task an agent learns to answer questions on the
basis of acoustic context. In order to promote research in this area, we
propose a data generation paradigm adapted from CLEVR (Johnson et al. 2017). We
generate acoustic scenes by leveraging a bank elementary sounds. We also
provide a number of functional programs that can be used to compose questions
and answers that exploit the relationships between the attributes of the
elementary sounds in each scene. We provide AQA datasets of various sizes as
well as the data generation code. As a preliminary experiment to validate our
data, we report the accuracy of current state of the art visual question
answering models when they are applied to the AQA task without modifications.
Although there is a plethora of question answering tasks based on text, image
or video data, to our knowledge, we are the first to propose answering
questions directly on audio streams. We hope this contribution will facilitate
the development of research in the area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10572</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10572</id><created>2018-11-02</created><authors><author><keyname>Garcia</keyname><forenames>Sergi</forenames></author><author><keyname>Hervas</keyname><forenames>Javier</forenames></author><author><keyname>Gasulla</keyname><forenames>Ivana</forenames></author></authors><title>Demonstration of multi-cavity optoelectronic oscillators based on
  multicore fibers</title><categories>physics.app-ph eess.SP physics.optics</categories><comments>4 pages</comments><journal-ref>Proceedings of 2017 IEEE International Topical Meeting on
  Microwave Photonics (MWP 2017), Beijing, China, 2017, pp. 1-4</journal-ref><doi>10.1109/MWP.2017.8168787</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report the first experimental demonstration of multi-cavity optoelectronic
oscillators where the different cavities are hosted in a single multicore
fiber. Different configurations are implemented on the same 20-m 7-core fiber
link, exploiting both unbalanced dual-cavity operation (loop lengths are a
multiple of a reference value) and multi-cavity Vernier operation (loop lengths
are slightly different).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10585</identifier>
 <datestamp>2018-11-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10585</id><created>2018-11-26</created><authors><author><keyname>Roth</keyname><forenames>Stefan</forenames></author><author><keyname>Kariminezhad</keyname><forenames>Ali</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Base-Stations Up in the Air: Multi-UAV Trajectory Control for Min-Rate
  Maximization in Uplink C-RAN</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the impact of unmanned aerial vehicles (UAVs)
trajectories on terrestrial users' spectral efficiency (SE). Assuming a strong
line of sight path to the users, the distance from all users to all UAVs
influence the outcome of an online trajectory optimization. The trajectory
should be designed in a way that the fairness rate is maximized over time. That
means, the UAVs travel in the directions that maximize the minimum of the
users' SE. From the free-space path-loss channel model, a data-rate gradient is
calculated and used to direct the UAVs in a long-term perspective towards the
local optimal solution on the two-dimensional spatial grid. Therefore, a
control system implementation is designed. Thereby, the UAVs follow the
data-rate gradient direction while having a more smooth trajectory compared
with a gradient method. The system can react to changes of the user locations
online; this system design captures the interaction between multiple UAV
trajectories by joint processing at the central unit, e.g., a ground base
station. Because of the wide spread of user locations, the UAVs end up in
optimal locations widely apart from each other. Besides, the SE expectancy is
enhancing continuously while moving along this trajectory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10673</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10673</id><created>2018-11-26</created><authors><author><keyname>Kim</keyname><forenames>Sungsoo</forenames></author><author><keyname>Park</keyname><forenames>Jin Soo</forenames></author><author><keyname>Bampis</keyname><forenames>Christos G.</forenames></author><author><keyname>Lee</keyname><forenames>Jaeseong</forenames></author><author><keyname>Markey</keyname><forenames>Mia K.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Bovik</keyname><forenames>Alan C.</forenames></author></authors><title>Adversarial Video Compression Guided by Soft Edge Detection</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
&quot;deep&quot; approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10678</identifier>
 <datestamp>2019-07-30</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10678</id><created>2018-10-23</created><updated>2019-07-28</updated><authors><author><keyname>Anwani</keyname><forenames>Navin</forenames></author><author><keyname>Rajendran</keyname><forenames>Bipin</forenames></author></authors><title>Training Multi-layer Spiking Neural Networks using NormAD based
  Spatio-Temporal Error Backpropagation</title><categories>cs.NE cs.LG eess.SP stat.ML</categories><comments>19 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking neural networks (SNNs) have garnered a great amount of interest for
supervised and unsupervised learning applications. This paper deals with the
problem of training multi-layer feedforward SNNs. The non-linear
integrate-and-fire dynamics employed by spiking neurons make it difficult to
train SNNs to generate desired spike trains in response to a given input. To
tackle this, first the problem of training a multi-layer SNN is formulated as
an optimization problem such that its objective function is based on the
deviation in membrane potential rather than the spike arrival instants. Then,
an optimization method named Normalized Approximate Descent (NormAD),
hand-crafted for such non-convex optimization problems, is employed to derive
the iterative synaptic weight update rule. Next, it is reformulated to
efficiently train multi-layer SNNs, and is shown to be effectively performing
spatio-temporal error backpropagation. The learning rule is validated by
training $2$-layer SNNs to solve a spike based formulation of the XOR problem
as well as training $3$-layer SNNs for generic spike based training problems.
Thus, the new algorithm is a key step towards building deep spiking neural
networks capable of efficient event-triggered learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10708</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10708</id><created>2018-11-26</created><authors><author><keyname>Lederle</keyname><forenames>Marcel</forenames></author><author><keyname>Wilhelm</keyname><forenames>Benjamin</forenames></author></authors><title>Combining High-Level Features of Raw Audio Waves and Mel-Spectrograms
  for Audio Tagging</title><categories>cs.SD cs.LG eess.AS</categories><comments>Detection and Classification of Acoustic Scenes and Events 2018
  (DCASE 2018), 19-20 November 2018, Surrey, UK</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we describe our contribution to Task 2 of the DCASE 2018 Audio
Challenge. While it has become ubiquitous to utilize an ensemble of machine
learning methods for classification tasks to obtain better predictive
performance, the majority of ensemble methods combine predictions rather than
learned features. We propose a single-model method that combines learned
high-level features computed from log-scaled mel-spectrograms and raw audio
data. These features are learned separately by two Convolutional Neural
Networks, one for each input type, and then combined by densely connected
layers within a single network. This relatively simple approach along with data
augmentation ranks among the best two percent in the Freesound General-Purpose
Audio Tagging Challenge on Kaggle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10736</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10736</id><created>2018-11-26</created><authors><author><keyname>Lugosch</keyname><forenames>Loren</forenames></author><author><keyname>Myer</keyname><forenames>Samuel</forenames></author><author><keyname>Tomar</keyname><forenames>Vikrant Singh</forenames></author></authors><title>DONUT: CTC-based Query-by-Example Keyword Spotting</title><categories>cs.LG cs.SD eess.AS stat.ML</categories><comments>Accepted to NeurIPS 2018 Workshop on Interpretability and Robustness
  for Audio, Speech, and Language</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyword spotting--or wakeword detection--is an essential feature for
hands-free operation of modern voice-controlled devices. With such devices
becoming ubiquitous, users might want to choose a personalized custom wakeword.
In this work, we present DONUT, a CTC-based algorithm for online
query-by-example keyword spotting that enables custom wakeword detection. The
algorithm works by recording a small number of training examples from the user,
generating a set of label sequence hypotheses from these training examples, and
detecting the wakeword by aggregating the scores of all the hypotheses given a
new audio recording. Our method combines the generalization and
interpretability of CTC-based keyword spotting with the user-adaptation and
convenience of a conventional query-by-example system. DONUT has low
computational requirements and is well-suited for both learning and inference
on embedded systems without requiring private user data to be uploaded to the
cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10778</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10778</id><created>2018-11-26</created><authors><author><keyname>Hu</keyname><forenames>Yue</forenames></author><author><keyname>Liu</keyname><forenames>Xiaohan</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>A Generalized Structured Low-Rank Matrix Completion Algorithm for MR
  Image Recovery</title><categories>eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent theory of mapping an image into a structured low-rank Toeplitz or
Hankel matrix has become an effective method to restore images. In this paper,
we introduce a generalized structured low-rank algorithm to recover images from
their undersampled Fourier coefficients using infimal convolution
regularizations. The image is modeled as the superposition of a piecewise
constant component and a piecewise linear component. The Fourier coefficients
of each component satisfy an annihilation relation, which results in a
structured Toeplitz matrix, respectively. We exploit the low-rank property of
the matrices to formulate a combined regularized optimization problem. In order
to solve the problem efficiently and to avoid the high memory demand resulting
from the large-scale Toeplitz matrices, we introduce a fast and memory
efficient algorithm based on the half-circulant approximation of the Toeplitz
matrix. We demonstrate our algorithm in the context of single and multi-channel
MR images recovery. Numerical experiments indicate that the proposed algorithm
provides improved recovery performance over the state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10812</identifier>
 <datestamp>2019-06-19</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10812</id><created>2018-11-26</created><updated>2019-06-17</updated><authors><author><keyname>Shon</keyname><forenames>Suwon</forenames></author><author><keyname>Lee</keyname><forenames>Younggun</forenames></author><author><keyname>Kim</keyname><forenames>Taesu</forenames></author></authors><title>Large-scale Speaker Retrieval on Random Speaker Variability Subspace</title><categories>eess.AS cs.IR</categories><comments>Interspeech 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a fast speaker search system to retrieve segments of the
same voice identity in the large-scale data. A recent study shows that Locality
Sensitive Hashing (LSH) enables quick retrieval of a relevant voice in the
large-scale data in conjunction with i-vector while maintaining accuracy. In
this paper, we proposed Random Speaker-variability Subspace (RSS) projection to
map a data into LSH based hash tables. We hypothesized that rather than
projecting on completely random subspace without considering data, projecting
on randomly generated speaker variability space would give more chance to put
the same speaker representation into the same hash bins, so we can use less
number of hash tables. Multiple RSS can be generated by randomly selecting a
subset of speakers from a large speaker cohort. From the experimental result,
the proposed approach shows 100 times and 7 times faster than the linear search
and LSH, respectively
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10813</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10813</id><created>2018-11-26</created><authors><author><keyname>Shon</keyname><forenames>Suwon</forenames></author><author><keyname>Oh</keyname><forenames>Tae-Hyun</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>Noise-tolerant Audio-visual Online Person Verification using an
  Attention-based Neural Network Fusion</title><categories>cs.CV eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a multi-modal online person verification system
using both speech and visual signals. Inspired by neuroscientific findings on
the association of voice and face, we propose an attention-based end-to-end
neural network that learns multi-sensory associations for the task of person
verification. The attention mechanism in our proposed network learns to
conditionally select a salient modality between speech and facial
representations that provides a balance between complementary inputs. By virtue
of this capability, the network is robust to missing or corrupted data from
either modality. In the VoxCeleb2 dataset, we show that our method performs
favorably against competing multi-modal methods. Even for extreme cases of
large corruption or an entirely missing modality, our method demonstrates
robustness over other unimodal methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10826</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10826</id><created>2018-11-27</created><authors><author><keyname>Thakur</keyname><forenames>Abhishek</forenames></author><author><keyname>Dhamija</keyname><forenames>Arnav</forenames></author><author><keyname>G</keyname><forenames>Tejeshwar Reddy</forenames></author></authors><title>VECTORS: Video communication through opportunistic relays and scalable
  video coding</title><categories>cs.MM eess.IV</categories><comments>13 pages, 6 figures, and under 3000 words for submission to the
  SoftwareX journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd-sourced video distribution is frequently of interest in the local
vicinity. In this paper, we propose a novel design to transfer such content
over opportunistic networks with adaptive quality encoding to achieve
reasonable delay bounds. The video segments are transmitted between source and
destination in a delay tolerant manner using the Nearby Connections Android
library. This implementation can be applied to multiple domains, including farm
monitoring, wildlife, and environmental tracking, disaster response scenarios,
etc. In this work, we present the design of an opportunistic contact based
system, and we discuss basic results for the trial runs within our institute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10902</identifier>
 <datestamp>2019-05-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10902</id><created>2018-11-27</created><updated>2019-05-28</updated><authors><author><keyname>Wang</keyname><forenames>Xiaoxiao</forenames></author><author><keyname>Guo</keyname><forenames>Xueying</forenames></author><author><keyname>Chuai</keyname><forenames>Jie</forenames></author><author><keyname>Chen</keyname><forenames>Zhitang</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author></authors><title>Kernel-based Multi-Task Contextual Bandits in Cellular Network
  Configuration</title><categories>cs.LG cs.NI eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular network configuration plays a critical role in network performance.
In current practice, network configuration depends heavily on field experience
of engineers and often remains static for a long period of time. This practice
is far from optimal. To address this limitation, online-learning-based
approaches have great potentials to automate and optimize network
configuration. Learning-based approaches face the challenges of learning a
highly complex function for each base station and balancing the fundamental
exploration-exploitation tradeoff while minimizing the exploration cost.
Fortunately, in cellular networks, base stations (BSs) often have similarities
even though they are not identical. To leverage such similarities, we propose
kernel-based multi-BS contextual bandit algorithm based on multi-task learning.
In the algorithm, we leverage the similarity among different BSs defined by
conditional kernel embedding. We present theoretical analysis of the proposed
algorithm in terms of regret and multi-task-learning efficiency. We evaluate
the effectiveness of our algorithm based on a simulator built by real traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10946</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10946</id><created>2018-11-27</created><authors><author><keyname>Sulun</keyname><forenames>Serkan</forenames></author></authors><title>Deep Learned Frame Prediction for Video Compression</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion compensation is one of the most essential methods for any video
compression algorithm. Video frame prediction is a task analogous to motion
compensation. In recent years, the task of frame prediction is undertaken by
deep neural networks (DNNs). In this thesis we create a DNN to perform learned
frame prediction and additionally implement a codec that contains our DNN. We
train our network using two methods for two different goals. Firstly we train
our network based on mean square error (MSE) only, aiming to obtain highest
PSNR values at frame prediction and video compression. Secondly we use
adversarial training to produce visually more realistic frame predictions. For
frame prediction, we compare our method with the baseline methods of frame
difference and 16x16 block motion compensation. For video compression we
further include x264 video codec in the comparison. We show that in frame
prediction, adversarial training produces frames that look sharper and more
realistic, compared MSE based training, but in video compression it
consistently performs worse. This proves that even though adversarial training
is useful for generating video frames that are more pleasing to the human eye,
they should not be employed for video compression. Moreover, our network
trained with MSE produces accurate frame predictions, and in quantitative
results, for both tasks, it produces comparable results in all videos and
outperforms other methods on average. More specifically, learned frame
prediction outperforms other methods in terms of rate-distortion performance in
case of high motion video, while the rate-distortion performance of our method
is competitive with that of x264 in low motion video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.10988</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.10988</id><created>2018-11-21</created><authors><author><keyname>Favory</keyname><forenames>Xavier</forenames></author><author><keyname>Fonseca</keyname><forenames>Eduardo</forenames></author><author><keyname>Font</keyname><forenames>Frederic</forenames></author><author><keyname>Serra</keyname><forenames>Xavier</forenames></author></authors><title>Facilitating the Manual Annotation of Sounds When Using Large Taxonomies</title><categories>cs.IR cs.HC cs.LG cs.SD eess.AS</categories><comments>5 pages, 5 figures, IEEE FRUCT International Workshop on Semantic
  Audio and the Internet of Things</comments><journal-ref>Proceedings of the 23rd Conference of Open Innovations Association
  FRUCT, Bologna, Italy. 2018. ISSN 2305-7254, ISBN 978-952-68653-6-2, FRUCT
  Oy, e-ISSN 2343-0737 (license CC BY-ND)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Properly annotated multimedia content is crucial for supporting advances in
many Information Retrieval applications. It enables, for instance, the
development of automatic tools for the annotation of large and diverse
multimedia collections. In the context of everyday sounds and online
collections, the content to describe is very diverse and involves many
different types of concepts, often organised in large hierarchical structures
called taxonomies. This makes the task of manually annotating content arduous.
In this paper, we present our user-centered development of two tools for the
manual annotation of audio content from a wide range of types. We conducted a
preliminary evaluation of functional prototypes involving real users. The goal
is to evaluate them in a real context, engage in discussions with users, and
inspire new ideas. A qualitative analysis was carried out including usability
questionnaires and semi-structured interviews. This revealed interesting
aspects to consider when developing tools for the manual annotation of audio
content with labels drawn from large hierarchical taxonomies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11074</identifier>
 <datestamp>2019-02-27</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11074</id><created>2018-11-27</created><updated>2019-02-26</updated><authors><author><keyname>Lu</keyname><forenames>Yao</forenames></author><author><keyname>Wu</keyname><forenames>Hau-tieng</forenames></author><author><keyname>Malik</keyname><forenames>John</forenames></author></authors><title>Recycling cardiogenic artifacts in impedance pneumography</title><categories>physics.data-an eess.SP stat.AP</categories><comments>21 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: Biomedical sensors often exhibit cardiogenic artifacts which, while
distorting the signal of interest, carry useful hemodynamic information. We
propose an algorithm to remove and extract hemodynamic information from these
cardiogenic artifacts. Methods: We apply a nonlinear time-frequency analysis
technique, the de-shape synchrosqueezing transform (dsSST), to adaptively
isolate the high- and low-frequency components of a single-channel signal. We
demonstrate this technique's effectiveness by removing and deriving hemodynamic
information from the cardiogenic artifact in an impedance pneumography (IP).
Results: The instantaneous heart rate is extracted, and the cardiac and
respiratory signals are reconstructed. Conclusions: The dsSST is suitable for
generating useful hemodynamic information from the cardiogenic artifact in a
single-channel IP. We propose that the usefulness of the dsSST as a recycling
tool extends to other biomedical sensors exhibiting cardiogenic artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11077</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11077</id><created>2018-11-27</created><authors><author><keyname>Burr</keyname><forenames>Alister</forenames></author><author><keyname>Bashar</keyname><forenames>Manijeh</forenames></author><author><keyname>Maryopi</keyname><forenames>Dick</forenames></author></authors><title>Ultra-dense Radio Access Networks for Smart Cities: Cloud-RAN, Fog-RAN
  and &quot;cell-free&quot; Massive MIMO</title><categories>eess.SP cs.IT math.IT</categories><comments>PIMRC18</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss the requirements for a radio access network
architecture for ultra-dense networks for &quot;smart city&quot; applications, and show
that coordination is required between access points to overcome the effects of
interference. We propose a new paradigm, Fog Massive MIMO, based on a
combination of the &quot;cell-free&quot; massive MIMO concept and the Fog Radio Access
Network (F-RAN). In particular we analyze the potential benefit of improved
coordination between APs over different coordination ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11078</identifier>
 <datestamp>2019-07-09</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11078</id><created>2018-11-27</created><updated>2019-07-08</updated><authors><author><keyname>Huang</keyname><forenames>Wen-Chin</forenames></author><author><keyname>Wu</keyname><forenames>Yi-Chiao</forenames></author><author><keyname>Hwang</keyname><forenames>Hsin-Te</forenames></author><author><keyname>Tobing</keyname><forenames>Patrick Lumban</forenames></author><author><keyname>Hayashi</keyname><forenames>Tomoki</forenames></author><author><keyname>Kobayashi</keyname><forenames>Kazuhiro</forenames></author><author><keyname>Toda</keyname><forenames>Tomoki</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Hsin-Min</forenames></author></authors><title>Refined WaveNet Vocoder for Variational Autoencoder Based Voice
  Conversion</title><categories>eess.AS cs.CL cs.SD</categories><comments>5 pages, 7 figures, 1 table. Accepted to EUSIPCO 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a refinement framework of WaveNet vocoders for
variational autoencoder (VAE) based voice conversion (VC), which reduces the
quality distortion caused by the mismatch between the training data and testing
data. Conventional WaveNet vocoders are trained with natural acoustic features
but conditioned on the converted features in the conversion stage for VC, and
such a mismatch often causes significant quality and similarity degradation. In
this work, we take advantage of the particular structure of VAEs to refine
WaveNet vocoders with the self-reconstructed features generated by VAE, which
are of similar characteristics with the converted features while having the
same temporal structure with the target natural features. We analyze these
features and show that the self-reconstructed features are similar to the
converted features. Objective and subjective experimental results demonstrate
the effectiveness of our proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11084</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11084</id><created>2018-10-24</created><authors><author><keyname>Zhong</keyname><forenames>Zhenghe</forenames></author><author><keyname>Zhang</keyname><forenames>Xinran</forenames></author><author><keyname>Zhang</keyname><forenames>Daihan</forenames></author><author><keyname>Chen</keyname><forenames>Huimiao</forenames></author><author><keyname>Gao</keyname><forenames>Chuning</forenames></author></authors><title>Simulation Based Evaluation and Optimization for PEV Charging Stations
  Deployment in Transportation Networks</title><categories>eess.SP math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plug-in electric vehicles are receiving global attention. However,
large-scale plug-in electric vehicles bring challenge to charging station
deployment. This paper provides a deployment evaluation method and a planning
method for plug-in electric vehicles charging stations based on simulation and
genetic algorithm. In this research, reasonable user behaviors changes i.e.,
detouring for recharging, is included in the method for more practical
application. A detailed logic for describing the plug-in electric vehicles
users' detour behaviors is designed in this paper for the charging station
deployment evaluation and further influence the later charging station planning
strategy formulation. Intuitively, by taking the behaviors change of plug-in
electric vehicles users after a charging station deployment is given into
account, our evaluation method is the more reasonable. Actually a series of
control experiments should be carried out to illustrate the different insights
from our method. However, in this paper, due to the limited space, we just
provide the result of our method in a relatively simple case with some
analyses. Different insights from our method may work as useful suggestion for
charging station plan in a city area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11085</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11085</id><created>2018-11-01</created><authors><author><keyname>Galli</keyname><forenames>Stefano</forenames></author><author><keyname>Liu</keyname><forenames>Ju</forenames></author><author><keyname>Zhang</keyname><forenames>Guanxi</forenames></author></authors><title>Bare Metal Wires as Open Waveguides, with Applications to 5G</title><categories>eess.SP</categories><comments>15 pages, 8 Figures, 27 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an old but unconventional waveguiding technique based on the use of
overhead bare metal Medium Voltage (MV) power lines as open waveguides and not
as transmission lines. This technique can be used in support of 5G applications
such as backhauling and broadband access in the mmW frequency range or above.
Although the analysis of open waveguides dates back a century or more, very few
communications oriented papers are available in the literature and there is
today no commonly agreed upon channel model. This paper offers a first step in
covering this gap by first solving numerically the characteristic equation
derived from Maxwell equations, then deriving an analytical expression for the
transfer function, and finally reporting for the first time capacity values for
bare metal open waveguides. A channel capacity of 1 Tbps over 100 m can be
achieved using a single overhead Medium Voltage power line, 1 W of total
transmitted power, and the 1-100 GHz band.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11087</identifier>
 <datestamp>2019-07-23</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11087</id><created>2018-11-12</created><updated>2019-07-19</updated><authors><author><keyname>Choi</keyname><forenames>Hayoung</forenames></author><author><keyname>He</keyname><forenames>Jinglian</forenames></author><author><keyname>Hu</keyname><forenames>Hang</forenames></author><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author></authors><title>Fast computation of von Neumann entropy for large-scale graphs via
  quadratic approximations</title><categories>cs.IT cs.DS cs.SI eess.SP math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The von Neumann graph entropy (VNGE) can be used as a measure of graph
complexity, which can be the measure of information divergence and distance
between graphs. However, computing VNGE is extensively demanding for a
large-scale graph. We propose novel quadratic approximations for fast computing
VNGE. Various inequalities for error between the quadratic approximations and
the exact VNGE are found. Our methods reduce the cubic complexity of VNGE to
linear complexity. Computational simulations on random graph models and various
real network datasets demonstrate superior performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11089</identifier>
 <datestamp>2018-12-05</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11089</id><created>2018-11-18</created><updated>2018-12-04</updated><authors><author><keyname>Baianifar</keyname><forenames>Mahdi</forenames></author></authors><title>Energy Efficiency Maximization in mmWave Wireless Networks with 3D
  Beamforming</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of three dimensional beamforming in
millimeter wave (mmWave) wireless networks. In particular, we study the impact
of base station (BS) antenna tilt angle optimization on the energy efficiency
(EE) of mmWave networks under two different scenarios: a homogeneous network
consisting of multiple macro base stations (MBSs), and a heterogeneous network
where several femto base stations are added within the coverage areas of the
MBSs. First, by adopting a stochastic geometry approach, we analyze the
coverage probability of both scenarios that incorporate 3DBF. Then, we derive
the EE of the networks as a function of the MBS antenna tilt angle. Next,
optimization problems are formulated to maximize the EE of the networks by
optimizing the tilt angle. Since the computational complexity of the optimal
solution is very high, near-optimal low-complexity methods are proposed for
solving the optimization problems. Simulation results show that in the mmWave
networks, the three dimensional beamforming technique with optimized tilt angle
can considerably improve the EE of the network. Also, the proposed low
complexity approach presents a performance close to the optimal solution but
with a significant reduced complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11090</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11090</id><created>2018-11-21</created><authors><author><keyname>Baghani</keyname><forenames>Mina</forenames></author><author><keyname>Parsaeefard</keyname><forenames>Saeedeh</forenames></author><author><keyname>Derakhshani</keyname><forenames>Mahsa</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author></authors><title>Dynamic Non-Orthogonal Multiple Access (NOMA) and Orthogonal Multiple
  Access (OMA) in 5G Wireless Networks</title><categories>eess.SP</categories><comments>28 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, facilitated via the flexible software defined structure of the
radio access units in 5G, we propose a novel dynamic multiple access technology
selection among orthogonal multiple access (OMA) and non-orthogonal multiple
access (NOMA) techniques for each subcarrier. For this setup, we formulate a
joint resource allocation problem where a new set of access technology
selection parameters along with power and subcarrier are allocated for each
user based on each user's channel state information. Here, we define a novel
utility function taking into account the rate and costs of access technologies.
This cost reflects both the complexity of performing successive interference
cancellation and the complexity incurred to guarantee a desired bit error rate.
This utility function can inherently demonstrate the trade-off between OMA and
NOMA. Due to non-convexity of our proposed resource allocation problem, we
resort to successive convex approximation where a two-step iterative algorithm
is applied in which a problem of the first step, called access technology
selection, is transformed into a linear integer programming problem, and the
nonconvex problem of the second step, referred to power allocation problem, is
solved via the difference-of-convex-functions (DC) programming. Moreover, the
closed-form solution for power allocation in the second step is derived. For
diverse network performance criteria such as rate, simulation results show that
the proposed new dynamic access technology selection outperforms
single-technology OMA or NOMA multiple access solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11091</identifier>
 <datestamp>2020-01-22</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11091</id><created>2018-11-21</created><updated>2020-01-20</updated><authors><author><keyname>Pr&#xe9;vost</keyname><forenames>Cl&#xe9;mence</forenames><affiliation>CRAN</affiliation></author><author><keyname>Usevich</keyname><forenames>Konstantin</forenames><affiliation>CRAN</affiliation></author><author><keyname>Comon</keyname><forenames>Pierre</forenames><affiliation>GIPSA-CICS</affiliation></author><author><keyname>Brie</keyname><forenames>David</forenames><affiliation>CRAN</affiliation></author></authors><title>Hyperspectral Super-Resolution with Coupled Tucker Approximation:
  Recoverability and SVD-based algorithms</title><categories>eess.SP</categories><comments>IEEE Transactions on Signal Processing, Institute of Electrical and
  Electronics Engineers, in Press</comments><proxy>ccsd</proxy><doi>10.1109/TSP.2020.2965305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach for hyperspectral super-resolution, that is based
on low-rank tensor approximation for a coupled low-rank multilinear (Tucker)
model. We show that the correct recovery holds for a wide range of multilinear
ranks. For coupled tensor approximation, we propose two SVD-based algorithms
that are simple and fast, but with a performance comparable to the
state-of-the-art methods. The approach is applicable to the case of unknown
spatial degradation and to the pansharpening problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11092</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11092</id><created>2018-11-21</created><authors><author><keyname>Hattab</keyname><forenames>Ghaith</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Spectrum Sharing Protocols based on Ultra-Narrowband Communications for
  Unlicensed Massive IoT</title><categories>eess.SP</categories><comments>This paper is accepted for publication in the IEEE International
  Symposium on Dynamic Spectrum Access Networks (DySPAN)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ultra-narrowband (UNB) communications is an emerging paradigm that tackles
two challenges to realizing massive Internet-of-things (IoT) connectivity over
the unlicensed spectrum: the intra-network sharing, i.e., how the spectrum is
shared among IoT devices, and the inter-network sharing, i.e., the coexistence
of the IoT network with other incumbent networks. Specifically, intra-network
sharing is enabled by using extremely narrowband signals to connect a massive
number of IoT devices without any prior network synchronization. Further, to
enhance robustness to incumbent interference, each IoT packet is sent multiple
times, each at a different frequency within a single band. Nevertheless, the
interplay between intra-network sharing and inter-technology coexistence at a
large scale remains unclear. Thus, in this paper, we develop an analytical
framework to model and analyze UNB networks. We use stochastic geometry to
derive the probability of successful transmission, identifying the impact of
intra- and inter-network interference on the performance. In addition to
analyzing the existing single-band access protocols, we present two multiband
schemes, where each BS listens to a single band for practical implementation.
Different access protocols are further compared in terms of the transmission
capacity, i.e., the maximum number of IoT devices a UNB protocol can support in
the presence of incumbent networks. Several design insights are gleaned from
the derived closed-form expressions, and simulation results are further
provided to validate them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11095</identifier>
 <datestamp>2019-06-26</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11095</id><created>2018-11-22</created><updated>2019-05-23</updated><authors><author><keyname>Zhuge</keyname><forenames>Qunbi</forenames></author><author><keyname>Zeng</keyname><forenames>Xiaobo</forenames></author><author><keyname>Lun</keyname><forenames>Huazhi</forenames></author><author><keyname>Cai</keyname><forenames>Meng</forenames></author><author><keyname>Liu</keyname><forenames>Xiaomin</forenames></author><author><keyname>Hu</keyname><forenames>Weisheng</forenames></author></authors><title>Application of Machine Learning in Fiber Nonlinearity Modeling and
  Monitoring for Elastic Optical Networks</title><categories>eess.SP stat.ML</categories><comments>There are some errors, as listed in the following, in the preliminary
  version(i.e. v1) of this paper with the title of Application of machine
  learning in fiber nonlinearity modeling and monitoring for elastic optical
  networks. In order to prevent confusing or some misleading decisions, we
  decide to withdraw this paper from arXiv. [The errors are listed in the
  abstract field]</comments><doi>10.1109/JLT.2019.2910143</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fiber nonlinear interference (NLI) modeling and monitoring are the key
building blocks to support elastic optical networks (EONs). In the past, they
were normally developed and investigated separately. Moreover, the accuracy of
the previously proposed methods still needs to be improved for heterogenous
dynamic optical networks. In this paper, we present the application of machine
learning (ML) in NLI modeling and monitoring. In particular, we first propose
to use ML approaches to calibrate the errors of current fiber nonlinearity
models. The Gaussian-noise (GN) model is used as an illustrative example, and
significant improvement is demonstrated with the aid of an artificial neural
network (ANN). Further, we propose to use ML to combine the modeling and
monitoring schemes for a better estimation of NLI variance.
  The following contents are the listed errors as mentioned in the comments for
reasons of withdrawal. (1) The works, as mentioned as the title, should be
addressed is about the elastic optical networks(EON), however, the simulation
setup and the results section are focused on the conventional wavelength
division multiplexing(WDM) networks. This error may confuse some researcher,
getting the misleading decision for the researches about the elastic optical
networks. (2) There exists some errors in the results rection, such as,
Fig.9(b) and (c) with the wrong captions may result in misleading decision. (3)
The split-step-Fourier-method(SSFM) presents good accuracy if the sufficiently
small steps are adopted in the calculation, however this paper has not
necessary contents and efforts to optimise the step-length of SSFM. This error
may confuse the accuracy of simulation results. Therefore, we decide to
withdraw this paper from arXiv. The correct and complete paper with the same
title was published in journal of lightwave technology with doi:
10.1109/JLT.2019.2910143.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11096</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11096</id><created>2018-11-24</created><authors><author><keyname>Ruan</keyname><forenames>Xiaoke</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>Yixiao</forenames></author><author><keyname>Zhang</keyname><forenames>Fan</forenames></author></authors><title>100G Data Center Interconnections with Silicon Dual-Drive Mach-Zehnder
  Modulator and Direct Detection</title><categories>eess.SP physics.app-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we experimentally demonstrate that a silicon dual-drive
Mach-Zehnder modulator (DD-MZM) has great potential for next-generation data
center interconnections (DCIs). For intra-data center interconnections, 120
Gb/s Nyquist 4-ary pulse amplitude modulation (PAM-4) signal is successfully
generated with a silicon DD-MZM operating at C-band and transmitted over 2 km
standard single-mode fiber (SSMF) with a bit error rate (BER) of 5.55x10-4. For
inter-data center interconnections, single sideband (SSB) modulation is chosen
to avoid power fading caused by fiber chromatic dispersion and square-law
detection. We report the generation and transmission of 112 Gb/s Nyquist SSB
PAM-4 signal by using the same silicon DD-MZM and Kramers-Kronig (KK) direct
detection. A two-tap digital post filter and maximum likelihood sequence
detection (MLSD) are applied to compensate for the limited system bandwidth.
After 80 km SSMF transmission, the BER is 2.46x10-3 that is below the 7% HD-FEC
threshold of 3.8x10-3. To the best of our knowledge, our work reports the
highest single-lane bitrate of 80 km SSB transmission based on a silicon
DD-MZM. Our study also shows the feasibility of silicon photonic modulator for
DCI applications in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11098</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11098</id><created>2018-11-24</created><authors><author><keyname>Amer</keyname><forenames>Ramy</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>ElSawy</keyname><forenames>Hesham</forenames></author><author><keyname>Butt</keyname><forenames>Majid</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author></authors><title>Caching to the Sky: Performance Analysis of Cache-Assisted CoMP for
  Cellular-Connected UAVs</title><categories>eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing connectivity to aerial users, such as cellular-connected unmanned
aerial vehicles (UAVs) or flying taxis, is a key challenge for tomorrow's
cellular systems. In this paper, the use of coordinated multi-point (CoMP)
transmission along with caching for providing seamless connectivity to aerial
users is investigated. In particular, a network of clustered cache-enabled
small base stations (SBSs) serving aerial users is considered in which a
requested content by an aerial user is cooperatively transmitted from
collaborative ground SBSs. For this network, a novel upper bound expression on
the coverage probability is derived as a function of the system parameters. The
effects of various system parameters such as collaboration distance and content
availability on the achievable performance are then investigated. Results
reveal that, when the antennas of the SBSs are tilted downwards, the coverage
probability of a high-altitude aerial user is upper bounded by that of a ground
user regardless of the transmission scheme. Moreover, it is shown that for a
low signal-to-interference-ratio (SIR) threshold, CoMP transmission improves
the coverage probability for aerial users from 10% to 70% under a collaboration
distance of 200 m.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11099</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11099</id><created>2018-11-24</created><authors><author><keyname>Amer</keyname><forenames>Ramy</forenames></author><author><keyname>ElSawy</keyname><forenames>Hesham</forenames></author><author><keyname>Kibi&#x142;da</keyname><forenames>Jacek</forenames></author><author><keyname>Butt</keyname><forenames>M. Majid</forenames></author><author><keyname>Marchetti</keyname><forenames>Nicola</forenames></author></authors><title>Cooperative Transmission and Probabilistic Caching for Clustered D2D
  Networks</title><categories>eess.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we aim at maximizing the cache offloading gain for a clustered
\ac{D2D} caching network by exploiting probabilistic caching and cooperative
transmission among the cluster devices. Devices with surplus memory
probabilistically cache a content from a known library. A requested content is
either brought from the device's local cache, cooperatively transmitted from
catering devices, or downloaded from the macro base station as a last resort.
Using stochastic geometry, we derive a closed-form expression for the
offloading gain and formulate the offloading maximization problem. In order to
simplify the objective function and obtain analytically tractable expressions,
we derive a lower bound on the offloading gain, for which a suboptimal solution
is obtained when considering a special case. Results reveal that the obtained
suboptimal solution can achieve up to 12% increase in the offloading gain
compared to the Zipf's caching technique. Besides, we show that the spatial
scaling parameters of the network, e.g., the density of clusters and distance
between devices in the same cluster, play a crucial role in identifying the
tradeoff between the content diversity gain and the cooperative transmission
gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11100</identifier>
 <datestamp>2019-10-02</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11100</id><created>2018-11-25</created><updated>2019-01-24</updated><authors><author><keyname>Jafari</keyname><forenames>Rana</forenames></author><author><keyname>Trebino</keyname><forenames>Rick</forenames></author></authors><title>100% Reliable Frequency-Resolved Optical Gating Pulse-Retrieval
  Algorithmic Approach</title><categories>eess.SP physics.optics</categories><doi>10.1109/JQE.2019.2920670</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency-resolved optical gating (FROG) is widely used to measure ultrashort
laser pulses, also providing an excellent indication of pulse-shape
instabilities by disagreement between measured and retrieved FROG traces. FROG,
however, requires -- but currently lacks -- an extremely reliable
pulse-retrieval algorithm. So, this work provides one. It uses a simple
procedure for directly retrieving the precise pulse spectrum from the measured
trace. Additionally, it implements a multi-grid scheme, also quickly yielding a
vastly improved guess for the spectral phase before implementing the entire
measured trace. As a result, it achieves 100% convergence for the three most
common variants of FROG for pulses with time-bandwidth products as high as 100,
even with traces contaminated with noise. Here we consider the
polarization-gate (PG) and transient-grating (TG) variants of FROG, which
measure amplified, UV, and broadly tunable pulses. Convergence occurs for all
of the &gt;20,000 simulated noisy PG/TG FROG traces considered and is also faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11102</identifier>
 <datestamp>2018-11-28</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11102</id><created>2018-11-07</created><authors><author><keyname>Molev-Shteiman</keyname><forenames>Arkady</forenames></author><author><keyname>Qi</keyname><forenames>Xiao-Feng</forenames></author></authors><title>Maximal Entropy Reduction Algorithm for SAR ADC Clock Compression</title><categories>eess.SP cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reduction of comparison cycles leads to power savings of a
successive-approximation-register (SAR) analog-to-digital converters (ADC). We
establish that the lowest average number of comparison cycles of a SAR ADC
approaches the entropy of the ADC output, and proposed a simple adaptive
algorithm that approaches this lower bound. Today's SAR ADC uses binary search,
which consumes more power than necessary for non-uniform input distributions
commonly found in practice. We refer to a SAR ADC employing such algorithm the
maximal entropy reduction (MER) ADC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11281</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11281</id><created>2018-11-27</created><authors><author><keyname>Biton</keyname><forenames>Shai</forenames></author><author><keyname>Gilboa</keyname><forenames>Guy</forenames></author></authors><title>Adaptive Anisotropic Total Variation - A Nonlinear Spectral Analysis</title><categories>eess.IV math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental concept in solving inverse problems is the use of regularizers,
which yield more physical and less-oscillatory solutions. Total variation (TV)
has been widely used as an edge-preserving regularizer. However, objects are
often over-regularized by TV, becoming blob-like convex structures of low
curvature. This phenomenon was explained mathematically in the analysis of
Andreau et al. They have shown that a TV regularizer can spatially preserve
perfectly sets which are nonlinear eigenfunctions of the form $\lambda u \in
\partial J_{TV}(u)$, where $\partial J_{TV}(u)$ is the TV subdifferential. For
TV, these shapes are convex sets of low-curvature. A compelling approach to
better preserve structures is to use anisotropic functionals, which adapt the
regularization in an image-driven manner, with strong regularization along
edges and low across them. This follows earlier ideas of Weickert on
anisotropic diffusion, which do not stem directly from functional minimization.
Adaptive anisotropic TV (A$^2$TV) was successfully used in several studies in
the past decade. However, until now there is no theory formulating the type of
structures which can be perfectly preserved. In this study we address this
question. We rely on a recently developed theory of Burger et al on nonlinear
spectral analysis of one-homogeneous functionals. We have that eigenfunction
sets, admitting $\lambda u \in \partial J_{A^2TV}(u)$, are perfectly preserved
under A$^2$TV-flow or minimization with $L^2$ square fidelity. We thus
investigate these eigenfunctions theoretically and numerically. We prove
non-convex sets can be eigenfunctions in certain conditions and provide
numerical results which characterize well the relations between the degree of
local anisotropy of the functional and the admitted maximal curvature....
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11307</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11307</id><created>2018-11-27</created><authors><author><keyname>Macartney</keyname><forenames>Craig</forenames></author><author><keyname>Weyde</keyname><forenames>Tillman</forenames></author></authors><title>Improved Speech Enhancement with the Wave-U-Net</title><categories>cs.SD cs.LG cs.NE eess.AS eess.SP</categories><comments>5 pages (including 1 for References), 1 figure, 2 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the use of the Wave-U-Net architecture for speech enhancement, a
model introduced by Stoller et al for the separation of music vocals and
accompaniment. This end-to-end learning method for audio source separation
operates directly in the time domain, permitting the integrated modelling of
phase information and being able to take large temporal contexts into account.
Our experiments show that the proposed method improves several metrics, namely
PESQ, CSIG, CBAK, COVL and SSNR, over the state-of-the-art with respect to the
speech enhancement task on the Voice Bank corpus (VCTK) dataset. We find that a
reduced number of hidden layers is sufficient for speech enhancement in
comparison to the original system designed for singing voice separation in
music. We see this initial result as an encouraging signal to further explore
speech enhancement in the time-domain, both as an end in itself and as a
pre-processing step to speech recognition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11389</identifier>
 <datestamp>2019-10-16</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11389</id><created>2018-11-28</created><updated>2019-10-14</updated><authors><author><keyname>Zhao</keyname><forenames>Bo</forenames></author><author><keyname>Meng</keyname><forenames>Lili</forenames></author><author><keyname>Yin</keyname><forenames>Weidong</forenames></author><author><keyname>Sigal</keyname><forenames>Leonid</forenames></author></authors><title>Image Generation from Layout</title><categories>cs.CV eess.IV</categories><comments>Accepted to CVPR 2019 (Oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite significant recent progress on generative models, controlled
generation of images depicting multiple and complex object layouts is still a
difficult problem. Among the core challenges are the diversity of appearance a
given object may possess and, as a result, exponential set of images consistent
with a specified layout. To address these challenges, we propose a novel
approach for layout-based image generation; we call it Layout2Im. Given the
coarse spatial layout (bounding boxes + object categories), our model can
generate a set of realistic images which have the correct objects in the
desired locations. The representation of each object is disentangled into a
specified/certain part (category) and an unspecified/uncertain part
(appearance). The category is encoded using a word embedding and the appearance
is distilled into a low-dimensional vector sampled from a normal distribution.
Individual object representations are composed together using convolutional
LSTM, to obtain an encoding of the complete layout, and then decoded to an
image. Several loss terms are introduced to encourage accurate and diverse
generation. The proposed Layout2Im model significantly outperforms the previous
state of the art, boosting the best reported inception score by 24.66% and
28.57% on the very challenging COCO-Stuff and Visual Genome datasets,
respectively. Extensive experiments also demonstrate our method's ability to
generate complex and diverse images with multiple objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11402</identifier>
 <datestamp>2019-01-01</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11402</id><created>2018-11-28</created><updated>2018-12-30</updated><authors><author><keyname>Latif</keyname><forenames>Siddique</forenames></author><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author></authors><title>Adversarial Machine Learning And Speech Emotion Recognition: Utilizing
  Generative Adversarial Networks For Robustness</title><categories>cs.LG cs.CR eess.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has undoubtedly offered tremendous improvements in the
performance of state-of-the-art speech emotion recognition (SER) systems.
However, recent research on adversarial examples poses enormous challenges on
the robustness of SER systems by showing the susceptibility of deep neural
networks to adversarial examples as they rely only on small and imperceptible
perturbations. In this study, we evaluate how adversarial examples can be used
to attack SER systems and propose the first black-box adversarial attack on SER
systems. We also explore potential defenses including adversarial training and
generative adversarial network (GAN) to enhance robustness. Experimental
evaluations suggest various interesting aspects of the effective utilization of
adversarial examples useful for achieving robustness for SER systems opening up
opportunities for researchers to further innovate in this space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11429</identifier>
 <datestamp>2019-08-06</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11429</id><created>2018-11-28</created><updated>2019-08-03</updated><authors><author><keyname>Tehrani</keyname><forenames>Mohammad Naseri</forenames></author><author><keyname>Farahmand</keyname><forenames>Shahrokh</forenames></author></authors><title>IoT Random Access in Massive MIMO: Exploiting Diversity in Sensing
  Matrices</title><categories>eess.SP</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, non-orthogonal codes have been advocated for IoT massive access.
Activity detection has been demonstrated to entail common support recovery in a
jointly sparse multiple measurement vector (MMV) problem and MMV algorithms
have been successfully applied offering various degrees of
complexity-performance trade-off. Targeting the small measurement per antenna
but large number of antennas setup, independent sensing matrices do offer
significant performance advantages. Unfortunately, the IoT random access
problem can not readily benefit from this concept as code matrix is fixed over
all receiving antennas. Our contributions towards addressing this challenge are
as follows. First, independent small-scale fading across antennas and users is
established as a possible source of sensing matrix decorrelation. Secondly, two
novel algorithms are proposed which exploit this partial de-correlation and
collect sensing matrix diversity. Enjoying a low-complexity, these methods do
offer great practical advantages as they target small measurement size, which
is indeed severely constrained due to limited coherence time/bandwidth, but
instead compensate for it by using a large array of antennas. Thirdly,
probability of failure (PoF) for these methods are rigorously derived and
corresponding measurement inequalities are presented. Fourthly, extensive
simulations are conducted to confirm the superior performance of these methods
versus state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11474</identifier>
 <datestamp>2020-02-25</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11474</id><created>2018-11-28</created><updated>2020-02-22</updated><authors><author><keyname>Pr&#xfc;her</keyname><forenames>Jakub</forenames></author><author><keyname>Karvonen</keyname><forenames>Toni</forenames></author><author><keyname>Oates</keyname><forenames>Chris J.</forenames></author><author><keyname>Straka</keyname><forenames>Ond&#x159;ej</forenames></author><author><keyname>S&#xe4;rkk&#xe4;</keyname><forenames>Simo</forenames></author></authors><title>Improved Calibration of Numerical Integration Error in Sigma-Point
  Filters</title><categories>stat.ML cs.LG eess.SP stat.ME</categories><comments>13 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The sigma-point filters, such as the UKF, which exploit numerical quadrature
to obtain an additional order of accuracy in the moment transformation step,
are popular alternatives to the ubiquitous EKF. The classical quadrature rules
used in the sigma-point filters are motivated via polynomial approximation of
the integrand, however in the applied context these assumptions cannot always
be justified. As a result, quadrature error can introduce bias into estimated
moments, for which there is no compensatory mechanism in the classical
sigma-point filters. This can lead in turn to estimates and predictions that
are poorly calibrated. In this article, we investigate the Bayes-Sard
quadrature method in the context of sigma-point filters, which enables
uncertainty due to quadrature error to be formalised within a probabilistic
model. Our first contribution is to derive the well-known classical quadratures
as special cases of the Bayes-Sard quadrature method. Then a general-purpose
moment transform is developed and utilised in the design of novel sigma-point
filters, so that uncertainty due to quadrature error is explicitly quantified.
Numerical experiments on a challenging tracking example with misspecified
initial conditions show that the additional uncertainty quantification built
into our method leads to better-calibrated state estimates with improved RMSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11482</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11482</id><created>2018-11-28</created><authors><author><keyname>Kong</keyname><forenames>Shu</forenames></author><author><keyname>Fowlkes</keyname><forenames>Charless</forenames></author></authors><title>Image Reconstruction with Predictive Filter Flow</title><categories>eess.IV cs.CV cs.GR cs.LG</categories><comments>https://www.ics.uci.edu/~skong2/pff.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple, interpretable framework for solving a wide range of
image reconstruction problems such as denoising and deconvolution. Given a
corrupted input image, the model synthesizes a spatially varying linear filter
which, when applied to the input image, reconstructs the desired output. The
model parameters are learned using supervised or self-supervised training. We
test this model on three tasks: non-uniform motion blur removal,
lossy-compression artifact reduction and single image super resolution. We
demonstrate that our model substantially outperforms state-of-the-art methods
on all these tasks and is significantly faster than optimization-based
approaches to deconvolution. Unlike models that directly predict output pixel
values, the predicted filter flow is controllable and interpretable, which we
demonstrate by visualizing the space of predicted filters for different tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11517</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11517</id><created>2018-11-28</created><authors><author><keyname>Chai</keyname><forenames>Li</forenames></author><author><keyname>Du</keyname><forenames>Jun</forenames></author><author><keyname>Lee</keyname><forenames>Chin-Hui</forenames></author></authors><title>Acoustics-guided evaluation (AGE): a new measure for estimating
  performance of speech enhancement algorithms for robust ASR</title><categories>eess.AS cs.SD</categories><comments>Submitted to ICASSP 2019</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One challenging problem of robust automatic speech recognition (ASR) is how
to measure the goodness of a speech enhancement algorithm (SEA) without
calculating the word error rate (WER) due to the high costs of manual
transcriptions, language modeling and decoding process. Traditional measures
like PESQ and STOI for evaluating the speech quality and intelligibility were
verified to have relatively low correlations with WER. In this study, a novel
acoustics-guided evaluation (AGE) measure is proposed for estimating
performance of SEAs for robust ASR. AGE consists of three consecutive steps,
namely the low-level representations via the feature extraction, high-level
representations via the nonlinear mapping with the acoustic model (AM), and the
final AGE calculation between the representations of clean speech and degraded
speech. Specifically, state posterior probabilities from neural network based
AM are adopted for the high-level representations and the cross-entropy
criterion is used to calculate AGE. Experiments demonstrate AGE could yield
consistently highest correlations with WER and give the most accurate
estimation of ASR performance compared with PESQ, STOI, and acoustic confidence
measure using Entropy. Potentially, AGE could be adopted to guide the parameter
optimization of deep learning based SEAs to further improve the recognition
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11529</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11529</id><created>2018-11-24</created><authors><author><keyname>Ahmad</keyname><forenames>M. Z.</forenames></author><author><keyname>Peters</keyname><forenames>J. F.</forenames></author></authors><title>Hyperconnected Relator Spaces. CW Complexes and Continuous Function
  Paths that are Hyperconnected</title><categories>math.GT eess.IV</categories><comments>15 pages, 5 figures</comments><msc-class>54E05, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces proximal cell complexes in a hyperconnected space.
Hyperconnectedness encodes how collections of path-connected sub-complexes in a
Alexandroff-Hopf-Whitehead CW space are near to or far from each other. Several
main results are given, namely, a hyper-connectedness form of CW (Closure
Finite Weak topology) complex, the existence of continuous functions that are
paths in hyperconnected relator spaces and hyperconnected chains with
overlapping interiors that are path graphs in a relator space. An application
of these results is given in terms of the definition of cycles using the
centroids of triangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1811.11575</identifier>
 <datestamp>2018-11-29</datestamp>
 <setSpec>eess</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1811.11575</id><created>2018-11-28</created><authors><author><keyname>Feuillen</keyname><forenames>Thomas</forenames></author><author><keyname>Xu</keyname><forenames>Chunlei</forenames></author><author><keyname>Louveaux</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author></authors><title>Quantity over Quality: Dithered Quantization for Compressive Radar
  Systems</title><categories>eess.SP</categories><comments>Submitted to RadarConf2019 as an invited paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a trade-off between the number of radar
observations (or measurements) and their resolution in the context of radar
range estimation. To this end, we introduce a novel estimation scheme that can
deal with strongly quantized received signals, going as low as 1-bit per signal
sample. We leverage for this a dithered quantized compressive sensing framework
that can be applied to classic radar processing and hardware. This allows us to
remove ambiguous scenarios prohibiting correct range estimation from
(undithered) quantized base-band radar signal. Two range estimation algorithms
are studied: Projected Back Projection (PBP) and Quantized Iterative Hard
Thresholding (QIHT). The effectiveness of the reconstruction methods combined
with the dithering strategy is shown through Monte Carlo simulations.
Furthermore we show that: (i), in dithered quantization, the accuracy of target
range estimation improves when the bit-rate (i.e., the total number of measured
bits) increases, whereas the accuracy of other undithered schemes saturate in
this case; and (ii), for fixed, low bit-rate scenarios, severely quantized
dithered schemes exhibit better performances than their full resolution
counterparts. These observations are confirmed using real measurements obtained
in a controlled environment, demonstrating the feasibility of the method in
real ranging applications.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="3000" completeListSize="16166">4250076|4001</resumptionToken>
</ListRecords>
</OAI-PMH>
