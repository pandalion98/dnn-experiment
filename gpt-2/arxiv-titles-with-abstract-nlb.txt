Multi-Sensor Fuzzy Data Fusion Using Sensors with Different Characteristics

This paper proposes a new approach to multi-sensor data fusion. It suggests
that aggregation of data from multiple sensors can be done more efficiently
when we consider information about sensors' different characteristics. Similar
to most research on effective sensors' characteristics, especially in control
systems, our focus is on sensors' accuracy and frequency response. A rule-based
fuzzy system is presented for fusion of raw data obtained from the sensors that
have complement characteristics in accuracy and bandwidth. Furthermore, a fuzzy
predictor system is suggested aiming for extreme accuracy which is a common
need in highly sensitive applications. Advantages of our proposed sensor fusion
system are shown by simulation of a control system utilizing the fusion system
for output estimation.



Warping Peirce Quincuncial Panoramas

The Peirce quincuncial projection is a mapping of the surface of a sphere to
the interior of a square. It is a conformal map except for four points on the
equator. These points of non-conformality cause significant artifacts in
photographic applications. In this paper, we propose an algorithm and
user-interface to mitigate these artifacts. Moreover, in order to facilitate an
interactive user-interface, we present a fast algorithm for calculating the
Peirce quincuncial projection of spherical imagery. We then promote the Peirce
quincuncial projection as a viable alternative to the more popular
stereographic projection in some scenarios.



Tracking Tetrahymena Pyriformis Cells using Decision Trees

Matching cells over time has long been the most difficult step in cell
tracking. In this paper, we approach this problem by recasting it as a
classification problem. We construct a feature set for each cell, and compute a
feature difference vector between a cell in the current frame and a cell in a
previous frame. Then we determine whether the two cells represent the same cell
over time by training decision trees as our binary classifiers. With the output
of decision trees, we are able to formulate an assignment problem for our cell
association task and solve it using a modified version of the Hungarian
algorithm.



Signal processing with Levy information

Levy processes, which have stationary independent increments, are ideal for
modelling the various types of noise that can arise in communication channels.
If a Levy process admits exponential moments, then there exists a parametric
family of measure changes called Esscher transformations. If the parameter is
replaced with an independent random variable, the true value of which
represents a "message", then under the transformed measure the original Levy
process takes on the character of an "information process". In this paper we
develop a theory of such Levy information processes. The underlying Levy
process, which we call the fiducial process, represents the "noise type". Each
such noise type is capable of carrying a message of a certain specification. A
number of examples are worked out in detail, including information processes of
the Brownian, Poisson, gamma, variance gamma, negative binomial, inverse
Gaussian, and normal inverse Gaussian type. Although in general there is no
additive decomposition of information into signal and noise, one is led
nevertheless for each noise type to a well-defined scheme for signal detection
and enhancement relevant to a variety of practical situations.



Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks

Recent studies have shown that deep neural networks (DNNs) perform
significantly better than shallow networks and Gaussian mixture models (GMMs)
on large vocabulary speech recognition tasks. In this paper, we argue that the
improved accuracy achieved by the DNNs is the result of their ability to
extract discriminative internal representations that are robust to the many
sources of variability in speech signals. We show that these representations
become increasingly insensitive to small perturbations in the input with
increasing network depth, which leads to better speech recognition performance
with deeper networks. We also show that DNNs cannot extrapolate to test samples
that are substantially different from the training examples. If the training
data are sufficiently representative, however, internal features learned by the
DNN are relatively stable with respect to speaker differences, bandwidth
differences, and environment distortion. This enables DNN-based recognizers to
perform as well or better than state-of-the-art systems based on GMMs or
shallow networks without the need for explicit model adaptation or feature
normalization.



Cognitive Random Stepped Frequency Radar with Sparse Recovery

Random stepped frequency (RSF) radar, which transmits random-frequency
pulses, can suppress the range ambiguity, improve convert detection, and
possess excellent electronic counter-countermeasures (ECCM) ability [1]. In
this paper, we apply a sparse recovery method to estimate the range and Doppler
of targets. We also propose a cognitive mechanism for RSF radar to further
enhance the performance of the sparse recovery method. The carrier frequencies
of transmitted pulses are adaptively designed in response to the observed
circumstance. We investigate the criterion to design carrier frequencies, and
efficient methods are then devised. Simulation results demonstrate that the
adaptive frequency-design mechanism significantly improves the performance of
target reconstruction in comparison with the non-adaptive mechanism.



Adaptive matching pursuit for off-grid compressed sensing

Compressive sensing (CS) can effectively recover a signal when it is sparse
in some discrete atoms. However, in some applications, signals are sparse in a
continuous parameter space, e.g., frequency space, rather than discrete atoms.
Usually, we divide the continuous parameter into finite discrete grid points
and build a dictionary from these grid points. However, the actual targets may
not exactly lie on the grid points no matter how densely the parameter is
grided, which introduces mismatch between the predefined dictionary and the
actual one. In this article, a novel method, namely adaptive matching pursuit
with constrained total least squares (AMP-CTLS), is proposed to find actual
atoms even if they are not included in the initial dictionary. In AMP-CTLS, the
grid and the dictionary are adaptively updated to better agree with
measurements. The convergence of the algorithm is discussed, and numerical
experiments demonstrate the advantages of AMP-CTLS.



Sparsity-Promoting Sensor Selection for Non-linear Measurement Models

Sensor selection is an important design problem in large-scale sensor
networks. Sensor selection can be interpreted as the problem of selecting the
best subset of sensors that guarantees a certain estimation performance. We
focus on observations that are related to a general non-linear model. The
proposed framework is valid as long as the observations are independent, and
its likelihood satisfies the regularity conditions. We use several functions of
the Cram\'er-Rao bound (CRB) as a performance measure. We formulate the sensor
selection problem as the design of a selection vector, which in its original
form is a nonconvex l0-(quasi) norm optimization problem. We present relaxed
sensor selection solvers that can be efficiently solved in polynomial time. We
also propose a projected subgradient algorithm that is attractive for
large-scale problems and also show how the algorithm can be easily distributed.
The proposed framework is illustrated with a number of examples related to
sensor placement design for localization.



Cramer-Rao Lower Bounds of Joint Delay-Doppler Estimation for an Extended Target

The problem on the Cramer-Rao Lower Bounds (CRLBs) for the joint time delay
and Doppler stretch estimation of an extended target is considered in this
paper. The integral representations of the CRLBs for both the time delay and
the Doppler stretch are derived. To facilitate computation and analysis, series
representations and approximations of the CRLBs are introduced. According to
these series representations, the impact of several waveform parameters on the
estimation accuracy is investigated, which reveals that the CRLB of the Doppler
stretch is inversely proportional to the effective time-bandwidth product of
the waveform. This conclusion generalizes a previous result in the narrowband
case. The popular wideband ambiguity function (WBAF) based delay-Doppler
estimator is evaluated and compared with the CRLBs through numerical
experiments. Our results indicate that the WBAF estimator, originally derived
from a single scatterer model, is not suitable for the parameter estimation of
an extended target.



Automatic Photo Adjustment Using Deep Neural Networks

Photo retouching enables photographers to invoke dramatic visual impressions
by artistically enhancing their photos through stylistic color and tone
adjustments. However, it is also a time-consuming and challenging task that
requires advanced skills beyond the abilities of casual photographers. Using an
automated algorithm is an appealing alternative to manual work but such an
algorithm faces many hurdles. Many photographic styles rely on subtle
adjustments that depend on the image content and even its semantics. Further,
these adjustments are often spatially varying. Because of these
characteristics, existing automatic algorithms are still limited and cover only
a subset of these challenges. Recently, deep machine learning has shown unique
abilities to address hard problems that resisted machine algorithms for long.
This motivated us to explore the use of deep learning in the context of photo
editing. In this paper, we explain how to formulate the automatic photo
adjustment problem in a way suitable for this approach. We also introduce an
image descriptor that accounts for the local semantics of an image. Our
experiments demonstrate that our deep learning formulation applied using these
descriptors successfully capture sophisticated photographic styles. In
particular and unlike previous techniques, it can model local adjustments that
depend on the image semantics. We show on several examples that this yields
results that are qualitatively and quantitatively better than previous work.



Implementation of an Automatic Syllabic Division Algorithm from Speech Files in Portuguese Language

A new algorithm for voice automatic syllabic splitting in the Portuguese
language is proposed, which is based on the envelope of the speech signal of
the input audio file. A computational implementation in MatlabTM is presented
and made available at the URL
http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its
straightforwardness, the proposed method is very attractive for embedded
systems (e.g. i-phones). It can also be used as a screen to assist more
sophisticated methods. Voice excerpts containing more than one syllable and
identified by the same envelope are named as super-syllables and they are
subsequently separated. The results indicate which samples corresponds to the
beginning and end of each detected syllable. Preliminary tests were performed
to fifty words at an identification rate circa 70% (further improvements may be
incorporated to treat particular phonemes). This algorithm is also useful in
voice command systems, as a tool in the teaching of Portuguese language or even
for patients with speech pathology.



A Matrix Laurent Series-based Fast Fourier Transform for Blocklengths N=4 (mod 8)

General guidelines for a new fast computation of blocklength 8m+4 DFTs are
presented, which is based on a Laurent series involving matrices. Results of
non-trivial real multiplicative complexity are presented for blocklengths N=64,
achieving lower multiplication counts than previously published FFTs. A
detailed description for the cases m=1 and m=2 is presented.



The Z Transform over Finite Fields

Finite field transforms have many applications and, in many cases, can be
implemented with a low computational complexity. In this paper, the Z Transform
over a finite field is introduced and some of its properties are presented.



A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation Recording

This paper presents a new approach for a vocoder design based on full
frequency masking by octaves in addition to a technique for spectral filling
via beta probability distribution. Some psycho-acoustic characteristics of
human hearing - inaudibility masking in frequency and phase - are used as a
basis for the proposed algorithm. The results confirm that this technique may
be useful to save bandwidth in applications requiring intelligibility. It is
recommended for the legal eavesdropping of long voice conversations.



A Flexible Implementation of a Matrix Laurent Series-Based 16-Point Fast Fourier and Hartley Transforms

This paper describes a flexible architecture for implementing a new fast
computation of the discrete Fourier and Hartley transforms, which is based on a
matrix Laurent series. The device calculates the transforms based on a single
bit selection operator. The hardware structure and synthesis are presented,
which handled a 16-point fast transform in 65 nsec, with a Xilinx SPARTAN 3E
device.



New Algorithms for Computing a Single Component of the Discrete Fourier Transform

This paper introduces the theory and hardware implementation of two new
algorithms for computing a single component of the discrete Fourier transform.
In terms of multiplicative complexity, both algorithms are more efficient, in
general, than the well known Goertzel Algorithm.



Video Inpainting of Complex Scenes

We propose an automatic video inpainting algorithm which relies on the
optimisation of a global, patch-based functional. Our algorithm is able to deal
with a variety of challenging situations which naturally arise in video
inpainting, such as the correct reconstruction of dynamic textures, multiple
moving objects and moving background. Furthermore, we achieve this in an order
of magnitude less execution time with respect to the state-of-the-art. We are
also able to achieve good quality results on high definition videos. Finally,
we provide specific algorithmic details to make implementation of our algorithm
as easy as possible. The resulting algorithm requires no segmentation or manual
input other than the definition of the inpainting mask, and can deal with a
wider variety of situations than is handled by previous work. 1. Introduction.
Advanced image and video editing techniques are increasingly common in the
image processing and computer vision world, and are also starting to be used in
media entertainment. One common and difficult task closely linked to the world
of video editing is image and video " inpainting ". Generally speaking, this is
the task of replacing the content of an image or video with some other content
which is visually pleasing. This subject has been extensively studied in the
case of images, to such an extent that commercial image inpainting products
destined for the general public are available, such as Photoshop's " Content
Aware fill " [1]. However, while some impressive results have been obtained in
the case of videos, the subject has been studied far less extensively than
image inpainting. This relative lack of research can largely be attributed to
high time complexity due to the added temporal dimension. Indeed, it has only
very recently become possible to produce good quality inpainting results on
high definition videos, and this only in a semi-automatic manner. Nevertheless,
high-quality video inpainting has many important and useful applications such
as film restoration, professional post-production in cinema and video editing
for personal use. For this reason, we believe that an automatic, generic video
inpainting algorithm would be extremely useful for both academic and
professional communities.



Multiscale edge detection and parametric shape modeling for boundary delineation in optoacoustic images

In this article, we present a novel scheme for segmenting the image boundary
(with the background) in optoacoustic small animal in vivo imaging systems. The
method utilizes a multiscale edge detection algorithm to generate a binary edge
map. A scale dependent morphological operation is employed to clean spurious
edges. Thereafter, an ellipse is fitted to the edge map through constrained
parametric transformations and iterative goodness of fit calculations. The
method delimits the tissue edges through the curve fitting model, which has
shown high levels of accuracy. Thus, this method enables segmentation of
optoacoutic images with minimal human intervention, by eliminating need of
scale selection for multiscale processing and seed point determination for
contour mapping.



Low PMEPR OFDM radar waveform design using the iterative least squares algorithm

This letter considers waveform design of orthogonal frequency division
multiplexing (OFDM) signal for radar applications, and aims at mitigating the
envelope fluctuation in OFDM. A novel method is proposed to reduce the
peak-to-mean envelope power ratio (PMEPR), which is commonly used to evaluate
the fluctuation. The proposed method is based on the tone reservation approach,
in which some bits or subcarriers of OFDM are allocated for decreasing PMEPR.
We introduce the coefficient of variation of envelopes (CVE) as the cost
function for waveform optimization, and develop an iterative least squares
algorithm. Minimizing CVE leads to distinct PMEPR reduction, and it is
guaranteed that the cost function monotonically decreases by applying the
iterative algorithm. Simulations demonstrate that the envelope is significantly
smoothed by the proposed method.



An Iterative Receiver for OFDM With Sparsity-Based Parametric Channel Estimation

In this work we design a receiver that iteratively passes soft information
between the channel estimation and data decoding stages. The receiver
incorporates sparsity-based parametric channel estimation. State-of-the-art
sparsity-based iterative receivers simplify the channel estimation problem by
restricting the multipath delays to a grid. Our receiver does not impose such a
restriction. As a result it does not suffer from the leakage effect, which
destroys sparsity. Communication at near capacity rates in high SNR requires a
large modulation order. Due to the close proximity of modulation symbols in
such systems, the grid-based approximation is of insufficient accuracy. We show
numerically that a state-of-the-art iterative receiver with grid-based sparse
channel estimation exhibits a bit-error-rate floor in the high SNR regime. On
the contrary, our receiver performs very close to the perfect channel state
information bound for all SNR values. We also demonstrate both theoretically
and numerically that parametric channel estimation works well in dense
channels, i.e., when the number of multipath components is large and each
individual component cannot be resolved.



A Pessimistic Approximation for the Fisher Information Measure

The problem of determining the intrinsic quality of a signal processing
system with respect to the inference of an unknown deterministic parameter
$\theta$ is considered. While the Fisher information measure $F(\theta)$ forms
a classical tool for such a problem, direct computation of the information
measure can become difficult in various situations. For the estimation
theoretic performance analysis of nonlinear measurement systems, the form of
the likelihood function can make the calculation of the information measure
$F(\theta)$ challenging. In situations where no closed-form expression of the
statistical system model is available, the analytical derivation of $F(\theta)$
is not possible at all. Based on the Cauchy-Schwarz inequality, we derive an
alternative information measure $S(\theta)$. It provides a lower bound on the
Fisher information $F(\theta)$ and has the property of being evaluated with the
mean, the variance, the skewness and the kurtosis of the system model at hand.
These entities usually exhibit good mathematical tractability or can be
determined at low-complexity by real-world measurements in a calibrated setup.
With various examples, we show that $S(\theta)$ provides a good conservative
approximation for $F(\theta)$ and outline different estimation theoretic
problems where the presented information bound turns out to be useful.



Visual Quality Enhancement in Optoacoustic Tomography using Active Contour Segmentation Priors

Segmentation of biomedical images is essential for studying and
characterizing anatomical structures, detection and evaluation of pathological
tissues. Segmentation has been further shown to enhance the reconstruction
performance in many tomographic imaging modalities by accounting for
heterogeneities of the excitation field and tissue properties in the imaged
region. This is particularly relevant in optoacoustic tomography, where
discontinuities in the optical and acoustic tissue properties, if not properly
accounted for, may result in deterioration of the imaging performance.
Efficient segmentation of optoacoustic images is often hampered by the
relatively low intrinsic contrast of large anatomical structures, which is
further impaired by the limited angular coverage of some commonly employed
tomographic imaging configurations. Herein, we analyze the performance of
active contour models for boundary segmentation in cross-sectional optoacoustic
tomography. The segmented mask is employed to construct a two compartment model
for the acoustic and optical parameters of the imaged tissues, which is
subsequently used to improve accuracy of the image reconstruction routines. The
performance of the suggested segmentation and modeling approach are showcased
in tissue-mimicking phantoms and small animal imaging experiments.



Highway Long Short-Term Memory RNNs for Distant Speech Recognition

In this paper, we extend the deep long short-term memory (DLSTM) recurrent
neural networks by introducing gated direct connections between memory cells in
adjacent layers. These direct links, called highway connections, enable
unimpeded information flow across different layers and thus alleviate the
gradient vanishing problem when building deeper LSTMs. We further introduce the
latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole
history while keeping the latency under control. Efficient algorithms are
proposed to train these novel networks using both frame and sequence
discriminative criteria. Experiments on the AMI distant speech recognition
(DSR) task indicate that we can train deeper LSTMs and achieve better
improvement from sequence training with highway LSTMs (HLSTMs). Our novel model
obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all
previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and
$5.3\%$ relative improvement respectively.



Prediction-Adaptation-Correction Recurrent Neural Networks for Low-Resource Language Speech Recognition

In this paper, we investigate the use of prediction-adaptation-correction
recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A
PAC-RNN is comprised of a pair of neural networks in which a {\it correction}
network uses auxiliary information given by a {\it prediction} network to help
estimate the state probability. The information from the correction network is
also used by the prediction network in a recurrent loop. Our model outperforms
other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.
Moreover, transfer learning from a language that is similar to the target
language can help improve performance further.



On Computational Complexity Reduction Methods for Kalman Filter Extensions

The Kalman filter and its extensions are used in a vast number of aerospace
and navigation applications for nonlinear state estimation of time series. In
the literature, different approaches have been proposed to exploit the
structure of the state and measurement models to reduce the computational
demand of the algorithms. In this tutorial, we survey existing code
optimization methods and present them using unified notation that allows them
to be used with various Kalman filter extensions. We develop the optimization
methods to cover a wider range of models, show how different structural
optimizations can be combined, and present new applications for the existing
optimizations. Furthermore, we present an example that shows that the
exploitation of the structure of the problem can lead to improved estimation
accuracy while reducing the computational load. This tutorial is intended for
persons who are familiar with Kalman filtering and want to get insights for
reducing the computational demand of different Kalman filter extensions.



Sensitivity Analysis for Binary Sampling Systems via Quantitative Fisher Information Lower Bounds

This article addresses the sensitivity of sensor systems with minimal signal
digitization complexity regarding the estimation of analog model parameters.
Digital measurements are exclusively available in a hard-limited form, and the
parameters of the analog received signals shall be inferred through efficient
algorithms. As a benchmark, the achievable estimation accuracy is to be
assessed based on theoretical error bounds. To this end, characterization of
the parametric likelihood is required, which forms a challenge for multivariate
binary distributions. In this context, we analyze the Fisher information matrix
of the exponential family and derive a conservative approximation for arbitrary
models. The conservative information matrix rests on a surrogate exponential
family, defined by two equivalences to the real data-generating system. This
probabilistic notion enables designing estimators that consistently achieve the
sensitivity level defined by the inverse of the conservative information matrix
without characterizing the distributions involved. For parameter estimation
with multivariate binary samples, using an equivalent quadratic exponential
distribution tames the computational complexity of the conservative information
matrix such that a quantitative assessment of the achievable error level
becomes tractable. We exploit this for the performance analysis concerning
signal parameter estimation with an array of low-complexity binary sensors by
examining the achievable sensitivity in comparison to an ideal system featuring
receivers supporting data acquisition with infinite amplitude resolution.
Additionally, we demonstrate data-driven sensitivity analysis through the
presented framework by learning the guaranteed achievable performance when
processing sensor data obtained with recursive binary sampling schemes as
implemented in $\Sigma\Delta$-modulating analog-to-digital converters.



Plug-and-Play Priors for Bright Field Electron Tomography and Sparse Interpolation

Many material and biological samples in scientific imaging are characterized
by non-local repeating structures. These are studied using scanning electron
microscopy and electron tomography. Sparse sampling of individual pixels in a
2D image acquisition geometry, or sparse sampling of projection images with
large tilt increments in a tomography experiment, can enable high speed data
acquisition and minimize sample damage caused by the electron beam.
  In this paper, we present an algorithm for electron tomographic
reconstruction and sparse image interpolation that exploits the non-local
redundancy in images. We adapt a framework, termed plug-and-play (P&P) priors,
to solve these imaging problems in a regularized inversion setting. The power
of the P&P approach is that it allows a wide array of modern denoising
algorithms to be used as a "prior model" for tomography and image
interpolation. We also present sufficient mathematical conditions that ensure
convergence of the P&P approach, and we use these insights to design a new
non-local means denoising algorithm. Finally, we demonstrate that the algorithm
produces higher quality reconstructions on both simulated and real electron
microscope data, along with improved convergence properties compared to other
methods.



Privacy, Secrecy, and Storage with Multiple Noisy Measurements of Identifiers

The key-leakage-storage region is derived for a generalization of a classic
two-terminal key agreement model. The additions to the model are that the
encoder observes a hidden, or noisy, version of the identifier, and that the
encoder and decoder can perform multiple measurements. To illustrate the
behavior of the region, the theory is applied to binary identifiers and noise
modeled via binary symmetric channels. In particular, the key-leakage-storage
region is simplified by applying Mrs. Gerber's lemma twice in different
directions to a Markov chain. The growth in the region as the number of
measurements increases is quantified. The amount by which the privacy-leakage
rate reduces for a hidden identifier as compared to a noise-free (visible)
identifier at the encoder is also given. If the encoder incorrectly models the
source as visible, it is shown that substantial secrecy leakage may occur and
the reliability of the reconstructed key might decrease.



Analysis of Random Pulse Repetition Interval Radar

Random pulse repetition interval (PRI) waveform arouses great interests in
the field of modern radars due to its ability to alleviate range and Doppler
ambiguities as well as enhance electronic counter-countermeasures (ECCM)
capabilities. Theoretical results pertaining to the statistical characteristics
of ambiguity function (AF) are derived in this work, indicating that the range
and Doppler ambiguities can be effectively suppressed by increasing the number
of pulses and the range of PRI jitters. This provides an important guidance in
terms of waveform design. As is well known, the significantly lifted sidelobe
pedestal induced by PRI randomization will degrade the performance of weak
target detection. Proceeding from that, we propose to employ orthogonal
matching pursuit (OMP) to overcome this issue. Simulation results demonstrate
that the OMP method can effectively lower the sidelobe pedestal of strong
target and improve the performance of weak target estimation.



Voltage stress minimization by optimal reactive power control

A standard operational requirement in power systems is that the voltage
magnitudes lie within prespecified bounds. Conventional engineering wisdom
suggests that such a tightly-regulated profile, imposed for system design
purposes and good operation of the network, should also guarantee a secure
system, operating far from static bifurcation instabilities such as voltage
collapse. In general however, these two objectives are distinct and must be
separately enforced. We formulate an optimization problem which maximizes the
distance to voltage collapse through injections of reactive power, subject to
power flow and operational voltage constraints. By exploiting a linear
approximation of the power flow equations we arrive at a convex reformulation
which can be efficiently solved for the optimal injections. We also address the
planning problem of allocating the resources by recasting our problem in a
sparsity-promoting framework that allows us to choose a desired trade-off
between optimality of injections and the number of required actuators. Finally,
we present a distributed algorithm to solve the optimization problem, showing
that it can be implemented on-line as a feedback controller. We illustrate the
performance of our results with the IEEE30 bus network.



Grading of Mammalian Cumulus Oocyte Complexes using Machine Learning for in Vitro Embryo Culture

Visual observation of Cumulus Oocyte Complexes provides only limited
information about its functional competence, whereas the molecular evaluations
methods are cumbersome or costly. Image analysis of mammalian oocytes can
provide attractive alternative to address this challenge. However, it is
complex, given the huge number of oocytes under inspection and the subjective
nature of the features inspected for identification. Supervised machine
learning methods like random forest with annotations from expert biologists can
make the analysis task standardized and reduces inter-subject variability. We
present a semi-automatic framework for predicting the class an oocyte belongs
to, based on multi-object parametric segmentation on the acquired microscopic
image followed by a feature based classification using random forests.



Subsampling for Graph Power Spectrum Estimation

In this paper we focus on subsampling stationary random processes that reside
on the vertices of undirected graphs. Second-order stationary graph signals are
obtained by filtering white noise and they admit a well-defined power spectrum.
Estimating the graph power spectrum forms a central component of stationary
graph signal processing and related inference tasks. We show that by sampling a
significantly smaller subset of vertices and using simple least squares, we can
reconstruct the power spectrum of the graph signal from the subsampled
observations, without any spectral priors. In addition, a near-optimal greedy
algorithm is developed to design the subsampling scheme.



Extended Object Tracking: Introduction, Overview and Applications

This article provides an elaborate overview of current research in extended
object tracking. We provide a clear definition of the extended object tracking
problem and discuss its delimitation to other types of object tracking. Next,
different aspects of extended object modelling are extensively discussed.
Subsequently, we give a tutorial introduction to two basic and well used
extended object tracking approaches - the random matrix approach and the Kalman
filter-based approach for star-convex shapes. The next part treats the tracking
of multiple extended objects and elaborates how the large number of feasible
association hypotheses can be tackled using both Random Finite Set (RFS) and
Non-RFS multi-object trackers. The article concludes with a summary of current
applications, where four example applications involving camera, X-band radar,
light detection and ranging (lidar), red-green-blue-depth (RGB-D) sensors are
highlighted.



On the Performance of Mismatched Data Detection in Large MIMO Systems

We investigate the performance of mismatched data detection in large
multiple-input multiple-output (MIMO) systems, where the prior distribution of
the transmit signal used in the data detector differs from the true prior. To
minimize the performance loss caused by this prior mismatch, we include a
tuning stage into our recently-proposed large MIMO approximate message passing
(LAMA) algorithm, which allows us to develop mismatched LAMA algorithms with
optimal as well as sub-optimal tuning. We show that carefully-selected priors
often enable simpler and computationally more efficient algorithms compared to
LAMA with the true prior while achieving near-optimal performance. A
performance analysis of our algorithms for a Gaussian prior and a uniform prior
within a hypercube covering the QAM constellation recovers classical and recent
results on linear and non-linear MIMO data detection, respectively.



Optimal Number of Choices in Rating Contexts

In many settings people must give numerical scores to entities from a small
discrete set. For instance, rating physical attractiveness from 1--5 on dating
sites, or papers from 1--10 for conference reviewing. We study the problem of
understanding when using a different number of options is optimal. We consider
the case when scores are uniform random and Gaussian. We study computationally
when using 2, 3, 4, 5, and 10 options out of a total of 100 is optimal in these
models (though our theoretical analysis is for a more general setting with $k$
choices from $n$ total options as well as a continuous underlying space). One
may expect that using more options would always improve performance in this
model, but we show that this is not necessarily the case, and that using fewer
choices---even just two---can surprisingly be optimal in certain situations.
While in theory for this setting it would be optimal to use all 100 options, in
practice this is prohibitive, and it is preferable to utilize a smaller number
of options due to humans' limited computational resources. Our results could
have many potential applications, as settings requiring entities to be ranked
by humans are ubiquitous. There could also be applications to other fields such
as signal or image processing where input values from a large set must be
mapped to output values in a smaller set.



Using instantaneous frequency and aperiodicity detection to estimate F0 for high-quality speech synthesis

This paper introduces a general and flexible framework for F0 and
aperiodicity (additive non periodic component) analysis, specifically intended
for high-quality speech synthesis and modification applications. The proposed
framework consists of three subsystems: instantaneous frequency estimator and
initial aperiodicity detector, F0 trajectory tracker, and F0 refinement and
aperiodicity extractor. A preliminary implementation of the proposed framework
substantially outperformed (by a factor of 10 in terms of RMS F0 estimation
error) existing F0 extractors in tracking ability of temporally varying F0
trajectories. The front end aperiodicity detector consists of a complex-valued
wavelet analysis filter with a highly selective temporal and spectral envelope.
This front end aperiodicity detector uses a new measure that quantifies the
deviation from periodicity. The measure is less sensitive to slow FM and AM and
closely correlates with the signal to noise ratio.



Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation

We propose a novel deep learning model, which supports permutation invariant
training (PIT), for speaker independent multi-talker speech separation,
commonly known as the cocktail-party problem. Different from most of the prior
arts that treat speech separation as a multi-class regression problem and the
deep clustering technique that considers it a segmentation (or clustering)
problem, our model optimizes for the separation regression error, ignoring the
order of mixing sources. This strategy cleverly solves the long-lasting label
permutation problem that has prevented progress on deep learning based
techniques for speech separation. Experiments on the equal-energy mixing setup
of a Danish corpus confirms the effectiveness of PIT. We believe improvements
built upon PIT can eventually solve the cocktail-party problem and enable
real-world adoption of, e.g., automatic meeting transcription and multi-party
human-computer interaction, where overlapping speech is common.



How Much Do Downlink Pilots Improve Cell-Free Massive MIMO?

In this paper, we analyze the benefits of including downlink pilots in a
cell-free massive MIMO system. We derive an approximate per-user achievable
downlink rate for conjugate beamforming processing, which takes into account
both uplink and downlink channel estimation errors, and power control. A
performance comparison is carried out, in terms of per-user net throughput,
considering cell-free massive MIMO operation with and without downlink
training, for different network densities. We take also into account the
performance improvement provided by max-min fairness power control in the
downlink. Numerical results show that, exploiting downlink pilots, the
performance can be considerably improved in low density networks over the
conventional scheme where the users rely on statistical channel knowledge only.
In high density networks, performance improvements are moderate.



On the Performance of Cell-Free Massive MIMO with Short-Term Power Constraints

In this paper we consider a time-division duplex cell-free massive
multiple-input multiple-output (MIMO) system where many distributed access
points (APs) simultaneously serve many users. A normalized conjugate
beamforming scheme, which satisfies short-term average power constraints at the
APs, is proposed and analyzed taking into account the effect of imperfect
channel information. We derive an approximate closed-form expression for the
per-user achievable downlink rate of this scheme. We also provide, analytically
and numerically, a performance comparison between the normalized conjugate
beamforming and the conventional conjugate beamforming scheme in [1] (which
satisfies long-term average power constraints). Normalized conjugate
beamforming scheme reduces the beamforming uncertainty gain, which comes from
the users' lack of the channel state information knowledge, and hence, it
improves the achievable downlink rate compared to the conventional conjugate
beamforming scheme.



Learning Sparse Graphs Under Smoothness Prior

In this paper, we are interested in learning the underlying graph structure
behind training data. Solving this basic problem is essential to carry out any
graph signal processing or machine learning task. To realize this, we assume
that the data is smooth with respect to the graph topology, and we parameterize
the graph topology using an edge sampling function. That is, the graph
Laplacian is expressed in terms of a sparse edge selection vector, which
provides an explicit handle to control the sparsity level of the graph. We
solve the sparse graph learning problem given some training data in both the
noiseless and noisy settings. Given the true smooth data, the posed sparse
graph learning problem can be solved optimally and is based on simple rank
ordering. Given the noisy data, we show that the joint sparse graph learning
and denoising problem can be simplified to designing only the sparse edge
selection vector, which can be solved using convex optimization.



The Microsoft 2016 Conversational Speech Recognition System

We describe Microsoft's conversational speech recognition system, in which we
combine recent developments in neural-network-based acoustic and language
modeling to advance the state of the art on the Switchboard recognition task.
Inspired by machine learning ensemble techniques, the system uses a range of
convolutional and recurrent neural networks. I-vector modeling and lattice-free
MMI training provide significant gains for all acoustic model architectures.
Language model rescoring with multiple forward and backward running RNNLMs, and
word posterior-based system combination provide a 20% boost. The best single
system uses a ResNet architecture acoustic model with RNNLM rescoring, and
achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The
combined system has an error rate of 6.2%, representing an improvement over
previously reported results on this benchmark task.



Complex Laplacian based Distributed Control for Multi-Agent Network

The work done in this paper, proposes a complex Laplacian-based distributed
control scheme for convergence in the multi-agent network. The proposed scheme
has been designated as cascade formulation. The proposed technique exploits the
traditional method of organizing large scattered networks into smaller
interconnected clusters to optimize information flow within the network. The
complex Laplacian-based approach results in a hierarchical structure, with
formation of a meta-cluster leading other clusters in the network. The proposed
formulation enables flexibility to constrain the eigen spectra of the overall
closed-loop dynamics, ensuring desired convergence rate and control input
intensity. The sufficient conditions ensuring globally stable formation for
proposed formulation are also asserted. Robustness of the proposed formulation
to uncertainties like loss in communication links and actuator failure has also
been discussed. The effectiveness of the proposed approach is illustrated by
simulating a finitely large network of thirty vehicles.



Macroscopic Modeling, Calibration, and Simulation of Managed Lane-Freeway Networks, Part I: Topological and Phenomenological Modeling

To help mitigate road congestion caused by the unrelenting growth of traffic
demand, many transit authorities have implemented managed lane policies.
Managed lanes typically run parallel to a freeway's standard, general-purpose
(GP) lanes, but are restricted to certain types of vehicles. It was originally
thought that managed lanes would improve the use of existing infrastructure
through incentivization of demand-management behaviors like carpooling, but
implementations have often been characterized by unpredicted phenomena that is
often to detrimental system performance.
  This paper presents several macroscopic traffic modeling tools we have used
for study of freeways equipped with managed lanes, or "managed lane-freeway
networks." The proposed framework is based on the widely-used first-order
kinematic wave theory. In this model, the GP and the managed lanes are modeled
as parallel links connected by nodes, where certain type of traffic may switch
between GP and managed lane links. Two types of managed lane topologies are
considered: full-access, where vehicles can switch between the GP and the
managed lanes anywhere; and separated, where such switching is allowed only at
certain locations called gates.
  We also describe methods to incorporate in three phenomena into our model
that are particular to managed lane-freeway networks. The inertia effect
reflects drivers' inclination to stay in their lane as long as possible and
switch only if this would obviously improve their travel condition. The
friction effect reflects the empirically-observed driver fear of moving fast in
a managed lane while traffic in the adjacent GP lanes moves slowly due to
congestion. The smoothing effect describes how managed lanes can increase
throughput at bottlenecks by reducing lane changes. We present simple models
for each of these phenomena that fit within the general macroscopic theory.



Approximate Gram-Matrix Interpolation for Wideband Massive MU-MIMO Systems

Numerous linear and non-linear data-detection and precoding algorithms for
wideband massive multi-user (MU) multiple-input multiple-output (MIMO) wireless
systems that rely on orthogonal frequency-division multiplexing (OFDM) or
single-carrier frequency-division multiple access (SC-FDMA) require the
computation of the Gram matrix for each active subcarrier. Computing the Gram
matrix for each active subcarrier, however, results in excessively high
computational complexity. In this paper, we propose novel, approximate
algorithms that significantly reduce the complexity of Gram-matrix computation
by simultaneously exploiting correlation across subcarriers and channel
hardening. We show analytically that a small fraction of Gram-matrix
computations in combination with approximate interpolation schemes are
sufficient to achieve near-optimal error-rate performance at low computational
complexity in massive MU-MIMO systems. We also demonstrate that the proposed
methods exhibit improved robustness against channel-estimation errors compared
to exact Gram-matrix interpolation algorithms that typically require high
computational complexity.



Covert Single-hop Communication in a Wireless Network with Distributed Artificial Noise Generation

Covert communication, also known as low probability of detection (LPD)
communication, prevents the adversary from knowing that a communication is
taking place. Recent work has demonstrated that, in a three-party scenario with
a transmitter (Alice), intended recipient (Bob), and adversary (Warden Willie),
the maximum number of bits that can be transmitted reliably from Alice to Bob
without detection by Willie, when additive white Gaussian noise (AWGN) channels
exist between all parties, is on the order of the square root of the number of
channel uses. In this paper, we begin consideration of network scenarios by
studying the case where there are additional "friendly" nodes present in the
environment that can produce artificial noise to aid in hiding the
communication. We establish achievability results by considering constructions
where the system node closest to the warden produces artificial noise and
demonstrate a significant improvement in the throughput achieved covertly,
without requiring close coordination between Alice and the noise-generating
node. Conversely, under mild restrictions on the communication strategy, we
demonstrate no higher covert throughput is possible. Extensions to the
consideration of the achievable covert throughput when multiple wardens
randomly located in the environment collaborate to attempt detection of the
transmitter are also considered.



A System Level Approach to Controller Synthesis

Biological and advanced cyberphysical control systems often have limited,
sparse, uncertain, and distributed communication and computing in addition to
sensing and actuation. Fortunately, the corresponding plants and performance
requirements are also sparse and structured, and this must be exploited to make
constrained controller design feasible and tractable. We introduce a new
"system level" (SL) approach involving three complementary SL elements. System
Level Parameterizations (SLPs) generalize state space and Youla
parameterizations of all stabilizing controllers and the responses they
achieve, and combine with System Level Constraints (SLCs) to parameterize the
largest known class of constrained stabilizing controllers that admit a convex
characterization, generalizing quadratic invariance (QI). SLPs also lead to a
generalization of detectability and stabilizability, suggesting the existence
of a rich separation structure, that when combined with SLCs, is naturally
applicable to structurally constrained controllers and systems. We further
provide a catalog of useful SLCs, most importantly including sparsity, delay,
and locality constraints on both communication and computing internal to the
controller, and external system performance. The resulting System Level
Synthesis (SLS) problems that arise define the broadest known class of
constrained optimal control problems that can be solved using convex
programming. An example illustrates how this system level approach can
systematically explore tradeoffs in controller performance, robustness, and
synthesis/implementation complexity.



Achieving Human Parity in Conversational Speech Recognition

Conversational speech recognition has served as a flagship speech recognition
task since the release of the Switchboard corpus in the 1990s. In this paper,
we measure the human error rate on the widely used NIST 2000 test set, and find
that our latest automated system has reached human parity. The error rate of
professional transcribers is 5.9% for the Switchboard portion of the data, in
which newly acquainted pairs of people discuss an assigned topic, and 11.3% for
the CallHome portion where friends and family members have open-ended
conversations. In both cases, our automated system establishes a new state of
the art, and edges past the human benchmark, achieving error rates of 5.8% and
11.0%, respectively. The key to our system's performance is the use of various
convolutional and LSTM acoustic model architectures, combined with a novel
spatial smoothing method and lattice-free MMI acoustic training, multiple
recurrent neural network language modeling approaches, and a systematic use of
system combination.



Approximate eigenvalue distribution of a cylindrically isotropic noise sample covariance matrix

The statistical behavior of the eigenvalues of the sample covariance matrix
(SCM) plays a key role in determining the performance of adaptive beamformers
(ABF) in presence of noise. This paper presents a method to compute the
approximate eigenvalue density function (EDF) for the SCM of a \cin{} field
when only a finite number of shapshots are available. The EDF of the ensemble
covariance matrix (ECM) is modeled as an atomic density with many fewer atoms
than the SCM size. The model results in substantial computational savings over
more direct methods of computing the EDF. The approximate EDF obtained from
this method agrees closely with histograms of eigenvalues obtained from
simulation.



Unit circle MVDR beamformer

The array polynomial is the z-transform of the array weights for a narrowband
planewave beamformer using a uniform linear array (ULA). Evaluating the array
polynomial on the unit circle in the complex plane yields the beampattern. The
locations of the polynomial zeros on the unit circle indicate the nulls of the
beampattern. For planewave signals measured with a ULA, the locations of the
ensemble MVDR polynomial zeros are constrained on the unit circle. However,
sample matrix inversion (SMI) MVDR polynomial zeros generally do not fall on
the unit circle. The proposed unit circle MVDR (UC MVDR) projects the zeros of
the SMI MVDR polynomial radially on the unit circle. This satisfies the
constraint on the zeros of ensemble MVDR polynomial. Numerical simulations show
that the UC MVDR beamformer suppresses interferers better than the SMI MVDR and
the diagonal loaded MVDR beamformer and also improves the white noise gain
(WNG).



Computationally Efficient Target Classification in Multispectral Image Data with Deep Neural Networks

Detecting and classifying targets in video streams from surveillance cameras
is a cumbersome, error-prone and expensive task. Often, the incurred costs are
prohibitive for real-time monitoring. This leads to data being stored locally
or transmitted to a central storage site for post-incident examination. The
required communication links and archiving of the video data are still
expensive and this setup excludes preemptive actions to respond to imminent
threats. An effective way to overcome these limitations is to build a smart
camera that transmits alerts when relevant video sequences are detected. Deep
neural networks (DNNs) have come to outperform humans in visual classifications
tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be
extended to make use of higher-dimensional input data such as multispectral
data. We explore this opportunity in terms of achievable accuracy and required
computational effort. To analyze the precision of DNNs for scene labeling in an
urban surveillance scenario we have created a dataset with 8 classes obtained
in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR
snapshot sensor to assess the potential of multispectral image data for target
classification. We evaluate several new DNNs, showing that the spectral
information fused together with the RGB frames can be used to improve the
accuracy of the system or to achieve similar accuracy with a 3x smaller
computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even
for scarcely occurring, but particularly interesting classes, such as cars, 75%
of the pixels are labeled correctly with errors occurring only around the
border of the objects. This high accuracy was obtained with a training set of
only 30 labeled images, paving the way for fast adaptation to various
application scenarios.



Stall Pattern Avoidance in Polynomial Product Codes

Product codes are a concatenated error-correction scheme that has been often
considered for applications requiring very low bit-error rates, which demand
that the error floor be decreased as much as possible. In this work, we
consider product codes constructed from polynomial algebraic codes, and propose
a novel low-complexity post-processing technique that is able to improve the
error-correction performance by orders of magnitude. We provide lower bounds
for the error rate achievable under post processing, and present simulation
results indicating that these bounds are tight.



Microseismic events enhancement and detection in sensor arrays using autocorrelation based filtering

Passive microseismic data are commonly buried in noise, which presents a
significant challenge for signal detection and recovery. For recordings from a
surface sensor array where each trace contains a time-delayed arrival from the
event, we propose an autocorrelation-based stacking method that designs a
denoising filter from all the traces, as well as a multi-channel detection
scheme. This approach circumvents the issue of time aligning the traces prior
to stacking because every trace's autocorrelation is centered at zero in the
lag domain. The effect of white noise is concentrated near zero lag, so the
filter design requires a predictable adjustment of the zero-lag value.
Truncation of the autocorrelation is employed to smooth the impulse response of
the denoising filter. In order to extend the applicability of the algorithm, we
also propose a noise prewhitening scheme that addresses cases with colored
noise. The simplicity and robustness of this method are validated with
synthetic and real seismic traces.



Kernel-based Reconstruction of Space-time Functions on Dynamic Graphs

Graph-based methods pervade the inference toolkits of numerous disciplines
including sociology, biology, neuroscience, physics, chemistry, and
engineering. A challenging problem encountered in this context pertains to
determining the attributes of a set of vertices given those of another subset
at possibly different time instants. Leveraging spatiotemporal dynamics can
drastically reduce the number of observed vertices, and hence the cost of
sampling. Alleviating the limited flexibility of existing approaches, the
present paper broadens the existing kernel-based graph function reconstruction
framework to accommodate time-evolving functions over possibly time-evolving
topologies. This approach inherits the versatility and generality of
kernel-based methods, for which no knowledge on distributions or second-order
statistics is required. Systematic guidelines are provided to construct two
families of space-time kernels with complementary strengths. The first
facilitates judicious control of regularization on a space-time frequency
plane, whereas the second can afford time-varying topologies. Batch and online
estimators are also put forth, and a novel kernel Kalman filter is developed to
obtain these estimates at affordable computational cost. Numerical tests with
real data sets corroborate the merits of the proposed methods relative to
competing alternatives.



Image biomarker standardisation initiative

The image biomarker standardisation initiative (IBSI) is an independent
international collaboration which works towards standardising the extraction of
image biomarkers from acquired imaging for the purpose of high-throughput
quantitative image analysis (radiomics). Lack of reproducibility and validation
of high-throughput quantitative image analysis studies is considered to be a
major challenge for the field. Part of this challenge lies in the scantiness of
consensus-based guidelines and definitions for the process of translating
acquired imaging into high-throughput image biomarkers. The IBSI therefore
seeks to provide image biomarker nomenclature and definitions, benchmark data
sets, and benchmark values to verify image processing and image biomarker
calculations, as well as reporting guidelines, for high-throughput image
analysis.



A new cosine series antialiasing function and its application to aliasing-free glottal source models for speech and singing synthesis

We formulated and implemented a procedure to generate aliasing-free
excitation source signals. It uses a new antialiasing filter in the continuous
time domain followed by an IIR digital filter for response equalization. We
introduced a cosine-series-based general design procedure for the new
antialiasing function. We applied this new procedure to implement the
antialiased Fujisaki-Ljungqvist model. We also applied it to revise our
previous implementation of the antialiased Fant-Liljencrants model. A
combination of these signals and a lattice implementation of the time varying
vocal tract model provides a reliable and flexible basis to test fo extractors
and source aperiodicity analysis methods. MATLAB implementations of these
antialiased excitation source models are available as part of our open source
tools for speech science.



Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling

Deep learning models (DLMs) are state-of-the-art techniques in speech
recognition. However, training good DLMs can be time consuming especially for
production-size models and corpora. Although several parallel training
algorithms have been proposed to improve training efficiency, there is no clear
guidance on which one to choose for the task in hand due to lack of systematic
and fair comparison among them. In this paper we aim at filling this gap by
comparing four popular parallel training algorithms in speech recognition,
namely asynchronous stochastic gradient descent (ASGD), blockwise model-update
filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging
stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using
feed-forward deep neural networks (DNNs) and convolutional, long short-term
memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the
top choice to train acoustic models since it is most stable, scales well with
number of GPUs, can achieve reproducible results, and in many cases even
outperforms single-GPU SGD. ASGD can be used as a substitute in some cases.



Multi-talker Speech Separation with Utterance-level Permutation Invariant Training of Deep Recurrent Neural Networks

In this paper we propose the utterance-level Permutation Invariant Training
(uPIT) technique. uPIT is a practically applicable, end-to-end, deep learning
based solution for speaker independent multi-talker speech separation.
Specifically, uPIT extends the recently proposed Permutation Invariant Training
(PIT) technique with an utterance-level cost function, hence eliminating the
need for solving an additional permutation problem during inference, which is
otherwise required by frame-level PIT. We achieve this using Recurrent Neural
Networks (RNNs) that, during training, minimize the utterance-level separation
error, hence forcing separated frames belonging to the same speaker to be
aligned to the same output stream. In practice, this allows RNNs, trained with
uPIT, to separate multi-talker mixed speech without any prior knowledge of
signal duration, number of speakers, speaker identity or gender. We evaluated
uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks
and found that uPIT outperforms techniques based on Non-negative Matrix
Factorization (NMF) and Computational Auditory Scene Analysis (CASA), and
compares favorably with Deep Clustering (DPCL) and the Deep Attractor Network
(DANet). Furthermore, we found that models trained with uPIT generalize well to
unseen speakers and languages. Finally, we found that a single model, trained
with uPIT, can handle both two-speaker, and three-speaker speech mixtures.



Optimal Precoders for Tracking the AoD and AoA of a mm-Wave Path

In millimeter-wave channels, most of the received energy is carried by a few
paths. Traditional precoders sweep the angle-of-departure (AoD) and
angle-of-arrival (AoA) space with directional precoders to identify directions
with largest power. Such precoders are heuristic and lead to sub-optimal
AoD/AoA estimation. We derive optimal precoders, minimizing the Cram\'{e}r-Rao
bound (CRB) of the AoD/AoA, assuming a fully digital architecture at the
transmitter and spatial filtering of a single path. The precoders are found by
solving a suitable convex optimization problem. We demonstrate that the
accuracy can be improved by at least a factor of two over traditional
precoders, and show that there is an optimal number of distinct precoders
beyond which the CRB does not improve.



Recognizing Multi-talker Speech with Permutation Invariant Training

In this paper, we propose a novel technique for direct recognition of
multiple speech streams given the single channel of mixed speech, without first
separating them. Our technique is based on permutation invariant training (PIT)
for automatic speech recognition (ASR). In PIT-ASR, we compute the average
cross entropy (CE) over all frames in the whole utterance for each possible
output-target assignment, pick the one with the minimum CE, and optimize for
that assignment. PIT-ASR forces all the frames of the same speaker to be
aligned with the same output layer. This strategy elegantly solves the label
permutation problem and speaker tracing problem in one shot. Our experiments on
artificially mixed AMI data showed that the proposed approach is very
promising.



Symbolic Models for Retarded Jump-Diffusion Systems

In this paper, we provide for the first time an automated,
correct-by-construction, controller synthesis scheme for a class of infinite
dimensional stochastic systems, namely, retarded jump-diffusion systems. First,
we construct finite abstractions approximately bisimilar to non-probabilistic
retarded systems corresponding to the original systems having some stability
property, namely, incremental input-to-state stability. Then, we provide a
result on quantifying the distance between output trajectory of the obtained
finite abstraction and that of the original retarded jump-diffusion system in a
probabilistic setting. Using the proposed result, one can refine the control
policy synthesized using finite abstractions to the original systems while
providing guarantee on the probability of satisfaction of high-level
requirements. Moreover, we provide sufficient conditions for the proposed
notion of incremental stability in terms of the existence of incremental
Lyapunov functions which reduce to some matrix inequalities for the linear
systems. Finally, the effectiveness of the proposed results is illustrated by
synthesizing a controller regulating the temperatures in a ten-room building
modelled as a delayed jump-diffusion system.



CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data

Extracting per-frame features using convolutional neural networks for
real-time processing of video data is currently mainly performed on powerful
GPU-accelerated workstations and compute clusters. However, there are many
applications such as smart surveillance cameras that require or would benefit
from on-site processing. To this end, we propose and evaluate a novel algorithm
for change-based evaluation of CNNs for video data recorded with a static
camera setting, exploiting the spatio-temporal sparsity of pixel changes. We
achieve an average speed-up of 8.6x over a cuDNN baseline on a realistic
benchmark with a negligible accuracy loss of less than 0.1% and no retraining
of the network. The resulting energy efficiency is 10x higher than that of
per-frame evaluation and reaches an equivalent of 328 GOp/s/W on the Tegra X1
platform.



ADMM Penalty Parameter Selection by Residual Balancing

Appropriate selection of the penalty parameter is crucial to obtaining good
performance from the Alternating Direction Method of Multipliers (ADMM). While
analytic results for optimal selection of this parameter are very limited,
there is a heuristic method that appears to be relatively successful in a
number of different problems. The contribution of this paper is to demonstrate
that their is a potentially serious flaw in this heuristic approach, and to
propose a modification that at least partially addresses it.



Graph Sampling for Covariance Estimation

In this paper the focus is on subsampling as well as reconstructing the
second-order statistics of signals residing on nodes of arbitrary undirected
graphs. Second-order stationary graph signals may be obtained by graph
filtering zero-mean white noise and they admit a well-defined power spectrum
whose shape is determined by the frequency response of the graph filter.
Estimating the graph power spectrum forms an important component of stationary
graph signal processing and related inference tasks such as Wiener prediction
or inpainting on graphs. The central result of this paper is that by sampling a
significantly smaller subset of vertices and using simple least squares, we can
reconstruct the second-order statistics of the graph signal from the subsampled
observations, and more importantly, without any spectral priors. To this end,
both a nonparametric approach as well as parametric approaches including moving
average and autoregressive models for the graph power spectrum are considered.
The results specialize for undirected circulant graphs in that the graph nodes
leading to the best compression rates are given by the so-called minimal sparse
rulers. A near-optimal greedy algorithm is developed to design the subsampling
scheme for the non-parametric and the moving average models, whereas a
particular subsampling scheme that allows linear estimation for the
autoregressive model is proposed. Numerical experiments on synthetic as well as
real datasets related to climatology and processing handwritten digits are
provided to demonstrate the developed theory.



Audio-based performance evaluation of squash players

In competitive sports it is often very hard to quantify the performance. A
player to score or overtake may depend on only millesimal of seconds or
millimeters. In racquet sports like tennis, table tennis and squash many events
will occur in a short time duration, whose recording and analysis can help
reveal the differences in performance. In this paper we show that it is
possible to architect a framework that utilizes the characteristic sound
patterns to precisely classify the types of and localize the positions of these
events. From these basic information the shot types and the ball speed along
the trajectories can be estimated. Comparing these estimates with the optimal
speed and target the precision of the shot can be defined. The detailed shot
statistics and precision information significantly enriches and improves data
available today. Feeding them back to the players and the coaches facilitates
to describe playing performance objectively and to improve strategy skills. The
framework is implemented, its hardware and software components are installed
and tested in a squash court.



On Bayesian Fisher Information Maximization for Distributed Vector Estimation

We consider the problem of distributed estimation of a Gaussian vector with
linear observation model. Each sensor makes a scalar noisy observation of the
unknown vector, quantizes its observation, maps it to a digitally modulated
symbol, and transmits the symbol over orthogonal power-constrained fading
channels to a fusion center (FC). The FC is tasked with fusing the received
signals from sensors and estimating the unknown vector. We derive the Bayesian
Fisher Information Matrix (FIM) for three types of receivers: (i) coherent
receiver (ii) noncoherent receiver with known channel envelopes (iii)
noncoherent receiver with known channel statistics only. We also derive the
Weiss-Weinstein bound (WWB). We formulate two constrained optimization
problems, namely maximizing trace and log-determinant of Bayesian FIM under
network transmit power constraint, with sensors transmit powers being the
optimization variables. We show that for coherent receiver, these problems are
concave. However, for noncoherent receivers, they are not necessarily concave.
The solution to the trace of Bayesian FIM maximization problem can be
implemented in a distributed fashion. We numerically investigate how the
FIM-max power allocation across sensors depends on the sensors observation
qualities and physical layer parameters as well as the network transmit power
constraint. Moreover, we evaluate the system performance in terms of MSE using
the solutions of FIM-max schemes, and compare it with the solution obtained
from minimizing the MSE of the LMMSE estimator (MSE-min scheme), and that of
uniform power allocation. These comparisons illustrate that, although the WWB
is tighter than the inverse of Bayesian FIM, it is still suitable to use
FIM-max schemes, since the performance loss in terms of the MSE of the LMMSE
estimator is not significant.



Comparison of Uniform and Random Sampling for Speech and Music Signals

In this paper, we will provide a comparison between uniform and random
sampling for speech and music signals. There are various sampling and recovery
methods for audio signals. Here, we only investigate uniform and random schemes
for sampling and basic low-pass filtering and iterative method with adaptive
thresholding for recovery. The simulation results indicate that uniform
sampling with cubic spline interpolation outperforms other sampling and
recovery methods.



Blind Detection of Polar Codes

Polar codes were recently chosen to protect the control channel information
in the next-generation mobile communication standard (5G) defined by the 3GPP.
As a result, receivers will have to implement blind detection of polar coded
frames in order to keep complexity, latency, and power consumption tractable.
As a newly proposed class of block codes, the problem of polar-code blind
detection has received very little attention. In this work, we propose a
low-complexity blind-detection algorithm for polar-encoded frames. We base this
algorithm on a novel detection metric with update rules that leverage the a
priori knowledge of the frozen-bit locations, exploiting the inherent
structures that these locations impose on a polar-encoded block of data. We
show that the proposed detection metric allows to clearly distinguish
polar-encoded frames from other types of data by considering the cumulative
distribution functions of the detection metric, and the receiver operating
characteristic. The presented results are tailored to the 5G standardization
effort discussions, i.e., we consider a short low-rate polar code concatenated
with a CRC.



On the Achievable Rates of Decentralized Equalization in Massive MU-MIMO Systems

Massive multi-user (MU) multiple-input multiple-output (MIMO) promises
significant gains in spectral efficiency compared to traditional, small-scale
MIMO technology. Linear equalization algorithms, such as zero forcing (ZF) or
minimum mean-square error (MMSE)-based methods, typically rely on centralized
processing at the base station (BS), which results in (i) excessively high
interconnect and chip input/output data rates, and (ii) high computational
complexity. In this paper, we investigate the achievable rates of decentralized
equalization that mitigates both of these issues. We consider two distinct BS
architectures that partition the antenna array into clusters, each associated
with independent radio-frequency chains and signal processing hardware, and the
results of each cluster are fused in a feedforward network. For both
architectures, we consider ZF, MMSE, and a novel, non-linear equalization
algorithm that builds upon approximate message passing (AMP), and we
theoretically analyze the achievable rates of these methods. Our results
demonstrate that decentralized equalization with our AMP-based methods incurs
no or only a negligible loss in terms of achievable rates compared to that of
centralized solutions.



Optimally-Tuned Nonparametric Linear Equalization for Massive MU-MIMO Systems

This paper deals with linear equalization in massive multi-user
multiple-input multiple-output (MU-MIMO) wireless systems. We first provide
simple conditions on the antenna configuration for which the well-known linear
minimum mean-square error (L-MMSE) equalizer provides near-optimal spectral
efficiency, and we analyze its performance in the presence of parameter
mismatches in the signal and/or noise powers. We then propose a novel,
optimally-tuned NOnParametric Equalizer (NOPE) for massive MU-MIMO systems,
which avoids knowledge of the transmit signal and noise powers altogether. We
show that NOPE achieves the same performance as that of the L-MMSE equalizer in
the large-antenna limit, and we demonstrate its efficacy in realistic,
finite-dimensional systems. From a practical perspective, NOPE is
computationally efficient and avoids dedicated training that is typically
required for parameter estimation



Neural Style Transfer: A Review

The seminal work of Gatys et al. demonstrated the power of Convolutional
Neural Networks (CNNs) in creating artistic imagery by separating and
recombining image content and style. This process of using CNNs to render a
content image in different styles is referred to as Neural Style Transfer
(NST). Since then, NST has become a trending topic both in academic literature
and industrial applications. It is receiving increasing attention and a variety
of approaches are proposed to either improve or extend the original NST
algorithm. In this paper, we aim to provide a comprehensive overview of the
current progress towards NST. We first propose a taxonomy of current algorithms
in the field of NST. Then, we present several evaluation methods and compare
different NST algorithms both qualitatively and quantitatively. The review
concludes with a discussion of various applications of NST and open problems
for future research. A list of papers discussed in this review, corresponding
codes, pre-trained models and more comparison results are publicly available at
https://github.com/ycjing/Neural-Style-Transfer-Papers.



Convolutional Sparse Representations with Gradient Penalties

While convolutional sparse representations enjoy a number of useful
properties, they have received limited attention for image reconstruction
problems. The present paper compares the performance of block-based and
convolutional sparse representations in the removal of Gaussian white noise.
While the usual formulation of the convolutional sparse coding problem is
slightly inferior to the block-based representations in this problem, the
performance of the convolutional form can be boosted beyond that of the
block-based form by the inclusion of suitable penalties on the gradients of the
coefficient maps.



Understanding MIDI: A Painless Tutorial on Midi Format

A short overview demystifying the midi audio format is presented. The goal is
to explain the file structure and how the instructions are used to produce a
music signal, both in the case of monophonic signals as for polyphonic signals.



Superfast Line Spectral Estimation

A number of recent works have proposed to solve the line spectral estimation
problem by applying off-the-grid extensions of sparse estimation techniques.
These methods are preferable over classical line spectral estimation algorithms
because they inherently estimate the model order. However, they all have
computation times which grow at least cubically in the problem size, thus
limiting their practical applicability in cases with large dimensions. To
alleviate this issue, we propose a low-complexity method for line spectral
estimation, which also draws on ideas from sparse estimation. Our method is
based on a Bayesian view of the problem. The signal covariance matrix is shown
to have Toeplitz structure, allowing superfast Toeplitz inversion to be used.
We demonstrate that our method achieves estimation accuracy at least as good as
current methods and that it does so while being orders of magnitudes faster.



Learning to Optimize: Training Deep Neural Networks for Wireless Resource Management

For the past couple of decades, numerical optimization has played a central
role in addressing wireless resource management problems such as power control
and beamformer design. However, optimization algorithms often entail
considerable complexity, which creates a serious gap between theoretical
design/analysis and real-time processing. To address this challenge, we propose
a new learning-based approach. The key idea is to treat the input and output of
a resource allocation algorithm as an unknown non-linear mapping and use a deep
neural network (DNN) to approximate it. If the non-linear mapping can be
learned accurately by a DNN of moderate size, then resource allocation can be
done in almost real time -- since passing the input through a DNN only requires
a small number of simple operations.
  In this work, we address both the thereotical and practical aspects of
DNN-based algorithm approximation with applications to wireless resource
management. We first pin down a class of optimization algorithms that are
`learnable' in theory by a fully connected DNN. Then, we focus on DNN-based
approximation to a popular power allocation algorithm named WMMSE (Shi {\it et
al} 2011). We show that using a DNN to approximate WMMSE can be fairly accurate
-- the approximation error $\epsilon$ depends mildly [in the order of
$\log(1/\epsilon)$] on the numbers of neurons and layers of the DNN. On the
implementation side, we use extensive numerical simulations to demonstrate that
DNNs can achieve orders of magnitude speedup in computational time compared to
state-of-the-art power allocation algorithms based on optimization.



Applied Koopman Operator Theory for Power Systems Technology

Koopman operator is a composition operator defined for a dynamical system
described by nonlinear differential or difference equation. Although the
original system is nonlinear and evolves on a finite-dimensional state space,
the Koopman operator itself is linear but infinite-dimensional (evolves on a
function space). This linear operator captures the full information of the
dynamics described by the original nonlinear system. In particular, spectral
properties of the Koopman operator play a crucial role in analyzing the
original system. In the first part of this paper, we review the so-called
Koopman operator theory for nonlinear dynamical systems, with emphasis on modal
decomposition and computation that are direct to wide applications. Then, in
the second part, we present a series of applications of the Koopman operator
theory to power systems technology. The applications are established as
data-centric methods, namely, how to use massive quantities of data obtained
numerically and experimentally, through spectral analysis of the Koopman
operator: coherency identification of swings in coupled synchronous generators,
precursor diagnostic of instabilities in the coupled swing dynamics, and
stability assessment of power systems without any use of mathematical models.
Future problems of this research direction are identified in the last
concluding part of this paper.



A modulation property of time-frequency derivatives of filtered phase and its application to aperiodicity and fo estimation

We introduce a simple and linear SNR (strictly speaking, periodic to random
power ratio) estimator (0dB to 80dB without additional
calibration/linearization) for providing reliable descriptions of aperiodicity
in speech corpus. The main idea of this method is to estimate the background
random noise level without directly extracting the background noise. The
proposed method is applicable to a wide variety of time windowing functions
with very low sidelobe levels. The estimate combines the frequency derivative
and the time-frequency derivative of the mapping from filter center frequency
to the output instantaneous frequency. This procedure can replace the
periodicity detection and aperiodicity estimation subsystems of recently
introduced open source vocoder, YANG vocoder. Source code of MATLAB
implementation of this method will also be open sourced.



A Bayesian Hyperprior Approach for Joint Image Denoising and Interpolation, with an Application to HDR Imaging

Recently, impressive denoising results have been achieved by Bayesian
approaches which assume Gaussian models for the image patches. This improvement
in performance can be attributed to the use of per-patch models. Unfortunately
such an approach is particularly unstable for most inverse problems beyond
denoising. In this work, we propose the use of a hyperprior to model image
patches, in order to stabilize the estimation procedure. There are two main
advantages to the proposed restoration scheme: Firstly it is adapted to
diagonal degradation matrices, and in particular to missing data problems (e.g.
inpainting of missing pixels or zooming). Secondly it can deal with signal
dependent noise models, particularly suited to digital cameras. As such, the
scheme is especially adapted to computational photography. In order to
illustrate this point, we provide an application to high dynamic range imaging
from a single image taken with a modified sensor, which shows the effectiveness
of the proposed scheme.



Online Convolutional Dictionary Learning

While a number of different algorithms have recently been proposed for
convolutional dictionary learning, this remains an expensive problem. The
single biggest impediment to learning from large training sets is the memory
requirements, which grow at least linearly with the size of the training set
since all existing methods are batch algorithms. The work reported here
addresses this limitation by extending online dictionary learning ideas to the
convolutional context.



Single-Channel Multi-talker Speech Recognition with Permutation Invariant Training

Although great progresses have been made in automatic speech recognition
(ASR), significant performance degradation is still observed when recognizing
multi-talker mixed speech. In this paper, we propose and evaluate several
architectures to address this problem under the assumption that only a single
channel of mixed signal is available. Our technique extends permutation
invariant training (PIT) by introducing the front-end feature separation module
with the minimum mean square error (MSE) criterion and the back-end recognition
module with the minimum cross entropy (CE) criterion. More specifically, during
training we compute the average MSE or CE over the whole utterance for each
possible utterance-level output-target assignment, pick the one with the
minimum MSE or CE, and optimize for that assignment. This strategy elegantly
solves the label permutation problem observed in the deep learning based
multi-talker mixed speech separation and recognition systems. The proposed
architectures are evaluated and compared on an artificially mixed AMI dataset
with both two- and three-talker mixed speech. The experimental results indicate
that our proposed architectures can cut the word error rate (WER) by 45.0% and
25.0% relatively against the state-of-the-art single-talker speech recognition
system across all speakers when their energies are comparable, for two- and
three-talker mixed speech, respectively. To our knowledge, this is the first
work on the multi-talker mixed speech recognition on the challenging
speaker-independent spontaneous large vocabulary continuous speech task.



Convolutional Sparse Coding: Boundary Handling Revisited

Two different approaches have recently been proposed for boundary handling in
convolutional sparse representations, avoiding potential boundary artifacts
arising from the circular boundary conditions implied by the use of frequency
domain solution methods by introducing a spatial mask into the convolutional
sparse coding problem. In the present paper we show that, under certain
circumstances, these methods fail in their design goal of avoiding boundary
artifacts. The reasons for this failure are discussed, a solution is proposed,
and the practical implications are illustrated in an image deblurring problem.



Stability and instability in saddle point dynamics -- Part I

We consider the problem of convergence to a saddle point of a concave-convex
function via gradient dynamics. Since first introduced by Arrow, Hurwicz and
Uzawa in [1] such dynamics have been extensively used in diverse areas, there
are, however, features that render their analysis non trivial. These include
the lack of convergence guarantees when the function considered is not strictly
concave-convex and also the non-smoothness of subgradient dynamics. Our aim in
this two part paper is to provide an explicit characterization to the
asymptotic behaviour of general gradient and subgradient dynamics applied to a
general concave-convex function. We show that despite the nonlinearity and
non-smoothness of these dynamics their $\omega$-limit set is comprised of
trajectories that solve only explicit linear ODEs that are characterized within
the paper.
  More precisely, in Part I an exact characterization is provided to the
asymptotic behaviour of unconstrained gradient dynamics. We also show that when
convergence to a saddle point is not guaranteed then the system behaviour can
be problematic, with arbitrarily small noise leading to an unbounded variance.
In Part II we consider a general class of subgradient dynamics that restrict
trajectories in an arbitrary convex domain, and show that when an equilibrium
point exists their limiting trajectories are solutions of subgradient dynamics
on only affine subspaces. The latter is a smooth class of dynamics with an
asymptotic behaviour exactly characterized in Part I, as solutions to explicit
linear ODEs. These results are used to formulate corresponding convergence
criteria and are demonstrated with several examples and applications presented
in Part II.



Restoring a smooth function from its noisy integrals

Numerical (and experimental) data analysis often requires the restoration of
a smooth function from a set of sampled integrals over finite bins. We present
the bin hierarchy method that efficiently computes the maximally smooth
function from the sampled integrals using essentially all the information
contained in the data. We perform extensive tests with different classes of
functions and levels of data quality, including Monte Carlo data suffering from
a severe sign problem and physical data for the Green's function of the
Fr\"ohlich polaron.



A Novel Sparse recovery based DOA estimation algorithm by relaxing the RIP constraint

Direction of Arrival (DOA) estimation of mixed uncorrelated and coherent
sources is a long existing challenge in array signal processing. Application of
compressive sensing to array signal processing has opened up an exciting class
of algorithms. The authors investigated the application of orthogonal matching
pursuit (OMP) for direction of Arrival (DOA) estimation for different
scenarios, especially to tackle the case of coherent sources and observed
inconsistencies in the results. In this paper, a modified OMP algorithm is
proposed to overcome these deficiencies by exploiting maximum variance based
criterion using only one snapshot. This criterion relaxes the imposed
restricted isometry property (RIP) on the measurement matrix to obtain the
sources and hence, reduces the sparsity of the input vector to the local OMP
algorithm. Moreover, it also tackles sources irrespective of their coherency.
The condition for the weak-1 RIP on decreased sparsity is derived and it is
shown that how the algorithm gives better result than the OMP algorithm. With
an addition to this, a simple method is also presented to calculate source
distance from the reference point in a uniform linear sensor array. Numerical
analysis demonstrates the effectiveness of the proposed algorithm.



A Graphical Characterization of Structurally Controllable Linear Systems with Dependent Parameters

One version of the concept of structural controllability defined for
single-input systems by Lin and subsequently generalized to multi-input systems
by others, states that a parameterized matrix pair $(A, B)$ whose nonzero
entries are distinct parameters, is structurally controllable if values can be
assigned to the parameters which cause the resulting matrix pair to be
controllable. In this paper the concept of structural controllability is
broadened to allow for the possibility that a parameter may appear in more than
one location in the pair $(A, B)$. Subject to a certain condition on the
parameterization called the "binary assumption", an explicit graph-theoretic
characterization of such matrix pairs is derived.



Maximum entropy based non-negative optoacoustic tomographic image reconstruction

Objective:Optoacoustic (photoacoustic) tomography is aimed at reconstructing
maps of the initial pressure rise induced by the absorption of light pulses in
tissue. In practice, due to inaccurate assumptions in the forward model, noise
and other experimental factors, the images are often afflicted by artifacts,
occasionally manifested as negative values. The aim of the work is to develop
an inversion method which reduces the occurrence of negative values and
improves the quantitative performance of optoacoustic imaging. Methods: We
present a novel method for optoacoustic tomography based on an entropy
maximization algorithm, which uses logarithmic regularization for attaining
non-negative reconstructions. The reconstruction image quality is further
improved using structural prior based fluence correction. Results: We report
the performance achieved by the entropy maximization scheme on numerical
simulation, experimental phantoms and in-vivo samples. Conclusion: The proposed
algorithm demonstrates superior reconstruction performance by delivering
non-negative pixel values with no visible distortion of anatomical structures.
Significance: Our method can enable quantitative optoacoustic imaging, and has
the potential to improve pre-clinical and translational imaging applications.



SPARCOM: Sparsity Based Super-Resolution Correlation Microscopy

In traditional optical imaging systems, the spatial resolution is limited by
the physics of diffraction, which acts as a low-pass filter. The information on
sub-wavelength features is carried by evanescent waves, never reaching the
camera, thereby posing a hard limit on resolution: the so-called diffraction
limit. Modern microscopic methods enable super-resolution, by employing
florescence techniques. State-of-the-art localization based fluorescence
subwavelength imaging techniques such as PALM and STORM achieve sub-diffraction
spatial resolution of several tens of nano-meters. However, they require tens
of thousands of exposures, which limits their temporal resolution. We have
recently proposed SPARCOM (sparsity based super-resolution correlation
microscopy), which exploits the sparse nature of the fluorophores distribution,
alongside a statistical prior of uncorrelated emissions, and showed that
SPARCOM achieves spatial resolution comparable to PALM/STORM, while capturing
the data hundreds of times faster. Here, we provide a detailed mathematical
formulation of SPARCOM, which in turn leads to an efficient numerical
implementation, suitable for large-scale problems. We further extend our method
to a general framework for sparsity based super-resolution imaging, in which
sparsity can be assumed in other domains such as wavelet or discrete-cosine,
leading to improved reconstructions in a variety of physical settings.



Automatic Organisation and Quality Analysis of User-Generated Content with Audio Fingerprinting

The increase of the quantity of user-generated content experienced in social
media has boosted the importance of analysing and organising the content by its
quality. Here, we propose a method that uses audio fingerprinting to organise
and infer the quality of user-generated audio content. The proposed method
detects the overlapping segments between different audio clips to organise and
cluster the data according to events, and to infer the audio quality of the
samples. A test setup with concert recordings manually crawled from YouTube is
used to validate the presented method. The results show that the proposed
method achieves better results than previous methods.



Automatic Organisation, Segmentation, and Filtering of User-Generated Audio Content

Using solely the information retrieved by audio fingerprinting techniques, we
propose methods to treat a possibly large dataset of user-generated audio
content, that (1) enable the grouping of several audio files that contain a
common audio excerpt (i.e., are relative to the same event), and (2) give
information about how those files are correlated in terms of time and quality
inside each event. Furthermore, we use supervised learning to detect incorrect
matches that may arise from the audio fingerprinting algorithm itself, whilst
ensuring our model learns with previous predictions. All the presented methods
were further validated by user-generated recordings of several different
concerts manually crawled from YouTube.



Analysis of Optimal Combining in Rician Fading with Co-channel Interference

Approximate Symbol error rate (SER), outage probability and rate expressions
are derived for receive diversity system employing optimum combining when both
the desired and the interfering signals are subjected to Rician fading, for the
cases of a) equal power uncorrelated interferers b) unequal power interferers
c) interferer correlation. The derived expressions are applicable for an
arbitrary number of receive antennas and interferers and for any quadrature
amplitude modulation (QAM) constellation. Furthermore, we derive a simple
closed form expression for SER in the interference-limited regime, for the
special case of Rayleigh faded interferers. A close match is observed between
the SER, outage probability and rate results obtained through the derived
analytical expressions and the ones obtained from Monte-Carlo simulations.



Convolutional Sparse Coding with Overlapping Group Norms

The most widely used form of convolutional sparse coding uses an $\ell_1$
regularization term. While this approach has been successful in a variety of
applications, a limitation of the $\ell_1$ penalty is that it is homogeneous
across the spatial and filter index dimensions of the sparse representation
array, so that sparsity cannot be separately controlled across these
dimensions. The present paper considers the consequences of replacing the
$\ell_1$ penalty with a mixed group norm, motivated by recent theoretical
results for convolutional sparse representations. Algorithms are developed for
solving the resulting problems, which are quite challenging, and the impact on
the performance of the denoising problem is evaluated. The mixed group norms
are found to perform very poorly in this application. While their performance
is greatly improved by introducing a weighting strategy, such a strategy also
improves the performance obtained from the much simpler and computationally
cheaper $\ell_1$ norm.



Joint Separation and Denoising of Noisy Multi-talker Speech using Recurrent Neural Networks and Permutation Invariant Training

In this paper we propose to use utterance-level Permutation Invariant
Training (uPIT) for speaker independent multi-talker speech separation and
denoising, simultaneously. Specifically, we train deep bi-directional Long
Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) using uPIT, for
single-channel speaker independent multi-talker speech separation in multiple
noisy conditions, including both synthetic and real-life noise signals. We
focus our experiments on generalizability and noise robustness of models that
rely on various types of a priori knowledge e.g. in terms of noise type and
number of simultaneous speakers. We show that deep bi-directional LSTM RNNs
trained using uPIT in noisy environments can improve the Signal-to-Distortion
Ratio (SDR) as well as the Extended Short-Time Objective Intelligibility
(ESTOI) measure, on the speaker independent multi-talker speech separation and
denoising task, for various noise types and Signal-to-Noise Ratios (SNRs).
Specifically, we first show that LSTM RNNs can achieve large SDR and ESTOI
improvements, when evaluated using known noise types, and that a single model
is capable of handling multiple noise types with only a slight decrease in
performance. Furthermore, we show that a single LSTM RNN can handle both
two-speaker and three-speaker noisy mixtures, without a priori knowledge about
the exact number of speakers. Finally, we show that LSTM RNNs trained using
uPIT generalize well to noise types not seen during training.



First and Second Order Methods for Online Convolutional Dictionary Learning

Convolutional sparse representations are a form of sparse representation with
a structured, translation invariant dictionary. Most convolutional dictionary
learning algorithms to date operate in batch mode, requiring simultaneous
access to all training images during the learning process, which results in
very high memory usage and severely limits the training data that can be used.
Very recently, however, a number of authors have considered the design of
online convolutional dictionary learning algorithms that offer far better
scaling of memory and computational cost with training set size than batch
methods. This paper extends our prior work, improving a number of aspects of
our previous algorithm; proposing an entirely new one, with better performance,
and that supports the inclusion of a spatial mask for learning from incomplete
data; and providing a rigorous theoretical analysis of these methods.



An order optimal policy for exploiting idle spectrum in cognitive radio networks

In this paper a spectrum sensing policy employing recency-based exploration
is proposed for cognitive radio networks. We formulate the problem of finding a
spectrum sensing policy for multi-band dynamic spectrum access as a stochastic
restless multi-armed bandit problem with stationary unknown reward
distributions. In cognitive radio networks the multi-armed bandit problem
arises when deciding where in the radio spectrum to look for idle frequencies
that could be efficiently exploited for data transmission. We consider two
models for the dynamics of the frequency bands: 1) the independent model where
the state of the band evolves randomly independently from the past and 2) the
Gilbert-Elliot model, where the states evolve according to a 2-state Markov
chain. It is shown that in these conditions the proposed sensing policy attains
asymptotically logarithmic weak regret. The policy proposed in this paper is an
index policy, in which the index of a frequency band is comprised of a sample
mean term and a recency-based exploration bonus term. The sample mean promotes
spectrum exploitation whereas the exploration bonus encourages for further
exploration for idle bands providing high data rates. The proposed recency
based approach readily allows constructing the exploration bonus such that it
will grow the time interval between consecutive sensing time instants of a
suboptimal band exponentially, which then leads to logarithmically increasing
weak regret. Simulation results confirming logarithmic weak regret are
presented and it is found that the proposed policy provides often improved
performance at low complexity over other state-of-the-art policies in the
literature.



Code Constructions for Physical Unclonable Functions and Biometric Secrecy Systems

The two-terminal key agreement problem with biometric or physical identifiers
is considered. Two linear code constructions based on Wyner-Ziv coding are
developed. The first construction uses random linear codes and achieves all
points of the key-leakage-storage regions of the generated-secret and
chosen-secret models. The second construction uses nested polar codes for
vector quantization during enrollment and for error correction during
reconstruction. Simulations show that the nested polar codes achieve
privacy-leakage and storage rates that improve on existing code designs. One
proposed code achieves a rate tuple that cannot be achieved by existing
methods.



Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification

Improving speech system performance in noisy environments remains a
challenging task, and speech enhancement (SE) is one of the effective
techniques to solve the problem. Motivated by the promising results of
generative adversarial networks (GANs) in a variety of image processing tasks,
we explore the potential of conditional GANs (cGANs) for SE, and in particular,
we make use of the image processing framework proposed by Isola et al. [1] to
learn a mapping from the spectrogram of noisy speech to an enhanced
counterpart. The SE cGAN consists of two networks, trained in an adversarial
manner: a generator that tries to enhance the input noisy spectrogram, and a
discriminator that tries to distinguish between enhanced spectrograms provided
by the generator and clean ones from the database using the noisy spectrogram
as a condition. We evaluate the performance of the cGAN method in terms of
perceptual evaluation of speech quality (PESQ), short-time objective
intelligibility (STOI), and equal error rate (EER) of speaker verification (an
example application). Experimental results show that the cGAN method overall
outperforms the classical short-time spectral amplitude minimum mean square
error (STSA-MMSE) SE algorithm, and is comparable to a deep neural
network-based SE approach (DNN-SE).



Variation Evolving for Optimal Control Computation, a Compact Way

A compact version of the variation evolving method (VEM) is developed in the
primal variable space for optimal control computation. Following the idea that
originates from the Lyapunov continuous-time dynamics stability theory in the
control field, the optimal solution is analogized to the stable equilibrium
point of a dynamic system and obtained asymptotically through the variation
motion. With the introduction of a virtual dimension, namely the variation
time, the evolution partial differential equation (EPDE), which seeks the
optimal solution with a theoretical guarantee, is developed for the optimal
control problem (OCP) with free terminal states, and the equivalent optimality
conditions with no employment of costates are established in the primal space.
These conditions show that the optimal feedback control law is generally not
analytically available because the optimal control is related to the future
states. Since the derived EPDE is suitable to be computed with the
semi-discrete method in the field of PDE numerical calculation, the optimal
solution may be obtained by solving the resulting finite-dimensional
initial-value problem (IVP).



Convolutional Dictionary Learning: A Comparative Review and New Algorithms

Convolutional sparse representations are a form of sparse representation with
a dictionary that has a structure that is equivalent to convolution with a set
of linear filters. While effective algorithms have recently been developed for
the convolutional sparse coding problem, the corresponding dictionary learning
problem is substantially more challenging. Furthermore, although a number of
different approaches have been proposed, the absence of thorough comparisons
between them makes it difficult to determine which of them represents the
current state of the art. The present work both addresses this deficiency and
proposes some new approaches that outperform existing ones in certain contexts.
A thorough set of performance comparisons indicates a very wide range of
performance differences among the existing and proposed methods, and clearly
identifies those that are the most effective.



Data Discovery and Anomaly Detection Using Atypicality: Signal Processing Methods

The aim of atypicality is to extract small, rare, unusual and interesting
pieces out of big data. This complements statistics about typical data to give
insight into data. In order to find such "interesting" parts of data, universal
approaches are required, since it is not known in advance what we are looking
for. We therefore base the atypicality criterion on codelength. In a prior
paper we developed the methodology for discrete-valued data, and the the
current paper extends this to real-valued data. This is done by using minimum
description length (MDL). We show that this shares a number of theoretical
properties with the discrete-valued case. We develop the methodology for a
number of "universal" signal processing models, and finally apply them to
recorded hydrophone data.



Support Spinor Machine

We generalize a support vector machine to a support spinor machine by using
the mathematical structure of wedge product over vector machine in order to
extend field from vector field to spinor field. The separated hyperplane is
extended to Kolmogorov space in time series data which allow us to extend a
structure of support vector machine to a support tensor machine and a support
tensor machine moduli space. Our performance test on support spinor machine is
done over one class classification of end point in physiology state of time
series data after empirical mode analysis and compared with support vector
machine test. We implement algorithm of support spinor machine by using
Holo-Hilbert amplitude modulation for fully nonlinear and nonstationary time
series data analysis.



Une v\'eritable approche $\ell_0$ pour l'apprentissage de dictionnaire

Sparse representation learning has recently gained a great success in signal
and image processing, thanks to recent advances in dictionary learning. To this
end, the $\ell_0$-norm is often used to control the sparsity level.
Nevertheless, optimization problems based on the $\ell_0$-norm are non-convex
and NP-hard. For these reasons, relaxation techniques have been attracting much
attention of researchers, by priorly targeting approximation solutions (e.g.
$\ell_1$-norm, pursuit strategies). On the contrary, this paper considers the
exact $\ell_0$-norm optimization problem and proves that it can be solved
effectively, despite of its complexity. The proposed method reformulates the
problem as a Mixed-Integer Quadratic Program (MIQP) and gets the global optimal
solution by applying existing optimization software. Because the main
difficulty of this approach is its computational time, two techniques are
introduced that improve the computational speed. Finally, our method is applied
to image denoising which shows its feasibility and relevance compared to the
state-of-the-art.



Bounds on Discrete Fourier Transform of Random Mask

This paper proposes some bounds on the maximum of magnitude of a random mask
in Fourier domain. The random mask is used in random sampling scheme. Having a
bound on the maximum value of a random mask in Fourier domain is very useful
for some iterative recovery methods that use thresholding operator. In this
paper, we propose some different bounds and compare them with the empirical
examples.



Adaptive Nonlinear RF Cancellation for Improved Isolation in Simultaneous Transmit-Receive Systems

This paper proposes an active radio frequency (RF) cancellation solution to
suppress the transmitter (TX) passband leakage signal in radio transceivers
supporting simultaneous transmission and reception. The proposed technique is
based on creating an opposite-phase baseband equivalent replica of the TX
leakage signal in the transceiver digital front-end through adaptive nonlinear
filtering of the known transmit data, to facilitate highly accurate
cancellation under a nonlinear TX power amplifier (PA). The active RF
cancellation is then accomplished by employing an auxiliary transmitter chain,
to generate the actual RF cancellation signal, and combining it with the
received signal at the receiver (RX) low noise amplifier (LNA) input. A
closed-loop parameter learning approach, based on the decorrelation principle,
is also developed to efficiently estimate the coefficients of the nonlinear
cancellation filter in the presence of a nonlinear TX PA with memory, finite
passive isolation, and a nonlinear RX LNA. The performance of the proposed
cancellation technique is evaluated through comprehensive RF measurements
adopting commercial LTE-Advanced transceiver hardware components. The results
show that the proposed technique can provide an additional suppression of up to
54 dB for the TX passband leakage signal at the RX LNA input, even at
considerably high transmit power levels and with wide transmission bandwidths.
Such novel cancellation solution can therefore substantially improve the TX-RX
isolation, hence reducing the requirements on passive isolation and RF
component linearity, as well as increasing the efficiency and flexibility of
the RF spectrum use in the emerging 5G radio networks.



Optimal Linear Precoding for Indoor Visible Light Communication System

Visible light communication (VLC) is an emerging technique that uses
light-emitting diodes (LED) to combine communication and illumination. It is
considered as a promising scheme for indoor wireless communication that can be
deployed at reduced costs while offering high data rate performance. In this
paper, we focus on the design of the downlink of a multi-user VLC system.
Inherent to multi-user systems is the interference caused by the broadcast
nature of the medium. Linear precoding based schemes are among the most popular
solutions that have recently been proposed to mitigate inter-user interference.
This paper focuses on the design of the optimal linear precoding scheme that
solves the max-min signal-to-interference-plus-noise ratio (SINR) problem. The
performance of the proposed precoding scheme is studied under different working
conditions and compared with the classical zero-forcing precoding. Simulations
have been provided to illustrate the high gain of the proposed scheme.



Optimal Training for Residual Self-Interference for Full Duplex One-way Relays

Channel estimation and optimal training sequence design for full-duplex
one-way relays are investigated. We propose a training scheme to estimate the
residual self-interference (RSI) channel and the channels between nodes
simultaneously. A maximum likelihood estimator is implemented with
Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. In the presence of RSI, the
overall source-to-destination channel becomes an inter-symbol-interference
(ISI) channel. With the help of estimates of the RSI channel, the destination
is able to cancel the ISI through equalization. We derive and analyze the
Cramer-Rao bound (CRB) in closed-form by using the asymptotic properties of
Toeplitz matrices. The optimal training sequence is obtained by minimizing the
CRB. Extensions for the fundamental one-way relay model to the
frequency-selective fading channels and the multiple relays case are also
considered. For the former, we propose a training scheme to estimate the
overall channel, and for the latter the CRB and the optimal number of relays
are derived when the distance between the source and the destination is fixed.
Simulations using LTE parameters corroborate our theoretical results.



MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment

Generating music has a few notable differences from generating images and
videos. First, music is an art of time, necessitating a temporal model. Second,
music is usually composed of multiple instruments/tracks with their own
temporal dynamics, but collectively they unfold over time interdependently.
Lastly, musical notes are often grouped into chords, arpeggios or melodies in
polyphonic music, and thereby introducing a chronological ordering of notes is
not naturally suitable. In this paper, we propose three models for symbolic
multi-track music generation under the framework of generative adversarial
networks (GANs). The three models, which differ in the underlying assumptions
and accordingly the network architectures, are referred to as the jamming
model, the composer model and the hybrid model. We trained the proposed models
on a dataset of over one hundred thousand bars of rock music and applied them
to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings.
A few intra-track and inter-track objective metrics are also proposed to
evaluate the generative results, in addition to a subjective user study. We
show that our models can generate coherent music of four bars right from
scratch (i.e. without human inputs). We also extend our models to human-AI
cooperative music generation: given a specific track composed by human, we can
generate four additional tracks to accompany it. All code, the dataset and the
rendered audio samples are available at https://salu133445.github.io/musegan/ .



Assessing Visual Quality of Omnidirectional Videos

In contrast with traditional video, omnidirectional video enables spherical
viewing direction with support for head-mounted displays, providing an
interactive and immersive experience. Unfortunately, to the best of our
knowledge, there are few visual quality assessment (VQA) methods, either
subjective or objective, for omnidirectional video coding. This paper proposes
both subjective and objective methods for assessing quality loss in encoding
omnidirectional video. Specifically, we first present a new database, which
includes the viewing direction data from several subjects watching
omnidirectional video sequences. Then, from our database, we find a high
consistency in viewing directions across different subjects. The viewing
directions are normally distributed in the center of the front regions, but
they sometimes fall into other regions, related to video content. Given this
finding, we present a subjective VQA method for measuring difference mean
opinion score (DMOS) of the whole and regional omnidirectional video, in terms
of overall DMOS (O-DMOS) and vectorized DMOS (V-DMOS), respectively. Moreover,
we propose two objective VQA methods for encoded omnidirectional video, in
light of human perception characteristics of omnidirectional video. One method
weighs the distortion of pixels with regard to their distances to the center of
front regions, which considers human preference in a panorama. The other method
predicts viewing directions according to video content, and then the predicted
viewing directions are leveraged to allocate weights to the distortion of each
pixel in our objective VQA method. Finally, our experimental results verify
that both the subjective and objective methods proposed in this paper advance
state-of-the-art VQA for omnidirectional video.



SIMO channel performance evaluation on indoor environment at 2.4 GHz

This work presents an experimental study of Single Input Multiple Output
(SIMO) channel performance in indoor radio propagation environment. Indoor
channel measurements at 2.4 GHz ISM frequency band have been performed using a
versatile channel sounder testbed platform. A single transmitting antenna, four
receiving antennas with two proposed geometries and a four-branch receiver
circuitry were used in order to achieve channel sounder measurements exploiting
baseband signal processing techniques. Deep investigation on SIMO wireless
channel performance was realized through three types of metrics which are
signal strength, gain coefficient and capacity. Performance results indicate
SIMO channel capacity enhancement and illustrate differences between the two
proposed geometries.



Photoacoustic Imaging using Combination of Eigenspace-Based Minimum Variance and Delay-Multiply-and-Sum Beamformers: Simulation Study

Delay and Sum (DAS), as the most common beamforming algorithm in
Photoacoustic Imaging (PAI), having a simple implementation, results in a
low-quality image. Delay Multiply and Sum (DMAS) was introduced to improve the
quality of the reconstructed images using DAS. However, the resolution
improvement is now well enough compared to high resolution adaptive
reconstruction methods such as Eigenspace- Based Minimum Variance (EIBMV). We
proposed to integrate the EIBMV inside the DMAS formula by replacing the
existing DAS algebra inside the expansion of DMAS, called EIBMV-DMAS. It is
shown that EIBMV-DMAS outperforms DMAS in the terms of levels of sidelobes and
width of mainlobe significantly. For instance, at the depth of 35 mm,
EIBMV-DMAS outperforms DMAS and EIBMV in the term of sidelobes for about 108
dB, 98 dB and 44 dB compared to DAS, DMAS, and EIBMV, respectively. The
quantitative comparison has been conducted using Full-Width-Half-Maximum (FWHM)
and Signal-to-Noise Ratio (SNR), and it was shown that EIBMV-DMAS reduces the
FWHM about 1.65 mm and improves the SNR about 15 dB, compared to DMAS.



Linear Computer-Music through Sequences over Galois Fields

It is shown how binary sequences can be associated with automatic composition
of monophonic pieces. We are concerned with the composition of e-music from
finite field structures. The information at the input may be either random or
information from a black-and-white, grayscale or color picture. New
e-compositions and music score are made available, including a new piece from
the famous Lenna picture: the score of the e-music <<Between Lenna's eyes in C
major.>> The corresponding stretch of music score are presented. Some
particular structures, including clock arithmetic (mod 12), GF(7), GF(8),
GF(13) and GF(17) are addressed. Further, multilevel block-codes are also used
in a new approach of e-music composition, engendering a particular style as an
e-composer. As an example, Pascal multilevel block codes recently introduced
are handled to generate a new style of electronic music over GF(13).



Modeling and Analysis of Power Line Communications for Application in Smart Grid

Smart grid is an energy infrastructure that increases energy efficiency by
using communication infrastructure, smart meters, smart appliances, automated
control and networking, and more. This paper focuses on the Power Line
Communication (PLC) aspect and technologies used in the smart grid. There are
various challenges and advancements in the smart grid; this research discusses
how PLC can improve smart grid performance. In order to provide applicable
results, practical PLC system parameters and other required data was obtained
from Florida Power and Light (FPL). Modeling of the PLC system with different
types of digital modulations was conducted using MATLAB/Simulink software and
Python. The benefits and design tradeoffs of Amplitude Shift Keying (ASK),
Frequency Shift Keying (FSK), and Phase Shift Keying (PSK) are discussed. The
modulation schemes are compared on the basis of their applicability to a
practical PLC network by comparing the results of the simulations



Optimized Structured Sparse Sensing Matrices for Compressive Sensing

We consider designing a robust structured sparse sensing matrix consisting of
a sparse matrix with a few non-zero entries per row and a dense base matrix for
capturing signals efficiently We design the robust structured sparse sensing
matrix through minimizing the distance between the Gram matrix of the
equivalent dictionary and the target Gram of matrix holding small mutual
coherence. Moreover, a regularization is added to enforce the robustness of the
optimized structured sparse sensing matrix to the sparse representation error
(SRE) of signals of interests. An alternating minimization algorithm with
global sequence convergence is proposed for solving the corresponding
optimization problem. Numerical experiments on synthetic data and natural
images show that the obtained structured sensing matrix results in a higher
signal reconstruction than a random dense sensing matrix.



Interdependence of Transmission Branch Parameters on the Voltage Levels

Transformers and transmission lines are critical components of a grid
network. This paper analyzes the statistical properties of the electrical
parameters of transmission branches and especially examines their
interdependence on the voltage levels. Some interesting findings include: (a)
with appropriate conversion of MVA rating, a transformers per unit reactance
exhibits consistent statistical pattern independent of voltage levels and
capacity; (b) the distributed reactance (ohms/km) of transmission lines also
has some consistent patterns regardless of voltage levels; (c) other parameters
such as the branch resistance, the MVA ratings, the transmission line length,
etc, manifest strong interdependence on the voltage levels which can be
approximated by a power function with different power constants. The results
will be useful in both creation of synthetic power grid test cases and
validation of existing grid models.



Image Acquisition System Using On Sensor Compressed Sampling Technique

Advances in CMOS technology have made high resolution image sensors possible.
These image sensor pose significant challenges in terms of the amount of raw
data generated, energy efficiency and frame rate. This paper presents a new
design methodology for an imaging system and a simplified novel image sensor
pixel design to be used in such system so that Compressed Sensing (CS)
technique can be implemented easily at the sensor level. This results in
significant energy savings as it not only cuts the raw data rate but also
reduces transistor count per pixel, decreases pixel size, increases fill
factor, simplifies ADC, JPEG encoder and JPEG decoder design and decreases
wiring as well as address decoder size by half. Thus CS has the potential to
increase the resolution of image sensors for a given technology and die size
while significantly decreasing the power consumption and design complexity. We
show that it has potential to reduce power consumption by about 23%-65%.



Covert Wireless Communication with Artificial Noise Generation

Covert communication conceals the transmission of the message from an
attentive adversary. Recent work on the limits of covert communication in
additive white Gaussian noise (AWGN) channels has demonstrated that a covert
transmitter (Alice) can reliably transmit a maximum of
$\mathcal{O}\left(\sqrt{n}\right)$ bits to a covert receiver (Bob) without
being detected by an adversary (Warden Willie) in $n$ channel uses. This paper
focuses on the scenario where other friendly nodes distributed according to a
two-dimensional Poisson point process with density $m$ are present in the
environment. We propose a strategy where the friendly node closest to the
adversary, without close coordination with Alice, produces artificial noise. We
show that this method allows Alice to reliably and covertly send
$\mathcal{O}(\min\{{n,m^{\gamma/2}\sqrt{n}}\})$ bits to Bob in $n$ channel
uses, where $\gamma$ is the path-loss exponent. Moreover, we also consider a
setting where there are $N_{\mathrm{w}}$ collaborating adversaries uniformly
and randomly located in the environment and show that in $n$ channel uses,
Alice can reliably and covertly send
$\mathcal{O}\left(\min\left\{n,\frac{m^{\gamma/2}
\sqrt{n}}{N_{\mathrm{w}}^{\gamma}}\right\}\right)$ bits to Bob when $\gamma
>2$, and $\mathcal{O}\left(\min\left\{n,\frac{m
\sqrt{n}}{N_{\mathrm{w}}^{2}\log^2 {N_{\mathrm{w}}}}\right\}\right)$ when
$\gamma = 2$. Conversely, we demonstrate that no higher covert throughput is
possible for $\gamma>2$.



Interconnection Strategies for Self-Calibration of Large Scale Antenna Arrays

In time-division duplexing (TDD) systems, massive multiple-input
multiple-output (MIMO) relies on the channel reciprocity to obtain the downlink
(DL) channel state information (CSI) with the uplink (UL) CSI. In practice, the
mismatches in the radio frequency (RF) analog circuits among different antennas
at the base station (BS) break the end-to-end UL and DL channel reciprocity.
Antenna calibration is necessary to avoid the severe performance degradation
with massive MIMO. Many calibration schemes are available to compensate the RF
gain mismatches and restore the channel reciprocity in TDD massive MIMO
systems. In this paper, we focus on the internal self-calibration scheme where
different BS antennas are interconnected via hardware transmission lines.
First, we study the resulting calibration performance for an arbitrary
interconnection strategy. Next, we obtain closed-form Cramer-Rao lower bound
(CRLB) expressions for each interconnection strategy at the BS with only (M-1)
transmission lines and M denotes the total number of BS antennas. Basing on the
derived results, we further prove that the star interconnection strategy is
optimal for internal self-calibration due to its lowest CRLB. In addition, we
also put forward efficient recursive algorithms to derive the corresponding
maximum-likelihood (ML) estimates of all the calibration coefficients.
Numerical simulation results are also included to corroborate our theoretical
analyses and results.



Broadband Multizone Sound Rendering by Jointly Optimizing the Sound Pressure and Particle Velocity

In this paper, a recently proposed approach to multizone sound field
synthesis, referred to as Joint Pressure and Velocity Matching (JPVM), is
investigated analytically using a spherical harmonics representation of the
sound field. The approach is motivated by the Kirchhoff-Helmholtz integral
equation and aims at controlling the sound field inside the local listening
zones by evoking the sound pressure and particle velocity on surrounding
contours. Based on the findings of the modal analysis, an improved version of
JPVM is proposed which provides both better performance and lower complexity.
In particular, it is shown analytically that the optimization of the tangential
component of the particle velocity vector, as is done in the original JPVM
approach, is very susceptible to errors and thus not pursued anymore. The
analysis furthermore provides fundamental insights as to how the spherical
harmonics used to describe the 3D variant sound field translate into 2D basis
functions as observed on the contours surrounding the zones. By means of
simulations, it is verified that discarding the tangential component of the
particle velocity vector ultimately leads to an improved performance. Finally,
the impact of sensor noise on the reproduction performance is assessed.



Having your cake and eating it too: Scripted workflows for image manipulation

The reproducibility issue in science has come under increased scrutiny. One
consistent suggestion lies in the use of scripted methods or workflows for data
analysis. Image analysis is one area in science in which little can be done in
scripted methods. The SWIIM Project (Scripted Workflows to Improve Image
Manipulation) is designed to generate workflows from popular image manipulation
tools. In the project, 2 approaches are being taken to construct workflows in
the image analysis area. First, the open-source tool GIMP is being enhanced to
produce an active log (which can be run on a stand-alone basis to perform the
same manipulation). Second, the R system Shiny tool is being used to construct
a graphical user interface (GUI) which works with EBImage code to modify
images, and to produce an active log which can perform the same operations.
This process has been successful to date, but is not complete. The basic method
for each component is discussed, and example code is shown.



Bolt Detection Signal Analysis Method Based on ICEEMD

The construction quality of the bolt is directly related to the safety of the
project, and as such, it must be tested. In this paper, the improved complete
ensemble empirical mode decomposition (ICEEMD) method is introduced to the bolt
detection signal analysis. The ICEEMD is used in order to decompose the anchor
detection signal according to the approximate entropy of each intrinsic mode
function (IMF). The noise of the IMFs is eliminated by the wavelet soft
threshold de-noising technique. Based on the approximate entropy, and the
wavelet de-noising principle, the ICEEMD-De anchor signal analysis method is
proposed. From the analysis of the vibration analog signal, as well as the bolt
detection signal, the result shows that the ICEEMD-De method is capable of
correctly separating the different IMFs under noisy conditions, and also that
the IMF can effectively identify the reflection signal of the end of the bolt.



Techniques and Challenges in Speech Synthesis

The aim of this project was to develop and implement an English language
Text-to-Speech synthesis system. This involved a study of mechanisms of human
speech production, a review of techniques in speech synthesis, and analysis of
tests used to evaluate the effectiveness of synthesized speech. It was
determined that a diphone synthesis system was the most effective choice for
the scope of this project. A method of automatically identifying and extracting
diphones from prompted speech was designed, allowing for the creation of a
diphone database by a speaker in less than 40 minutes. CMUdict was used to
determine the pronunciation of known words. A system for smoothing the
transitions between diphone recordings was designed and implemented. CMUdict
was then used to train a maximum-likelihood prediction system to determine the
correct pronunciation of unknown English language alphabetic words. Then, a
Part Of Speech tagger was designed to find the lexical class of words within a
sentence.
  A method of altering the pitch, duration, and volume of the produced voice
over time was designed, being a combination of the TD-PSOLA algorithm and a
novel approach referred to as Unvoiced Speech Duration Shifting. This minimises
distortion of the voice when shifting the pitch or duration, while maximising
computational efficiency by operating in the time domain. This approach was
used to add correct lexical stress to vowels within words. A text tokenisation
system was developed to handle arbitrary text input, allowing pronunciation of
numerical input tokens and use of appropriate pauses for punctuation. Methods
for further improving sentence level speech naturalness were discussed.
Finally, the system was tested with listeners for its intelligibility and
naturalness.



Spectral and Energy Efficiency of Superimposed Pilots in Uplink Massive MIMO

Next generation wireless networks aim at providing substantial improvements
in spectral efficiency (SE) and energy efficiency (EE). Massive MIMO has been
proved to be a viable technology to achieve these goals by spatially
multiplexing several users using many base station (BS) antennas. A potential
limitation of Massive MIMO in multicell systems is pilot contamination, which
arises in the channel estimation process from the interference caused by
reusing pilots in neighboring cells. A standard method to reduce pilot
contamination, known as regular pilot (RP), is to adjust the length of pilot
sequences while transmitting data and pilot symbols disjointly. An alternative
method, called superimposed pilot (SP), sends a superposition of pilot and data
symbols. This allows to use longer pilots which, in turn, reduces pilot
contamination. We consider the uplink of a multicell Massive MIMO network using
maximum ratio combining detection and compare RP and SP in terms of SE and EE.
To this end, we derive rigorous closed-form achievable rates with SP under a
practical random BS deployment. We prove that the reduction of pilot
contamination with SP is outweighed by the additional coherent and non-coherent
interference. Numerical results show that when both methods are optimized, RP
achieves comparable SE and EE to SP in practical scenarios.



Estimate Exchange over Network is Good for Distributed Hard Thresholding Pursuit

We investigate an existing distributed algorithm for learning sparse signals
or data over networks. The algorithm is iterative and exchanges intermediate
estimates of a sparse signal over a network. This learning strategy using
exchange of intermediate estimates over the network requires a limited
communication overhead for information transmission. Our objective in this
article is to show that the strategy is good for learning in spite of limited
communication. In pursuit of this objective, we first provide a restricted
isometry property (RIP)-based theoretical analysis on convergence of the
iterative algorithm. Then, using simulations, we show that the algorithm
provides competitive performance in learning sparse signals vis-a-vis an
existing alternate distributed algorithm. The alternate distributed algorithm
exchanges more information including observations and system parameters.



Single-pixel imaging with Morlet wavelet correlated random patterns

Single-pixel imaging is an indirect imaging technique which utilizes
simplified optical hardware and advanced computational methods. It offers novel
solutions for hyper-spectral imaging, polarimetric imaging, three-dimensional
imaging, holographic imaging, optical encryption and imaging through scattering
media. The main limitations for its use come from relatively high measurement
and reconstruction times. In this paper we propose to reduce the required
signal acquisition time by using a novel sampling scheme based on a random
selection of Morlet wavelets convolved with white noise. While such functions
exhibit random properties, they are locally determined by Morlet wavelet
parameters. The proposed method is equivalent to random sampling of the
properly selected part of the feature space, which maps the measured images
accurately both in the spatial and spatial frequency domains. We compare both
numerically and experimentally the image quality obtained with our sampling
protocol against widely-used sampling with Walsh-Hadamard or noiselet
functions. The results show considerable improvement over the former methods,
enabling single-pixel imaging at low compression rates on the order of a few
percent.



Heterogeneous Networked Data Recovery from Compressive Measurements Using a Copula Prior

Large-scale data collection by means of wireless sensor network and
internet-of-things technology poses various challenges in view of the
limitations in transmission, computation, and energy resources of the
associated wireless devices. Compressive data gathering based on compressed
sensing has been proven a well-suited solution to the problem. Existing designs
exploit the spatiotemporal correlations among data collected by a specific
sensing modality. However, many applications, such as environmental monitoring,
involve collecting heterogeneous data that are intrinsically correlated. In
this study, we propose to leverage the correlation from multiple heterogeneous
signals when recovering the data from compressive measurements. To this end, we
propose a novel recovery algorithm---built upon belief-propagation
principles---that leverages correlated information from multiple heterogeneous
signals. To efficiently capture the statistical dependencies among diverse
sensor data, the proposed algorithm uses the statistical model of copula
functions. Experiments with heterogeneous air-pollution sensor measurements
show that the proposed design provides significant performance improvements
against state-of-the-art compressive data gathering and recovery schemes that
use classical compressed sensing, compressed sensing with side information, and
distributed compressed sensing.



Measurement of amplitude of the moir\'e patterns in digital autostereoscopic 3D display

The article presents the experimental measurements of the amplitude of the
moir\'e patterns in a digital autostereoscopic barrier-type 3D display across a
wide angular range with a small increment. The period and orientation of the
moir\'e patterns were also measured as functions of the angle. Simultaneous
branches are observed and analyzed. The theoretical interpretation is also
given. The results can help preventing or minimizing the moir\'e effect in
displays.



SNR-based adaptive acquisition method for fast Fourier ptychographic microscopy

Fourier ptychographic microscopy (FPM) is a computational imaging technique
with both high resolution and large field-of-view. However, the effective
numerical aperture (NA) achievable with a typical LED panel is ambiguous and
usually relies on the repeated tests of different illumination NAs. The imaging
quality of each raw image usually depends on the visual assessments, which is
subjective and inaccurate especially for those dark field images. Moreover, the
acquisition process is really time-consuming.In this paper, we propose a
SNR-based adaptive acquisition method for quantitative evaluation and adaptive
collection of each raw image according to the signal-to-noise ration (SNR)
value, to improve the FPM's acquisition efficiency and automatically obtain the
maximum achievable NA, reducing the time of collection, storage and subsequent
calculation. The widely used EPRY-FPM algorithm is applied without adding any
algorithm complexity and computational burden. The performance has been
demonstrated in both USAF targets and biological samples with different imaging
sensors respectively, which have either Poisson or Gaussian noises model.
Further combined with the sparse LEDs strategy, the number of collection images
can be shorten to around 25 frames while the former needs 361 images, the
reduction ratio can reach over 90%. This method will make FPM more practical
and automatic, and can also be used in different configurations of FPM.



Elliptification of Rectangular Imagery

We present and discuss different algorithms for converting rectangular
imagery into elliptical regions. We mainly focus on methods that use
mathematical mappings with explicit and invertible equations. The key idea is
to start with invertible mappings between the square and the circular disc then
extend it to handle rectangles and ellipses. This extension can be done by
simply removing the eccentricity and reintroducing it back after using a chosen
square-to-disc mapping.



Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data

We present a factorized hierarchical variational autoencoder, which learns
disentangled and interpretable representations from sequential data without
supervision. Specifically, we exploit the multi-scale nature of information in
sequential data by formulating it explicitly within a factorized hierarchical
graphical model that imposes sequence-dependent priors and sequence-independent
priors to different sets of latent variables. The model is evaluated on two
speech corpora to demonstrate, qualitatively, its ability to transform speakers
or linguistic content by manipulating different sets of latent variables; and
quantitatively, its ability to outperform an i-vector baseline for speaker
verification and reduce the word error rate by as much as 35% in mismatched
train/test scenarios for automatic speech recognition tasks.



Neural Network Alternatives to Convolutive Audio Models for Source Separation

Convolutive Non-Negative Matrix Factorization model factorizes a given audio
spectrogram using frequency templates with a temporal dimension. In this paper,
we present a convolutional auto-encoder model that acts as a neural network
alternative to convolutive NMF. Using the modeling flexibility granted by
neural networks, we also explore the idea of using a Recurrent Neural Network
in the encoder. Experimental results on speech mixtures from TIMIT dataset
indicate that the convolutive architecture provides a significant improvement
in separation performance in terms of BSSeval metrics.



Linear-Array Photoacoustic Imaging Using Minimum Variance-Based Delay Multiply and Sum Adaptive Beamforming Algorithm

In Photoacoustic imaging (PA), Delay-and-Sum (DAS) beamformer is a common
beamforming algorithm having a simple implementation. However, it results in a
poor resolution and high sidelobes. To address these challenges, a new
algorithm namely Delay-Multiply-and-Sum (DMAS) was introduced having lower
sidelobes compared to DAS. To improve the resolution of DMAS, a novel
beamformer is introduced using Minimum Variance (MV) adaptive beamforming
combined with DMAS, so-called Minimum Variance-Based DMAS (MVB-DMAS). It is
shown that expanding the DMAS equation results in multiple terms representing a
DAS algebra. It is proposed to use the MV adaptive beamformer instead of the
existing DAS. MVB-DMAS is evaluated numerically and experimentally. In
particular, at the depth of 45 mm MVB-DMAS results in about 31 dB, 18 dB and 8
dB sidelobes reduction compared to DAS, MV and DMAS, respectively. The
quantitative results of the simulations show that MVB-DMAS leads to improvement
in full-width-half-maximum about 96 %, 94 % and 45 % and signal-to-noise ratio
about 89 %, 15 % and 35 % compared to DAS, DMAS, MV, respectively. In
particular, at the depth of 33 mm of the experimental images, MVB-DMAS results
in about 20 dB sidelobes reduction in comparison with other beamformers.



Reliability assessment of microgrid with renewable generation and prioritized loads

With the increase in awareness about the climate change, there has been a
tremendous shift towards utilizing renewable energy sources (RES). In this
regard, smart grid technologies have been presented to facilitate higher
penetration of RES. Microgrids are the key components of the smart grids.
Microgrids allow integration of various distributed energy resources (DER) such
as the distributed generation (DGs) and energy storage systems (ESSs) into the
distribution system and hence remove or delay the need for distribution
expansion. One of the crucial requirements for utilities is to ensure that the
system reliability is maintained with the inclusion of microgrid topology.
Therefore, this paper evaluates the reliability of a microgrid containing
prioritized loads and distributed RES through a hybrid analytical-simulation
method. The stochasticity of RES introduces complexity to the reliability
evaluation. The method takes into account the variability of RES through Monte-
Carlo state sampling simulation. The results indicate the reliability
enhancement of the overall system in the presence of the microgrid topology. In
particular, the highest priority load has the largest improvement in the
reliability indices. Furthermore, sensitivity analysis is performed to
understand the effects of the failure of microgrid islanding in the case of a
fault in the upstream network.



Cloud-aided collaborative estimation by ADMM-RLS algorithms for connected vehicle prognostics

As the connectivity of consumer devices is rapidly growing and cloud
computing technologies are becoming more widespread, cloud-aided techniques for
parameter estimation can be designed to exploit the theoretically unlimited
storage memory and computational power of the cloud, while relying on
information provided by multiple sources. With the ultimate goal of developing
monitoring and diagnostic strategies, this report focuses on the design of a
Recursive Least-Squares (RLS) based estimator for identification over a group
of devices connected to the cloud. The proposed approach, that relies on
Node-to-Cloud-to-Node (N2C2N) transmissions, is designed so that: (i) estimates
of the unknown parameters are computed locally and (ii) the local estimates are
refined on the cloud. The proposed approach requires minimal changes to local
(pre-existing) RLS estimators.



Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks

A method for statistical parametric speech synthesis incorporating generative
adversarial networks (GANs) is proposed. Although powerful deep neural networks
(DNNs) techniques can be applied to artificially synthesize speech waveform,
the synthetic speech quality is low compared with that of natural speech. One
of the issues causing the quality degradation is an over-smoothing effect often
observed in the generated speech parameters. A GAN introduced in this paper
consists of two neural networks: a discriminator to distinguish natural and
generated samples, and a generator to deceive the discriminator. In the
proposed framework incorporating the GANs, the discriminator is trained to
distinguish natural and generated speech parameters, while the acoustic models
are trained to minimize the weighted sum of the conventional minimum generation
loss and an adversarial loss for deceiving the discriminator. Since the
objective of the GANs is to minimize the divergence (i.e., distribution
difference) between the natural and generated speech parameters, the proposed
method effectively alleviates the over-smoothing effect on the generated speech
parameters. We evaluated the effectiveness for text-to-speech and voice
conversion, and found that the proposed method can generate more natural
spectral parameters and $F_0$ than conventional minimum generation error
training algorithm regardless its hyper-parameter settings. Furthermore, we
investigated the effect of the divergence of various GANs, and found that a
Wasserstein GAN minimizing the Earth-Mover's distance works the best in terms
of improving synthetic speech quality.



Time-Reversal Routing for Dispersion Code Multiple Access (DCMA) Communications

We present the modeling and characterization of a time-reversal routing
dispersion code multiple access (TR-DCMA) system. We show that this system
maintains the low complexity advantage of DCMA transceivers while offering
dynamic adaptivity for practial communication scenarios. We first derive the
mathematical model and explain operation principles of the system, and then
characterize its interference, signal to interference ratio, and bit error
probability characteristics.



Magnetless Circulators Based on Spatiotemporal Modulation of Bandstop Filters in a Delta Topology

In this paper, we discuss the design rationale and guidelines to build
magnet-less circulators based on spatio-temporal modulation of resonant
junctions consisting of first-order bandstop filters connected in a delta
topology. Without modulation, the junction does not allow transmission between
its ports, however, when the natural oscillation frequencies of the constituent
LC filters are modulated in time with a suitable phase pattern, a synthetic
angular-momentum bias can be effectively imparted to the junction and a
transmission window opens at one of the output ports, thus realizing a
circulator. We develop a rigorous small-signal linear model and find analytical
expressions for the harmonic S-parameters of the proposed circuit, which
significantly facilitate the design process. We validate the theory with
simulations and further discuss the large signal response, including power
handling and non-linearity, and the noise performance. Finally, we present
measured results with unprecedented performance in all metrics for a PCB
prototype using a Rogers board and off-the-shelf discrete components.



Pseudo-Linear Time-Invariant Magnetless Circulators Based on Differential Spatiotemporal Modulation of Resonant Junctions

In this paper, we present voltage- and current-mode differential magnetless
non-reciprocal devices obtained by pairing two single-ended (SE) circulators,
each consisting of three first-order bandpass or bandstop LC filters, connected
in either a wye or a delta topology. The resonant poles of each SE circulator
are modulated in time with 120 deg phase-shifted periodic signals, resulting in
synthetic angular-momentum biasing achieved through spatiotemporal modulation
(STM). We tailor the two SE circulators to exhibit a constant 180 deg phase
difference between their STM biases. Unlike conventional differential
time-variant circuits, for which only the even or odd spurs are rejected, we
show that the proposed configuration cancels out all intermodulation (IM)
products, thus making them operate alike linear time-invariant (LTI) circuits
for an external observer. In turn, this property enhances all metrics of the
resulting circulator, overcoming the limitations of SE architectures, and
improving insertion loss, impedance matching, bandwidth and noise figure. We
show that this differential architecture also significantly relaxes the
required modulation parameters, both in frequency and amplitude. We develop a
rigorous small-signal model to guide the design of the proposed circuits and to
get insights into their pseudo-LTI characteristics. Then, we validate the
theory with simulations and measurements showing remarkable performance
compared to the current state of the art of magnetless non-reciprocal devices.



Neuromorphic adaptive edge-preserving denoising filter

In this paper, we present on-sensor neuromorphic vision hardware
implementation of denoising spatial filter. The mean or median spatial filters
with fixed window shape are known for its denoising ability, however, have the
drawback of blurring the object edges. The effect of blurring increases with an
increase in window size. To preserve the edge information, we propose an
adaptive spatial filter that uses neuron's ability to detect similar pixels and
calculates the mean. The analog input differences of neighborhood pixels are
converted to the chain of pulses with voltage controlled oscillator and applied
as neuron input. When the input pulses charge the neuron to equal or greater
level than its threshold, the neuron will fire, and pixels are identified as
similar. The sequence of the neuron's responses for pixels is stored in the
serial-in-parallel-out shift register. The outputs of shift registers are used
as input to the selector switches of an averaging circuit making this an
adaptive mean operation resulting in an edge preserving mean filter. System
level simulation of the hardware is conducted using 150 images from Caltech
database with added Gaussian noise to test the robustness of edge-preserving
and denoising ability of the proposed filter. Threshold values of the hardware
neuron were adjusted so that the proposed edge-preserving spatial filter
achieves optimal performance in terms of PSNR and MSE, and these results
outperforms that of the conventional mean and median filters.



Performance Bounds for Parameter Estimation under Misspecified Models: Fundamental findings and applications

Inferring information from a set of acquired data is the main objective of
any signal processing (SP) method. In particular, the common problem of
estimating the value of a vector of parameters from a set of noisy measurements
is at the core of a plethora of scientific and technological advances in the
last decades; for example, wireless communications, radar and sonar,
biomedicine, image processing, and seismology, just to name a few. Developing
an estimation algorithm often begins by assuming a statistical model for the
measured data, i.e. a probability density function (pdf) which if correct,
fully characterizes the behaviour of the collected data/measurements.
Experience with real data, however, often exposes the limitations of any
assumed data model since modelling errors at some level are always present.
Consequently, the true data model and the model assumed to derive the
estimation algorithm could differ. When this happens, the model is said to be
mismatched or misspecified. Therefore, understanding the possible performance
loss or regret that an estimation algorithm could experience under model
misspecification is of crucial importance for any SP practitioner. Further,
understanding the limits on the performance of any estimator subject to model
misspecification is of practical interest. Motivated by the widespread and
practical need to assess the performance of a mismatched estimator, the goal of
this paper is to help to bring attention to the main theoretical findings on
estimation theory, and in particular on lower bounds under model
misspecification, that have been published in the statistical and econometrical
literature in the last fifty years. Secondly, some applications are discussed
to illustrate the broad range of areas and problems to which this framework
extends, and consequently the numerous opportunities available for SP
researchers.



A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech Enhancement

Despite noise suppression being a mature area in signal processing, it
remains highly dependent on fine tuning of estimator algorithms and parameters.
In this paper, we demonstrate a hybrid DSP/deep learning approach to noise
suppression. A deep neural network with four hidden layers is used to estimate
ideal critical band gains, while a more traditional pitch filter attenuates
noise between pitch harmonics. The approach achieves significantly higher
quality than a traditional minimum mean squared error spectral estimator, while
keeping the complexity low enough for real-time operation at 48 kHz on a
low-power processor.



Compressed Air Energy Storage-Part I: An Accurate Bi-linear Cavern Model

Compressed air energy storage (CAES) is suitable for large-scale energy
storage and can help to increase the penetration of wind power in power
systems. A CAES plant consists of compressors, expanders, caverns, and a
motor/generator set. Currently used cavern models for CAES are either accurate
but highly non-linear or linear but inaccurate. Highly non-linear cavern models
cannot be directly utilized in power system optimization problems. In this
regard, an accurate bi-linear cavern model for CAES is proposed in this first
paper of a two-part series. The charging and discharging processes in a cavern
are divided into several virtual states and then the first law of
thermodynamics and ideal gas law are used to derive a cavern model, i.e., model
for the variation of temperature and pressure in these processes. Thereafter,
the heat transfer between the air in the cavern and the cavern wall is
considered and integrated into the cavern model. By subsequently eliminating
several negligible terms, the cavern model reduces to a bi-linear (linear)
model for CAES with multiple (single) time steps. The accuracy of the proposed
cavern model is verified via comparison with an accurate non-linear model.



Compressed Air Energy Storage-Part II: Application to Power System Unit Commitment

Unit commitment (UC) is one of the most important power system operation
problems. To integrate higher penetration of wind power into power systems,
more compressed air energy storage (CAES) plants are being built. Existing
cavern models for the CAES used in power system optimization problems are not
accurate, which may lead to infeasible solutions, e.g., the air pressure in the
cavern is outside its operating range. In this regard, an accurate CAES model
is proposed for the UC problem based on the accurate bi-linear cavern model
proposed in the first paper of this two-part series. The minimum switch time
between the charging and discharging processes of CAES is considered. The whole
model, i.e., the UC model with an accurate CAES model, is a large-scale mixed
integer bi-linear programming problem. To reduce the complexity of the whole
model, three strategies are proposed to reduce the number of bi-linear terms
without sacrificing accuracy. McCormick relaxation and piecewise linearization
are then used to linearize the whole model. To decrease the solution time, a
method to obtain an initial solution of the linearized model is proposed. A
modified RTS-79 system is used to verify the effectiveness of the whole model
and the solution methodology.



Massive MIMO 1-Bit DAC Transmission: A Low-Complexity Symbol Scaling Approach

We study multi-user massive multiple-input single-output (MISO) systems and
focus on downlink transmission, where the base station (BS) employs a large
antenna array with low-cost 1-bit digital-to-analog converters (DACs). The
direct combination of existing beamforming schemes with 1-bit DACs is shown to
lead to an error floor at medium-to-high SNR regime, due to the coarse
quantization of the DACs with limited precision. In this paper, based on the
constructive interference we consider both a quantized linear beamforming
scheme where we analytically obtain the optimal beamforming matrix, and a
non-linear mapping scheme where we directly design the transmit signal vector.
Due to the 1-bit quantization, the formulated optimization for the non-linear
mapping scheme is shown to be non-convex. To solve this problem, the non-convex
constraints of the 1-bit DACs are firstly relaxed, followed by an element-wise
normalization to satisfy the 1-bit DAC transmission. We further propose a
low-complexity symbol scaling scheme that consists of three stages, in which
the quantized transmit signal on each antenna element is selected sequentially.
Numerical results show that the proposed symbol scaling scheme achieves a
comparable performance to the optimization-based non-linear mapping approach,
while its corresponding complexity is negligible compared to that of the
non-linear scheme.



Biosignal Analysis with Matching-Pursuit Based Adaptive Chirplet Transform

Chirping phenomena, in which the instantaneous frequencies of a signal change
with time, are abundant in signals related to biological systems. Biosignals
are non-stationary in nature and the time-frequency analysis is a viable tool
to analyze them. It is well understood that Gaussian chirplet function is
critical in describing chirp signals. Despite the theory of adaptive chirplet
transform (ACT) has been established for more than two decades and is well
accepted in the community of signal processing, application of ACT to
bio-/biomedical signal analysis is still quite limited, probably because that
the power of ACT, as an emerging tool for biosignal analysis, has not yet been
fully appreciated by the researchers in the field of biomedical engineering. In
this paper, we describe a novel ACT algorithm based on the "coarse-refinement"
scheme. Namely, the initial estimate of a chirplet is implemented with the
matching-pursuit (MP) algorithm and subsequently it is refined using the
expectation-maximization (EM) algorithm, which we coin as MPEM algorithm. We
emphasize the robustness enhancement of the algorithm in face of noise, which
is important to biosignal analysis, as they are usually embedded in strong
background noise. We then demonstrate the capability of our algorithm by
applying it to the analysis of representative biosignals, including visual
evoked potentials (bioelectrical signals), audible heart sounds and bat
ultrasonic echolocation signals (bioacoustic signals), and human speech. The
results show that the MPEM algorithm provides more compact representation of
signals under investigation and clearer visualization of their time-frequency
structures, indicating considerable promise of ACT in biosignal analysis. The
MATLAB code repository is hosted on GitHub for free download
(https://github.com/jiecui/mpact).



Predicting interviewee attitude and body language from speech descriptors

This present research investigated the relationship between personal
impressions and the acoustic nonverbal communication conveyed by employees
being interviewed. First, we investigated the extent to which different
conversation topics addressed during the interview induced changes in the
interviewees' acoustic parameters. Next, we attempted to predict the observed
and self-assessed attitudes and body language of the interviewees based on the
acoustic data. The results showed that topicality caused significant deviations
in the acoustic parameters statistics, but the ability to predict the personal
perceptions of the interviewees based on their acoustic non-verbal
communication was relatively independent of topicality, due to the natural
redundancy inherent in acoustic attributes. Our findings suggest that joint
modeling of speech and visual cues may improve the assessment of interviewee
profiles.



Compressed Sensing by Shortest-Solution Guided Decimation

Compressed sensing is an important problem in many fields of science and
engineering. It reconstructs signals by finding sparse solutions to
underdetermined linear equations. In this work we propose a deterministic and
non-parametric algorithm SSD (Shortest-Solution guided Decimation) to construct
support of the sparse solution under the guidance of the dense least-squares
solution of the recursively decimated linear equation. The most significant
feature of SSD is its insensitivity to correlations in the sampling matrix.
Using extensive numerical experiments we show that SSD greatly outperforms
L1-norm based methods, Orthogonal Least Squares, Orthogonal Matching Pursuit,
and Approximate Message Passing when the sampling matrix contains strong
correlations. This nice property of correlation tolerance makes SSD a versatile
and robust tool for different types of real-world signal acquisition tasks.



Simple Signal Extension Method for Discrete Wavelet Transform

Discrete wavelet transform of finite-length signals must necessarily handle
the signal boundaries. The state-of-the-art approaches treat such boundaries in
a complicated and inflexible way, using special prolog or epilog phases. This
holds true in particular for images decomposed into a number of scales,
exemplary in JPEG 2000 coding system. In this paper, the state-of-the-art
approaches are extended to perform the treatment using a compact streaming
core, possibly in multi-scale fashion. We present the core focused on CDF 5/3
wavelet and the symmetric border extension method, both employed in the JPEG
2000. As a result of our work, every input sample is visited only once, while
the results are produced immediately, i.e. without buffering.



Coexistence between Communication and Radar Systems - A Survey

Data traffic demand in cellular networks has been tremendously growing and
has led to creating congested RF environment. Accordingly, innovative
approaches for spectrum sharing have been proposed and implemented to
accommodate several systems within the same frequency band. Spectrum sharing
between radar and communication systems is one of the important research and
development areas. In this paper, we present the fundamental spectrum sharing
concepts and technologies, then we provide an updated and comprehensive survey
of spectrum sharing techniques that have been developed to enable some of the
wireless communication systems to coexist in the same band as radar systems.



Two asymptotic approaches for the exponential signal and harmonic noise in Singular Spectrum Analysis

The general theoretical approach to the asymptotic extraction of the signal
series from the perturbed signal with the help of Singular Spectrum Analysis
(briefly, SSA) was already outlined in Nekrutkin 2010, SII, v. 3, 297--319.
  In this paper we consider the example of such an analysis applied to the
increasing exponential signal and the sinusoidal noise. It is proved that if
the signal rapidly tends to infinity, then the so-called reconstruction errors
of SSA do not uniformly tend to zero as the series length tends to infinity.
More precisely, in this case any finite number of last terms of the error
series do not tend to any finite or infinite values.
  On the contrary, for the "discretization" scheme with the bounded from above
exponential signal, all elements of the error series tend to zero.
  This effect shows that the discretization model can be an effective tool in
the theoretical SSA considerations with increasing signals.



Asymptotic robustness of Kelly's GLRT and Adaptive Matched Filter detector under model misspecification

A fundamental assumption underling any Hypothesis Testing (HT) problem is
that the available data follow the parametric model assumed to derive the test
statistic. Nevertheless, a perfect match between the true and the assumed data
models cannot be achieved in many practical applications. In all these cases,
it is advisable to use a robust decision test, i.e. a test whose statistic
preserves (at least asymptotically) the same probability density function (pdf)
for a suitable set of possible input data models under the null hypothesis.
Building upon the seminal work of Kent (1982), in this paper we investigate the
impact of the model mismatch in a recurring HT problem in radar signal
processing applications: testing the mean of a set of Complex Elliptically
Symmetric (CES) distributed random vectors under a possible misspecified,
Gaussian data model. In particular, by using this general misspecified
framework, a new look to two popular detectors, the Kelly's Generalized
Likelihood Ration Test (GLRT) and the Adaptive Matched Filter (AMF), is
provided and their robustness properties investigated.



Learning a Predictive Model for Music Using PULSE

Predictive models for music are studied by researchers of algorithmic
composition, the cognitive sciences and machine learning. They serve as base
models for composition, can simulate human prediction and provide a
multidisciplinary application domain for learning algorithms. A particularly
well established and constantly advanced subtask is the prediction of
monophonic melodies. As melodies typically involve non-Markovian dependencies
their prediction requires a capable learning algorithm. In this thesis, I apply
the recent feature discovery and learning method PULSE to the realm of symbolic
music modeling. PULSE is comprised of a feature generating operation and
L1-regularized optimization. These are used to iteratively expand and cull the
feature set, effectively exploring feature spaces that are too large for common
feature selection approaches. I design a general Python framework for PULSE,
propose task-optimized feature generating operations and various
music-theoretically motivated features that are evaluated on a standard corpus
of monophonic folk and chorale melodies. The proposed method significantly
outperforms comparable state-of-the-art models. I further discuss the free
parameters of the learning algorithm and analyze the feature composition of the
learned models. The models learned by PULSE afford an easy inspection and are
musicologically interpreted for the first time.



A Centralized Power Control and Management Method for Grid-Connected Photovoltaic (PV)-Battery Systems

Distributed Generation (DG) is an effective way of integrating renewable
energy sources to conventional power grid, which improves the reliability and
efficiency of power systems. Photovoltaic (PV) systems are ideal DGs thanks to
their attractive benefits, such as availability of solar energy and low
installation costs. Battery groups are used in PV systems to balance the power
flows and eliminate power fluctuations due to change of operating condition,
e.g., irradiance and temperature variation. In an attempt to effectively manage
the power flows, this paper presents a novel power control and management
system for grid-connected PV-Battery systems. The proposed system realizes the
maximum power point tracking (MPPT) of the PV panels, stabilization of the DC
bus voltage for load plug-and-play access, balance among the power flows, and
quick response of both active and reactive power demands.



Research on several key technologies in practical speech emotion recognition

In this dissertation the practical speech emotion recognition technology is
studied, including several cognitive related emotion types, namely fidgetiness,
confidence and tiredness. The high quality of naturalistic emotional speech
data is the basis of this research. The following techniques are used for
inducing practical emotional speech: cognitive task, computer game, noise
stimulation, sleep deprivation and movie clips.
  A practical speech emotion recognition system is studied based on Gaussian
mixture model. A two-class classifier set is adopted for performance
improvement under the small sample case. Considering the context information in
continuous emotional speech, a Gaussian mixture model embedded with Markov
networks is proposed.
  A further study is carried out for system robustness analysis. First, noise
reduction algorithm based on auditory masking properties is fist introduced to
the practical speech emotion recognition. Second, to deal with the complicated
unknown emotion types under real situation, an emotion recognition method with
rejection ability is proposed, which enhanced the system compatibility against
unknown emotion samples. Third, coping with the difficulties brought by a large
number of unknown speakers, an emotional feature normalization method based on
speaker-sensitive feature clustering is proposed. Fourth, by adding the
electrocardiogram channel, a bi-modal emotion recognition system based on
speech signals and electrocardiogram signals is first introduced.
  The speech emotion recognition methods studied in this dissertation may be
extended into the cross-language speech emotion recognition and the whispered
speech emotion recognition.



Real time text localization for Indoor Mobile Robot Navigation

Scene text is an important feature to be extracted, especially in
vision-based mobile robot navigation as many potential landmarks such as
nameplates and information signs contain text. In this paper, a novel two-step
text localization method for Indoor Mobile Robot Navigation is introduced. This
method is based on morphological operators and machine learning techniques and
can be used in real time environments. The proposed method has two steps. At
First, a new set of morphological operators is applied with a particular
sequence to extract high contrast areas that have high probability of text
existence. Using of morphological operators has many advantages such as: high
computation speed, being invariant to several geometrical transformations like
translation, rotations, and scaling, and being able to extract all areas
containing text. After extracting text candidate regions, a set of nine
features are extracted for accurate detection and deletion of the regions that
don't have text. These features are descriptors for texture properties and are
computed in real time. Then, we use a SVM classifier to detect the existence of
text in the region. Performance of the proposed algorithm is compared against a
number of widely used text localization algorithms and the results show that
this method can quickly and effectively localize and extract text regions from
real scenes and can be used in mobile robot navigation under an indoor
environment to detect text based landmarks.



On the Complex Network Structure of Musical Pieces: Analysis of Some Use Cases from Different Music Genres

This paper focuses on the modeling of musical melodies as networks. Notes of
a melody can be treated as nodes of a network. Connections are created whenever
notes are played in sequence. We analyze some main tracks coming from different
music genres, with melodies played using different musical instruments. We find
out that the considered networks are, in general, scale free networks and
exhibit the small world property. We measure the main metrics and assess
whether these networks can be considered as formed by sub-communities. Outcomes
confirm that peculiar features of the tracks can be extracted from this
analysis methodology. This approach can have an impact in several multimedia
applications such as music didactics, multimedia entertainment, and digital
music generation.



A Generative Model for Score Normalization in Speaker Recognition

We propose a theoretical framework for thinking about score normalization,
which confirms that normalization is not needed under (admittedly fragile)
ideal conditions. If, however, these conditions are not met, e.g. under
data-set shift between training and runtime, our theory reveals dependencies
between scores that could be exploited by strategies such as score
normalization. Indeed, it has been demonstrated over and over experimentally,
that various ad-hoc score normalization recipes do work. We present a first
attempt at using probability theory to design a generative score-space
normalization model which gives similar improvements to ZT-norm on the
text-dependent RSR 2015 database.



Efficient Convolutional Neural Network For Audio Event Detection

Wireless distributed systems as used in sensor networks, Internet-of-Things
and cyber-physical systems, impose high requirements on resource efficiency.
Advanced preprocessing and classification of data at the network edge can help
to decrease the communication demand and to reduce the amount of data to be
processed centrally. In the area of distributed acoustic sensing, the
combination of algorithms with a high classification rate and
resource-constraint embedded systems is essential. Unfortunately, algorithms
for acoustic event detection have a high memory and computational demand and
are not suited for execution at the network edge. This paper addresses these
aspects by applying structural optimizations to a convolutional neural network
for audio event detection to reduce the memory requirement by a factor of more
than 500 and the computational effort by a factor of 2.1 while performing 9.2%
better.



Modeling Transmission and Radiation Effects when Exploiting Power Line Networks for Communication

Power distribution grids are exploited by Power Line Communication (PLC)
technology to convey high frequency data signals. The natural conformation of
such power line networks causes a relevant part of the high frequency signals
traveling through them to be radiated instead of being conducted. This causes
not only electromagnetic interference (EMI) with devices positioned next to
power line cables, but also a consistent deterioration of the signal integrity.
Since existing PLC channel models do not take into account losses due to
radiation phenomena, this paper responds to the need of developing accurate
network simulators. A thorough analysis is herein presented about the conducted
and radiated effects on the signal integrity, digging into differential mode to
common mode signal conversion due to network imbalances. The outcome of this
work allows each network element to be described by a mixed-mode transmission
matrix. Furthermore, the classical per-unit-length equivalent circuit of
transmission lines is extended to incorporate radiation resistances. The
results of this paper lay the foundations for future developments of
comprehensive power line network models that incorporate conducted and radiated
phenomena.



Propagation based phase retrieval of simulated intensity measurements using artificial neural networks

Determining the phase of a wave from intensity measurements has many
applications in fields such as electron microscopy, visible light optics, and
medical imaging. Propagation based phase retrieval, where the phase is obtained
from defocused images, has shown significant promise. There are, however,
limitations in the accuracy of the retrieved phase arising from such methods.
Sources of error include shot noise, image misalignment, and diffraction
artifacts. We explore the use of artificial neural networks (ANNs) to improve
the accuracy of propagation based phase retrieval algorithms applied to
simulated intensity measurements. We employ a phase retrieval algorithm based
on the transport-of-intensity equation to obtain the phase from simulated
micrographs of procedurally generated specimens. We then train an ANN with
pairs of retrieved and exact phases, and use the trained ANN to process a test
set of retrieved phase maps. The total error in the phase is significantly
reduced using this method. We also discuss a variety of potential extensions to
this work.



An Opportunistic-Non Orthogonal Multiple Access based Cooperative Relaying system over Rician Fading Channels

Non-orthogonal Multiple Access (NOMA) has become a salient technology for
improving the spectral efficiency of the next generation 5G wireless
communication networks. In this paper, the achievable average rate of an
Opportunistic Non-Orthogonal Multiple Access (O-NOMA) based Cooperative
Relaying System (CRS) is studied under Rician fading channels with Channel
State Information (CSI) available at the source terminal. Based on CSI, for
opportunistic transmission, the source immediately chooses either the direct
transmission or the cooperative NOMA transmission using the relay, which can
provide better achievable average rate performance than the existing
Conventional-NOMA (C-NOMA) based CRS with no CSI at the source node.
Furthermore, a mathematical expression is also derived for the achievable
average rate and the results are compared with C-NOMA based CRS with no CSI at
the transmitter end, over a range of increasing power allocation coefficients,
transmit Signal-to-Noise Ratios (SNRs) and average channel powers. Numerical
results show that the CRS using O-NOMA with CSI achieves better spectral
efficiency in terms of the achievable average rate than the Conventional-NOMA
based CRS without CSI. To check the consistency of the derived analytical
results, Monte Carlo simulations are performed which verify that the results
are consistent and matched well with the simulation results.



Achievable Information Rates for Fiber Optics: Applications and Computations

In this paper, achievable information rates (AIR) for fiber optical
communications are discussed. It is shown that AIRs such as the mutual
information and generalized mutual information are good design metrics for
coded optical systems. The theoretical predictions of AIRs are compared to the
performance of modern codes including low-parity density check (LDPC) and polar
codes. Two different computation methods for these AIRs are also discussed:
Monte-Carlo integration and Gauss-Hermite quadrature. Closed-form ready-to-use
approximations for such computations are provided for arbitrary constellations
and the multidimensional AWGN channel. The computation of AIRs in optical
experiments and simulations is also discussed.



Analysis and Design of Cost-Effective, High-Throughput LDPC Decoders

This paper introduces a new approach to cost-effective, high-throughput
hardware designs for Low Density Parity Check (LDPC) decoders. The proposed
approach, called Non-Surjective Finite Alphabet Iterative Decoders (NS-FAIDs),
exploits the robustness of message-passing LDPC decoders to inaccuracies in the
calculation of exchanged messages, and it is shown to provide a unified
framework for several designs previously proposed in the literature. NS-FAIDs
are optimized by density evolution for regular and irregular LDPC codes, and
are shown to provide different trade-offs between hardware complexity and
decoding performance. Two hardware architectures targeting high-throughput
applications are also proposed, integrating both Min-Sum (MS) and NS-FAID
decoding kernels. ASIC post synthesis implementation results on 65nm CMOS
technology show that NS-FAIDs yield significant improvements in the throughput
to area ratio, by up to 58.75% with respect to the MS decoder, with even better
or only slightly degraded error correction performance.



Application of Compressive Sensing Techniques in Distributed Sensor Networks: A Survey

In this survey paper, our goal is to discuss recent advances of compressive
sensing (CS) based solutions in wireless sensor networks (WSNs) including the
main ongoing/recent research efforts, challenges and research trends in this
area. In WSNs, CS based techniques are well motivated by not only the sparsity
prior observed in different forms but also by the requirement of efficient
in-network processing in terms of transmit power and communication bandwidth
even with nonsparse signals. In order to apply CS in a variety of WSN
applications efficiently, there are several factors to be considered beyond the
standard CS framework. We start the discussion with a brief introduction to the
theory of CS and then describe the motivational factors behind the potential
use of CS in WSN applications. Then, we identify three main areas along which
the standard CS framework is extended so that CS can be efficiently applied to
solve a variety of problems specific to WSNs. In particular, we emphasize on
the significance of extending the CS framework to (i). take communication
constraints into account while designing projection matrices and reconstruction
algorithms for signal reconstruction in centralized as well in decentralized
settings, (ii) solve a variety of inference problems such as detection,
classification and parameter estimation, with compressed data without signal
reconstruction and (iii) take practical communication aspects such as
measurement quantization, physical layer secrecy constraints, and imperfect
channel conditions into account. Finally, open research issues and challenges
are discussed in order to provide perspectives for future research directions.



Frequency offset tolerant synchronization signal design in NB-IoT

Timing detection is the first step and very important in wireless
communication systems. Timing detection performance is usually affected by the
frequency offset. Therefore, it is a challenge to design the synchronization
signal in massive narrowband Internet of Things (NB-IoT) scenarios where the
frequency offset is usually large due to the low cost requirement. In this
paper, we firstly proposed a new general synchronization signal structure with
a couple of sequences which are conjugated to remove the potential timing error
arose from large frequency offset. Then, we analyze the suitable sequence for
our proposed synchronization signal structure and discuss a special ZC sequence
as an example. Finally, the simulation results demonstrate our proposed
synchronization signal can work well when the frequency offset is large. It
means that our proposed synchronization signal design is very suitable for the
massive NB-IoT.



Microcomb-based true-time-delay network for microwave beamforming with arbitrary beam pattern control

Microwave phased array antennas (PAAs) are very attractive to defense
applications and high-speed wireless communications for their abilities of fast
beam scanning and complex beam pattern control. However, traditional PAAs based
on phase shifters suffer from the beam-squint problem and have limited
bandwidths. True-time-delay (TTD) beamforming based on low-loss photonic delay
lines can solve this problem. But it is still quite challenging to build
large-scale photonic TTD beamformers due to their high hardware complexity. In
this paper, we demonstrate a photonic TTD beamforming network based on a
miniature microresonator frequency comb (microcomb) source and dispersive time
delay. A method incorporating optical phase modulation and programmable
spectral shaping is proposed for positive and negative apodization weighting to
achieve arbitrary microwave beam pattern control. The experimentally
demonstrated TTD beamforming network can support a PAA with 21 elements. The
microwave frequency range is $\mathbf{8\sim20\ {GHz}}$, and the beam scanning
range is $\mathbf{\pm 60.2^\circ}$. Detailed measurements of the microwave
amplitudes and phases are performed. The beamforming performances of Gaussian,
rectangular beams and beam notch steering are evaluated through simulations by
assuming a uniform radiating antenna array. The scheme can potentially support
larger PAAs with hundreds of elements by increasing the number of comb lines
with broadband microcomb generation.



Real-Time Wind Noise Detection and Suppression with Neural-Based Signal Reconstruction for Mult-Channel, Low-Power Devices

Active wind noise detection and suppression techniques are a new and
essential paradigm for enhancing ASR-based functionality with smart glasses, in
addition to other wearable and smart devices in the broader IoT (Internet of
things). In this paper, we develop two separate algorithms for wind noise
detection and suppression, respectively, operational in a challenging,
low-energy regime. Together, these algorithms comprise a robust wind noise
suppression system. In the first case, we advance a real-time wind detection
algorithm (RTWD) that uses two distinct sets of low-dimensional signal features
to discriminate the presence of wind noise with high accuracy. For wind noise
suppression, we employ an additional algorithm - attentive neural wind
suppression (ANWS) - that utilizes a neural network to reconstruct the wearer
speech signal from wind-corrupted audio in the spectral regions that are most
adversely affected by wind noise. Finally, we test our algorithms through
real-time experiments using low-power, multi-microphone devices with a wind
simulator under challenging detection criteria and a variety of wind
intensities.



UTD-CRSS Submission for MGB-3 Arabic Dialect Identification: Front-end and Back-end Advancements on Broadcast Speech

This study presents systems submitted by the University of Texas at Dallas,
Center for Robust Speech Systems (UTD-CRSS) to the MGB-3 Arabic Dialect
Identification (ADI) subtask. This task is defined to discriminate between five
dialects of Arabic, including Egyptian, Gulf, Levantine, North African, and
Modern Standard Arabic. We develop multiple single systems with different
front-end representations and back-end classifiers. At the front-end level,
feature extraction methods such as Mel-frequency cepstral coefficients (MFCCs)
and two types of bottleneck features (BNF) are studied for an i-Vector
framework. As for the back-end level, Gaussian back-end (GB), and Generative
Adversarial Networks (GANs) classifiers are applied alternately. The best
submission (contrastive) is achieved for the ADI subtask with an accuracy of
76.94% by augmenting the randomly chosen part of the development dataset.
Further, with a post evaluation correction in the submitted system, final
accuracy is increased to 79.76%, which represents the best performance achieved
so far for the challenge on the test dataset.



PLDA-Based Diarization of Telephone Conversations

This paper investigates the application of the probabilistic linear
discriminant analysis (PLDA) to speaker diarization of telephone conversations.
We introduce using a variational Bayes (VB) approach for inference under a PLDA
model for modeling segmental i-vectors in speaker diarization. Deterministic
annealing (DA) algorithm is imposed in order to avoid local optimal solutions
in VB iterations. We compare our proposed system with a well-known system that
applies k-means clustering on principal component analysis (PCA) coefficients
of segmental i-vectors. We used summed channel telephone data from the National
Institute of Standards and Technology (NIST) 2008 Speaker Recognition
Evaluation (SRE) as the test set in order to evaluate the performance of the
proposed system. We achieve about 20% relative improvement in Diarization Error
Rate (DER) compared to the baseline system.



Enhanced Linear-array Photoacoustic Beamforming using Modified Coherence Factor

Photoacoustic imaging (PAI) is a promising medical imaging modality providing
the spatial resolution of ultrasound (US) imaging and the contrast of pure
optical imaging. For linear-array PAI, a beamformer has to be used as the
reconstruction algorithm. Delay-and-sum (DAS) is the most prevalent beamforming
algorithm in PAI. However, using DAS beamformer leads to low resolution images
along with significant effects of the off-axis signals. Coherence factor (CF)
is a weighting method in which each pixel of the reconstructed image is
weighted, based on the spatial spectrum of the aperture, to improve the
contrast. In this paper, it has been shown that the numerator of the formula of
CF contains a DAS algebra, and it was proposed to use the
delay-multiply-and-sum (DMAS) beamformer instead of the available DAS on the
numerator. The proposed weighting technique, modified CF (MCF), has been
evaluated numerically and experimentally compared to CF, and it was shown that
MCF leads to lower sidelobes and better detectable targets. The quantitative
results of the experiment (using wire targets) show that MCF leads to for about
45% and 40% improvement, in comparison with CF, in the terms of signal-to-noise
ratio and full-width-half-maximum, respectively.



BER Performance of Uplink Massive MIMO With Low-Resolution ADCs

Massive multiple-input multiple-output (MIMO) is a promising technology for
next generation wireless communication systems (5G). In this technology, Base
Station (BS) is equipped with a large number of antennas. Employing high
resolution analog-to-digital converters (ADCs) for all antennas may cause high
costs and high power consumption for the BS. By performing numerical results,
we evaluate the use of low-resolution ADCs for uplink massive MIMO by analyzing
Bit Error Rate (BER) performance for different detection techniques (MMSE, ZF)
and different modulations (QPSK, 16-QAM) to find an optimal quantization
resolution. Our results reveal that the BER performance of uplink massive MIMO
systems with a few-bit resolution ADCs is comparable to the case of having full
precision ADCs. We found that the optimum choice of quantization level (number
of bits in ADCs) depends on the modulation technique and the number of antennas
at the BS.



Beam Switching Techniques for Millimeter Wave Vehicle to Infrastructure Communications

Beam alignment for millimeter wave (mm Wave) vehicular communications is
challenging due to the high mobility of vehicles. Recent studies have proposed
some beam switching techniques at Road Side Unit (RSU) for vehicle to
infrastructure (V2I) communications, employing initial position and speed
information of vehicles, that are sent through Dedicated Short Range
Communications (DSRC) to the RSU. However, inaccuracies of the provided
information lead to beam misalignment. Some beam design parameters are
suggested in the literature to combat this effect. But how these parameters
should be tuned? Here, we evaluate the effect of all these parameters, and
propose a beam design efficiency metric to perform beam alignment in the
presence of the estimation errors, and to improve the performance by choosing
the right design parameters.



Large-scale weakly supervised audio classification using gated convolutional neural network

In this paper, we present a gated convolutional neural network and a temporal
attention-based localization method for audio classification, which won the 1st
place in the large-scale weakly supervised sound event detection task of
Detection and Classification of Acoustic Scenes and Events (DCASE) 2017
challenge. The audio clips in this task, which are extracted from YouTube
videos, are manually labeled with one or a few audio tags but without
timestamps of the audio events, which is called as weakly labeled data. Two
sub-tasks are defined in this challenge including audio tagging and sound event
detection using this weakly labeled data. A convolutional recurrent neural
network (CRNN) with learnable gated linear units (GLUs) non-linearity applied
on the log Mel spectrogram is proposed. In addition, a temporal attention
method is proposed along the frames to predicate the locations of each audio
event in a chunk from the weakly labeled data. We ranked the 1st and the 2nd as
a team in these two sub-tasks of DCASE 2017 challenge with F value 55.6\% and
Equal error 0.73, respectively.



Performance analysis of FSO using relays and spatial diversity under log-normal fading channel

The performance analysis of free space optical communication (FSO) system
using relays and spatial diversity at the source is studied in this paper. The
effect of atmospheric turbulence and attenuation, caused by different weather
conditions and geometric losses, has also been considered for analysis. The
exact closed-form expressions are presented for bit error rate (BER) of M-ary
quadrature amplitude modulation (M-QAM) technique for multi-hop multiple-input
single-output (MISO) FSO system under log-normal fading channel. Furthermore,
the link performance of multi-hop MISO and multi-hop single-input and
single-output (SISO) FSO systems are compared to the different systems using
on-off keying (OOK), repetition codes (RCs) and M-ary pulse amplitude
modulation (M-PAM) techniques. A significant performance enhancement in terms
of BER analysis and SNR gains is shown for multi-hop MISO and multi-hop SISO
FSO systems with M-QAM over other existing systems with different modulation
schemes. Moreover, Monte-Carlo simulations are used to validate the accuracy
and consistency of the derived analytical results. Numerical results show that
M-QAM modulated multi-hop MISO and multi-hop SISO FSO system with relays and
spatial diversity outperforms other systems while having the same spectral
efficiency of each system.



Statistically Segregated k-Space Sampling for Accelerating Multiple-Acquisition MRI

A central limitation of multiple-acquisition magnetic resonance imaging (MRI)
is the degradation in scan efficiency as the number of distinct datasets grows.
Sparse recovery techniques can alleviate this limitation via randomly
undersampled acquisitions. A frequent sampling strategy is to prescribe for
each acquisition a different random pattern drawn from a common sampling
density. However, naive random patterns often contain gaps or clusters across
the acquisition dimension that in turn can degrade reconstruction quality or
reduce scan efficiency. To address this problem, a statistically-segregated
sampling method is proposed for multiple-acquisition MRI. This method generates
multiple patterns sequentially, while adaptively modifying the sampling density
to minimize k-space overlap across patterns. As a result, it improves
incoherence across acquisitions while still maintaining similar sampling
density across the radial dimension of k-space. Comprehensive simulations and
in vivo results are presented for phase-cycled balanced steady-state free
precession and multi-echo T$_2$-weighted imaging. Segregated sampling achieves
significantly improved quality in both Fourier and compressed-sensing
reconstructions of multiple-acquisition datasets.



Shannon information storage in noisy phase-modulated fringes and fringe-data compression by phase-shifting algorithms

Optical phase-modulated fringe-patterns are usually digitized with XxY pixels
and 8 bits/pixel (or higher) gray-levels. The digitized 8 bits/pixel are
raw-data bits, not Shannon information bits. Here we show that noisy
fringe-patterns store much less Shannon information than the capacity of the
digitizing camera. This means that high signal-to-noise ratio (S/N) cameras may
waste to noise most bits/pixel. For example one would not use smartphone
cameras for high quality phase-metrology, because of their lower (S/N) images.
However smartphones digitize high-resolution (12 megapixel) images, and as we
show here, the information storage of an image depends on its bandwidth and its
(S/N). The standard formalism for measuring information are the Shannon-entropy
H, and the Shannon capacity theorem (SCT). According to SCT, low (S/N) images
may be compensated with a larger fringe-bandwidth to obtain high-information
phase measurements. So broad bandwidth fringes may give high quality phase, in
spite of digitizing low (S/N) fringe images. Most real-life images are
redundant, they have smooth zones where the pixel-value do not change much, and
data compression algorithms are paramount for image transmission/storage.
Shannon's capacity theorem is used to gauge competing image compression
algorithms. Here we show that phase-modulated phase-shifted fringes are highly
correlated, and as a consequence, phase-shifting algorithms (PSAs) may be used
as fringe-data compressors. Therefore a PSA may compress a large number of
phase-shifted fringes into a single complex-valued image. This is important in
spaceborne optical/RADAR phase-telemetry where downlink is severely limited by
huge distance and low-power downlink. That is, instead of transmitting M
phase-shifted fringes, one only transmit the phase-demodulated signal as
compressed sensing data.



Equalization Methods for NLIN Mitigation

We investigate the potential of adaptive equalization techniques to mitigate
inter-channel nonlinear interference noise (NLIN). We derive a lower bound on
the mutual information of a system using adaptive equalization, showing that
the channel estimation error determines the equalizer's performance. We develop
an adaptive equalization scheme which uses the statistics of the NLIN to obtain
optimal detection, based on Kalman filtering and maximum likelihood sequence
estimation (MLSE). This scheme outperforms commonly used equalizers and
significantly increases performance.



The Dependence of Frequency Distributions on Multiple Meanings of Words, Codes and Signs

The dependence of the frequency distributions due to multiple meanings of
words in a text is investigated by deleting letters. By coding the words with
fewer letters the number of meanings per coded word increases. This increase is
measured and used as an input in a predictive theory. For a text written in
English, the word-frequency distribution is broad and fat-tailed, whereas if
the words are only represented by their first letter the distribution becomes
exponential. Both distribution are well predicted by the theory, as is the
whole sequence obtained by consecutively representing the words by the first
L=6,5,4,3,2,1 letters. Comparisons of texts written by Chinese characters and
the same texts written by letter-codes are made and the similarity of the
corresponding frequency-distributions are interpreted as a consequence of the
multiple meanings of Chinese characters. This further implies that the
difference of the shape for word-frequencies for an English text written by
letters and a Chinese text written by Chinese characters is due to the coding
and not to the language per se.



A simple and fast frequency domain analysis method for calculating the frequency response and linearity of electro-optic microring modulators

A fast and simple frequency domain method is introduced for the analysis of
microring modulator response using the Jacobi Anger expansion method. Resonance
frequency modulated microring (FMMR) modulators and coupling modulated
microring modulators (CMMR) are analyzed using this method. The linearity of
these modulators is analyzed. The third order intercept point (IP3) is
calculated for CMMR devices and compared to Mach Zehnder interferometer (MZI)
modulator devices. It is shown that CMMR devices can achieve a 12dB higher IP3
compared to MZI devices. CMMR devices have high second order nonlinearity,
while MZI devices second order nonlinearity is zero. A novel geometry based on
dual CMMR modulators is introduced to improve the second order nonlinearity of
CMMR modulators.



Optimal Resource Allocation in Ultra-low Power Fog-computing SWIPT-based Networks

In this paper, we consider a fog computing system consisting of a
multi-antenna access point (AP), an ultra-low power (ULP) single antenna device
and a fog server. The ULP device is assumed to be capable of both energy
harvesting (EH) and information decoding (ID) using a time-switching
simultaneous wireless information and power transfer (SWIPT) scheme. The ULP
device deploys the harvested energy for ID and either local computing or
offloading the computations to the fog server depending on which strategy is
most energy efficient. In this scenario, we optimize the time slots devoted to
EH, ID and local computation as well as the time slot and power required for
the offloading to minimize the energy cost of the ULP device. Numerical results
are provided to study the effectiveness of the optimized fog computing system
and the relevant challenges.



Phased Array-Based Sub-Nyquist Sampling for Joint Wideband Spectrum Sensing and Direction-of-Arrival Estimation

In this paper, we study the problem of joint wideband spectrum sensing and
direction-of-arrival (DoA) estimation in a sub-Nyquist sampling framework.
Specifically, considering a scenario where a few uncorrelated narrowband
signals spread over a wide (say, several GHz) frequency band, our objective is
to estimate the carrier frequencies and the DoAs associated with the narrowband
sources, as well as reconstruct the power spectra of these narrowband signals.
To overcome the sampling rate bottleneck for wideband spectrum sensing, we
propose a new phased-array based sub-Nyquist sampling architecture with
variable time delays, where a uniform linear array (ULA) is employed and the
received signal at each antenna is delayed by a variable amount of time and
then sampled by a synchronized low-rate analog-digital converter (ADC). Based
on the collected sub-Nyquist samples, we calculate a set of cross-correlation
matrices with different time lags, and develop a CANDECOMP/PARAFAC (CP)
decomposition-based method for joint DoA, carrier frequency and power spectrum
recovery. Perfect recovery conditions for the associated parameters and the
power spectrum are analyzed. Our analysis reveals that our proposed method does
not require to place any sparse constraint on the wideband spectrum, only needs
the sampling rate to be greater than the bandwidth of the narrowband source
signal with the largest bandwidth among all sources. Simulation results show
that our proposed method can achieve an estimation accuracy close to the
associated Cram\'{e}r-Rao bounds (CRBs) using only a small number of data
samples.



125 Gbps Pre-Compensated Nonlinear Frequency-Division Multiplexed Transmission

Record-high data rate of 125 Gb/s and SE over 2 bits/s/Hz in burst-mode
single-polarization NFDM transmissions were achieved over 976 km of SSMF with
EDFA-only amplification by transmitting and processing 222 32 QAM-modulated
nonlinear subcarriers simultaneously



Performance Analysis of Coherent and Noncoherent Modulation under I/Q Imbalance

In-phase/quadrature-phase Imbalance (IQI) is considered a major
performance-limiting impairment in direct-conversion transceivers. Its effects
become even more pronounced at higher carrier frequencies such as the
millimeter-wave frequency bands being considered for 5G systems. In this paper,
we quantify the effects of IQI on the performance of different modulation
schemes under multipath fading channels. This is realized by developing a
general framework for the symbol error rate (SER) analysis of coherent phase
shift keying, noncoherent differential phase shift keying and noncoherent
frequency shift keying under IQI effects. In this context, the moment
generating function of the signal-to-interference-plus-noise-ratio is first
derived for both single-carrier and multi-carrier systems suffering from
transmitter (TX) IQI only, receiver (RX) IQI only and joint TX/RX IQI.
Capitalizing on this, we derive analytic expressions for the SER of the
different modulation schemes. These expressions are corroborated by comparisons
with corresponding results from computer simulations and they provide insights
into the dependence of IQI on the system parameters. We demonstrate that the
effects of IQI differ considerably depending on the considered system as some
cases of single-carrier transmission appear robust to IQI, whereas
multi-carrier systems experiencing IQI at the RX require compensation in order
to achieve a reliable communication link.



Proactive Doppler Shift Compensation in Vehicular Cyber-Physical Systems

In vehicular cyber-physical systems (CPS), safety information, including
vehicular speed and location information, is shared among vehicles via wireless
waves at specific frequency. This helps control vehicle to alleviate traffic
congestion and road accidents. However, Doppler shift existing between vehicles
with high relative speed causes an apparent frequency shift for the received
wireless wave, which consequently decreases the reliability of the recovered
safety information and jeopardizes the safety of vehicular CPS. Passive
confrontation of Doppler shift at the receiver side is not applicable due to
multiple Doppler shifts at each receiver. In this paper, we provide a proactive
Doppler shift compensation algorithm based on the probabilistic graphical
model. Each vehicle pre-compensates its carrier frequency individually so that
there is no frequency shift from the desired carrier frequency between each
pair of transceiver. The pre-compensated offset for each vehicle is computed in
a distributed fashion in order to be adaptive to the distributed and dynamic
topology of vehicular CPS. Besides, the updating procedure is designed in a
broadcasting fashion to reduce communication burden. It is rigorously proved
that the proposed algorithm is convergence guaranteed even for systems with
packet drops and random communication delays. Simulations based on real map and
transportation data verify the accuracy and convergence property of the
proposed algorithm. It is shown that this method achieves almost the optimal
frequency compensation accuracy with an error approaching the Cram\'{e}r-Rao
lower bound.



GPR signal de-noise method based on variational mode decomposition

Compared with traditional empirical mode decomposition (EMD) methods,
variational mode decomposition (VMD) has strong theoretical foundation and high
operational efficiency. The VMD method is introduced to ground penetrating
radar (GPR) signal processing. The characteristics of GPR signals validate the
method of signal de-noising based on the VMD principle. The validity and
accuracy of the method are further verified via Ricker wavelet and forward
model GPR de-noising experiments. The method of VMD is evaluated in comparison
with traditional wavelet transform (WT) and EEMD (ensemble EMD) methods. The
method is subsequently used to analyze a GPR signal from a practical
engineering case. The results show that the method can effectively remove the
noise in the GPR data, and can obtain high signal-to-noise ratios (SNR) even
under strong background noise.



Dynamic Uplink/Downlink Resource Management in Flexible Duplex-Enabled Wireless Networks

Flexible duplex is proposed to adapt to the channel and traffic asymmetry for
future wireless networks. In this paper, we propose two novel algorithms within
the flexible duplex framework for joint uplink and downlink resource allocation
in multi-cell scenario, named SAFP and RMDI, based on the awareness of
interference coupling among wireless links. Numerical results show significant
performance gain over the baseline system with fixed uplink/downlink resource
configuration, and over the dynamic TDD scheme that independently adapts the
configuration to time-varying traffic volume in each cell. The proposed
algorithms achieve two-fold increase when compared with the baseline scheme,
measured by the worst-case quality of service satisfaction level, under a low
level of traffic asymmetry. The gain is more significant when the traffic is
highly asymmetric, as it achieves three-fold increase.



Improving Resource Efficiency with Partial Resource Muting for Future Wireless Networks

We propose novel resource allocation algorithms that have the objective of
finding a good tradeoff between resource reuse and interference avoidance in
wireless networks. To this end, we first study properties of functions that
relate the resource budget available to network elements to the optimal utility
and to the optimal resource efficiency obtained by solving max-min utility
optimization problems. From the asymptotic behavior of these functions, we
obtain a transition point that indicates whether a network is operating in an
efficient noise-limited regime or in an inefficient interference-limited regime
for a given resource budget. For networks operating in the inefficient regime,
we propose a novel partial resource muting scheme to improve the efficiency of
the resource utilization. The framework is very general. It can be applied not
only to the downlink of 4G networks, but also to 5G networks equipped with
flexible duplex mechanisms. Numerical results show significant performance
gains of the proposed scheme compared to the solution to the max-min utility
optimization problem with full frequency reuse.



The Impact of Transceiver Noise on Digital Nonlinearity Compensation

The efficiency of digital nonlinearity compensation (NLC) is analyzed in the
presence of noise arising from amplified spontaneous emission noise (ASE) as
well as from a non-ideal transceiver subsystem. Its impact on signal-to-noise
ratio (SNR) and reach increase is studied with particular emphasis on split
NLC, where the digital back-propagation algorithm is divided between
transmitter and receiver. An analytical model is presented to compute the SNR's
for non-ideal transmission systems with arbitrary split NLC configurations.
When signal-signal nonlinearities are compensated, the performance limitation
arises from residual signal-noise interactions. These interactions consist of
nonlinear beating between the signal and co-propagating ASE and transceiver
noise. While transceiver noise-signal beating is usually dominant for short
transmission distances, ASE noise-signal beating is dominant for larger
transmission distances. It is shown that both regimes behave differently with
respect to the optimal NLC split ratio and their respective reach gains.
Additionally, simple formulas for the prediction of the optimal NLC split ratio
and the reach increase in those two regimes are reported. It is found that
split NLC offers negligible gain with respect to conventional digital
back-propagation (DBP) for distances less than 1000 km using standard
single-mode fibers and a transceiver (back-to-back) SNR of 26 dB, when
transmitter and receiver inject the same amount of noise. However, when
transmitter and receiver inject an unequal amount of noise, reach gains of 56%
on top of DBP are achievable by properly tailoring the split NLC algorithm. The
theoretical findings are confirmed by numerical simulations.



Steepest Descent Multimodulus Algorithm for Blind Signal Retrieval in QAM Systems

We present steepest descent (SD) implementation of multimodulus algorithm
(MMA2-2) for blind signal retrieval in digital communication systems. In
comparison to stochastic approximate (gradient descent) realization, the
proposed SD implementation of MMA2-2 equalizer mitigates inter-symbol
interference with relatively smooth convergence and superior steady-state
performance.



Cache Placement in Fog-RANs: From Centralized to Distributed Algorithms

To deal with the rapid growth of high-speed and/or ultra-low latency data
traffic for massive mobile users, fog radio access networks (Fog-RANs) have
emerged as a promising architecture for next-generation wireless networks. In
Fog-RANs, the edge nodes and user terminals possess storage, computation and
communication functionalities to various degrees, which provides high
flexibility for network operation, i.e., from fully centralized to fully
distributed operation. In this paper, we study the cache placement problem in
Fog-RANs, by taking into account flexible physical-layer transmission schemes
and diverse content preferences of different users. We develop both centralized
and distributed transmission aware cache placement strategies to minimize
users' average download delay subject to the storage capacity constraints. In
the centralized mode, the cache placement problem is transformed into a matroid
constrained submodular maximization problem, and an approximation algorithm is
proposed to find a solution within a constant factor to the optimum. In the
distributed mode, a belief propagation based distributed algorithm is proposed
to provide a suboptimal solution, with iterative updates at each BS based on
locally collected information. Simulation results show that by exploiting
caching and cooperation gains, the proposed transmission aware caching
algorithms can greatly reduce the users' average download delay.



User-centric C-RAN Architecture for Ultra-dense 5G Networks: Challenges and Methodologies

Ultra-dense networks (UDN) constitute one of the most promising techniques of
supporting the 5G mobile system. By deploying more small cells in a fixed area,
the average distance between users and access points can be significantly
reduced, hence a dense spatial frequency reuse can be exploited. However,
severe interference is the major obstacle in UDN. Most of the contributions
deal with the interference by relying on cooperative game theory. This paper
advocates the application of dense user-centric C-RAN philosophy to UDN, thanks
to the recent development of cloud computing techniques. Under dense C-RAN,
centralized signal processing can be invoked for supporting CoMP transmission.
We summarize the main challenges in dense user-centric C-RANs. One of the most
challenging issues is the requirement of the global CSI for the sake of
cooperative transmission. We investigate this requirement by only relying on
partial CSI, namely, on inter-cluster large-scale CSI. Furthermore, the
estimation of the intra-cluster CSI is considered, including the pilot
allocation and robust transmission. Finally, we highlight several promising
research directions to make the dense user-centric C-RAN become a reality, with
special emphasis on the application of the `big data' techniques.



A Novel Mataheuristic based Interference Alignment for K-User Interference Channel : A Comparative Study

This paper presents a new Interference Alignment (IA) scheme for K-User
Multiple Input Multiple Output (MIMO) Interference Channel (IC) based on two
metaheuristics, namely Particle Swarm Optimization (PSO) and Artificial Bee
Colony (ABC) Algorithm. Tackling interference is an essential issue in wireless
communications to which Interference Alignment (IA) provides a promising
solution. However, IA still lacks of explicit and straightforward design
procedures. In fact, most of IA procedures aim to minimize a certain
Interference Leakage (IL) which measures the effect of the interference on the
network, this results in complex optimization tasks involving a large amount of
decision variables, together with a problem of convergence of the IA solutions.
In this paper the IA optimization is performed using PSO, ABC and their
cooperative counterparts, more suitable for large scale optimization. A
comparison between the four algorithms is also carried out. The cooperative
proposed approaches seem to be promising.



PSO and CPSO Based Interference Alignment for K-User MIMO Interference Channel

This paper investigates how to use a metaheuristic based technique, namely
Particle Swarm Optimization (PSO), in carrying out of Interference Alignment
(IA) for $K$-User MIMO Interference Channel (IC). Despite its increasing
popularity, mainly in wireless communications, IA lacks of explicit and
straightforward design procedures. Indeed, IA design results in complex
optimization tasks involving a large amount of decision variables, together
with a problem of convergence of the IA solutions. In this paper the IA
optimization is performed using PSO and Cooperative PSO (CPSO) more suitable
for large scale optimization, a comparison between the two versions is also
carried out. This approach seems to be promising.



Automated and Robust Quantification of Colocalization in Dual-Color Fluorescence Microscopy: A Nonparametric Statistical Approach

Colocalization is a powerful tool to study the interactions between
fluorescently labeled molecules in biological fluorescence microscopy. However,
existing techniques for colocalization analysis have not undergone continued
development especially in regards to robust statistical support. In this paper,
we examine two of the most popular quantification techniques for colocalization
and argue that they could be improved upon using ideas from nonparametric
statistics and scan statistics. In particular, we propose a new colocalization
metric that is robust, easily implementable, and optimal in a rigorous
statistical testing framework. Application to several benchmark datasets, as
well as biological examples, further demonstrates the usefulness of the
proposed technique.



Resolution limits on visual speech recognition

Visual-only speech recognition is dependent upon a number of factors that can
be difficult to control, such as: lighting; identity; motion; emotion and
expression. But some factors, such as video resolution are controllable, so it
is surprising that there is not yet a systematic study of the effect of
resolution on lip-reading. Here we use a new data set, the Rosetta Raven data,
to train and test recognizers so we can measure the affect of video resolution
on recognition accuracy. We conclude that, contrary to common practice,
resolution need not be that great for automatic lip-reading. However it is
highly unlikely that automatic lip-reading can work reliably when the distance
between the bottom of the lower lip and the top of the upper lip is less than
four pixels at rest.



Some observations on computer lip-reading: moving from the dream to the reality

In the quest for greater computer lip-reading performance there are a number
of tacit assumptions which are either present in the datasets (high resolution
for example) or in the methods (recognition of spoken visual units called
visemes for example). Here we review these and other assumptions and show the
surprising result that computer lip-reading is not heavily constrained by video
resolution, pose, lighting and other practical factors. However, the working
assumption that visemes, which are the visual equivalent of phonemes, are the
best unit for recognition does need further examination. We conclude that
visemes, which were defined over a century ago, are unlikely to be optimal for
a modern computer lip-reading system.



Which phoneme-to-viseme maps best improve visual-only computer lip-reading?

A critical assumption of all current visual speech recognition systems is
that there are visual speech units called visemes which can be mapped to units
of acoustic speech, the phonemes. Despite there being a number of published
maps it is infrequent to see the effectiveness of these tested, particularly on
visual-only lip-reading (many works use audio-visual speech). Here we examine
120 mappings and consider if any are stable across talkers. We show a method
for devising maps based on phoneme confusions from an automated lip-reading
system, and we present new mappings that show improvements for individual
talkers.



Photonic machine learning implementation for signal recovery in optical communications

Machine learning techniques have proven very efficient in assorted
classification tasks. Nevertheless, processing time-dependent high-speed
signals can turn into an extremely challenging task, especially when these
signals have been nonlinearly distorted. Recently, analogue hardware concepts
using nonlinear transient responses have been gaining significant interest for
fast information processing. Here, we introduce a simplified photonic reservoir
computing scheme for data classification of severely distorted optical
communication signals after extended fibre transmission. To this end, we
convert the direct bit detection process into a pattern recognition problem.
Using an experimental implementation of our photonic reservoir computer, we
demonstrate an improvement in bit-error-rate by two orders of magnitude,
compared to directly classifying the transmitted signal. This improvement
corresponds to an extension of the communication range by over 75%. While we do
not yet reach full real-time post-processing at telecom rates, we discuss how
future designs might close the gap.



Speaker-independent machine lip-reading with speaker-dependent viseme classifiers

In machine lip-reading, which is identification of speech from visual-only
information, there is evidence to show that visual speech is highly dependent
upon the speaker [1]. Here, we use a phoneme-clustering method to form new
phoneme-to-viseme maps for both individual and multiple speakers. We use these
maps to examine how similarly speakers talk visually. We conclude that broadly
speaking, speakers have the same repertoire of mouth gestures, where they
differ is in the use of the gestures.



A Graph Signal Processing View on Functional Brain Imaging

Modern neuroimaging techniques provide us with unique views on brain
structure and function; i.e., how the brain is wired, and where and when
activity takes place. Data acquired using these techniques can be analyzed in
terms of its network structure to reveal organizing principles at the systems
level. Graph representations are versatile models where nodes are associated to
brain regions and edges to structural or functional connections. Structural
graphs model neural pathways in white matter that are the anatomical backbone
between regions. Functional graphs are built based on functional connectivity,
which is a pairwise measure of statistical interdependency between activity
traces of regions. Therefore, most research to date has focused on analyzing
these graphs reflecting structure or function.
  Graph signal processing (GSP) is an emerging area of research where signals
recorded at the nodes of the graph are studied atop the underlying graph
structure. An increasing number of fundamental operations have been generalized
to the graph setting, allowing to analyze the signals from a new viewpoint.
Here, we review GSP for brain imaging data and discuss their potential to
integrate brain structure, contained in the graph itself, with brain function,
residing in the graph signals. We review how brain activity can be meaningfully
filtered based on concepts of spectral modes derived from brain structure. We
also derive other operations such as surrogate data generation or
decompositions informed by cognitive systems. In sum, GSP offers a novel
framework for the analysis of brain imaging data.



Finding phonemes: improving machine lip-reading

In machine lip-reading there is continued debate and research around the
correct classes to be used for recognition. In this paper we use a structured
approach for devising speaker-dependent viseme classes, which enables the
creation of a set of phoneme-to-viseme maps where each has a different quantity
of visemes ranging from two to 45. Viseme classes are based upon the mapping of
articulated phonemes, which have been confused during phoneme recognition, into
viseme groups. Using these maps, with the LiLIR dataset, we show the effect of
changing the viseme map size in speaker-dependent machine lip-reading, measured
by word recognition correctness and so demonstrate that word recognition with
phoneme classifiers is not just possible, but often better than word
recognition with viseme classifiers. Furthermore, there are intermediate units
between visemes and phonemes which are better still.



Decoding visemes: improving machine lipreading

To undertake machine lip-reading, we try to recognise speech from a visual
signal. Current work often uses viseme classification supported by language
models with varying degrees of success. A few recent works suggest phoneme
classification, in the right circumstances, can outperform viseme
classification. In this work we present a novel two-pass method of training
phoneme classifiers which uses previously trained visemes in the first pass.
With our new training algorithm, we show classification performance which
significantly improves on previous lip-reading results.



Decoding visemes: improving machine lipreading

Machine lipreading (MLR) is speech recognition from visual cues and a niche
research problem in speech processing & computer vision. Current challenges
fall into two groups: the content of the video, such as rate of speech or; the
parameters of the video recording e.g, video resolution. We show that HD video
is not needed to successfully lipread with a computer. The term "viseme" is
used in machine lipreading to represent a visual cue or gesture which
corresponds to a subgroup of phonemes where the phonemes are visually
indistinguishable. A phoneme is the smallest sound one can utter, because there
are more phonemes per viseme, maps between units show a many-to-one
relationship. Many maps have been presented, we compare these and our results
show Lee's is best. We propose a new method of speaker-dependent
phoneme-to-viseme maps and compare these to Lee's. Our results show the
sensitivity of phoneme clustering and we use our new knowledge to augment a
conventional MLR system. It has been observed in MLR, that classifiers need
training on test subjects to achieve accuracy. Thus machine lipreading is
highly speaker-dependent. Conversely speaker independence is robust
classification of non-training speakers. We investigate the dependence of
phoneme-to-viseme maps between speakers and show there is not a high
variability of visemes, but there is high variability in trajectory between
visemes of individual speakers with the same ground truth. This implies a
dependency upon the number of visemes within each set for each individual. We
show that prior phoneme-to-viseme maps rarely have enough visemes and the
optimal size, which varies by speaker, ranges from 11-35. Finally we decode
from visemes back to phonemes and into words. Our novel approach uses the
optimum range visemes within hierarchical training of phoneme classifiers and
demonstrates a significant increase in classification accuracy.



Visual speech recognition: aligning terminologies for better understanding

We are at an exciting time for machine lipreading. Traditional research
stemmed from the adaptation of audio recognition systems. But now, the computer
vision community is also participating. This joining of two previously
disparate areas with different perspectives on computer lipreading is creating
opportunities for collaborations, but in doing so the literature is
experiencing challenges in knowledge sharing due to multiple uses of terms and
phrases and the range of methods for scoring results.
  In particular we highlight three areas with the intention to improve
communication between those researching lipreading; the effects of
interchanging between speech reading and lipreading; speaker dependence across
train, validation, and test splits; and the use of accuracy, correctness,
errors, and varying units (phonemes, visemes, words, and sentences) to measure
system performance. We make recommendations as to how we can be more
consistent.



Visual gesture variability between talkers in continuous visual speech

Recent adoption of deep learning methods to the field of machine lipreading
research gives us two options to pursue to improve system performance. Either,
we develop end-to-end systems holistically or, we experiment to further our
understanding of the visual speech signal. The latter option is more difficult
but this knowledge would enable researchers to both improve systems and apply
the new knowledge to other domains such as speech therapy. One challenge in
lipreading systems is the correct labeling of the classifiers. These labels map
an estimated function between visemes on the lips and the phonemes uttered.
Here we ask if such maps are speaker-dependent? Prior work investigated
isolated word recognition from speaker-dependent (SD) visemes, we extend this
to continuous speech. Benchmarked against SD results, and the isolated words
performance, we test with RMAV dataset speakers and observe that with
continuous speech, the trajectory between visemes has a greater negative effect
on the speaker differentiation.



Understanding the visual speech signal

For machines to lipread, or understand speech from lip movement, they decode
lip-motions (known as visemes) into the spoken sounds. We investigate the
visual speech channel to further our understanding of visemes. This has
applications beyond machine lipreading; speech therapists, animators, and
psychologists can benefit from this work. We explain the influence of speaker
individuality, and demonstrate how one can use visemes to boost lipreading.



Improving Compression Based Dissimilarity Measure for Music Score Analysis

In this paper, we propose a way to improve the compression based
dissimilarity measure, CDM. We propose to use a modified value of the file
size, where the original CDM uses an unmodified file size. Our application is a
music score analysis. We have chosen piano pieces from five different
composers. We have selected 75 famous pieces (15 pieces for each composer). We
computed the distances among all pieces by using the modified CDM. We use the
K-nearest neighbor method when we estimate the composer of each piece of music.
The modified CDM shows improved accuracy. The difference is statistically
significant.



Independent Low-Rank Matrix Analysis Based on Parametric Majorization-Equalization Algorithm

In this paper, we propose a new optimization method for independent low-rank
matrix analysis (ILRMA) based on a parametric majorization-equalization
algorithm. ILRMA is an efficient blind source separation technique that
simultaneously estimates a spatial demixing matrix (spatial model) and the
power spectrograms of each estimated source (source model). In ILRMA, since
both models are alternately optimized by iterative update rules, the difference
in the convergence speeds between these models often results in a poor local
solution. To solve this problem, we introduce a new parameter that controls the
convergence speed of the source model and find the best balance between the
optimizations in the spatial and source models for ILRMA.



Cell Detection by Functional Inverse Diffusion and Non-negative Group Sparsity$-$Part I: Modeling and Inverse Problems

In this two-part paper, we present a novel framework and methodology to
analyze data from certain image-based biochemical assays, e.g., ELISPOT and
Fluorospot assays. In this first part, we start by presenting a physical
partial differential equations (PDE) model up to image acquisition for these
biochemical assays. Then, we use the PDEs' Green function to derive a novel
parametrization of the acquired images. This parametrization allows us to
propose a functional optimization problem to address inverse diffusion. In
particular, we propose a non-negative group-sparsity regularized optimization
problem with the goal of localizing and characterizing the biological cells
involved in the said assays. We continue by proposing a suitable discretization
scheme that enables both the generation of synthetic data and implementable
algorithms to address inverse diffusion. We end Part I by providing a
preliminary comparison between the results of our methodology and an expert
human labeler on real data. Part II is devoted to providing an accelerated
proximal gradient algorithm to solve the proposed problem and to the empirical
validation of our methodology.



Cram\'er-Rao Bounds for Blind Multichannel Estimation

In some estimation problems, not all the parameters can be identified, which
results in singularity of the Fisher Information Matrix (FIM). The Cram\'er-Rao
Bound (CRB), which is the inverse of the FIM, is then not defined. To
regularize the estimation problem, one can impose constraints on the parameters
and derive the corresponding CRBs. The correspondence between local
identifiability and FIM regularity is studied here. Furthermore the number of
FIM singularities is shown to be equal to the number of independent constraints
necessary to have a well-defined constrained CRB and local identifiability. In
general, many sets of constraints can render the parameters identifiable,
giving different values for the CRB, that are not always relevant. When the
constraints can be chosen, we propose a constrained CRB, the pseudo-inverse of
the FIM, which gives, for a minimum number of constraints, the lowest bound on
the mean squared estimation error. These results are applied to two approaches
to blind FIR multichannel estimation which allow identification of the channel
up to a scale or phase factor. These two approaches correspond to deterministic
and Gaussian models for the unknown channel inputs. The singularities of the
FIMs and local identifiability are studied and the corresponding constrained
CRBs are derived and interpreted.



Cell Detection by Functional Inverse Diffusion and Non-negative Group Sparsity$-$Part II: Proximal Optimization and Performance Evaluation

In this two-part paper, we present a novel framework and methodology to
analyze data from certain image-based biochemical assays, e.g., ELISPOT and
Fluorospot assays. In this second part, we focus on our algorithmic
contributions. We provide an algorithm for functional inverse diffusion that
solves the variational problem we posed in Part I. As part of the derivation of
this algorithm, we present the proximal operator for the non-negative
group-sparsity regularizer, which is a novel result that is of interest in
itself, also in comparison to previous results on the proximal operator of a
sum of functions. We then present a discretized approximated implementation of
our algorithm and evaluate it both in terms of operational cell-detection
metrics and in terms of distributional optimal-transport metrics.



Lung sound classification using local binary pattern

Lung sounds contain vital information about pulmonary pathology. In this
paper, we use short-term spectral characteristics of lung sounds to recognize
associated diseases. Motivated by the success of auditory perception based
techniques in speech signal classification, we represent time-frequency
information of lung sounds using mel-scale warped spectral coefficients, called
here as mel-frequency spectral coefficients (MFSCs). Next, we employ local
binary pattern analysis (LBP) to capture texture information of the MFSCs, and
the feature vectors are subsequently derived using histogram representation.
The proposed features are used with three well-known classifiers in this field:
k-nearest neighbor (kNN), artificial neural network (ANN), and support vector
machine (SVM). Also, the performance was tested with multiple SVM kernels. We
conduct extensive performance evaluation experiments using two databases which
include normal and adventitious sounds. Results show that the proposed features
with SVM and also with kNN classifier outperform commonly used wavelet-based
features as well as our previously investigated mel-frequency cepstral
coefficients (MFCCs) based statistical features, specifically in abnormal sound
detection. Proposed features also yield better results than morphological
features and energy features computed from rational dilation wavelet
coefficients. The Bhattacharyya kernel performs considerably better than other
kernels. Further, we optimize the configuration of the proposed feature
extraction algorithm. Finally, we have applied mRMR (minimum-redundancy
maximum-relevancy) based feature selection method to remove redundancy in the
feature vector which makes the proposed method computationally more efficient
without any degradation in the performance. The overall performance gain is up
to 24.5% as compared to the standard wavelet feature based system.



Eigenspace-Based Minimum Variance Adaptive Beamformer Combined with Delay Multiply and Sum: Experimental Study

Delay and sum (DAS) is the most common beamforming algorithm in linear-array
photoacoustic imaging (PAI) as a result of its simple implementation. However,
it leads to a low resolution and high sidelobes. Delay multiply and sum (DMAS)
was used to address the incapabilities of DAS, providing a higher image
quality. However, the resolution improvement is not well enough compared to
eigenspace-based minimum variance (EIBMV). In this paper, the EIBMV beamformer
has been combined with DMAS algebra, called EIBMV-DMAS, using the expansion of
DMAS algorithm. The proposed method is used as the reconstruction algorithm in
linear-array PAI. EIBMV-DMAS is experimentally evaluated where the quantitative
and qualitative results show that it outperforms DAS, DMAS and EIBMV. The
proposed method degrades the sidelobes for about 365 %, 221 % and 40 %,
compared to DAS, DMAS and EIBMV, respectively. Moreover, EIBMV-DMAS improves
the SNR about 158 %, 63 % and 20 %, respectively.



Finite Time Identification in Unstable Linear Systems

Identification of the parameters of stable linear dynamical systems is a
well-studied problem in the literature, both in the low and high-dimensional
settings. However, there are hardly any results for the unstable case,
especially regarding finite time bounds. For this setting, classical results on
least-squares estimation of the dynamics parameters are not applicable and
therefore new concepts and technical approaches need to be developed to address
the issue. Unstable linear systems arise in key real applications in control
theory, econometrics, and finance. This study establishes finite time bounds
for the identification error of the least-squares estimates for a fairly large
class of heavy-tailed noise distributions, and transition matrices of such
systems. The results relate the time length (samples) required for estimation
to a function of the problem dimension and key characteristics of the true
underlying transition matrix and the noise distribution. To establish them,
appropriate concentration inequalities for random matrices and for sequences of
martingale differences are leveraged.



Head shadow enhancement with low-frequency beamforming improves sound localization and speech perception for simulated bimodal listeners

Many hearing-impaired listeners struggle to localize sounds due to poor
availability of binaural cues. Listeners with a cochlear implant and a
contralateral hearing aid -- so-called bimodal listeners -- are amongst the
worst performers, as both interaural time and level differences are poorly
transmitted. We present a new method to enhance head shadow in the low
frequencies. Head shadow enhancement is achieved with a fixed beamformer with
contralateral attenuation in each ear. The method results in interaural level
differences which vary monotonically with angle. It also improves low-frequency
signal-to-noise ratios in conditions with spatially separated speech and noise.
We validated the method in two experiments with acoustic simulations of bimodal
listening. In the localization experiment, performance improved from 50.5{\deg}
to 26.8{\deg} root-mean-square error compared with standard omni-directional
microphones. In the speech-in-noise experiment, speech was presented from the
frontal direction. Speech reception thresholds improved by 15.7 dB SNR when the
noise was presented from the cochlear implant side, improved by 7.6 dB SNR when
the noise was presented from the hearing aid side, and was not affected when
noise was presented from all directions. Apart from bimodal listeners, the
method might also be promising for bilateral cochlear implant or hearing aid
users. Its low computational complexity makes the method suitable for
application in current clinical devices.
  Keywords: head shadow enhancement, enhancement of interaural level
differences, sound localization, directional hearing, speech in noise, speech
intelligibility
  PACS: 43.60.Fg, 43.66.Pn, 43.66.Qp, 43.66.Rq, 43.66.Ts, 43.71.-k, 43.71.Es,
43.71.Ky



Semantic speech retrieval with a visually grounded model of untranscribed speech

There is growing interest in models that can learn from unlabelled speech
paired with visual context. This setting is relevant for low-resource speech
processing, robotics, and human language acquisition research. Here we study
how a visually grounded speech model, trained on images of scenes paired with
spoken captions, captures aspects of semantics. We use an external image tagger
to generate soft text labels from images, which serve as targets for a neural
model that maps untranscribed speech to (semantic) keyword labels. We introduce
a newly collected data set of human semantic relevance judgements and an
associated task, semantic speech retrieval, where the goal is to search for
spoken utterances that are semantically relevant to a given text query. Without
seeing any text, the model trained on parallel speech and images achieves a
precision of almost 60% on its top ten semantic retrievals. Compared to a
supervised model trained on transcriptions, our model matches human judgements
better by some measures, especially in retrieving non-verbatim semantic
matches. We perform an extensive analysis of the model and its resulting
representations.



Analysis of simultaneous 3D positioning and attitude estimation of a planar coil using inductive coupling

In this paper, simultaneous estimation of 3D position and attitude of a
single coil using a set of anchors, with known position and magnetic dipole, is
analyzed. Effect of noise and geometric properties of the anchors'
constellation is considered. Several parameters are analyzed and discussed,
including placement of anchors in a single or in multiple orthogonal planes. It
is shown that adding space and orientation diversity anchors may lead to a more
robust performance when the mobile node attitude changes in time.



Quality factor of a transmission line coupled coplanar waveguide resonator

We investigate analytically the coupling of a coplanar waveguide resonator to
a coplanar waveguide feedline. Using a conformal mapping technique we obtain an
expression for the characteristic mode impedances and coupling coefficients of
an asymmetric multi-conductor transmission line. Leading order terms for the
external quality factor and frequency shift are calculated. The obtained
analytical results are relevant for designing circuit-QED quantum systems and
frequency division multiplexing of superconducting bolometers, detectors and
similar microwave-range multi-pixel devices.



About attenuation of videopulse in nonlinear transmission lines with ideal dielectric

The attenuation of the video pulse with monotonically increasing input
voltage in a transmission lines with an ideal dielectric can be characterized
by "ohmic" voltage drop $U_\sigma$ along the electrodes with finite
conductivity. The exact analytical formulas for the calculations $U_\sigma$ in
coaxial and strip lines with and without taking into account the strong skin
effect have been obtained. These formulas do not depend on the dispersion and
the degree of nonlinearity of the dielectric and therefore is suitable for
evaluating of shock electromagnetic waves attenuation



Rigorous Q Factor Formulation and Characterization for Nonlinear Oscillators

In this paper, we discuss the definition of Q factor for nonlinear
oscillators. While available definitions of Q are often limited to linear
resonators or oscillators with specific topologies, our definition is
applicable to any oscillator as a figure of merit for its amplitude stability.
It can be formulated rigorously and computed numerically from oscillator
equations. With this definition, we calculate and analyze the Q factors of
several oscillators of different types. The results confirm that the proposed Q
formulation is a useful addition to the characterization techniques for
oscillators.



Thermal Source Localization Through Infinite-Dimensional Compressed Sensing

We propose a scheme utilizing ideas from infinite dimensional compressed
sensing for thermal source localization. Using the soft recovery framework of
one of the authors, we provide rigorous theoretical guarantees for the recovery
performance. In particular, we extend the framework in order to also include
noisy measurements. Further, we conduct numerical experiments, showing that our
proposed method has strong performance, in a wide range of settings. These
include scenarios with few sensors, off-grid source positioning and high noise
levels, both in one and two dimensions.



Stochastic model for the 3D microstructure of pristine and cyclically aged cathodes in Li-ion batteries

It is well-known that the microstructure of electrodes in lithium-ion
batteries strongly affects their performance. Vice versa, the microstructure
can exhibit strong changes during the usage of the battery due to aging
effects. For a better understanding of these effects, mathematical analysis and
modeling has turned out to be of great help. In particular, stochastic 3D
microstructure models have proven to be a powerful and very flexible tool to
generate various kinds of particle-based structures. Recently, such models have
been proposed for the microstructure of anodes in lithium-ion energy and power
cells. In the present paper, we describe a stochastic modeling approach for the
3D microstructure of cathodes in a lithium-ion energy cell, which differs
significantly from the one observed in anodes. The model for the cathode data
enhances the ideas of the anode models, which have been developed so far. It is
calibrated using 3D tomographic image data from pristine as well as two aged
cathodes. A validation based on morphological image characteristics shows that
the model is able to realistically describe both, the microstructure of
pristine and aged cathodes. Thus, we conclude that the model is suitable to
generate virtual, but realistic microstructures of lithium-ion cathodes.



Light Field Retargeting for Multi-Panel Displays

Light fields preserve angular information which can be retargeted to
multi-panel depth displays. Due to limited aperture size and constrained
spatial-angular sampling of many light field capture systems, the displayed
light fields provide only a narrow viewing zone in which parallax views can be
supported. In addition, multi-panel displays typically have a reduced number of
panels being able to coarsely sample depth content resulting in a layered
appearance of light fields. We propose a light field retargeting technique for
multi-panel displays that enhances the perceived parallax and achieves seamless
transition over different depths and viewing angles. This is accomplished by
slicing the captured light fields according to their depth content, boosting
the parallax, and blending the results across the panels. Displayed views are
synthesized and aligned dynamically according to the position of the viewer.
The proposed technique is outlined, simulated and verified experimentally on a
three-panel aerial display.



Embroidered Antenna Characterization for Passive UHF RFID Tags

For smart clothing integration with the wireless system based on radio
frequency (RF) backscattering, we demonstrate an ultra-high frequency (UHF)
antenna constructed from embroidered conductive threads. Sewn into a fabric
backing, the T-match antenna design mimics a commercial UHF RFID tag, which was
also used for comparative testing. Bonded to the fabric antenna is the
integrated circuit chip dissected from another commercial RFID tag, which
allows for testing the tags under normal EPC Gen 2 operating conditions. We
find that, despite of the high resistive loss of the antenna and inexact
impedance matching, the fabric antenna works reasonably well as a UHF antenna
both in standalone RFID testing, and during variety of ways of wearing under
sweaters or as wristbands. The embroidering pattern does not affect much the
feel and comfort from either side of the fabrics by our sewing method.



Generating Nontrivial Melodies for Music as a Service

We present a hybrid neural network and rule-based system that generates pop
music. Music produced by pure rule-based systems often sounds mechanical. Music
produced by machine learning sounds better, but still lacks hierarchical
temporal structure. We restore temporal hierarchy by augmenting machine
learning with a temporal production grammar, which generates the music's
overall structure and chord progressions. A compatible melody is then generated
by a conditional variational recurrent autoencoder. The autoencoder is trained
with eight-measure segments from a corpus of 10,000 MIDI files, each of which
has had its melody track and chord progressions identified heuristically. The
autoencoder maps melody into a multi-dimensional feature space, conditioned by
the underlying chord progression. A melody is then generated by feeding a
random sample from that space to the autoencoder's decoder, along with the
chord progression generated by the grammar. The autoencoder can make musically
plausible variations on an existing melody, suitable for recurring motifs. It
can also reharmonize a melody to a new chord progression, keeping the rhythm
and contour. The generated music compares favorably with that generated by
other academic and commercial software designed for the music-as-a-service
industry.



End-to-end DNN Based Speaker Recognition Inspired by i-vector and PLDA

Recently several end-to-end speaker verification systems based on deep neural
networks (DNNs) have been proposed. These systems have been proven to be
competitive for text-dependent tasks as well as for text-independent tasks with
short utterances. However, for text-independent tasks with longer utterances,
end-to-end systems are still outperformed by standard i-vector + PLDA systems.
In this work, we develop an end-to-end speaker verification system that is
initialized to mimic an i-vector + PLDA baseline. The system is then further
trained in an end-to-end manner but regularized so that it does not deviate too
far from the initial system. In this way we mitigate overfitting which normally
limits the performance of end-to-end systems. The proposed system outperforms
the i-vector + PLDA baseline on both long and short duration utterances.



Dictionary-Free MRI PERK: Parameter Estimation via Regression with Kernels

This paper introduces a fast, general method for dictionary-free parameter
estimation in quantitative magnetic resonance imaging (QMRI) via regression
with kernels (PERK). PERK first uses prior distributions and the nonlinear MR
signal model to simulate many parameter-measurement pairs. Inspired by machine
learning, PERK then takes these parameter-measurement pairs as labeled training
points and learns from them a nonlinear regression function using kernel
functions and convex optimization. PERK admits a simple implementation as
per-voxel nonlinear lifting of MRI measurements followed by linear minimum
mean-squared error regression. We demonstrate PERK for $T_1,T_2$ estimation, a
well-studied application where it is simple to compare PERK estimates against
dictionary-based grid search estimates. Numerical simulations as well as
single-slice phantom and in vivo experiments demonstrate that PERK and grid
search produce comparable $T_1,T_2$ estimates in white and gray matter, but
PERK is consistently at least $23\times$ faster. This acceleration factor will
increase by several orders of magnitude for full-volume QMRI estimation
problems involving more latent parameters per voxel.



The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments

This paper introduces the contents and the possible usage of the
DIRHA-ENGLISH multi-microphone corpus, recently realized under the EC DIRHA
project. The reference scenario is a domestic environment equipped with a large
number of microphones and microphone arrays distributed in space.
  The corpus is composed of both real and simulated material, and it includes
12 US and 12 UK English native speakers. Each speaker uttered different sets of
phonetically-rich sentences, newspaper articles, conversational speech,
keywords, and commands. From this material, a large set of 1-minute sequences
was generated, which also includes typical domestic background noise as well as
inter/intra-room reverberation effects. Dev and test sets were derived, which
represent a very precious material for different studies on multi-microphone
speech processing and distant-speech recognition. Various tasks and
corresponding Kaldi recipes have already been developed.
  The paper reports a first set of baseline results obtained using different
techniques, including Deep Neural Networks (DNN), aligned with the
state-of-the-art at international level.



LTE Spectrum Sharing Research Testbed: Integrated Hardware, Software, Network and Data

This paper presents Virginia Tech's wireless testbed supporting research on
long-term evolution (LTE) signaling and radio frequency (RF) spectrum
coexistence. LTE is continuously refined and new features released. As the
communications contexts for LTE expand, new research problems arise and include
operation in harsh RF signaling environments and coexistence with other radios.
Our testbed provides an integrated research tool for investigating these and
other research problems; it allows analyzing the severity of the problem,
designing and rapidly prototyping solutions, and assessing them with
standard-compliant equipment and test procedures. The modular testbed
integrates general-purpose software-defined radio hardware, LTE-specific test
equipment, RF components, free open-source and commercial LTE software, a
configurable RF network and recorded radar waveform samples. It supports RF
channel emulated and over-the-air radiated modes. The testbed can be remotely
accessed and configured. An RF switching network allows for designing many
different experiments that can involve a variety of real and virtual radios
with support for multiple-input multiple-output (MIMO) antenna operation. We
present the testbed, the research it has enabled and some valuable lessons that
we learned and that may help designing, developing, and operating future
wireless testbeds.



Partially Asynchronous Distributed Unmixing of Hyperspectral Images

So far, the problem of unmixing large or multitemporal hyperspectral datasets
has been specifically addressed in the remote sensing literature only by a few
dedicated strategies. Among them, some attempts have been made within a
distributed estimation framework, in particular relying on the alternating
direction method of multipliers (ADMM). In this paper, we propose to study the
interest of a partially asynchronous distributed unmixing procedure based on a
recently proposed asynchronous algorithm. Under standard assumptions, the
proposed algorithm inherits its convergence properties from recent
contributions in non-convex optimization, while allowing the problem of
interest to be efficiently addressed. Comparisons with a distributed
synchronous counterpart of the proposed unmixing procedure allow its interest
to be assessed on synthetic and real data. Besides, thanks to its genericity
and flexibility, the procedure investigated in this work can be implemented to
address various matrix factorization problems.



Radiation Pattern Synthesis Using Hybrid Fourier- Woodward-Lawson-Neural Networks for Reliable MIMO Antenna Systems

In this paper, we implement hybrid Woodward-Lawson-Neural Networks and
weighted Fourier method to synthesize antenna arrays. The neural networks (NN)
is applied here to simplify the modeling of MIMO antenna arrays by assessing
phases. The main problem is obviously to find optimal weights of the linear
antenna array elements giving radiation pattern with minimum sidelobe level
(SLL) and hence ameliorating the antenna array performance. To attain this
purpose, an antenna array for reliable Multiple-Input Multiple-Output (MIMO)
applications with frequency at 2.45 GHz is implemented. To validate the
suggested method, many examples of uniformly excited array patterns with the
main beam are put in the direction of the useful signal. The
Woodward-Lawson-Neural Networks synthesis method permits to find out
interesting analytical equations for the synthesis of an antenna array and
highlights the flexibility between the system parameters in input and those in
output. The performance of this hybrid optimization underlines how well the
system is suitable for a wireless communication and how it participates in
reducing interference, as well.



Robust Radar Detection of a Mismatched Steering Vector Embedded in Compound Gaussian Clutter

The problem of radar detection in compound Gaussian clutter when a radar
signature is not completely known has not been considered yet and is addressed
in this paper. We proposed a robust technique to detect, based on the
generalized likelihood ratio test, a point-like target embedded in compound
Gaussian clutter. Employing an array of antennas, we assume that the actual
steering vector departs from the nominal one, but lies in a known interval. The
detection is then secured by employing a semi-definite programming. It is
confirmed via simulation that the proposed detector experiences a negligible
detection loss compared to an adaptive normalized matched filter in a perfectly
matched case, but outperforms in cases of mismatched signal. Remarkably, the
proposed detector possesses constant false alarm rate with respect to the
clutter covariance matrix.



A Semi-Automated Technique for Internal Jugular Vein Segmentation in Ultrasound Images Using Active Contours

The assessment of the blood volume is crucial for the management of many
acute and chronic diseases. Recent studies have shown that circulating blood
volume correlates with the cross-sectional area (CSA) of the internal jugular
vein (IJV) estimated from ultrasound imagery. In this paper, a semi-automatic
segmentation algorithm is proposed using a combination of region growing and
active contour techniques to provide a fast and accurate segmentation of IJV
ultrasound videos. The algorithm is applied to track and segment the IJV across
a range of image qualities, shapes, and temporal variation. The experimental
results show that the algorithm performs well compared to expert manual
segmentation and outperforms several published algorithms incorporating speckle
tracking.



Image Improvement in Linear-Array Photoacoustic Imaging using High Resolution Coherence Factor Weighting Technique

In Photoacoustic imaging (PAI), the most prevalent beamforming algorithm is
delay-and-sum (DAS) due to its simple implementation. However, it results in a
low quality image affected by the high level of sidelobes. Coherence factor
(CF) can be used to address the sidelobes in the reconstructed images by DAS,
but the resolution improvement is not good enough compared to the high
resolution beamformers such as minimum variance (MV). As a weighting algorithm
in linear-array PAI, it was proposed to use high-resolution-CF (HRCF) weighting
technique in which MV is used instead of the existing DAS in the formula of the
conventional CF. The higher performance of HRCF was proved numerically and
experimentally. The quantitative results obtained with the simulations show
that at the depth of 40 mm, in comparison with DAS+CF and MV+CF, HRCF improves
the full-width-half-maximum of about 91 % and 15 % and the signal-to-noise
ratio about 40 % and 14 %, respectively. Moreover, the contrast ratio at the
depth of 20 mm has been improved about 62 % and 21 % by HRCF, compared to
DAS+CF and MV+CF, respectively



A Fast and Efficient Near-Lossless Image Compression using Zipper Transformation

Near-lossless image compression-decompression scheme is proposed in this
paper using Zipper Transformation (ZT) and inverse zipper transformation (iZT).
The proposed ZT exploits the conjugate symmetry property of Discrete Fourier
Transformation (DFT). The proposed transformation is implemented using two
different configurations: the interlacing and concatenating ZT. In order to
quantify the efficacy of the proposed transformation, we benchmark with
Discrete Cosine Transformation (DCT) and Fast Walsh Hadamard Transformation
(FWHT) in terms of lossless compression capability and computational cost.
Numerical simulations show that ZT-based compression algorithm is
near-lossless, compresses better, and offers faster implementation than both
DCT and FWHT. Also, interlacing and concatenating ZT are shown to yield similar
results in most of the test cases considered.



Range-Spread Targets Detection in Unknown Doppler Shift via Semi-Definite Programming

Based on the technique of generalized likelihood ratio test, we address
detection schemes for Doppler-shifted range-spread targets in Gaussian noise.
First, a detection scheme is derived by solving the maximization associated
with the estimation of unknown Doppler frequency with semi-definite
programming. To lower the computational complexity of the detector, we then
consider a simplification of the detector by adopting maximization over a
relaxed space. Both of the proposed detectors are shown to have constant false
alarm rate via numerical or theoretical analysis. The detection performance of
the proposed detector based on the semi-definite programming is shown to be
almost the same as that of the conventional detector designed for known Doppler
frequency.



A report on sound event detection with different binaural features

In this paper, we compare the performance of using binaural audio features in
place of single-channel features for sound event detection. Three different
binaural features are studied and evaluated on the publicly available TUT Sound
Events 2017 dataset of length 70 minutes. Sound event detection is performed
separately with single-channel and binaural features using stacked
convolutional and recurrent neural network and the evaluation is reported using
standard metrics of error rate and F-score. The studied binaural features are
seen to consistently perform equal to or better than the single-channel
features with respect to error rate metric.



Sound event detection using weakly labeled dataset with stacked convolutional and recurrent neural network

This paper proposes a neural network architecture and training scheme to
learn the start and end time of sound events (strong labels) in an audio
recording given just the list of sound events existing in the audio without
time information (weak labels). We achieve this by using a stacked
convolutional and recurrent neural network with two prediction layers in
sequence one for the strong followed by the weak label. The network is trained
using frame-wise log mel-band energy as the input audio feature, and weak
labels provided in the dataset as labels for the weak label prediction layer.
Strong labels are generated by replicating the weak labels as many number of
times as the frames in the input audio feature, and used for strong label layer
during training. We propose to control what the network learns from the weak
and strong labels by different weighting for the loss computed in the two
prediction layers. The proposed method is evaluated on a publicly available
dataset of 155 hours with 17 sound event classes. The method achieves the best
error rate of 0.84 for strong labels and F-score of 43.3% for weak labels on
the unseen test split.



Modelling Power Network: State Estimation and Correction

It is presented a simple algorithm for power network state estimation and
correction (fault detection) employing standard methodology.



Advanced Satellite-based Frequency Transfer at the 10^{-16} Level

Advanced satellite-based frequency transfers by TWCP and IPPP have been
performed between NICT and KRISS. We confirm that the disagreement between them
is less than 1x10^{-16} at an averaging time of several days. Additionally, an
intercontinental frequency ratio measurement of Sr and Yb optical lattice
clocks was directly performed by TWCP. We achieved an uncertainty at the
mid-10^{-16} level after a total measurement time of 12 hours. The frequency
ratio was consistent with the recently reported values within the uncertainty.



Python Non-Uniform Fast Fourier Transform (PyNUFFT): multi-dimensional non-Cartesian image reconstruction package for heterogeneous platforms and applications to MRI

This paper reports the development of a Python Non-Uniform Fast Fourier
Transform (PyNUFFT) package, which accelerates non-Cartesian image
reconstruction on heterogeneous platforms. Scientific computing with Python
encompasses a mature and integrated environment. The NUFFT algorithm has been
extensively used for non-Cartesian image reconstruction but previously there
was no native Python NUFFT library. The current PyNUFFT software enables
multi-dimensional NUFFT on heterogeneous platforms. The PyNUFFT also provides
several solvers, including the conjugate gradient method, $\ell$1
total-variation regularized ordinary least square (L1TV-OLS) and $\ell$1
total-variation regularized least absolute deviation (L1TV-LAD).
Metaprogramming libraries were employed to accelerate PyNUFFT. The PyNUFFT
package has been tested on multi-core CPU and GPU, with acceleration factors of
6.3 - 9.5$\times$ on a 32 thread CPU platform and 5.4 - 13$\times$ on the GPU.



Windowed Overlapped frequency-domain Block Filtering Approach for Direct Sequence Signal Acquisition

This paper applies a windowed frequency-domain overlapped block filtering
approach for the acquisition of direct sequence signals. The windows, as a
novel viewpoint, not only allow pulse shaping without a front end pulse shaping
filter, but also improve the performance of the spectrum sensing unit which can
efficiently be implemented into this frequency-domain receiver and may further
be used for spectrum sensing in cognitive radios or narrowband interference
cancellation in military radios. The proposed receiver is applicable for
initial time synchronization of different signals containing a preamble. These
signals include single carrier, constant-envelope single-carrier, multi-carrier
and even generalized-multi-carrier signals, which makes the proposed receiver
structure a universal unit. Furthermore, the receiver can be used to perform
filtering with long codes and compute the sliding correlation of an unknown
periodic preamble. It can further be modified to handle large Doppler shifts.
We will also demonstrate the computational complexity and analysis of the
acquisition performance in Rayleigh and Rician fading channels.



Contaminated speech training methods for robust DNN-HMM distant speech recognition

Despite the significant progress made in the last years, state-of-the-art
speech recognition technologies provide a satisfactory performance only in the
close-talking condition. Robustness of distant speech recognition in adverse
acoustic conditions, on the other hand, remains a crucial open issue for future
applications of human-machine interaction. To this end, several advances in
speech enhancement, acoustic scene analysis as well as acoustic modeling, have
recently contributed to improve the state-of-the-art in the field. One of the
most effective approaches to derive a robust acoustic modeling is based on
using contaminated speech, which proved helpful in reducing the acoustic
mismatch between training and testing conditions.
  In this paper, we revise this classical approach in the context of modern
DNN-HMM systems, and propose the adoption of three methods, namely, asymmetric
context windowing, close-talk based supervision, and close-talk based
pre-training. The experimental results, obtained using both real and simulated
data, show a significant advantage in using these three methods, overall
providing a 15% error rate reduction compared to the baseline systems. The same
trend in performance is confirmed either using a high-quality training set of
small size, and a large one.



Quantized Spectral Compressed Sensing: Cramer-Rao Bounds and Recovery Algorithms

Efficient estimation of wideband spectrum is of great importance for
applications such as cognitive radio. Recently, sub-Nyquist sampling schemes
based on compressed sensing have been proposed to greatly reduce the sampling
rate. However, the important issue of quantization has not been fully
addressed, particularly for high-resolution spectrum and parameter estimation.
In this paper, we aim to recover spectrally-sparse signals and the
corresponding parameters, such as frequency and amplitudes, from heavy
quantizations of their noisy complex-valued random linear measurements, e.g.
only the quadrant information. We first characterize the Cramer-Rao bound under
Gaussian noise, which highlights the trade-off between sample complexity and
bit depth under different signal-to-noise ratios for a fixed budget of bits.
Next, we propose a new algorithm based on atomic norm soft thresholding for
signal recovery, which is equivalent to proximal mapping of properly designed
surrogate signals with respect to the atomic norm that motivates spectral
sparsity. The proposed algorithm can be applied to both the single measurement
vector case, as well as the multiple measurement vector case. It is shown that
under the Gaussian measurement model, the spectral signals can be reconstructed
accurately with high probability, as soon as the number of quantized
measurements exceeds the order of K log n, where K is the level of spectral
sparsity and $n$ is the signal dimension. Finally, numerical simulations are
provided to validate the proposed approaches.



Deep learning in remote sensing: a review

Standing at the paradigm shift towards data-intensive science, machine
learning techniques are becoming increasingly important. In particular, as a
major breakthrough in the field, deep learning has proven as an extremely
powerful tool in many fields. Shall we embrace deep learning as the key to all?
Or, should we resist a 'black-box' solution? There are controversial opinions
in the remote sensing community. In this article, we analyze the challenges of
using deep learning for remote sensing data analysis, review the recent
advances, and provide resources to make deep learning in remote sensing
ridiculously simple to start with. More importantly, we advocate remote sensing
scientists to bring their expertise into deep learning, and use it as an
implicit general model to tackle unprecedented large-scale influential
challenges, such as climate change and urbanization.



PROSE: Perceptual Risk Optimization for Speech Enhancement

The goal in speech enhancement is to obtain an estimate of clean speech
starting from the noisy signal by minimizing a chosen distortion measure, which
results in an estimate that depends on the unknown clean signal or its
statistics. Since access to such prior knowledge is limited or not possible in
practice, one has to estimate the clean signal statistics. In this paper, we
develop a new risk minimization framework for speech enhancement, in which, one
optimizes an unbiased estimate of the distortion/risk instead of the actual
risk. The estimated risk is expressed solely as a function of the noisy
observations. We consider several perceptually relevant distortion measures and
develop corresponding unbiased estimates under realistic assumptions on the
noise distribution and a priori signal-to-noise ratio (SNR). Minimizing the
risk estimates gives rise to the corresponding denoisers, which are nonlinear
functions of the a posteriori SNR. Perceptual evaluation of speech quality
(PESQ), average segmental SNR (SSNR) computations, and listening tests show
that the proposed risk optimization approach employing Itakura-Saito and
weighted hyperbolic cosine distortions gives better performance than the other
distortion measures. For SNRs greater than 5 dB, the proposed approach gives
superior denoising performance over the benchmark techniques based on the
Wiener filter, log-MMSE minimization, and Bayesian nonnegative matrix
factorization.



A Review of Convolutional Neural Networks for Inverse Problems in Imaging

In this survey paper, we review recent uses of convolution neural networks
(CNNs) to solve inverse problems in imaging. It has recently become feasible to
train deep CNNs on large databases of images, and they have shown outstanding
performance on object classification and segmentation tasks. Motivated by these
successes, researchers have begun to apply CNNs to the resolution of inverse
problems such as denoising, deconvolution, super-resolution, and medical image
reconstruction, and they have started to report improvements over
state-of-the-art methods, including sparsity-based techniques such as
compressed sensing. Here, we review the recent experimental work in these
areas, with a focus on the critical design decisions: Where does the training
data come from? What is the architecture of the CNN? and How is the learning
problem formulated and solved? We also bring together a few key theoretical
papers that offer perspective on why CNNs are appropriate for inverse problems
and point to some next steps in the field.



An Elementary Introduction to Kalman Filtering

Kalman filtering is a classic state estimation technique used in application
areas such as signal processing and autonomous control of vehicles. It is now
being used to solve problems in computer systems such as controlling the
voltage and frequency of processors.
  Although there are many presentations of Kalman filtering in the literature,
they usually deal with particular systems like autonomous robots or linear
systems with Gaussian noise, which makes it difficult to understand the general
principles behind Kalman filtering. In this paper, we first present the
abstract ideas behind Kalman filtering at a level accessible to anyone with a
basic knowledge of probability theory and calculus, and then show how these
concepts can be applied to the particular problem of state estimation in linear
systems. This separation of concepts from applications should make it easier to
understand Kalman filtering and to apply it to other problems in computer
systems.



Dispenser Concept for Unmanned Aerial Vehicles (UAV, Drone, UAS)

System, design and methodology to load and dispense different articles from
an autonomous aircraft are disclosed. In one embodiment, the design of a unique
detachable dispenser for delivery of articles is described along with an
intelligent methodology of loading and delivering the articles to and from the
dispenser. Design of the dispenser, interaction of the dispenser with the
flight control unit and ground control or base-station, and interaction of the
base station with the sender or recipient of the article, are also described.



Pyroomacoustics: A Python package for audio room simulations and array processing algorithms

We present pyroomacoustics, a software package aimed at the rapid development
and testing of audio array processing algorithms. The content of the package
can be divided into three main components: an intuitive Python object-oriented
interface to quickly construct different simulation scenarios involving
multiple sound sources and microphones in 2D and 3D rooms; a fast C
implementation of the image source model for general polyhedral rooms to
efficiently generate room impulse responses and simulate the propagation
between sources and receivers; and finally, reference implementations of
popular algorithms for beamforming, direction finding, and adaptive filtering.
Together, they form a package with the potential to speed up the time to market
of new algorithms by significantly reducing the implementation overhead in the
performance evaluation step.



Algebraic Image Processing

We propose an approach to image processing related to algebraic operators
acting in the space of images. In view of the interest in the applications in
optics and computer science, mathematical aspects of the paper have been
simplified as much as possible. Underlying theory, related to rigged Hilbert
spaces and Lie algebras, is discussed elsewhere



Identification of Legged Locomotion via Model-Based and Data-Driven Approaches

In the first part of this thesis, we present our efforts on experimental
validation of the predictive performance of mechanics-based mathematical models
on a physical one-legged hopping robot platform. We extend upon a recently
proposed approximate analytical solution developed for the lossy spring--mass
models for a real robotic system and perform a parametric system identification
to carefully identify the system parameters in the proposed model. We also
present our assessments on the predictive performance of the proposed
approximate analytical solution on our one-legged hopping robot data.
  The second part considers estimating state space models of legged locomotion
using input--output data. To accomplish this, we first propose a state space
identification method to estimate time periodic state and input matrices of a
hybrid LTP system under full state measurement assumption. We then release this
assumption and proceed with subspace identification methods to estimate LTP
state space realizations for unknown stable LTP systems. We utilize bilinear
(Tustin) transformation and frequency domain lifting methods to generalize our
solutions to different LTP system models. Our results provide a basis towards
identification of state space models for legged locomotion.



A Spatial-Spectral Interference Model for Dense Finite-Area 5G mmWave Networks

With the overcrowded sub-6 GHz bands, millimeter wave (mmWave) bands offer a
promising alternative for the next generation wireless standard, i.e., 5G.
However, the susceptibility of mmWave signals to severe pathloss and shadowing
requires the use of highly directional antennas to overcome such adverse
characteristics. Building a network with directional beams changes the
interference behavior, since, narrow beams are vulnerable to blockages. Such
sensitivity to blockages causes uncertainty in the active interfering node
locations. Configuration uncertainty may also manifest in the spectral domain
while applying dynamic channel and frequency assignment to support 5G
applications. In this paper, we first propose a blockage model considering
mmWave specifications. Subsequently, using the proposed blockage model, we
derive a spatial-spectral interference model for dense finite-area 5G mmWave
networks. The proposed interference model considers both spatial and spectral
randomness in node configuration. Finally, the error performance of the network
from an arbitrarily located user perspective is calculated in terms of bit
error rate (BER) and outage probability metrics. The analytical results are
validated via Monte-Carlo simulations. It is shown that considering mmWave
specifications and also randomness in both spectral and spatial node
configurations leads to a noticeably different interference profile.



Audio Concept Classification with Hierarchical Deep Neural Networks

Audio-based multimedia retrieval tasks may identify semantic information in
audio streams, i.e., audio concepts (such as music, laughter, or a revving
engine). Conventional Gaussian-Mixture-Models have had some success in
classifying a reduced set of audio concepts. However, multi-class
classification can benefit from context window analysis and the discriminating
power of deeper architectures. Although deep learning has shown promise in
various applications such as speech and object recognition, it has not yet met
the expectations for other fields such as audio concept classification. This
paper explores, for the first time, the potential of deep learning in
classifying audio concepts on User-Generated Content videos. The proposed
system is comprised of two cascaded neural networks in a hierarchical
configuration to analyze the short- and long-term context information. Our
system outperforms a GMM approach by a relative 54%, a Neural Network by 33%,
and a Deep Neural Network by 12% on the TRECVID-MED database



A Spatial-Spectral Interference Model for Millimeter Wave 5G Applications

The potential of the millimeter wave (mmWave) band in meeting the ever
growing demand for high data rate and capacity in emerging fifth generation
(5G) wireless networks is well-established. Since mmWave systems are expected
to use highly directional antennas with very focused beams to overcome severe
pathloss and shadowing in this band, the nature of signal propagation in mmWave
wireless networks may differ from current networks. One factor that is
influenced by such propagation characteristics is the interference behavior,
which is also impacted by simultaneous use of the unlicensed portion of the
spectrum by multiple users. Therefore, considering the propagation
characteristics in the mmWave band, we propose a spatial-spectral interference
model for 5G mmWave applications, in the presence of Poisson field of blockages
and interferers operating in licensed and unlicensed mmWave spectrum.
Consequently, the average bit error rate of the network is calculated.
Simulation is also carried out to verify the outcomes of the paper.



White Gaussian Noise Based Capacity Estimate and Characterization of Fiber-Optic Links

We use white Gaussian noise as a test signal for single-mode and multimode
transmission links and estimate the link capacity based on a calculation of
mutual information. We also extract the complex amplitude channel estimations
and mode-dependent loss with high accuracy.



Partial Zero-Forcing for Multi-Way Relay Networks

The ever increasing demands for mobile network access have resulted in a
significant increase in bandwidth usage. By improving the system spectral
efficiency, multi-way relay networks (MWRNs) provide promising approaches to
address this challenge. In this paper, we propose a novel linear beamforming
design, namely partial zero-forcing (PZF), for MWRNs with a
multiple-input-multiple-output (MIMO) relay. Compared to zero-forcing (ZF), PZF
relaxes the constraints on the relay beamforming matrix such that only partial
user-interference, instead of all, is canceled at the relay. The users
eliminate the remaining interferences through self-interference and successive
interference cancellation. A sum-rate maximization problem is formulated and
solved to exploit the extra degrees-of-freedom resulted from PZF. Simulation
results show that the proposed PZF relay beamforming design achieves
significantly higher network sum-rates than the existing linear beamforming
designs.



Performance Analysis of Energy Consumption in Cache-Enabled Multicast D2D Communications

Device-to-Device (D2D) communication as a promising technology in 5G cellular
networks provides the communication of the users in the vicinity and thereby
decreases end-to-end delay and power consumption. In addition to the
aforementioned advantages, it also supports the high-speed data transmission
services such as content delivery. In this paper, we consider the D2D multicast
communications opportunity in the D2D-cellular hybrid network, in which that
one transmitter targets multiple receivers at the same time. We provide the
analysis for the proposed system by using tools from stochastic geometry, to
calculate the cache hitting probability of the receivers as well as the energy
consumption of the hybrid network aiming to seek the optimal number of caching
contents in the D2D multicast opportunities.



On the Runtime-Efficacy Trade-off of Anomaly Detection Techniques for Real-Time Streaming Data

Ever growing volume and velocity of data coupled with decreasing attention
span of end users underscore the critical need for real-time analytics. In this
regard, anomaly detection plays a key role as an application as well as a means
to verify data fidelity. Although the subject of anomaly detection has been
researched for over 100 years in a multitude of disciplines such as, but not
limited to, astronomy, statistics, manufacturing, econometrics, marketing, most
of the existing techniques cannot be used as is on real-time data streams.
Further, the lack of characterization of performance -- both with respect to
real-timeliness and accuracy -- on production data sets makes model selection
very challenging. To this end, we present an in-depth analysis, geared towards
real-time streaming data, of anomaly detection techniques. Given the
requirements with respect to real-timeliness and accuracy, the analysis
presented in this paper should serve as a guide for selection of the "best"
anomaly detection technique. To the best of our knowledge, this is the first
characterization of anomaly detection techniques proposed in very diverse set
of fields, using production data sets corresponding to a wide set of
application domains.



An FBAR Circulator

This letter presents the experimental demonstration of a film bulk acoustic
resonator (FBAR) circulator at 2.5 GHz. The circulator is based on
spatio-temporal modulation of the series resonant frequency of FBARs using
varactors and exhibits a large isolation of 76 dB at 2.5 GHz. The FBAR chip
(0.25 mm2) consists of three identical FBARs connected in wye configuration.
The FBAR0s quality factor (Q) of 1250 and piezoelectric coupling coefficient kt
2 of 3% relaxes the modulation requirements, achieving non-reciprocity with
small modulationto- RF frequency ratio bettter than 1:800 (3 MHz:2.5 GHz).



Reconfigurable Antennas in mmWave MIMO Systems

The key obstacle to achieving the full potential of the millimeter wave
(mmWave) band has been the poor propagation characteristics of wireless signals
in this band. One approach to overcome this issue is to use antennas that can
support higher gains while providing beam adaptability and diversity, i.e.,
reconfigurable antennas. In this article, we present a new architecture for
mmWave multiple-input multiple-output (MIMO) communications that uses a new
class of reconfigurable antennas. More specifically, the proposed lens-based
antennas can support multiple radiation patterns while using a single radio
frequency chain. Moreover, by using a beam selection network, each antenna beam
can be steered in the desired direction. Further, using the proposed
reconfigurable antenna in a MIMO architecture, we propose a new signal
processing algorithm that uses the additional degrees of freedom provided by
the antennas to overcome propagation issues at mmWave frequencies. Our
simulation results show that the proposed reconfigurable antenna MIMO
architecture significantly enhances the performance of mmWave communication
systems.



Beam Domain Massive MIMO for Optical Wireless Communications with Transmit Lens

This paper presents a novel massive multiple-input multiple-output (MIMO)
transmission in beam domain for optical wireless communications. The optical
base station equipped with massive optical transmitters communicates with a
number of user terminals (UTs) through a transmit lens. Focusing on LED
transmitters, we analyze light refraction of the lens and establish a channel
model for optical massive MIMO transmissions. For a large number of LEDs,
channel vectors of different UTs become asymptotically orthogonal. We
investigate the maximum ratio transmission and regularized zero-forcing
precoding in the optical massive MIMO system, and propose a linear precoding
design to maximize the sum rate. We further design the precoding when the
number of transmitters grows asymptotically large, and show that beam division
multiple access (BDMA) transmission achieves the asymptotically optimal
performance for sum rate maximization. Unlike optical MIMO without a transmit
lens, BDMA can increase the sum rate proportionally to $2K$ and $K$ under the
total and per transmitter power constraints, respectively, where $K$ is the
number of UTs. In the non-asymptotic case, we prove the orthogonality
conditions of the optimal power allocation in beam domain and propose efficient
beam allocation algorithms. Numerical results confirm the significantly
improved performance of our proposed beam domain optical massive MIMO
communication approaches.



Joint Radio Resource Allocation, 3D Placement and User Association of Aerial Base Stations in IoT Networks

In this paper, a novel method for joint radio resource allocation (RRA),
three-dimensional placement (3DP), and user association of aerial base stations
(ABSs) as a main problem in the internet of things (IoT) networks is proposed.
In our proposed model, we consider two schemes: a) line of sight (LoS) b)
generalized. In the LoS scheme, all the ABSs should see the IoT users as LoS.
In the generalized scheme, ABSs can see some of the IoT users as LoS and some
of them as NLoS. The main goal of this paper is to minimize the overal transmit
power of the IoT users while satisfying some quality of service (QoS)
constraints in uplink scenario. To solve the optimization problems and to
convert the main problems with high complexity into the subproblems with lower
complexity, we decompose them into two subproblems namely 3DP subproblem and
joint RRA and user association (JRU) subproblem. The methods which we use to
solve our proposed optimization problems are Semi Definite Relaxation (SDR) and
Geometric Programming (GP). Finally, using simulations, we evaluate the
performance of the proposed schemes for different values of the network
parameters.



Estimating Phase Duration for SPaT Messages

A SPaT (Signal Phase and Timing) message describes for each lane the current
phase at a signalized intersection together with an estimate of the residual
time of that phase. Accurate SPaT messages can be used to construct a speed
profile for a vehicle that reduces its fuel consumption as it approaches or
leaves an intersection. This paper presents SPaT estimation algorithms at an
intersection with a semi-actuated signal, using real-time signal phase
measurements. The algorithms are evaluated using high-resolution data from two
intersections in Montgomery County, MD. The algorithms can be readily
implemented at signal controllers. The study supports three findings. First,
real-time information dramatically improves the accuracy of the prediction of
the residual time compared with prediction based on historical data alone.
Second, as time increases the prediction of the residual time may increase or
decrease. Third, as drivers differently weight errors in predicting `end of
green' and `end of red', drivers on two different approaches may prefer
different estimates of the residual time of the same phase.



Measurements and Characterisation of Surface Scattering at 60 GHz

This paper presents the analysis and characterization of the surface
scattering process for both specular and diffused components. The study is
focused on the investigation of various building materials each having a
different roughness, at a central frequency of 60GHz. Very large signal
strength variations in first order scattered components is observed as the user
moves over very short distances. This is due to the small-scale fading caused
by rough surface scatterers. Furthermore, it is shown that the diffused
scattering depends on the material roughness, the angle of incidence and the
distance from the surface. Finally, results indicate that reflections from
rough materials may suffer from high depolarization, a phenomenon that can
potentially be exploited in order to improve the performance of mm-Wave systems
using polarization diversity.



Non-Orthogonal Multiple Access for FSO Backhauling

We consider a free space optical (FSO) backhauling system which consists of
two base stations (BSs) and one central unit (CU). We propose to employ
non-orthogonal multiple access (NOMA) for FSO backhauling where both BSs
transmit at the same time and in the same frequency band to the same
photodetector at the CU. We develop a dynamic NOMA scheme which determines the
optimal decoding order as a function of the channel state information at the CU
and the quality of service requirements of the BSs, such that the outage
probabilities of both BSs are jointly minimized. Moreover, we analyze the
performance of the proposed NOMA scheme in terms of the outage probability over
Gamma-Gamma FSO turbulence channels. We further derive closed-form expressions
for the outage probability for the high signal-to-noise ratio regime. Our
simulation results confirm the analytical derivations and reveal that the
proposed dynamic NOMA scheme significantly outperforms orthogonal transmission
and existing NOMA schemes.



Requirements for Secure Clock Synchronization

This paper establishes a fundamental theory of secure clock synchronization.
Accurate clock synchronization is the backbone of systems managing power
distribution, financial transactions, telecommunication operations, database
services, etc. Some clock synchronization (time transfer) systems, such as the
Global Navigation Satellite Systems (GNSS), are based on one-way communication
from a master to a slave clock. Others, such as the Network Transport Protocol
(NTP), and the IEEE 1588 Precision Time Protocol (PTP), involve two-way
communication between the master and slave. This paper shows that all one-way
time transfer protocols are vulnerable to replay attacks that can potentially
compromise timing information. A set of conditions for secure two-way clock
synchronization is proposed and proved to be necessary and sufficient. It is
shown that IEEE 1588 PTP, although a two-way synchronization protocol, is not
compliant with these conditions, and is therefore insecure. Requirements for
secure IEEE 1588 PTP are proposed, and a second example protocol is offered to
illustrate the range of compliant systems.



Densely Connected Convolutional Networks and Signal Quality Analysis to Detect Atrial Fibrillation Using Short Single-Lead ECG Recordings

The development of new technology such as wearables that record high-quality
single channel ECG, provides an opportunity for ECG screening in a larger
population, especially for atrial fibrillation screening. The main goal of this
study is to develop an automatic classification algorithm for normal sinus
rhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise from a
single channel short ECG segment (9-60 seconds). For this purpose, signal
quality index (SQI) along with dense convolutional neural networks was used.
Two convolutional neural network (CNN) models (main model that accepts 15
seconds ECG and secondary model that processes 9 seconds shorter ECG) were
trained using the training data set. If the recording is determined to be of
low quality by SQI, it is immediately classified as noisy. Otherwise, it is
transformed to a time-frequency representation and classified with the CNN as
NSR, AF, O, or noise. At the final step, a feature-based post-processing
algorithm classifies the rhythm as either NSR or O in case the CNN model's
discrimination between the two is indeterminate. The best result achieved at
the official phase of the PhysioNet/CinC challenge on the blind test set was
0.80 (F1 for NSR, AF, and O were 0.90, 0.80, and 0.70, respectively).



Compressed Sensing, ASBSR-method of image sampling and reconstruction and the problem of digital image acquisition with the lowest possible sampling rate

The problem of minimization of the number of measurements needed for digital
image acquisition and reconstruction with a given accuracy is addressed. Basics
of the sampling theory are outlined to show that the lower bound of signal
sampling rate sufficient for signal reconstruction with a given accuracy is
equal to the spectrum sparsity of the signal sparse approximation that has this
accuracy. It is revealed that the compressed sensing approach, which was
advanced as a solution to the sampling rate minimization problem, is far from
reaching the sampling rate theoretical minimum. Potentials and limitations of
compressed sensing are demystified using a simple and intutive model, A method
of image Arbitrary Sampling and Bounded Spectrum Reconstruction (ASBSR-method)
is described that allows to draw near the image sampling rate theoretical
minimum. Presented and discussed are also results of experimental verification
of the ASBSR-method and its possible applicability extensions to solving
various underdetermined inverse problems such as color image demosaicing, image
in-painting, image reconstruction from their sparsely sampled or decimated
projections, image reconstruction from the modulus of its Fourier spectrum, and
image reconstruction from its sparse samples in Fourier domain



Distributed Coordinated Multicell Beamforming for Wireless Cellular Networks Powered by Renewables: A Stochastic ADMM Approach

The integration of renewable energy sources (RES) has facilitated efficient
and sustainable resource allocation for wireless communication systems. In this
paper, a novel framework is introduced to develop coordinated multicell
beamforming (CMBF) design for wireless cellular networks powered by a smart
microgrid, where the BSs are equipped with RES harvesting devices and can
perform two-way (i.e., buying/selling) energy trading with the main grid. To
this end, new models are put forth to account for the stochastic RES
harvesting, two-way energy trading, and conditional value-at-risk (CVaR) based
energy transaction cost. Capitalizing on these models, we propose a distributed
CMBF solution to minimize the grid-wide transaction cost subject to user
quality-of-service (QoS) constraints. Specifically, relying on state-of-the-art
optimization tools, we show that the relevant task can be formulated as a
convex problem that is well suited for development of a distributed solver. To
cope with stochastic availability of the RES, the stochastic alternating
direction method of multipliers (ADMM) is then leveraged to develop a novel
distributed CMBF scheme. It is established that the proposed scheme is
guaranteed to yield the optimal CMBF solution, with only local channel state
information available at each BS and limited information exchange among the
BSs. Numerical results are provided to corroborate the merits of the proposed
scheme.



Intentional Aliasing Method to Improve Sub-Nyquist Sampling System

A modulated wideband converter (MWC) has been introduced as a sub-Nyquist
sampler that exploits a set of fast alternating pseudo random (PR) signals.
Through parallel sampling branches, an MWC compresses a multiband spectrum by
mixing it with PR signals in the time domain, and acquires its sub-Nyquist
samples. Previously, the ratio of compression was fully dependent on the
specifications of PR signals. That is, to further reduce the sampling rate
without information loss, faster and longer-period PR signals were needed.
However, the implementation of such PR signal generators results in high power
consumption and large fabrication area. In this paper, we propose a novel
aliased modulated wideband converter (AMWC), which can further reduce the
sampling rate of MWC with fixed PR signals. The main idea is to induce
intentional signal aliasing at the analog-to-digital converter (ADC). In
addition to the first spectral compression by the signal mixer, the intentional
aliasing compresses the mixed spectrum once again. We demonstrate that AMWC
reduces the number of sampling branches and the rate of ADC for lossless
sub-Nyquist sampling without needing to upgrade the speed or period of PR
signals. Conversely, for a given fixed number of sampling branches and sampling
rate, AMWC improves the performance of signal reconstruction.



Towards CT-quality Ultrasound Imaging using Deep Learning

The cost-effectiveness and practical harmlessness of ultrasound imaging have
made it one of the most widespread tools for medical diagnosis. Unfortunately,
the beam-forming based image formation produces granular speckle noise,
blurring, shading and other artifacts. To overcome these effects, the ultimate
goal would be to reconstruct the tissue acoustic properties by solving a full
wave propagation inverse problem. In this work, we make a step towards this
goal, using Multi-Resolution Convolutional Neural Networks (CNN). As a result,
we are able to reconstruct CT-quality images from the reflected ultrasound
radio-frequency(RF) data obtained by simulation from real CT scans of a human
body. We also show that CNN is able to imitate existing computationally heavy
despeckling methods, thereby saving orders of magnitude in computations and
making them amenable to real-time applications.



Beat by Beat: Classifying Cardiac Arrhythmias with Recurrent Neural Networks

With tens of thousands of electrocardiogram (ECG) records processed by mobile
cardiac event recorders every day, heart rhythm classification algorithms are
an important tool for the continuous monitoring of patients at risk. We utilise
an annotated dataset of 12,186 single-lead ECG recordings to build a diverse
ensemble of recurrent neural networks (RNNs) that is able to distinguish
between normal sinus rhythms, atrial fibrillation, other types of arrhythmia
and signals that are too noisy to interpret. In order to ease learning over the
temporal dimension, we introduce a novel task formulation that harnesses the
natural segmentation of ECG signals into heartbeats to drastically reduce the
number of time steps per sequence. Additionally, we extend our RNNs with an
attention mechanism that enables us to reason about which heartbeats our RNNs
focus on to make their decisions. Through the use of attention, our model
maintains a high degree of interpretability, while also achieving
state-of-the-art classification performance with an average F1 score of 0.79 on
an unseen test set (n=3,658).



Coupling Brain-Tumor Biophysical Models and Diffeomorphic Image Registration

We present the SIBIA (Scalable Integrated Biophysics-based Image Analysis)
framework for joint image registration and biophysical inversion and we apply
it to analyse MR images of glioblastomas (primary brain tumors). In particular,
we consider the following problem. Given the segmentation of a normal brain MRI
and the segmentation of a cancer patient MRI, we wish to determine tumor growth
parameters and a registration map so that if we "grow a tumor" (using our tumor
model) in the normal segmented image and then register it to the patient
segmented image, then the registration mismatch is as small as possible. We
call this "the coupled problem" because it two-way couples the biophysical
inversion and registration problems. In the image registration step we solve a
large-deformation diffeomorphic registration problem parameterized by an
Eulerian velocity field. In the biophysical inversion step we estimate
parameters in a reaction-diffusion tumor growth model that is formulated as a
partial differential equation.
  In this paper, our contributions are the introduction of a PDE-constrained
optimization formulation of the coupled problem, the derivation of the
optimality conditions, and the derivation of a Picard iterative scheme for the
solution of the coupled problem. In addition, we perform several tests to
experimentally assess the performance of our method on synthetic and clinical
datasets. We demonstrate the convergence of the SIBIA optimization solver in
different usage scenarios. We demonstrate that we can accurately solve the
coupled problem in three dimensions ($256^3$ resolution) in a few minutes using
11 dual-x86 nodes. Also, we demonstrate that, with our coupled approach, we can
successfully register normal MRI to tumor-bearing MRI while obtaining Dice
coefficients that match those achieved when registering of normal-to-normal
MRI.



Social Learning Against Data Falsification in Sensor Networks

Although surveillance and sensor networks play a key role in Internet of
Things, sensor nodes are usually vulnerable to tampering due to their
widespread locations. In this letter we consider data falsification attacks
where an smart attacker takes control of critical nodes within the network,
including nodes serving as fusion centers. In order to face this critical
security thread, we propose a data aggregation scheme based on social learning,
resembling the way in which agents make decisions in social networks. Our
results suggest that social learning enables network resilience, even when a
significant portion of the nodes have been compromised by the attacker.
Finally, we show the suitability of our scheme to sensor networks by developing
a low-complexity algorithm to facilitate the social learning data fusion rule
in devices with restricted computational power.



Design Considerations of a Sub-50 {\mu}W Receiver Front-end for Implantable Devices in MedRadio Band

Emerging health-monitor applications, such as information transmission
through multi-channel neural implants, image and video communication from
inside the body etc., calls for ultra-low active power (<50${\mu}$W) high
data-rate, energy-scalable, highly energy-efficient (pJ/bit) radios. Previous
literature has strongly focused on low average power duty-cycled radios or low
power but low-date radios. In this paper, we investigate power performance
trade-off of each front-end component in a conventional radio including active
matching, down-conversion and RF/IF amplification and prioritize them based on
highest performance/energy metric. The analysis reveals 50${\Omega}$ active
matching and RF gain is prohibitive for 50${\mu}$W power-budget. A mixer-first
architecture with an N-path mixer and a self-biased inverter based baseband
LNA, designed in TSMC 65nm technology show that sub 50${\mu}$W performance can
be achieved up to 10Mbps (< 5pJ/b) with OOK modulation.



Power Minimization Techniques in Distributed Base Station Antenna Systems using Non-Orthogonal Multiple Access

This paper introduces new approaches for combining non-orthogonal multiple
access (NOMA) with distributed base station (DBS) deployments. The purpose of
the study is to unlock the true potentials of DBS systems in the NOMA context,
since all previous works dealing with power minimization in NOMA are performed
in the CBS (centralized base station) context. This work targets a minimization
of the total transmit power in each cell, under user rate and power
multiplexing constraints. Different techniques are designed for the joint
allocation of subcarriers, antennas and power, with a particular care given to
insuring a moderate complexity. Results show an important gain in the total
transmit power obtained by the DBS-NOMA combination, with respect to both
DBS-OMA (orthogonal multiple access) and CBS-NOMA deployment scenarios.



Representation Learning of Music Using Artist Labels

In music domain, feature learning has been conducted mainly in two ways:
unsupervised learning based on sparse representations or supervised learning by
semantic labels such as music genre. However, finding discriminative features
in an unsupervised way is challenging and supervised feature learning using
semantic labels may involve noisy or expensive annotation. In this paper, we
present a supervised feature learning approach using artist labels annotated in
every single track as objective meta data. We propose two deep convolutional
neural networks (DCNN) to learn the deep artist features. One is a plain DCNN
trained with the whole artist labels simultaneously, and the other is a Siamese
DCNN trained with a subset of the artist labels based on the artist identity.
We apply the trained models to music classification and retrieval tasks in
transfer learning settings. The results show that our approach is comparable to
previous state-of-the-art methods, indicating that the proposed approach
captures general music audio features as much as the models learned with
semantic labels. Also, we discuss the advantages and disadvantages of the two
models.



A Fast Blind Impulse Detector for Bernoulli-Gaussian Noise in Underspread Channel

The Bernoulli-Gaussian (BG) model is practical to characterize impulsive
noises that widely exist in various communication systems. To estimate the BG
model parameters from noise measurements, a precise impulse detection is
essential. In this paper, we propose a novel blind impulse detector, which is
proven to be fast and accurate for BG noise in underspread communication
channels.



Electric Vehicle Battery Swapping Station

Providing adequate charging infrastructure plays a momentous role in rapid
proliferation of Electric Vehicles (EVs). Easy access to such infrastructure
would remove various obstacles regarding limited EV mobility range. A Battery
Swapping Station (BSS) is an effective approach in supplying power to the EVs,
while mitigating long waiting times in a Battery Charging Station (BCS). In
contrast with the BCS, the BSS charges the batteries in advance and prepares
them to be swapped in a considerably short time. Considering that these
stations can serve as an intermediate entity between the EV owners and the
power system, they can potentially provide unique benefits to the power system.
This paper investigates the advantages of building the BSS from various
perspectives. Accordingly, a model for the scheduling of battery charging from
the station owner perspective is proposed. An illustrative example is provided
to show how the proposed model would help BSS owners in managing their assets
through scheduling battery charging time.



Visualizing Sensor Network Coverage with Location Uncertainty

We present an interactive visualization system for exploring the coverage in
sensor networks with uncertain sensor locations. We consider a simple case of
uncertainty where the location of each sensor is confined to a discrete number
of points sampled uniformly at random from a region with a fixed radius.
Employing techniques from topological data analysis, we model and visualize
network coverage by quantifying the uncertainty defined on its simplicial
complex representations. We demonstrate the capabilities and effectiveness of
our tool via the exploration of randomly distributed sensor networks.



TDoA Based Positioning using Ultrasound Signals and Wireless Nodes

In this paper, a positioning technique based on Time Difference of Arrival
(TDoA) measurements is analyzed. The proposed approach is designed to consent
range and position estimation, using ultrasound transmissions of a stream of
chirp pulses, received by a set of wireless nodes. A potential source of
inaccuracy introduced by lack of synchronization between transmitting node and
receiving nodes is identified and characterized. An algorithm to identify and
correct such inaccuracies is presented.



Accurate Estimation of a Coil Magnetic Dipole Moment

In this paper, a technique for accurate estimation of the moment of magnetic
dipole is proposed. The achievable accuracy is investigated, as a function of
measurement noise affecting estimation of magnetic field cartesian components.
The proposed technique is validated both via simulations and experimentally.



An Interactive System for Exhibitions in a Science and Technology Center

This paper presents the development of a system for realizing interactive
exhibitions in the context of a science and technology center. The core
functionality of the system is provided by a positioning subsystem comprised of
a fixed infrastructure of transmitters and a sensor worn by a user. The
operating principle of the positioning system is based on inductive coupling of
resonators. Information about the position of the user is transferred to an
information system for processing and displaying. Possible use cases include
interactive games, information retrieval interfaces and educational scenarios.



Best Linear Approximation of Wiener Systems Using Multilevel Signals: Theory and Experiments

The problem of measuring the best linear approximation of a nonlinear system
by means of multilevel excitation sequences is analyzed. A comparison between
different types of sequences applied at the input of Wiener systems is provided
by numerical simulations and by experiments on a practical circuit including an
analog filter and a clipping nonlinearity. The performance of the sequences is
compared with a white Gaussian noise signal for reference purposes. The
theoretical characterization of the best linear approximation when using
randomized constrained sequences is derived analytically for the cubic
nonlinearity case. Numerical and experimental results show that the randomized
constrained approach for designing ternary sequences has a low sensitivity to
both even and odd order nonlinearities, resulting in a response close to the
actual response of the underlying linear system.



Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning

We present Deep Voice 3, a fully-convolutional attention-based neural
text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural
speech synthesis systems in naturalness while training ten times faster. We
scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more
than eight hundred hours of audio from over two thousand speakers. In addition,
we identify common error modes of attention-based speech synthesis networks,
demonstrate how to mitigate them, and compare several different waveform
synthesis methods. We also describe how to scale inference to ten million
queries per day on one single-GPU server.



On-the-fly Adaptive $k$-Space Sampling for Linear MRI Reconstruction Using Moment-Based Spectral Analysis

In high-dimensional magnetic resonance imaging applications, time-consuming,
sequential acquisition of data samples in the spatial frequency domain
($k$-space) can often be accelerated by accounting for dependencies along
imaging dimensions other than space in linear reconstruction, at the cost of
noise amplification that depends on the sampling pattern. Examples are
support-constrained, parallel, and dynamic MRI, and $k$-space sampling
strategies are primarily driven by image-domain metrics that are expensive to
compute for arbitrary sampling patterns. It remains challenging to provide
systematic and computationally efficient automatic designs of arbitrary
multidimensional Cartesian sampling patterns that mitigate noise amplification,
given the subspace to which the object is confined. To address this problem,
this work introduces a theoretical framework that describes local geometric
properties of the sampling pattern and relates these properties to a measure of
the spread in the eigenvalues of the information matrix described by its first
two spectral moments. This new criterion is then used for very efficient
optimization of complex multidimensional sampling patterns that does not
require reconstructing images or explicitly mapping noise amplification.
Experiments with in vivo data show strong agreement between this criterion and
traditional, comprehensive image-domain- and $k$-space-based metrics,
indicating the potential of the approach for computationally efficient
(on-the-fly), automatic, and adaptive design of sampling patterns.



Heat Kernel Smoothing in Irregular Image Domains

We present the discrete version of heat kernel smoothing on graph data
structure. The method is used to smooth data in an irregularly shaped domains
in 3D images.
  New statistical properties are derived. As an application, we show how to
filter out data in the lung blood vessel trees obtained from computed
tomography. The method can be further used in representing the complex vessel
trees parametrically and extracting the skeleton representation of the trees.



Deep Triphone Embedding Improves Phoneme Recognition

In this paper, we present a novel Deep Triphone Embedding (DTE)
representation derived from Deep Neural Network (DNN) to encapsulate the
discriminative information present in the adjoining speech frames. DTEs are
generated using a four hidden layer DNN with 3000 nodes in each hidden layer at
the first-stage. This DNN is trained with the tied-triphone classification
accuracy as an optimization criterion. Thereafter, we retain the activation
vectors (3000) of the last hidden layer, for each speech MFCC frame, and
perform dimension reduction to further obtain a 300 dimensional representation,
which we termed as DTE. DTEs along with MFCC features are fed into a
second-stage four hidden layer DNN, which is subsequently trained for the task
of tied-triphone classification. Both DNNs are trained using tri-phone labels
generated from a tied-state triphone HMM-GMM system, by performing a
forced-alignment between the transcriptions and MFCC feature frames. We conduct
the experiments on publicly available TED-LIUM speech corpus. The results show
that the proposed DTE method provides an improvement of absolute 2.11% in
phoneme recognition, when compared with a competitive hybrid tied-state
triphone HMM-DNN system.



A Binary Wyner-Ziv Code Design Based on Compound LDGM-LDPC Structures

In this paper, a practical coding scheme is designed for the binary Wyner-Ziv
(WZ) problem by using nested low-density generator-matrix (LDGM) and
low-density parity-check (LDPC) codes. This scheme contains two steps in the
encoding procedure. The first step involves applying the binary quantization by
employing LDGM codes and the second one is using the syndrome-coding technique
by utilizing LDPC codes. The decoding algorithm of the proposed scheme is based
on the Sum-Product (SP) algorithm with the help of a side information available
at the decoder side. It is theoretically shown that the compound structure has
the capability of achieving the WZ bound. The proposed method approaches this
bound by utilizing the iterative message-passing algorithms in both encoding
and decoding, although theoretical results show that it is asymptotically
achievable.



Phase Locking Value revisited: teaching new tricks to an old dog

Despite the increase in calculation power in the last decades, the estimation
of brain connectivity is still a tedious task. The high computational cost of
the algorithms escalates with the square of the number of signals evaluated,
usually in the range of thousands. In this work we propose a re-formulation of
a widely used algorithm that allows the estimation of whole brain connectivity
in much smaller times. We start from the original implementation of Phase
Locking Value (PLV) and re-formulated it in a highly computational efficient
way. Besides, this formulation stresses its strong similarity with coherence,
which we used to introduce two new metrics insensitive to zero lag
synchronization, the imaginary part of PLV (iPLV) and its corrected counterpart
(ciPLV). The new implementation of PLV avoids some highly CPU-expensive
operations, and achieved a 100-fold speedup over the original algorithm. The
new derived metrics were highly robust in the presence of volume conduction.
ciPLV, in particular, proved capable of ignoring zero-lag connectivity, while
correctly estimating nonzero-lag connectivity. Our implementation of PLV makes
it possible to calculate whole-brain connectivity in much shorter times. The
results of the simulations using ciPLV suggest that this metric is ideal to
measure synchronization in the presence of volume conduction or source leakage
effects.



Optimal Experiment Design for Magnetic Resonance Fingerprinting: Cram\'er-Rao Bound Meets Spin Dynamics

Magnetic resonance (MR) fingerprinting is a new quantitative imaging
paradigm, which simultaneously acquires multiple MR tissue parameter maps in a
single experiment. In this paper, we present an estimation-theoretic framework
to perform experiment design for MR fingerprinting. Specifically, we describe a
discrete-time dynamic system to model spin dynamics, and derive an
estimation-theoretic bound, i.e., the Cramer-Rao bound (CRB), to characterize
the signal-to-noise ratio (SNR) efficiency of an MR fingerprinting experiment.
We then formulate an optimal experiment design problem, which determines a
sequence of acquisition parameters to encode MR tissue parameters with the
maximal SNR efficiency, while respecting the physical constraints and other
constraints from the image decoding/reconstruction process. We evaluate the
performance of the proposed approach with numerical simulations, phantom
experiments, and in vivo experiments. We demonstrate that the optimized
experiments substantially reduce data acquisition time and/or improve parameter
estimation. For example, the optimized experiments achieve about a factor of
two improvement in the accuracy of $T_2$ maps, while keeping similar or
slightly better accuracy of $T_1$ maps. Finally, as a remarkable observation,
we find that the sequence of optimized acquisition parameters appears to be
highly structured rather than randomly/pseudo-randomly varying as is prescribed
in the conventional MR fingerprinting experiments.



Detection of Generalized Synchronization using Echo State Networks

Generalized synchronization between coupled dynamical systems is a phenomenon
of relevance in applications that range from secure communications to
physiological modelling. Here we test the capabilities of reservoir computing
and, in particular, echo state networks for the detection of generalized
synchronization. A nonlinear dynamical system consisting of two coupled
R\"ossler chaotic attractors is used to generate temporal series consisting of
time-locked generalized synchronized sequences interleaved by unsynchronized
ones. Correctly tuned, echo state networks are able to efficiently discriminate
between unsynchronized and synchronized sequences. Compared to other
state-of-the-art techniques of synchronization detection, the online
capabilities of the proposed ESN based methodology make it a promising choice
for real-time applications aiming to monitor dynamical synchronization changes
in continuous signals.



A Survey on Hardware Implementations of Elliptic Curve Cryptosystems

In the past two decades, Elliptic Curve Cryptography (ECC) have become
increasingly advanced. ECC, with much smaller key sizes, offers equivalent
security when compared to other asymmetric cryptosystems. In this survey, an
comprehensive overview of hardware implementations of ECC is provided. We first
discuss different elliptic curves, point multiplication algorithms and
underling finite field operations over binary fields F2m and prime fields Fp
which are used in the literature for hardware implementation. Then methods,
steps and considerations of ECC implementation are presented. The
implementations of the ECC are categorized in two main groups based on
implementation technologies consist of field programmable gate array (FPGA)
based implementations and application specific integrated circuit (ASIC)
implementations. Therefore, in these categories to have a better presentation
and comparison, the implementations are presented and distinguished based on
type of finite fields. The best and newest structures in the literature are
described in more details for overall presentation of architectures and
approaches in each group of implementations. High-speed implementation is an
important factor in the ECC applications such as network servers. Also in smart
cards, Wireless Sensor Networks (WSN) and Radio Frequency Identification (RFID)
tags require to low-cost and lightweight implementations. Therefore,
implementation methods related to these applications are explored. In addition,
a classification of the previous works in terms of scalability, flexibility,
performance and cost effectiveness is provided. Finally, some words and
techniques about future works that should be considered are provided.



Silver Standard Masks for Data Augmentation Applied to Deep-Learning-Based Skull-Stripping

The bottleneck of convolutional neural networks (CNN) for medical imaging is
the number of annotated data required for training. Manual segmentation is
considered to be the "gold-standard". However, medical imaging datasets with
expert manual segmentation are scarce as this step is time-consuming and
expensive. We propose in this work the use of what we refer to as silver
standard masks for data augmentation in deep-learning-based skull-stripping
also known as brain extraction. We generated the silver standard masks using
the consensus algorithm Simultaneous Truth and Performance Level Estimation
(STAPLE). We evaluated CNN models generated by the silver and gold standard
masks. Then, we validated the silver standard masks for CNNs training in one
dataset, and showed its generalization to two other datasets. Our results
indicated that models generated with silver standard masks are comparable to
models generated with gold standard masks and have better generalizability.
Moreover, our results also indicate that silver standard masks could be used to
augment the input dataset at training stage, reducing the need for manual
segmentation at this step.



Pulse rate estimation using imaging photoplethysmography: generic framework and comparison of methods on a publicly available dataset

Objective: to establish an algorithmic framework and a benchmark dataset for
comparing methods of pulse rate estimation using imaging photoplethysmography
(iPPG). Approach: first we reveal essential steps of pulse rate estimation from
facial video and review methods applied at each of the steps. Then we
investigate performance of these methods for DEAP dataset
www.eecs.qmul.ac.uk/mmv/datasets/deap/ containing facial videos and reference
contact photoplethysmograms. Main results: best assessment precision is
achieved when pulse rate is estimated using continuous wavelet transform from
iPPG extracted by the POS method (overall mean absolute error below 2 heart
beats per minute). Significance: we provide a generic framework for theoretical
comparison of methods for pulse rate estimation from iPPG and report results
for the most popular methods on a publicly available dataset that can be used
as a benchmark.



Listening to the World Improves Speech Command Recognition

We study transfer learning in convolutional network architectures applied to
the task of recognizing audio, such as environmental sound events and speech
commands. Our key finding is that not only is it possible to transfer
representations from an unrelated task like environmental sound classification
to a voice-focused task like speech command recognition, but also that doing so
improves accuracies significantly. We also investigate the effect of increased
model capacity for transfer learning audio, by first validating known results
from the field of Computer Vision of achieving better accuracies with
increasingly deeper networks on two audio datasets: UrbanSound8k and the newly
released Google Speech Commands dataset. Then we propose a simple multiscale
input representation using dilated convolutions and show that it is able to
aggregate larger contexts and increase classification performance. Further, the
models trained using a combination of transfer learning and multiscale input
representations need only 40% of the training data to achieve similar
accuracies as a freshly trained model with 100% of the training data. Finally,
we demonstrate a positive interaction effect for the multiscale input and
transfer learning, making a case for the joint application of the two
techniques.



Platoon Stability and Safety Analysis of Cooperative Adaptive Cruise Control under Wireless Rician Fading Channels and Jamming Attacks

Cooperative Adaptive Cruise Control (CACC) is considered as a key enabling
technology to automatically regulate the inter-vehicle distances in a vehicle
platoon to improve traffic efficiency while maintaining safety. Although the
wireless communication and physical processes in the existing CACC systems are
integrated in one control framework, the coupling between wireless
communication reliability and system states is not well modeled. Furthermore,
the research on the impact of jamming attacks on the system stability and
safety is largely open. In this paper, we conduct a comprehensive analysis on
the stability and safety of the platoon under the wireless Rician fading
channel model and jamming attacks. The effect of Rician fading and jamming on
the communication reliability is incorporated in the modeling of string
dynamics such that it captures its state dependency. Time-domain definition of
string stability is utilized to delineate the impact of Rician fading and
jamming on the CACC system's functionality and string stability. Attacker's
possible locations at which it can destabilize the string is further studied
based on the proposed model. From the safety perspective, reachable states
(i.e., inter-vehicle distances) of the CACC system under unreliable wireless
fading channels and jamming attacks is studied. Safety verification is
investigated by examining the inter-vehicle distance trajectories. We propose a
methodology to compute the upper and lower bounds of the trajectories of
inter-vehicle distances between the lead vehicle and its follower. We conduct
extensive simulations to evaluate the system stability and safety under jamming
attacks in different scenarios. We identify that channel fading can degrade the
performance of the CACC system, and the platoon's safety is highly sensitive to
jamming attacks.



A Distributed Dynamic Programming-based Solution for Load Management in Smart Grids

Load management is being recognized as an important option for active user
participation in the energy market. Traditional load management methods usually
require a centralized powerful control center and a two-way communication
network between the system operators and energy end-users. The increasing user
participation in smart grids may limit their applications. In this paper, a
distributed solution for load management in emerging smart grids is proposed.
The load management problem is formulated as a constrained optimization problem
aiming at maximizing the overall utility of users while meeting the requirement
for load reduction requested by the system operator, and is solved by using a
distributed dynamic programming algorithm. The algorithm is implemented via a
distributed framework and thus can deliver a highly desired distributed
solution. It avoids the required use of a centralized coordinator or control
center, and can achieve satisfactory outcomes for load management. Simulation
results with various test systems demonstrate its effectiveness.



Hand Gesture Recognition Using Ultrasonic Waves

This paper presents a new method for detecting and classifying a predefined
set of hand gestures using a single transmitter and a single receiver utilizing
a linearly frequency modulated ultrasonic signal. Gestures are identified based
on estimated range and received signal strength (RSS) of reflected signal from
the hand. Support Vector Machine (SVM) was used for gesture detection and
classification. The system was tested using experimental setup and achieved an
average accuracy of 88%.



On the Conditioning of the Spherical Harmonic Matrix for Spatial Audio Applications

In this paper, we attempt to study the conditioning of the Spherical Harmonic
Matrix (SHM), which is widely used in the discrete, limited order orthogonal
representation of sound fields. SHM's has been widely used in the audio
applications like spatial sound reproduction using loudspeakers, orthogonal
representation of Head Related Transfer Functions (HRTFs) etc. The conditioning
behaviour of the SHM depends on the sampling positions chosen in the 3D space.
Identification of the optimal sampling points in the continuous 3D space that
results in a well-conditioned SHM for any number of sampling points is a highly
challenging task. In this work, an attempt has been made to solve a discrete
version of the above problem using optimization based techniques. The discrete
problem is, to identify the optimal sampling points from a discrete set of
densely sampled positions of the 3D space, that minimizes the condition number
of SHM. This method has been subsequently utilized for identifying the geometry
of loudspeakers in the spatial sound reproduction, and in the selection of
spatial sampling configurations for HRTF measurement. The application specific
requirements have been formulated as additional constraints of the optimization
problem. Recently developed mixed-integer optimization solvers have been used
in solving the formulated problem. The performance of the obtained sampling
position in each application is compared with the existing configurations.
Objective measures like condition number, D-measure, and spectral distortion
are used to study the performance of the sampling configurations resulting from
the proposed and the existing methods. It is observed that the proposed
solution is able to find the sampling points that results in a better
conditioned SHM and also maintains all the application specific requirements.



Inferring Room Semantics Using Acoustic Monitoring

Having knowledge of the environmental context of the user i.e. the knowledge
of the users' indoor location and the semantics of their environment, can
facilitate the development of many of location-aware applications. In this
paper, we propose an acoustic monitoring technique that infers semantic
knowledge about an indoor space \emph{over time,} using audio recordings from
it. Our technique uses the impulse response of these spaces as well as the
ambient sounds produced in them in order to determine a semantic label for
them. As we process more recordings, we update our \emph{confidence} in the
assigned label. We evaluate our technique on a dataset of single-speaker human
speech recordings obtained in different types of rooms at three university
buildings. In our evaluation, the confidence\emph{ }for the true label
generally outstripped the confidence for all other labels and in some cases
converged to 100\% with less than 30 samples.



Online Training of LSTM Networks in Distributed Systems for Variable Length Data Sequences

In this brief paper, we investigate online training of Long Short Term Memory
(LSTM) architectures in a distributed network of nodes, where each node employs
an LSTM based structure for online regression. In particular, each node
sequentially receives a variable length data sequence with its label and can
only exchange information with its neighbors to train the LSTM architecture. We
first provide a generic LSTM based regression structure for each node. In order
to train this structure, we put the LSTM equations in a nonlinear state space
form for each node and then introduce a highly effective and efficient
Distributed Particle Filtering (DPF) based training algorithm. We also
introduce a Distributed Extended Kalman Filtering (DEKF) based training
algorithm for comparison. Here, our DPF based training algorithm guarantees
convergence to the performance of the optimal LSTM coefficients in the mean
square error (MSE) sense under certain conditions. We achieve this performance
with communication and computational complexity in the order of the first order
gradient based methods. Through both simulated and real life examples, we
illustrate significant performance improvements with respect to the state of
the art methods.



Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention

This paper describes a novel text-to-speech (TTS) technique based on deep
convolutional neural networks (CNN), without any recurrent units. Recurrent
neural network (RNN) has been a standard technique to model sequential data
recently, and this technique has been used in some cutting-edge neural TTS
techniques. However, training RNN component often requires a very powerful
computer, or very long time typically several days or weeks. Recent other
studies, on the other hand, have shown that CNN-based sequence synthesis can be
much faster than RNN-based techniques, because of high parallelizability. The
objective of this paper is to show an alternative neural TTS system, based only
on CNN, that can alleviate these economic costs of training. In our experiment,
the proposed Deep Convolutional TTS can be sufficiently trained only in a night
(15 hours), using an ordinary gaming PC equipped with two GPUs, while the
quality of the synthesized speech was almost acceptable.



Trace norm regularization and faster inference for embedded speech recognition RNNs

We propose and evaluate new techniques for compressing and speeding up dense
matrix multiplications as found in the fully connected and recurrent layers of
neural networks for embedded large vocabulary continuous speech recognition
(LVCSR). For compression, we introduce and study a trace norm regularization
technique for training low rank factored versions of matrix multiplications.
Compared to standard low rank training, we show that our method leads to good
accuracy versus number of parameter trade-offs and can be used to speed up
training of large models. For speedup, we enable faster inference on ARM
processors through new open sourced kernels optimized for small batch sizes,
resulting in 3x to 7x speed ups over the widely used gemmlowp library. Beyond
LVCSR, we expect our techniques and kernels to be more generally applicable to
embedded neural networks with large fully connected or recurrent layers.



End-to-End Optimized Speech Coding with Deep Neural Networks

Modern compression algorithms are often the result of laborious
domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took
years to develop and were largely hand-designed. We present a deep neural
network model which optimizes all the steps of a wideband speech coding
pipeline (compression, quantization, entropy coding, and decompression)
end-to-end directly from raw speech data -- no manual feature engineering
necessary, and it trains in hours. In testing, our DNN-based coder performs on
par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps).
It also runs in realtime on a 3.8GhZ Intel CPU.



Relative Transfer Function Inverse Regression from Low Dimensional Manifold

In room acoustic environments, the Relative Transfer Functions (RTFs) are
controlled by few underlying modes of variability. Accordingly, they are
confined to a low-dimensional manifold. In this letter, we investigate a RTF
inverse regression problem, the task of which is to generate the
high-dimensional responses from their low-dimensional representations. The
problem is addressed from a pure data-driven perspective and a supervised Deep
Neural Network (DNN) model is applied to learn a mapping from the
source-receiver poses (positions and orientations) to the frequency domain RTF
vectors. The experiments show promising results: the model achieves lower
prediction error of the RTF than the free field assumption. However, it fails
to compete with the linear interpolation technique in small sampling distances.



Merging the Bernoulli-Gaussian and Symmetric Alpha-Stable Models for Impulsive Noises in Narrowband Power Line Channels

To model impulsive noise in power line channels, both the Bernoulli-Gaussian
model and the symmetric alpha-stable model are usually applied. Towards a merge
of existing noise measurement databases and a simplification of communication
system design, the compatibility between the two models is of interest. In this
paper, we show that they can be approximately converted to each other under
certain constrains, although never generally unified. Based on this, we propose
a fast model conversion.



Pseudo-Zernike Moments Based Sparse Representations for SAR Image Classification

We propose radar image classification via pseudo-Zernike moments based sparse
representations. We exploit invariance properties of pseudo-Zernike moments to
augment redundancy in the sparsity representative dictionary by introducing
auxiliary atoms. We employ complex radar signatures. We prove the validity of
our proposed methods on the publicly available MSTAR dataset.



Unsupervised and Semi-supervised Anomaly Detection with LSTM Neural Networks

We investigate anomaly detection in an unsupervised framework and introduce
Long Short Term Memory (LSTM) neural network based algorithms. In particular,
given variable length data sequences, we first pass these sequences through our
LSTM based structure and obtain fixed length sequences. We then find a decision
function for our anomaly detectors based on the One Class Support Vector
Machines (OC-SVM) and Support Vector Data Description (SVDD) algorithms. As the
first time in the literature, we jointly train and optimize the parameters of
the LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective
gradient and quadratic programming based training methods. To apply the
gradient based training method, we modify the original objective criteria of
the OC-SVM and SVDD algorithms, where we prove the convergence of the modified
objective criteria to the original criteria. We also provide extensions of our
unsupervised formulation to the semi-supervised and fully supervised
frameworks. Thus, we obtain anomaly detection algorithms that can process
variable length data sequences while providing high performance, especially for
time series data. Our approach is generic so that we also apply this approach
to the Gated Recurrent Unit (GRU) architecture by directly replacing our LSTM
based structure with the GRU based structure. In our experiments, we illustrate
significant performance gains achieved by our algorithms with respect to the
conventional methods.



Convergence Analysis of l0-RLS Adaptive Filter

This paper presents first and second order convergence analysis of the
sparsity aware l0-RLS adaptive filter. The theorems 1 and 2 state the steady
state value of mean and mean square deviation of the adaptive filter weight
vector.



Image registration of low signal-to-noise cryo-STEM data

Combining multiple fast image acquisitions to mitigate scan noise and drift
artifacts has proven essential for picometer precision, quantitative analysis
of atomic resolution scanning transmission electron microscopy (STEM) data. For
very low signal-to-noise ratio (SNR) image stacks - frequently required for
undistorted imaging at liquid nitrogen temperatures - image registration is
particularly delicate, and standard approaches may either fail, or produce
subtly specious reconstructed lattice images. We present an approach which
effectively registers and averages image stacks which are challenging due to
their low-SNR and propensity for unit cell misalignments. Registering all
possible image pairs in a multi-image stack leads to significant information
surplus. In combination with a simple physical picture of stage drift, this
enables identification of incorrect image registrations, and determination of
the optimal image shifts from the complete set of relative shifts. We
demonstrate the effectiveness of our approach on experimental, cryogenic STEM
datasets, highlighting subtle artifacts endemic to low-SNR lattice images and
how they can be avoided. High-SNR average images with information transfer out
to 0.72 A are achieved at 300 kV and with the sample cooled to near liquid
nitrogen temperature.



A Link Quality Model for Generalised Frequency Division Multiplexing

5G systems aim to achieve extremely high data rates, low end-to-end latency
and ultra-low power consumption. Recently, there has been considerable interest
in the design of 5G physical layer waveforms. One important candidate is
Generalised Frequency Division Multiplexing (GFDM). In order to evaluate its
performance and features, system-level studies should be undertaken in a range
of scenarios. These studies, however, require highly complex computations if
they are performed using bit-level simulators. In this paper, the Mutual
Information (MI) based link quality model (PHY abstraction), which has been
regularly used to implement system-level studies for Orthogonal Frequency
Division Multiplexing (OFDM), is applied to GFDM. The performance of the GFDM
waveform using this model and the bit-level simulation performance is measured
using different channel types. Moreover, a system-level study for a GFDM based
LTE-A system in a realistic scenario, using both a bit-level simulator and this
abstraction model, has been studied and compared. The results reveal the
accuracy of this model using realistic channel data. Based on these results,
the PHY abstraction technique can be applied to evaluate the performance of
GFDM based systems in an effective manner with low complexity. The maximum
difference in the Packet Error Rate (PER) and throughput results in the
abstraction case compared to bit-level simulation does not exceed 4% whilst
offering a simulation time saving reduction of around 62,000 times.



On Power Allocation for Distributed Detection with Correlated Observations and Linear Fusion

We consider a binary hypothesis testing problem in an inhomogeneous wireless
sensor network, where a fusion center (FC) makes a global decision on the
underlying hypothesis. We assume sensors observations are correlated Gaussian
and sensors are unaware of this correlation when making decisions. Sensors send
their modulated decisions over fading channels, subject to individual and/or
total transmit power constraints. For parallel-access channel (PAC) and
multiple-access channel (MAC) models, we derive modified deflection coefficient
(MDC) of the test statistic at the FC with coherent reception.We propose a
transmit power allocation scheme, which maximizes MDC of the test statistic,
under three different sets of transmit power constraints: total power
constraint, individual and total power constraints, individual power
constraints only. When analytical solutions to our constrained optimization
problems are elusive, we discuss how these problems can be converted to convex
ones. We study how correlation among sensors observations, reliability of local
decisions, communication channel model and channel qualities and transmit power
constraints affect the reliability of the global decision and power allocation
of inhomogeneous sensors.



A Refined Analysis of the Gap between Expected Rate for Partial CSIT and the Massive MIMO Rate Limit

Optimal BeamFormers (BFs) that maximize the Weighted Sum Rate (WSR) for a
Multiple-Input Multiple-Output (MIMO) interference broadcast channel (IBC)
remains an important research area. Under practical scenarios, the problem is
compounded by the fact that only partial channel state information at the
transmitter (CSIT) is available. Hence, a typical choice of the optimization
metric is the Expected Weighted Sum Rate (EWSR). However, the presence of the
expectation operator makes the optimization a daunting task. On the other hand,
for the particular, but significant, special case of massive MIMO (MaMIMO), the
EWSR converges to Expected Signal covariance Expected Interference covariance
based WSR (ESEI-WSR) and this metric is more amenable to optimization.
Recently, [1] considered a multi-user Multiple-Input Single-Output (MISO)
scenario and proposed approximating the EWSR by ESEI-WSR. They then derived a
constant bound for this approximation. This paper performs a refined analysis
of the gap between EWSR and ESEI-WSR criteria for finite antenna dimensions.



Near-Optimal Sparse Sensing for Gaussian Detection with Correlated Observations

Detection of a signal under noise is a classical signal processing problem.
When monitoring spatial phenomena under a fixed budget, i.e., either physical,
economical or computational constraints, the selection of a subset of available
sensors, referred to as sparse sensing, that meets both the budget and
performance requirements is highly desirable. Unfortunately, the subset
selection problem for detection under dependent observations is combinatorial
in nature and suboptimal subset selection algorithms must be employed. In this
work, different from the widely used convex relaxation of the problem, we
leverage submodularity, the diminishing returns property, to provide practical
near optimal algorithms suitable for large-scale subset selection. This is
achieved by means of low-complexity greedy algorithms, which incur a reduced
computational complexity compared to their convex counterparts.



Offset-Based Beamforming: A New Approach to Robust Downlink Transmission

The design of a set of beamformers for the multiuser multiple-input
single-output (MISO) downlink that provides the receivers with prespecified
levels of quality-of-service (QoS) can be quite challenging when the channel
state information is not perfectly known at the base station. The constraint of
having the SINR meet or exceed a given threshold with high probability is
intractable in general, which results in problems that are fundamentally hard
to solve. In this paper, we will develop a high quality approximation of the
SINR outage constraint that, along with a semidefinite relaxation, enables us
to formulate the beamformer design problem as a convex optimization problem
that can be efficiently solved. For systems in which the uncertainty size is
small, a further approximation yields algorithms based on iterative evaluations
of closed-form expressions that have substantially lower computational cost.
Since finding the beamforming directions incurs most of the computational load
of these algorithms, analogous power loading algorithms for predefined
beamforming directions are developed and their performance is shown to be close
to optimal. When the system contains a large number of antennas, the proposed
power loading can be obtained at a computational cost that grows only linearly
in the number of antennas. The proposed power loading algorithm provides an
explicit relationship between the outage probability required and the power
consumed, which allows us to precisely control the power consumption, and
automatically identifies users who are consuming most of the power resources.
The flexibility of the proposed approach is illustrated by developing a power
loading technique that minimizes an average notion of outage.



Lip2AudSpec: Speech reconstruction from silent lip movements video

In this study, we propose a deep neural network for reconstructing
intelligible speech from silent lip movement videos. We use auditory
spectrogram as spectral representation of speech and its corresponding sound
generation method resulting in a more natural sounding reconstructed speech.
Our proposed network consists of an autoencoder to extract bottleneck features
from the auditory spectrogram which is then used as target to our main lip
reading network comprising of CNN, LSTM and fully connected layers. Our
experiments show that the autoencoder is able to reconstruct the original
auditory spectrogram with a 98% correlation and also improves the quality of
reconstructed speech from the main lip reading network. Our model, trained
jointly on different speakers is able to extract individual speaker
characteristics and gives promising results of reconstructing intelligible
speech with superior word recognition accuracy.



Estimation of Rain Attenuation at EHF bands for Earth-to-Satellite Links in Bangladesh

Due to heavy congestion in lower frequency bands, engineers are looking for
new frequency bands to support new services that require higher data rates,
which in turn needs broader bandwidths. To meet this requirement, extremely
high frequency (EHF), particularly Q (36 to 46 GHz) and V (46 to 56 GHz) bands,
is the best viable solution because of its complete availability. The most
serious challenge the EHF band poses is the attenuation caused by rain. This
paper investigates the effect of the rain on Q and V bands' performances in
Bangladeshi climatic conditions. The rain attenuations of the two bands are
predicted for the four main regions of Bangladesh using ITU rain attenuation
model. The measured rain statistics is used for this prediction. It is observed
that the attenuation due to rain in the Q/V band reaches up to 150 dB which is
much higher than that of the currently used Ka band. The variability of the
rain attenuation is also investigated over different sessions of Bangladesh.
The attenuation varies from 40 dB to 170 dB depending on the months. Finally,
the amount of rain fade required to compensate the high rain attenuation is
also predicted for different elevation angles.



Probabilistic Available Delivery Capability Assessment of General Distribution Network with Renewables

Rapid increase of renewable energy sources and electric vehicles in utility
distribution feeders introduces more and more uncertainties. To investigate how
such uncertainties may affect the available delivery capability (ADC) of the
distribution network, it is imperative to employ a probabilistic analysis
framework. In this paper, a formulation for probabilistic ADC incorporating
renewable generators and load variations is proposed; a computationally
efficient method to solve the probabilistic ADC is presented, which combines
the up-to-date sparse polynomial chaos expansion (PCE) and the continuation
method. A numerical example in the IEEE 13 node test feeder is given to
demonstrate the accuracy and efficiency of the proposed method.



A Study Of Optimal False Information Injection Attack On Dynamic State Estimation in Multi-Sensor Systems

In this paper, the impact of false information injection is investigated for
linear dynamic systems with multiple sensors. It is assumed that the system is
unsuspecting the existence of false information and the adversary is trying to
maximize the negative effect of the false information on Kalman filter's
estimation performance. The false information attack under different conditions
is mathematically characterized. For the adversary, many closed-form results
for the optimal attack strategies that maximize Kalman filter's estimation
error are theoretically derived. It is shown that by choosing the optimal
correlation coefficients among the bias noises and allocating power optimally
among sensors, the adversary could significantly increase Kalman filter's
estimation errors. To be concrete, a target tracking system is used as an
example in the paper. From the adversary's point of view, the best attack
strategies are obtained under different scenarios, including a single-sensor
system with both position and velocity measurements, and a multi-sensor system
with position and velocity measurements. Under a constraint on the total power
of the injected bias noises, the optimal solutions are solved from two
perspectives: trace and determinant. Numerical results are also provided in
order to illustrate the effectiveness of the proposed attack strategies.



Acoustic Landmarks Contain More Information About the Phone String than Other Frames for Automatic Speech Recognition with Deep Neural Network Acoustic Model

Most mainstream Automatic Speech Recognition (ASR) systems consider all
feature frames equally important. However, acoustic landmark theory is based on
a contradictory idea, that some frames are more important than others. Acoustic
landmark theory exploits quantal non-linearities in the articulatory-acoustic
and acoustic-perceptual relations to define landmark times at which the speech
spectrum abruptly changes or reaches an extremum; frames overlapping landmarks
have been demonstrated to be sufficient for speech perception. In this work, we
conduct experiments on the TIMIT corpus, with both GMM and DNN based ASR
systems and find that frames containing landmarks are more informative for ASR
than others. We find that altering the level of emphasis on landmarks by
re-weighting acoustic likelihood tends to reduce the phone error rate (PER).
Furthermore, by leveraging the landmark as a heuristic, one of our hybrid DNN
frame dropping strategies maintained a PER within 0.44% of optimal when scoring
less than half (45.8% to be precise) of the frames. This hybrid strategy
out-performs other non-heuristic-based methods and demonstrate the potential of
landmarks for reducing computation.



Separation of Moving Sound Sources Using Multichannel NMF and Acoustic Tracking

In this paper we propose a method for separation of moving sound sources. The
method is based on first tracking the sources and then estimation of source
spectrograms using multichannel non-negative matrix factorization (NMF) and
extracting the sources from the mixture by single-channel Wiener filtering. We
propose a novel multichannel NMF model with time-varying mixing of the sources
denoted by spatial covariance matrices (SCM) and provide update equations for
optimizing model parameters minimizing squared Frobenius norm. The SCMs of the
model are obtained based on estimated directions of arrival of tracked sources
at each time frame. The evaluation is based on established objective separation
criteria and using real recordings of two and three simultaneous moving sound
sources. The compared methods include conventional beamforming and ideal ratio
mask separation. The proposed method is shown to exceed the separation quality
of other evaluated blind approaches according to all measured quantities.
Additionally, we evaluate the method's susceptibility towards tracking errors
by comparing the separation quality achieved using annotated ground truth
source trajectories.



Direction of arrival estimation for multiple sound sources using convolutional recurrent neural network

This paper proposes a deep neural network for estimating the directions of
arrival (DOA) of multiple sound sources. The proposed stacked convolutional and
recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS)
along with the DOA estimates in both azimuth and elevation. We avoid any
explicit feature extraction step by using the magnitudes and phases of the
spectrograms of all the channels as input to the network. The proposed DOAnet
is evaluated by estimating the DOAs of multiple concurrently present sources in
anechoic, matched and unmatched reverberant conditions. The results show that
the proposed DOAnet is capable of estimating the number of sources and their
respective DOAs with good precision and generate SPS with high signal-to-noise
ratio.



Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion Recognition

Long short-term memory (LSTM) is normally used in recurrent neural network
(RNN) as basic recurrent unit. However,conventional LSTM assumes that the state
at current time step depends on previous time step. This assumption constraints
the time dependency modeling capability. In this study, we propose a new
variation of LSTM, advanced LSTM (A-LSTM), for better temporal context
modeling. We employ A-LSTM in weighted pooling RNN for emotion recognition. The
A-LSTM outperforms the conventional LSTM by 5.5% relatively. The A-LSTM based
weighted pooling RNN can also complement the state-of-the-art emotion
classification framework. This shows the advantage of A-LSTM.



An Automated Window Selection Procedure For DFT Based Detection Schemes To Reduce Windowing SNR Loss

The classical spectrum analysis methods utilize window functions to reduce
the masking effect of a strong spectral component over weaker components. The
main cost of side-lobe reduction is the reduction of signal-to-noise ratio
(SNR) level of the output spectrum. We present a single snapshot method which
optimizes the selection of most suitable window function among a finite set of
candidate windows, say rectangle, Hamming, Blackman windows, for each spectral
bin. The main goal is to utilize different window functions at each spectral
output depending on the interference level encountered at that spectral bin so
as to reduce the SNR loss associated with the windowing operation. Stated
differently, the windows with strong interference suppression capabilities are
utilized only when a sufficiently powerful interferer is corrupting the
spectral bin of interest is present, i.e. only when this window is needed. The
achieved reduction in the windowing SNR loss can be important for the detection
of low SNR targets.



BridgeNets: Student-Teacher Transfer Learning Based on Recursive Neural Networks and its Application to Distant Speech Recognition

Despite the remarkable progress achieved on automatic speech recognition,
recognizing far-field speeches mixed with various noise sources is still a
challenging task. In this paper, we introduce novel student-teacher transfer
learning, BridgeNet which can provide a solution to improve distant speech
recognition. There are two key features in BridgeNet. First, BridgeNet extends
traditional student-teacher frameworks by providing multiple hints from a
teacher network. Hints are not limited to the soft labels from a teacher
network. Teacher's intermediate feature representations can better guide a
student network to learn how to denoise or dereverberate noisy input. Second,
the proposed recursive architecture in the BridgeNet can iteratively improve
denoising and recognition performance. The experimental results of BridgeNet
showed significant improvements in tackling the distant speech recognition
problem, where it achieved up to 13.24% relative WER reductions on AMI corpus
compared to a baseline neural network without teacher's hints.



Unified Functorial Signal Representation III: Foundations, Redundancy, $L^0$ and $L^2$ functors

In this paper we propose and lay the foundations of a functorial framework
for representing signals. By incorporating additional category-theoretic
relative and generative perspective alongside the classic set-theoretic measure
theory the fundamental concepts of redundancy, compression are formulated in a
novel authentic arrow-theoretic way. The existing classic framework
representing a signal as a vector of appropriate linear space is shown as a
special case of the proposed framework.
  Next in the context of signal-spaces as a categories we study the various
covariant and contravariant forms of $L^0$ and $L^2$ functors using categories
of measurable or measure spaces and their opposites involving Boolean and
measure algebras along with partial extension. Finally we contribute a novel
definition of intra-signal redundancy using general concept of isomorphism
arrow in a category covering the translation case and others as special cases.
Through category-theory we provide a simple yet precise explanation for the
well-known heuristic of lossless differential encoding standards yielding
better compressions in image types such as line drawings, iconic image, text
etc; as compared to classic representation techniques such as JPEG which choose
bases or frames in a global Hilbert space.



Reconfigurable Power Electronics Topologies

This paper presents two novel topologies for automatically transforming power
converter topology from three-phase 3-level cascaded H-bridge to three-phase
2-level converter design. These techniques are implemented by flicking specific
switches to rearrange circuit connections. The switches can be controlled by
signals in order to realize automation.



Distributed Change Detection via Average Consensus over Networks

Distributed change-point detection has been a fundamental problem when
performing real-time monitoring using sensor-networks. We propose a distributed
detection algorithm, where each sensor only exchanges CUSUM statistic with
their neighbors based on the average consensus scheme, and an alarm is raised
when local consensus statistic exceeds a pre-specified global threshold. We
provide theoretical performance bounds showing that the performance of the
fully distributed scheme can match the centralized algorithms under some mild
conditions. Numerical experiments demonstrate the good performance of the
algorithm especially in detecting asynchronous changes.



Single wavelength 480 Gb/s direct detection over 80km SSMF enabled by Stokes Vector Kramers Kronig transceiver

We propose 4D modulation with directed detection employing a novel
Stokes-Vector Kramers-Kronig transceiver. It shows that employing Stokes vector
receiver, transmitted digital carrier and Kramers-Kronig detection offers an
effective way to de-rotate polarization multiplexed complex double side band
signal without using a local oscillator at receiver. The impact of system
parameters and configurations including carrier-to-signal-power ratio, guard
band of the digital carrier, oversampling ratio and real MIMO is experimentally
investigated. Finally, a record 480 Gb/s data rate over 80 km SSMF is achieved
in a 60 Gbaud PDM-16QAM single carrier experiment with a BER below the
threshold of 2.0x10-2



Jointly Tracking and Separating Speech Sources Using Multiple Features and the generalized labeled multi-Bernoulli Framework

This paper proposes a novel joint multi-speaker tracking-and-separation
method based on the generalized labeled multi-Bernoulli (GLMB) multi-target
tracking filter, using sound mixtures recorded by microphones. Standard
multi-speaker tracking algorithms usually only track speaker locations, and
ambiguity occurs when speakers are spatially close. The proposed multi-feature
GLMB tracking filter treats the set of vectors of associated speaker features
(location, pitch and sound) as the multi-target multi-feature observation,
characterizes transitioning features with corresponding transition models and
overall likelihood function, thus jointly tracks and separates each
multi-feature speaker, and addresses the spatial ambiguity problem. Numerical
evaluation verifies that the proposed method can correctly track locations of
multiple speakers and meanwhile separate speech signals.



Investigation of Frame Alignments for GMM-based Digit-prompted Speaker Verification

Frame alignments can be computed by different methods in GMM-based speaker
verification. By incorporating a phonetic Gaussian mixture model (PGMM), we are
able to compare the performance using alignments extracted from the deep neural
networks (DNN) and the conventional hidden Markov model (HMM) in digit-prompted
speaker verification. Based on the different characteristics of these two
alignments, we present a novel content verification method to improve the
system security without much computational overhead. Our experiments on the
RSR2015 Part-3 digit-prompted task show that, the DNN based alignment performs
on par with the HMM alignment. The results also demonstrate the effectiveness
of the proposed Kullback-Leibler (KL) divergence based scoring to reject speech
with incorrect pass-phrases.



Shift-enabled graphs: Graphs where shift-invariant filters are representable as polynomials of shift operations

In digital signal processing, shift-invariant filters can be represented as a
polynomial expansion of a shift operation,that is, the Z-transform
representation. When extended to graph signal processing (GSP), this would mean
that a shift-invariant graph filter can be represented as a polynomial of the
adjacency (shift) matrix of the graph. However, the characteristic and minimum
polynomials of the adjacency matrix must be identical for the property to hold.
While it has been suggested that this condition might be ignored as it is
always possible to find a polynomial transform to represent the original
adjacency matrix by another adjacency matrix that satisfies the condition, this
letter shows that a filter that is shift invariant in terms of the original
graph may not be shift invariant anymore under the modified graph and vice
versa. We introduce the notion of "shift-enabled graph" for graphs that satisfy
the aforementioned condition, and present a concrete example of a graph that is
not "shift-enabled" and a shift-invariant filter that is not a polynomial of
the shift operation matrix. The result provides a deeper understanding of
shift-invariant filters when applied in GSP and shows that further
investigation of shift-enabled graphs is needed to make it applicable to
practical scenarios.



Sample-level CNN Architectures for Music Auto-tagging Using Raw Waveforms

Recent work has shown that the end-to-end approach using convolutional neural
network (CNN) is effective in various types of machine learning tasks. For
audio signals, the approach takes raw waveforms as input using an 1-D
convolution layer. In this paper, we improve the 1-D CNN architecture for music
auto-tagging by adopting building blocks from state-of-the-art image
classification models, ResNets and SENets, and adding multi-level feature
aggregation to it. We compare different combinations of the modules in building
CNN architectures. The results show that they achieve significant improvements
over previous state-of-the-art models on the MagnaTagATune dataset and
comparable results on Million Song Dataset. Furthermore, we analyze and
visualize our model to show how the 1-D CNN operates.



Feasibility Study of OFDM-MFSK Modulation Scheme for Smart Metering Technology

The Orthogonal Frequency Division Multiplexing based M-ary Frequency Shift
Keying (OFDM-MFSK) is a noncoherent modulation scheme which merges MFSK with
the OFDM waveform. It is designed to improve the receiver sensitivity in the
hard environments where channel estimation is very difficult to perform. In
this paper, the OFDM-MFSK is suggested for the smart metering technology and
its performance is measured and compared with the ordinary OFDM-BPSK. Our
results show that, depending on the MFSK size value (M), the Packet Error Rate
(PER) has dramatically improved for OFDM-MFSK. Additionally, the adaptive
OFDM-MFSK, which selects the best M value that gives the minimum PER and higher
throughput for each Smart Meter (SM), has better coverage than OFDM-BPSK.
Although its throughput and capacity are lower than OFDMBPSK, the connected SMs
per sector are higher. Based on the smart metering technology requirements
which imply the need for high coverage and low amount of data exchanged between
the network and the SMs, The OFDM-MFSK can be efficiently used in this
technology.



Generalized End-to-End Loss for Speaker Verification

In this paper, we propose a new loss function called generalized end-to-end
(GE2E) loss, which makes the training of speaker verification models more
efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike
TE2E, the GE2E loss function updates the network in a way that emphasizes
examples that are difficult to verify at each step of the training process.
Additionally, the GE2E loss does not require an initial stage of example
selection. With these properties, our model with the new loss function
decreases speaker verification EER by more than 10%, while reducing the
training time by 60% at the same time. We also introduce the MultiReader
technique, which allows us to do domain adaptation - training a more accurate
model that supports multiple keywords (i.e. "OK Google" and "Hey Google") as
well as multiple dialects.



Speaker Diarization with LSTM

For many years, i-vector based audio embedding techniques were the dominant
approach for speaker verification and speaker diarization applications.
However, mirroring the rise of deep learning in various domains, neural network
based audio embeddings, also known as d-vectors, have consistently demonstrated
superior speaker verification performance. In this paper, we build on the
success of d-vector based speaker verification systems to develop a new
d-vector based approach to speaker diarization. Specifically, we combine
LSTM-based d-vector audio embeddings with recent work in non-parametric
clustering to obtain a state-of-the-art speaker diarization system. Our system
is evaluated on three standard public datasets, suggesting that d-vector based
diarization systems offer significant advantages over traditional i-vector
based systems. We achieved a 12.0% diarization error rate on NIST SRE 2000
CALLHOME, while our model is trained with out-of-domain data from voice search
logs.



Attention-Based Models for Text-Dependent Speaker Verification

Attention-based models have recently shown great performance on a range of
tasks, such as speech recognition, machine translation, and image captioning
due to their ability to summarize relevant information that expands through the
entire length of an input sequence. In this paper, we analyze the usage of
attention mechanisms to the problem of sequence summarization in our end-to-end
text-dependent speaker recognition system. We explore different topologies and
their variants of the attention layer, and compare different pooling methods on
the attention weights. Ultimately, we show that attention-based models can
improves the Equal Error Rate (EER) of our speaker verification system by
relatively 14% compared to our non-attention LSTM baseline model.



Sequence-to-Sequence ASR Optimization via Reinforcement Learning

Despite the success of sequence-to-sequence approaches in automatic speech
recognition (ASR) systems, the models still suffer from several problems,
mainly due to the mismatch between the training and inference conditions. In
the sequence-to-sequence architecture, the model is trained to predict the
grapheme of the current time-step given the input of speech signal and the
ground-truth grapheme history of the previous time-steps. However, it remains
unclear how well the model approximates real-world speech during inference.
Thus, generating the whole transcription from scratch based on previous
predictions is complicated and errors can propagate over time. Furthermore, the
model is optimized to maximize the likelihood of training data instead of error
rate evaluation metrics that actually quantify recognition quality. This paper
presents an alternative strategy for training sequence-to-sequence ASR models
by adopting the idea of reinforcement learning (RL). Unlike the standard
training scheme with maximum likelihood estimation, our proposed approach
utilizes the policy gradient algorithm. We can (1) sample the whole
transcription based on the model's prediction in the training process and (2)
directly optimize the model with negative Levenshtein distance as the reward.
Experimental results demonstrate that we significantly improved the performance
compared to a model trained only with maximum likelihood estimation.



Probabilistic Distribution Power Flow Based on Finite Smoothing of Data Samples Considering Plug-in Hybrid Electric Vehicles

The ever increasing penetration of plug-in hybrid electric vehicles in
distribution systems has triggered the need for a more accurate and at the same
time fast solution to probabilistic distribution power flow problem. In this
paper a novel algorithm is introduced based on finite sample points to
determine probabilistic density function of probabilistic distribution power
flow results. A modified probabilistic charging behavior of plug-in hybrid
electric vehicles at charging stations and their overlap with residential peak
load is evaluated in probabilistic distribution power flow problem. The
proposed algorithm is faster than Monte Carlo Simulation and at the same time
keeps adequate accuracy. It is applied to solve probabilistic distribution
power flow for two dimensionally different test systems and is compared with
recent probabilistic solutions. Simulation results show the accuracy and
efficiency of the proposed algorithm to calculate probability density function
of uncertain outputs.



Generalized gradient optimization over lossy networks for partition-based estimation

We address the problem of distributed convex unconstrained optimization over
networks characterized by asynchronous and possibly lossy communications. We
analyze the case where the global cost function is the sum of locally coupled
local strictly convex cost functions. As discussed in detail in a motivating
example, this class of optimization objectives is, for example, typical in
localization problems and in partition-based state estimation. Inspired by a
generalized gradient descent strategy, namely the block Jacobi iteration, we
propose a novel solution which is amenable for a distributed implementation and
which, under a suitable condition on the step size, is provably locally
resilient to communication failures. The theoretical analysis relies on the
separation of time scales and Lyapunov theory. In addition, to show the
flexibility of the proposed algorithm, we derive a resilient gradient descent
iteration and a resilient generalized gradient for quadratic programming as two
natural particularizations of our strategy. In this second case, global
robustness is provided. Finally, the proposed algorithm is numerically tested
on the IEEE 123 nodes distribution feeder in the context of partition-based
smart grid robust state estimation in the presence of measurements outliers.



A Framework for Over-the-air Reciprocity Calibration for TDD Massive MIMO Systems

One of the biggest challenges in operating massive multiple-input
multiple-output systems is the acquisition of accurate channel state
information at the transmitter. To take up this challenge, time division duplex
is more favorable thanks to its channel reciprocity between downlink and
uplink. However, while the propagation channel over the air is reciprocal, the
radio-frequency front-ends in the transceivers are not. Therefore, calibration
is required to compensate the RF hardware asymmetry.
  Although various over-the-air calibration methods exist to address the above
problem, this paper offers a unified representation of these algorithms,
providing a higher level view on the calibration problem, and introduces
innovations on calibration methods. We present a novel family of calibration
methods, based on antenna grouping, which improve accuracy and speed up the
calibration process compared to existing methods. We then provide the
Cram\'er-Rao bound as the performance evaluation benchmark and compare maximum
likelihood and least squares estimators. We also differentiate between coherent
and non-coherent accumulation of calibration measurements, and point out that
enabling non-coherent accumulation allows the training to be spread in time,
minimizing impact to the data service. Overall, these results have special
value in allowing to design reciprocity calibration techniques that are both
accurate and resource-effective.



Weighted Proportional Fair Scheduling for Downlink Non-Orthogonal Multiple Access

In this paper, a weighted proportional fair (PF) scheduling method is
proposed in the context of non-orthogonal multiple access (NOMA) with
successive interference cancellation (SIC) at the receiver side. The new scheme
introduces weights that adapt the classical PF metric to the NOMA scenario,
improving performance indicators and enabling new services. The distinguishing
value of the proposal resides in its ability to improve long term fairness and
total system throughput while achieving a high level of fairness in every
scheduling slot. Finally, it is shown that the additional complexity caused by
the weight calculation has only a limited impact on the overall scheduler
complexity while simulation results confirm the claimed improvements making the
proposal an appealing alternative for resource allocation in a cellular
downlink system.



Sound Source Localization in a Multipath Environment Using Convolutional Neural Networks

The propagation of sound in a shallow water environment is characterized by
boundary reflections from the sea surface and sea floor. These reflections
result in multiple (indirect) sound propagation paths, which can degrade the
performance of passive sound source localization methods. This paper proposes
the use of convolutional neural networks (CNNs) for the localization of sources
of broadband acoustic radiated noise (such as motor vessels) in shallow water
multipath environments. It is shown that CNNs operating on cepstrogram and
generalized cross-correlogram inputs are able to more reliably estimate the
instantaneous range and bearing of transiting motor vessels when the source
localization performance of conventional passive ranging methods is degraded.
The ensuing improvement in source localization performance is demonstrated
using real data collected during an at-sea experiment.



Content-based Representations of audio using Siamese neural networks

In this paper, we focus on the problem of content-based retrieval for audio,
which aims to retrieve all semantically similar audio recordings for a given
audio clip query. This problem is similar to the problem of query by example of
audio, which aims to retrieve media samples from a database, which are similar
to the user-provided example. We propose a novel approach which encodes the
audio into a vector representation using Siamese Neural Networks. The goal is
to obtain an encoding similar for files belonging to the same audio class, thus
allowing retrieval of semantically similar audio. Using simple similarity
measures such as those based on simple euclidean distance and cosine similarity
we show that these representations can be very effectively used for retrieving
recordings similar in audio content.



SCMA with Low Complexity Symmetric Codebook Design for Visible Light Communication

Sparse code multiple access (SCMA) is attracting significant research
interests currently, which is considered as a promising multiple access
technique for 5G systems. It serves as a good candidate for the future
communication network with massive nodes due to its capability of handling user
overloading. Introducing SCMA to visible light communication (VLC) can provide
another opportunity on design of transmission protocols for the communication
network with massive nodes due to the limited communication range of VLC, which
reduces the interference intensity. However, when applying SCMA in VLC systems,
we need to modify the SCMA codebook to accommodate the real and positive signal
requirement for VLC.We apply multidimensional constellation design methods to
SCMA codebook. To reduce the design complexity, we also propose a symmetric
codebook design. For all the proposed design approaches, the minimum Euclidean
distance aims to be maximized. Our symmetric codebook design can reduce design
and detection complexity simultaneously. Simulation results show that our
design implies fast convergence with respect to the number of iterations, and
outperforms the design that simply modifies the existing approaches to VLC
signal requirements.



On the Taut String Interpretation of the One-dimensional Rudin-Osher-Fatemi Model: A New Proof, a Fundamental Estimate and Some Applications

A new proof of the equivalence of the Taut String Algorithm and the
one-dimensional Rudin-Osher-Fatemi model is presented. Based on duality and the
projection theorem in Hilbert space, the proof is strictly elementary.
Existence and uniqueness of solutions to both denoising models follow as
by-products. The standard convergence properties of the denoised signal, as the
regularizing parameter tends to zero, are recalled and efficient proofs
provided. Moreover, a new and fundamental bound on the denoised signal is
derived. This bound implies, among other things, the strong convergence (in the
space of functions of bounded variation) of the denoised signal to the insignal
as the regularization parameter vanishes. The methods developed in the paper
can be modified to cover other interesting applications such as isotonic
regression.



Respiratory and cardiac monitoring at night using a wrist wearable optical system

Sleep monitoring provides valuable insights into the general health of an
individual and helps in the diagnostic of sleep-derived illnesses.
Polysomnography, is considered the gold standard for such task. However, it is
very unwieldy and therefore not suitable for long-term analysis. Here, we
present a non-intrusive wearable system that, by using photoplethysmography, it
can estimate beat-to-beat intervals, pulse rate, and breathing rate reliably
during the night. The performance of the proposed approach was evaluated
empirically in the Department of Psychology at the University of Fribourg. Each
participant was wearing two smart-bracelets from Ava as well as a complete
polysomnographic setup as reference. The resulting mean absolute errors are
17.4 ms (MAPE 1.8%) for the beat-to-beat intervals, 0.13 beats-per-minute (MAPE
0.20%) for the pulse rate, and 0.9 breaths-per-minute (MAPE 6.7%) for the
breath rate.



Full-Duplex Non-Orthogonal Multiple Access for Modern Wireless Networks

Non-orthogonal multiple access (NOMA) is an interesting concept to provide
higher capacity for future wireless communications. In this article, we
consider the feasibility and benefits of combining full-duplex operation with
NOMA for modern communication systems. Specifically, we provide a comprehensive
overview on application of full-duplex NOMA in cellular networks, cooperative
and cognitive radio networks, and characterize gains possible due to
full-duplex operation. Accordingly, we discuss challenges, particularly the
self-interference and inter-user interference and provide potential solutions
to interference mitigation and quality-of-service provision based on
beamforming, power control, and link scheduling. We further discuss future
research challenges and interesting directions to pursue to bring full-duplex
NOMA into maturity and use in practice.



Estimation and Control over Cognitive Radio Channels with Distributed and Dynamic Spectral Activity

Since its first inception by Joseph Mitola III in 1998 cognitive radio (CR)
systems have seen an explosion of papers in the communication community.
However, the interaction of CR and control has remained vastly unexplored. In
fact, when combined with control theory CR may pave the way for new and
exciting control and communication applications. In this paper, the control and
estimation problem via the well known two switch model which represents a CR
link is considered. In particular, The optimal linear estimator subject to a CR
link between the sensor and the estimator is derived. Furthermore, it is shown
that in the Linear Quadratic Gaussian (LQG) Control law for a closed-loop
system over double CR links is not linear in the state estimate. Consequently,
the separation principle is shown to be violated. Several conditions of
stochastic stability are also discussed. Illustrative numerical examples are
provided to show the effectiveness of the results.



New sufficient conditions of signal recovery with tight frames via $l_1$-analysis

The paper discusses the recovery of signals in the case that signals are
nearly sparse with respect to a tight frame $D$ by means of the $l_1$-analysis
approach. We establish several new sufficient conditions regarding the
$D$-restricted isometry property to ensure stable reconstruction of signals
that are approximately sparse with respect to $D$. It is shown that if the
measurement matrix $\Phi$ fulfils the condition $\delta_{ts}<t/(4-t)$ for
$0<t<4/3$, then signals which are approximately sparse with respect to $D$ can
be stably recovered by the $l_1$-analysis method. In the case of $D=I$, the
bound is sharp, see Cai and Zhang's work \cite{Cai and Zhang 2014}. When $t=1$,
the present bound improves the condition $\delta_s<0.307$ from Lin et al.'s
reuslt to $\delta_s<1/3$.



A sharp sufficient condition of block signal recovery via $l_2/l_1$-minimization

This work gains a sharp sufficient condition on the block restricted isometry
property for the recovery of sparse signal. Under the certain assumption, the
signal with block structure can be stably recovered in the present of noisy
case and the block sparse signal can be exactly reconstructed in the noise-free
case. Besides, an example is proposed to exhibit the condition is sharp. As
byproduct, when $t=1$, the result improves the bound of block restricted
isometry constant $\delta_{s|\mathcal{I}}$ in Lin and Li (Acta Math. Sin. Engl.
Ser. 29(7): 1401-1412, 2013).



High efficiency compression for object detection

Image and video compression has traditionally been tailored to human vision.
However, modern applications such as visual analytics and surveillance rely on
computers seeing and analyzing the images before (or instead of) humans. For
these applications, it is important to adjust compression to computer vision.
In this paper we present a bit allocation and rate control strategy that is
tailored to object detection. Using the initial convolutional layers of a
state-of-the-art object detector, we create an importance map that can guide
bit allocation to areas that are important for object detection. The proposed
method enables bit rate savings of 7% or more compared to default HEVC, at the
equivalent object detection rate.



Onsets and Frames: Dual-Objective Piano Transcription

We advance the state of the art in polyphonic piano music transcription by
using a deep convolutional and recurrent neural network which is trained to
jointly predict onsets and frames. Our model predicts pitch onset events and
then uses those predictions to condition framewise pitch predictions. During
inference, we restrict the predictions from the framewise detector by not
allowing a new note to start unless the onset detector also agrees that an
onset for that pitch is present in the frame. We focus on improving onsets and
offsets together instead of either in isolation as we believe this correlates
better with human musical perception. Our approach results in over a 100%
relative improvement in note F1 score (with offsets) on the MAPS dataset.
Furthermore, we extend the model to predict relative velocities of normalized
audio which results in more natural-sounding transcriptions.



Augmented Slepians: Bandlimited Functions that Counterbalance Energy in Selected Intervals

Slepian functions provide a solution to the optimization problem of joint
time-frequency localization. Here, this concept is extended by using a
generalized optimization criterion that favors energy concentration in one
interval while penalizing energy in another interval, leading to the
"augmented" Slepian functions. Mathematical foundations together with examples
are presented in order to illustrate the most interesting properties that these
generalized Slepian functions show. Also the relevance of this novel
energy-concentration criterion is discussed along with some of its
applications.



Deep Learning for Frame Error Probability Prediction in BICM-OFDM Systems

In the context of wireless communications, we propose a deep learning
approach to learn the mapping from the instantaneous state of a frequency
selective fading channel to the corresponding frame error probability (FEP) for
an arbitrary set of transmission parameters. We propose an abstract model of a
bit interleaved coded modulation (BICM) orthogonal frequency division
multiplexing (OFDM) link chain and show that the maximum likelihood (ML)
estimator of the model parameters estimates the true FEP distribution. Further,
we exploit deep neural networks as a general purpose tool to implement our
model and propose a training scheme for which, even while training with the
binary frame error events (i.e., ACKs / NACKs), the network outputs converge to
the FEP conditioned on the input channel state. We provide simulation results
that demonstrate gains in the FEP prediction accuracy with our approach as
compared to the traditional effective exponential SIR metric (EESM) approach
for a range of channel code rates, and show that these gains can be exploited
to increase the link throughput.



Nebula: F0 Estimation and Voicing Detection by Modeling the Statistical Properties of Feature Extractors

A F0 and voicing status estimation algorithm for high quality speech
analysis/synthesis is proposed. This problem is approached from a different
perspective that models the behavior of feature extractors under noise, instead
of directly modeling speech signals. Under time-frequency locality assumptions,
the joint distribution of extracted features and target F0 can be characterized
by training a bank of Gaussian mixture models (GMM) on artificial data
generated from Monte-Carlo simulations. The trained GMMs can then be used to
generate a set of conditional distributions on the predicted F0, which are then
combined and post-processed by Viterbi algorithm to give a final F0 trajectory.
Evaluation on CSTR and CMU Arctic speech databases shows that the proposed
method, trained on fully synthetic data, achieves lower gross error rates than
state-of-the-art methods.



Audio style transfer

'Style transfer' among images has recently emerged as a very active research
topic, fuelled by the power of convolution neural networks (CNNs), and has
become fast a very popular technology in social media. This paper investigates
the analogous problem in the audio domain: How to transfer the style of a
reference audio signal to a target audio content? We propose a flexible
framework for the task, which uses a sound texture model to extract statistics
characterizing the reference audio style, followed by an optimization-based
audio texture synthesis to modify the target content. In contrast to mainstream
optimization-based visual transfer method, the proposed process is initialized
by the target content instead of random noise and the optimized loss is only
about texture, not structure. These differences proved key for audio style
transfer in our experiments. In order to extract features of interest, we
investigate different architectures, whether pre-trained on other tasks, as
done in image style transfer, or engineered based on the human auditory system.
Experimental results on different types of audio signal confirm the potential
of the proposed approach.



Polyphonic Music Generation with Sequence Generative Adversarial Networks

We propose an application of sequence generative adversarial networks
(SeqGAN), which are generative adversarial networks for discrete sequence
generation, for creating polyphonic musical sequences. Instead of a monophonic
melody generation suggested in the original work, we present an efficient
representation of a polyphony MIDI file that simultaneously captures chords and
melodies with dynamic timings. The proposed method condenses duration, octaves,
and keys of both melodies and chords into a single word vector representation,
and recurrent neural networks learn to predict distributions of sequences from
the embedded musical word space. We experiment with the original method and the
least squares method to the discriminator, which is known to stabilize the
training of GANs. The network can create sequences that are musically coherent
and shows an improved quantitative and qualitative measures. We also report
that careful optimization of reinforcement learning signals of the model is
crucial for general application of the model.



SVSGAN: Singing Voice Separation via Generative Adversarial Network

Separating two sources from an audio mixture is an important task with many
applications. It is a challenging problem since only one signal channel is
available for analysis. In this paper, we propose a novel framework for singing
voice separation using the generative adversarial network (GAN) with a
time-frequency masking function. The mixture spectra is considered to be a
distribution and is mapped to the clean spectra which is also considered a
distribtution. The approximation of distributions between mixture spectra and
clean spectra is performed during the adversarial training process. In contrast
with current deep learning approaches for source separation, the parameters of
the proposed framework are first initialized in a supervised setting and then
optimized by the training procedure of GAN in an unsupervised setting.
Experimental results on three datasets (MIR-1K, iKala and DSD100) show that
performance can be improved by the proposed framework consisting of
conventional networks.



Statistical Speech Enhancement Based on Probabilistic Integration of Variational Autoencoder and Non-Negative Matrix Factorization

This paper presents a statistical method of single-channel speech enhancement
that uses a variational autoencoder (VAE) as a prior distribution on clean
speech. A standard approach to speech enhancement is to train a deep neural
network (DNN) to take noisy speech as input and output clean speech. Although
this supervised approach requires a very large amount of pair data for
training, it is not robust against unknown environments. Another approach is to
use non-negative matrix factorization (NMF) based on basis spectra trained on
clean speech in advance and those adapted to noise on the fly. This
semi-supervised approach, however, causes considerable signal distortion in
enhanced speech due to the unrealistic assumption that speech spectrograms are
linear combinations of the basis spectra. Replacing the poor linear generative
model of clean speech in NMF with a VAE---a powerful nonlinear deep generative
model---trained on clean speech, we formulate a unified probabilistic
generative model of noisy speech. Given noisy speech as observed data, we can
sample clean speech from its posterior distribution. The proposed method
outperformed the conventional DNN-based method in unseen noisy environments.



Multi-Resolution Fully Convolutional Neural Networks for Monaural Audio Source Separation

In deep neural networks with convolutional layers, each layer typically has
fixed-size/single-resolution receptive field (RF). Convolutional layers with a
large RF capture global information from the input features, while layers with
small RF size capture local details with high resolution from the input
features. In this work, we introduce novel deep multi-resolution fully
convolutional neural networks (MR-FCNN), where each layer has different RF
sizes to extract multi-resolution features that capture the global and local
details information from its input features. The proposed MR-FCNN is applied to
separate a target audio source from a mixture of many audio sources.
Experimental results show that using MR-FCNN improves the performance compared
to feedforward deep neural networks (DNNs) and single resolution deep fully
convolutional neural networks (FCNNs) on the audio source separation problem.



Melody Generation for Pop Music via Word Representation of Musical Properties

Automatic melody generation for pop music has been a long-time aspiration for
both AI researchers and musicians. However, learning to generate euphonious
melody has turned out to be highly challenging due to a number of factors.
Representation of multivariate property of notes has been one of the primary
challenges. It is also difficult to remain in the permissible spectrum of
musical variety, outside of which would be perceived as a plain random play
without auditory pleasantness. Observing the conventional structure of pop
music poses further challenges. In this paper, we propose to represent each
note and its properties as a unique `word,' thus lessening the prospect of
misalignments between the properties, as well as reducing the complexity of
learning. We also enforce regularization policies on the range of notes, thus
encouraging the generated melody to stay close to what humans would find easy
to follow. Furthermore, we generate melody conditioned on song part
information, thus replicating the overall structure of a full song.
Experimental results demonstrate that our model can generate auditorily
pleasant songs that are more indistinguishable from human-written ones than
previous models.



Finite sample performance of linear least squares estimators under sub-Gaussian martingale difference noise

Linear Least Squares is a very well known technique for parameter estimation,
which is used even when sub-optimal, because of its very low computational
requirements and the fact that exact knowledge of the noise statistics is not
required. Surprisingly, bounding the probability of large errors with finitely
many samples has been left open, especially when dealing with correlated noise
with unknown covariance. In this paper we analyze the finite sample performance
of the linear least squares estimator under sub-Gaussian martingale difference
noise. In order to analyze this important question we used concentration of
measure bounds. When applying these bounds we obtained tight bounds on the tail
of the estimator's distribution. We show the fast exponential convergence of
the number of samples required to ensure a given accuracy with high
probability. We provide probability tail bounds on the estimation error's norm.
Our analysis method is simple and uses simple $L_{\infty}$ type bounds on the
estimation error. The tightness of the bounds is tested through simulation. The
proposed bounds make it possible to predict the number of samples required for
least squares estimation even when least squares is sub-optimal and used for
computational simplicity. The finite sample analysis of least squares models
with this general noise model is novel.



Spatially Adaptive Colocalization Analysis in Dual-Color Fluorescence Microscopy

Colocalization analysis aims to study complex spatial associations between
bio-molecules via optical imaging techniques. However, existing colocalization
analysis workflows only assess an average degree of colocalization within a
certain region of interest and ignore the unique and valuable spatial
information offered by microscopy. In the current work, we introduce a new
framework for colocalization analysis that allows us to quantify colocalization
levels at each individual location and automatically identify pixels or regions
where colocalization occurs. The framework, referred to as spatially adaptive
colocalization analysis (SACA), integrates a pixel-wise local kernel model for
colocalization quantification and a multi-scale adaptive propagation-separation
strategy for utilizing spatial information to detect colocalization in a
spatially adaptive fashion. Applications to simulated and real biological
datasets demonstrate the practical merits of SACA in what we hope to be an
easily applicable and robust colocalization analysis method. In addition,
theoretical properties of SACA are investigated to provide rigorous statistical
justification.



User Environment Detection with Acoustic Sensors Embedded on Mobile Devices for the Recognition of Activities of Daily Living

The detection of the environment where user is located, is of extreme use for
the identification of Activities of Daily Living (ADL). ADL can be identified
by use of the sensors available in many off-the-shelf mobile devices, including
magnetic and motion, and the environment can be also identified using acoustic
sensors. The study presented in this paper is divided in two parts: firstly, we
discuss the recognition of the environment using acoustic sensors (i.e.,
microphone), and secondly, we fuse this information with motion and magnetic
sensors (i.e., motion and magnetic sensors) for the recognition of standing
activities of daily living. The recognition of the environments and the ADL are
performed using pattern recognition techniques, in order to develop a system
that includes data acquisition, data processing, data fusion, and artificial
intelligence methods. The artificial intelligence methods explored in this
study are composed by different types of Artificial Neural Networks (ANN),
comparing the different types of ANN and selecting the best methods to
implement in the different stages of the system developed. Conclusions point to
the use of Deep Neural Networks (DNN) with normalized data for the
identification of ADL with 85.89% of accuracy, the use of Feedforward neural
networks with non-normalized data for the identification of the environments
with 86.50% of accuracy, and the use of DNN with normalized data for the
identification of standing activities with 100% of accuracy.



Closed Form Solutions of Combinatorial Graph Laplacian Estimation under Acyclic Topology Constraints

How to obtain a graph from data samples is an important problem in graph
signal processing. One way to formulate this graph learning problem is based on
Gaussian maximum likelihood estimation, possibly under particular topology
constraints. To solve this problem, we typically require iterative convex
optimization solvers. In this paper, we show that when the target graph
topology does not contain any cycle, then the solution has a closed form in
terms of the empirical covariance matrix. This enables us to efficiently
construct a tree graph from data, even if there is only a single data sample
available. We also provide an error bound of the objective function when we use
the same solution to approximate a cyclic graph. As an example, we consider an
image denoising problem, in which for each input image we construct a graph
based on the theoretical result. We then apply low-pass graph filters based on
this graph. Experimental results show that the weights given by the graph
learning solution lead to better denoising results than the bilateral weights
under some conditions.



Reducing Model Complexity for DNN Based Large-Scale Audio Classification

Audio classification is the task of identifying the sound categories that are
associated with a given audio signal. This paper presents an investigation on
large-scale audio classification based on the recently released AudioSet
database. AudioSet comprises 2 millions of audio samples from YouTube, which
are human-annotated with 527 sound category labels. Audio classification
experiments with the balanced training set and the evaluation set of AudioSet
are carried out by applying different types of neural network models. The
classification performance and the model complexity of these models are
compared and analyzed. While the CNN models show better performance than MLP
and RNN, its model complexity is relatively high and undesirable for practical
use. We propose two different strategies that aim at constructing
low-dimensional embedding feature extractors and hence reducing the number of
model parameters. It is shown that the simplified CNN model has only 1/22 model
parameters of the original model, with only a slight degradation of
performance.



Transient Behavior of Redox Flow Battery Connected to Circuit Based on Global Phase Structure

A Redox Flow Battery (RFB) is one of the promising energy storage systems in
power grid. An RFB has many advantages such as a quick response, a large
capacity, and a scalability. Due to these advantages, an RFB can operate in
mixed time scale. Actually, it has been demonstrated that an RFB can be used
for load leveling, compensating sag, and smoothing the output of the renewable
sources. An analysis on transient behaviors of an RFB is a key issue for these
applications. An RFB is governed by electrical, chemical, and fluid dynamics.
The hybrid structure makes the analysis difficult. To analyze transient
behaviors of an RFB, the exact model is necessary. In this paper, we focus on a
change in a concentration of ions in the electrolyte, and simulate the change
with a model which is mainly based on chemical kinetics. The simulation results
introduces transient behaviors of an RFB in a response to a load variation.
There are found three kinds of typical transient behaviors including
oscillations. As results, it is clarified that the complex transient behaviors,
due to slow and fast dynamics in the system, arise by the quick response to
load.



Learned Convolutional Sparse Coding

We propose a convolutional recurrent sparse auto-encoder model. The model
consists of a sparse encoder, which is a convolutional extension of the learned
ISTA (LISTA) method, and a linear convolutional decoder. Our strategy offers a
simple method for learning a task-driven sparse convolutional dictionary (CD),
and producing an approximate convolutional sparse code (CSC) over the learned
dictionary. We trained the model to minimize reconstruction loss via gradient
decent with back-propagation and have achieved competitive results to KSVD
image denoising and to leading CSC methods in image inpainting requiring only a
small fraction of their run-time.



Shift-Invariant Kernel Additive Modelling for Audio Source Separation

A major goal in blind source separation to identify and separate sources is
to model their inherent characteristics. While most state-of-the-art approaches
are supervised methods trained on large datasets, interest in non-data-driven
approaches such as Kernel Additive Modelling (KAM) remains high due to their
interpretability and adaptability. KAM performs the separation of a given
source applying robust statistics on the time-frequency bins selected by a
source-specific kernel function, commonly the K-NN function. This choice
assumes that the source of interest repeats in both time and frequency. In
practice, this assumption does not always hold. Therefore, we introduce a
shift-invariant kernel function capable of identifying similar spectral content
even under frequency shifts. This way, we can considerably increase the amount
of suitable sound material available to the robust statistics. While this leads
to an increase in separation performance, a basic formulation, however, is
computationally expensive. Therefore, we additionally present acceleration
techniques that lower the overall computational complexity.



Full-info Training for Deep Speaker Feature Learning

In recent studies, it has shown that speaker patterns can be learned from
very short speech segments (e.g., 0.3 seconds) by a carefully designed
convolutional & time-delay deep neural network (CT-DNN) model. By enforcing the
model to discriminate the speakers in the training data, frame-level speaker
features can be derived from the last hidden layer. In spite of its good
performance, a potential problem of the present model is that it involves a
parametric classifier, i.e., the last affine layer, which may consume some
discriminative knowledge, thus leading to `information leak' for the feature
learning. This paper presents a full-info training approach that discards the
parametric classifier and enforces all the discriminative knowledge learned by
the feature net. Our experiments on the Fisher database demonstrate that this
new training scheme can produce more coherent features, leading to consistent
and notable performance improvement on the speaker verification task.



Non-Linear Digital Self-Interference Cancellation for In-Band Full-Duplex Radios Using Neural Networks

Full-duplex systems require very strong self-interference cancellation in
order to operate correctly and a significant part of the self-interference
signal is due to non-linear effects created by various transceiver impairments.
As such, linear cancellation alone is usually not sufficient and sophisticated
non-linear cancellation algorithms have been proposed in the literature. In
this work, we investigate the use of a neural network as an alternative to the
traditional non-linear cancellation method that is based on polynomial basis
functions. Measurement results from a full-duplex testbed demonstrate that a
small and simple feed-forward neural network canceler works exceptionally well,
as it can match the performance of the polynomial non-linear canceler with
significantly lower computational complexity.



Sustainable Green Networking: Exploiting Degrees of Freedom towards Energy-Efficient 5G Systems

The carbon footprint concern in the development and deployment of 5G new
radio systems has drawn the attention to several stakeholders. In this article,
we analyze the critical power consuming component of all candidate 5G system
architectures-the power amplifier (PA)-and propose PA-centric resource
management solutions for green 5G communications. We discuss the impact of
ongoing trends in cellular communications on sustainable green networking and
analyze two communications architectures that allow exploiting the extra
degrees-of-freedom (DoF) from multi-antenna and massive antenna deployments:
small cells/distributed antenna network and massive MIMO. For small cell
systems with a moderate number of antennas, we propose a peak to average power
ratio-aware resource allocation scheme for joint orthogonal frequency and space
division multiple access. For massive MIMO systems, we develop a highly
parallel recurrent neural network for energy-efficient precoding. Simulation
results for representative 5G deployment scenarios demonstrate an energy
efficiency improvement of one order of magnitude or higher with respect to
current state-of-the-art solutions.



Tensor Valued Common and Individual Feature Extraction: Multi-dimensional Perspective

A novel method for common and individual feature analysis from exceedingly
large-scale data is proposed, in order to ensure the tractability of both the
computation and storage and thus mitigate the curse of dimensionality, a major
bottleneck in modern data science. This is achieved by making use of the
inherent redundancy in so-called multi-block data structures, which represent
multiple observations of the same phenomenon taken at different times, angles
or recording conditions. Upon providing an intrinsic link between the
properties of the outer vector product and extracted features in tensor
decompositions (TDs), the proposed common and individual information extraction
from multi-block data is performed through imposing physical meaning to
otherwise unconstrained factorisation approaches. This is shown to dramatically
reduce the dimensionality of search spaces for subsequent classification
procedures and to yield greatly enhanced accuracy. Simulations on a multi-class
classification task of large-scale extraction of individual features from a
collection of partially related real-world images demonstrate the advantages of
the "blessing of dimensionality" associated with TDs.



Event-Triggered Diffusion Kalman Filters

Distributed state estimation strongly depends on collaborative signal
processing, which often requires excessive communication and computation to be
executed on resource-constrained sensor nodes. To address this problem, we
propose an event-triggered diffusion Kalman filter, which collects measurements
and exchanges messages between nodes based on a local signal indicating the
estimation error. On this basis, we develop an energy-aware state estimation
algorithm that regulates the resource consumption in wireless networks and
ensures the effectiveness of every consumed resource. The proposed algorithm
does not require the nodes to share its local covariance matrices, and thereby
allows considerably reducing the number of transmission messages. To confirm
its efficiency, we apply the proposed algorithm to the distributed simultaneous
localization and time synchronization problem and evaluate it on a physical
testbed of a mobile quadrotor node and stationary custom ultra-wideband
wireless devices. The obtained experimental results indicate that the proposed
algorithm allows saving 86% of the communication overhead associated with the
original diffusion Kalman filter while causing deterioration of performance by
16% only. We make the Matlab code and the real testing data available online.



TasNet: time-domain audio separation network for real-time, single-channel speech separation

Robust speech processing in multi-talker environments requires effective
speech separation. Recent deep learning systems have made significant progress
toward solving this problem, yet it remains challenging particularly in
real-time, short latency applications. Most methods attempt to construct a mask
for each source in time-frequency representation of the mixture signal which is
not necessarily an optimal representation for speech separation. In addition,
time-frequency decomposition results in inherent problems such as
phase/magnitude decoupling and long time window which is required to achieve
sufficient frequency resolution. We propose Time-domain Audio Separation
Network (TasNet) to overcome these limitations. We directly model the signal in
the time-domain using an encoder-decoder framework and perform the source
separation on nonnegative encoder outputs. This method removes the frequency
decomposition step and reduces the separation problem to estimation of source
masks on encoder outputs which is then synthesized by the decoder. Our system
outperforms the current state-of-the-art causal and noncausal speech separation
algorithms, reduces the computational cost of speech separation, and
significantly reduces the minimum required latency of the output. This makes
TasNet suitable for applications where low-power, real-time implementation is
desirable such as in hearable and telecommunication devices.



Decoupled Heterogeneous Networks with Millimeter Wave Small Cells

Deploying sub-6GHz network together with millimeter wave (mmWave) is a
promising solution to simultaneously achieve sufficient coverage and high data
rate. In the heterogeneous networks (HetNets), the traditional coupled access,
i.e., the users are constrained to be associated with the same base station in
both downlink and uplink, is no longer optimal, and the concept of downlink and
uplink decoupling has recently been proposed. In this paper, we propose an
analytical framework to investigate the traditional sub-6GHz HetNets
integrating with mmWave small cells (SCells) with decoupled access, where both
the uplink power control and mmWave interference are taken into account. Using
the tools from stochastic geometry, the performance metrics of
signal-to-interference-plus-noise ratio coverage probability, user-perceived
rate coverage probability, and area sum rate are derived. The impact of the
densification of different SCells on the network performance is also analyzed
to give insights on the network design. Simulation results validate the
accuracy of our analysis, and reveal that mmWave interference can not be
neglected when the mmWave SCells are extremely dense and that different kinds
of SCells have various effects on the network performance and thus need to be
organized properly.



The sum of tensor networks

Tensor networks (TNs) have been gaining interest as multiway data analysis
tools owing to their ability to tackle the curse of dimensionality and to
represent tensors as smaller-scale interconnections of their intrinsic
features. However, despite the obvious advantages, the current treatment of TNs
as stand-alone entities does not take full benefit of their underlying
structure and the associated feature localization. To this end, embarking upon
the analogy with a feature fusion, we propose a rigorous framework for the
combination of TNs, focusing on their summation as the natural way for their
combination. This allows for feature combination for any number of tensors, as
long as their TN representation topologies are isomorphic. The benefits of the
proposed framework are demonstrated on the classification of several groups of
partially related images, where it outperforms standard machine learning
algorithms.



Performance Evaluation of Channel Decoding With Deep Neural Networks

With the demand of high data rate and low latency in fifth generation (5G),
deep neural network decoder (NND) has become a promising candidate due to its
capability of one-shot decoding and parallel computing. In this paper, three
types of NND, i.e., multi-layer perceptron (MLP), convolution neural network
(CNN) and recurrent neural network (RNN), are proposed with the same parameter
magnitude. The performance of these deep neural networks are evaluated through
extensive simulation. Numerical results show that RNN has the best decoding
performance, yet at the price of the highest computational overhead. Moreover,
we find there exists a saturation length for each type of neural network, which
is caused by their restricted learning abilities.



Framework for evaluation of sound event detection in web videos

The largest source of sound events is web videos. Most videos lack sound
event labels at segment level, however, a significant number of them do respond
to text queries, from a match found using metadata by search engines. In this
paper we explore the extent to which a search query can be used as the true
label for detection of sound events in videos. We present a framework for
large-scale sound event recognition on web videos. The framework crawls videos
using search queries corresponding to 78 sound event labels drawn from three
datasets. The datasets are used to train three classifiers, and we obtain a
prediction on 3.7 million web video segments. We evaluated performance using
the search query as true label and compare it with human labeling. Both types
of ground truth exhibited close performance, to within 10%, and similar
performance trend with increasing number of evaluated segments. Hence, our
experiments show potential for using search query as a preliminary true label
for sound event recognition in web videos.



Does Phase Matter For Monaural Source Separation?

The "cocktail party" problem of fully separating multiple sources from a
single channel audio waveform remains unsolved. Current biological
understanding of neural encoding suggests that phase information is preserved
and utilized at every stage of the auditory pathway. However, current
computational approaches primarily discard phase information in order to mask
amplitude spectrograms of sound. In this paper, we seek to address whether
preserving phase information in spectral representations of sound provides
better results in monaural separation of vocals from a musical track by using a
neurally plausible sparse generative model. Our results demonstrate that
preserving phase information reduces artifacts in the separated tracks, as
quantified by the signal to artifact ratio (GSAR). Furthermore, our proposed
method achieves state-of-the-art performance for source separation, as
quantified by a mean signal to interference ratio (GSIR) of 19.46.



Audio Set classification with attention model: A probabilistic perspective

This paper investigates the classification of the Audio Set dataset. Audio
Set is a large scale weakly labelled dataset of sound clips. Previous work used
multiple instance learning (MIL) to classify weakly labelled data. In MIL, a
bag consists of several instances, and a bag is labelled positive if at least
one instances in the audio clip is positive. A bag is labelled negative if all
the instances in the bag are negative. We propose an attention model to tackle
the MIL problem and explain this attention model from a novel probabilistic
perspective. We define a probability space on each bag, where each instance in
the bag has a trainable probability measure for each class. Then the
classification of a bag is the expectation of the classification output of the
instances in the bag with respect to the learned probability measure.
Experimental results show that our proposed attention model modeled by fully
connected deep neural network obtains mAP of 0.327 on Audio Set dataset,
outperforming the Google's baseline of 0.314 and recurrent neural network of
0.325.



Energy-Delay Efficient Power Control in Wireless Networks

This work aims at developing a power control framework to jointly optimize
energy efficiency (measured in bit/Joule) and delay in wireless networks. A
multi-objective approach is taken to deal with both performance metrics, while
ensuring a minimum quality-of-service to each user in the network. Each user in
the network is modeled as a rational agent that engages in a generalized
non-cooperative game. Feasibility conditions are derived for the existence of
each player's best response, and used to show that if these conditions are met,
the game best response dynamics will converge to a unique Nash equilibrium.
Based on these results, a convergent power control algorithm is derived, which
can be implemented in a fully decentralized fashion. Next, a centralized power
control algorithm is proposed, which also serves as a benchmark for the
proposed decentralized solution. Due to the non-convexity of the centralized
problem, the tool of maximum block improvement is used, to trade-off complexity
with optimality.



Diffraction Influence on the Field of View and Resolution of Three-Dimensional Integral Imaging

The influence of the diffraction limit on the field of view of
three-dimensional integral imaging (InI) systems is estimated by calculating
the resolution of the InI system along arbitrarily tilted directions. The
deteriorating effects of diffraction on the resolution are quantified in this
manner. Two different three-dimensional scenes are recorded by real/virtual and
focused imaging modes. The recorded scenes are reconstructed at different
tilted planes and the obtained results for the resolution and field of view of
the system are verified. It is shown that the diffraction effects severely
affect the resolution of InI in the real/virtual mode when the tilted angle of
viewing is increased. It is also shown that the resolution of InI in the
focused mode is more robust to the unwanted effects of diffraction even though
it is much lower than the resolution of InI in the real/virtual mode.



Optimization of phase retrieval in the Fresnel domain by the modified Gerchberg-Saxton algorithm

The modified Gerchberg-Saxton algorithm (MGSA) is one of the standard methods
for phase retrieval. In this work we apply the MGSA in the paraxial domain. For
three given physical parameters - i.e. wavelength, propagation distance and
pixel size the computational width in the Fresnel-Transform is fixed. This
width can be larger than the real dimension of the input or output images.
Consequently, it can induce a padding around the real input and output without
given amplitude (intensity) values. To solve this problem, we propose a very
simple and efficient solution and compare it to other approaches. We
demonstrate that the new modified GSA provides almost perfect results without
losing the time efficiency of the simplest method.



Learning flexible representations of stochastic processes on graphs

Graph convolutional networks adapt the architecture of convolutional neural
networks to learn rich representations of data supported on arbitrary graphs by
replacing the convolution operations of convolutional neural networks with
graph-dependent linear operations. However, these graph-dependent linear
operations are developed for scalar functions supported on undirected graphs.
We propose a class of linear operations for stochastic (time-varying) processes
on directed (or undirected) graphs to be used in graph convolutional networks.
We propose a parameterization of such linear operations using functional
calculus to achieve arbitrarily low learning complexity. The proposed approach
is shown to model richer behaviors and display greater flexibility in learning
representations than product graph methods.



Convolutional Drift Networks for Video Classification

Analyzing spatio-temporal data like video is a challenging task that requires
processing visual and temporal information effectively. Convolutional Neural
Networks have shown promise as baseline fixed feature extractors through
transfer learning, a technique that helps minimize the training cost on visual
information. Temporal information is often handled using hand-crafted features
or Recurrent Neural Networks, but this can be overly specific or prohibitively
complex. Building a fully trainable system that can efficiently analyze
spatio-temporal data without hand-crafted features or complex training is an
open challenge. We present a new neural network architecture to address this
challenge, the Convolutional Drift Network (CDN). Our CDN architecture combines
the visual feature extraction power of deep Convolutional Neural Networks with
the intrinsically efficient temporal processing provided by Reservoir
Computing. In this introductory paper on the CDN, we provide a very simple
baseline implementation tested on two egocentric (first-person) video activity
datasets.We achieve video-level activity classification results on-par with
state-of-the art methods. Notably, performance on this complex spatio-temporal
task was produced by only training a single feed-forward layer in the CDN.



Background Subtraction via Fast Robust Matrix Completion

Background subtraction is the primary task of the majority of video
inspection systems. The most important part of the background subtraction which
is common among different algorithms is background modeling. In this regard,
our paper addresses the problem of background modeling in a computationally
efficient way, which is important for current eruption of "big data" processing
coming from high resolution multi-channel videos. Our model is based on the
assumption that background in natural images lies on a low-dimensional
subspace. We formulated and solved this problem in a low-rank matrix completion
framework. In modeling the background, we benefited from the in-face extended
Frank-Wolfe algorithm for solving a defined convex optimization problem. We
evaluated our fast robust matrix completion (fRMC) method on both background
models challenge (BMC) and Stuttgart artificial background subtraction (SABS)
datasets. The results were compared with the robust principle component
analysis (RPCA) and low-rank robust matrix completion (RMC) methods, both
solved by inexact augmented Lagrangian multiplier (IALM). The results showed
faster computation, at least twice as when IALM solver is used, while having a
comparable accuracy even better in some challenges, in subtracting the
backgrounds in order to detect moving objects in the scene.



Optimal-speed algorithms for localization of random pulsed point sources generating super short pulses

The time-optimal technique of spatial localization of the random pulsed-point
source that has the uniform distribution density on search interval and
indicating itself by generation of the instant impulses (delta functions) at
random time points is developed. Localization is carried out by means of the
receiver having view window freely reconstructed in time. The created
algorithms are generalized to the search carried out by system consisting of
several receivers.



RF Wireless Power Transfer: Regreening Future Networks

Green radio communication is an emerging topic since the overall footprint of
information and communication technology (ICT) services is predicted to triple
between 2007 and 2020. Given this research line, energy harvesting (EH) and
wireless power transfer (WPT) networks can be evaluated as promising
approaches. In this paper, an overview of recent trends for future green
networks on the platforms of EH and WPT is provided. By rethinking the
application of radio frequency (RF)-WPT, a new concept, namely green RF-WTP, is
introduced. Accordingly, opening challenges and promising combinations among
current technologies, such as small-cell, millimeter (mm)-wave, and Internet of
Things (IoT) networks, are discussed in details to seek solutions for the
question with how to re-green the future networks?



Fast Path Localization on Graphs via Multiscale Viterbi Decoding

We consider a problem of localizing a path-signal that evolves over time on a
graph. A path-signal can be viewed as the trajectory of a moving agent on a
graph in several consecutive time points. Combining dynamic programming and
graph partitioning, we propose a path-localization algorithm with significantly
reduced computational complexity. We analyze the localization error for the
proposed approach both in the Hamming distance and the destination's distance
between the path estimate and the true path using numerical bounds. Unlike
usual theoretical bounds that only apply to restricted graph models, the
obtained numerical bounds apply to all graphs and all non-overlapping
graph-partitioning schemes. In random geometric graphs, we are able to derive a
closed-form expression for the localization error bound, and a tradeoff between
localization error and the computational complexity. Finally, we compare the
proposed technique with the maximum likelihood estimate under the path
constraint in terms of computational complexity and localization error, and
show significant speedup (100 times) with comparable localization error (4
times) on a graph from real data. Variants of the proposed technique can be
applied to tracking, road congestion monitoring, and brain signal processing.



Knowledge Transfer from Weakly Labeled Audio using Convolutional Neural Network for Sound Events and Scenes

In this work we propose approaches to effectively transfer knowledge from
weakly labeled web audio data. We first describe a convolutional neural network
(CNN) based framework for sound event detection and classification using weakly
labeled audio data. Our model trains efficiently from audios of variable
lengths; hence, it is well suited for transfer learning. We then propose
methods to learn representations using this model which can be effectively used
for solving the target task. We study both transductive and inductive transfer
learning tasks, showing the effectiveness of our methods for both domain and
task adaptation. We show that the learned representations using the proposed
CNN model generalizes well enough to reach human level accuracy on ESC-50 sound
events dataset and set state of art results on this dataset. We further use
them for acoustic scene classification task and once again show that our
proposed approaches suit well for this task as well. We also show that our
methods are helpful in capturing semantic meanings and relations as well.
Moreover, in this process we also set state-of-art results on Audioset dataset,
relying on balanced training set.



Monaural Singing Voice Separation with Skip-Filtering Connections and Recurrent Inference of Time-Frequency Mask

Singing voice separation based on deep learning relies on the usage of
time-frequency masking. In many cases the masking process is not a learnable
function or is not encapsulated into the deep learning optimization.
Consequently, most of the existing methods rely on a post processing step using
the generalized Wiener filtering. This work proposes a method that learns and
optimizes (during training) a source-dependent mask and does not need the
aforementioned post processing step. We introduce a recurrent inference
algorithm, a sparse transformation step to improve the mask generation process,
and a learned denoising filter. Obtained results show an increase of 0.49 dB
for the signal to distortion ratio and 0.30 dB for the signal to interference
ratio, compared to previous state-of-the-art approaches for monaural singing
voice separation.



Linearly Constrained Kalman Filter For Linear Discrete State-Space Models

For linear discrete state-space (LDSS) models, under certain conditions, the
linear least mean squares filter estimate has a convenient recursive
predictor/corrector format, aka the Kalman filter (KF). The aim of the paper is
to introduce the general form of the linearly constrained KF (LCKF) for LDSS
models, which encompasses the linearly constrained minimum variance estimator
(LCMVE). Thus the LCKF opens access to the abundant litterature on LCMVE in the
deterministic framework which can be transposed to the stochastic framework.
Therefore, among other things, the LCKF may provide alternative solutions to
$H_{\infty }$ filter and unbiased finite impulse response filter to robustify
the KF, which performance are sensible to misspecified noise or uncertainties
in the system matrices



Machine Learning Approach to RF Transmitter Identification

With the development and widespread use of wireless devices in recent years
(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has
become extremely crowded. In order to counter security threats posed by rogue
or unknown transmitters, it is important to identify RF transmitters not by the
data content of the transmissions but based on the intrinsic physical
characteristics of the transmitters. RF waveforms represent a particular
challenge because of the extremely high data rates involved and the potentially
large number of transmitters present in a given location. These factors outline
the need for rapid fingerprinting and identification methods that go beyond the
traditional hand-engineered approaches. In this study, we investigate the use
of machine learning (ML) strategies to the classification and identification
problems, and the use of wavelets to reduce the amount of data required. Four
different ML strategies are evaluated: deep neural nets (DNN), convolutional
neural nets (CNN), support vector machines (SVM), and multi-stage training
(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method
preconditioned by wavelets was by far the most accurate, achieving 100%
classification accuracy of transmitters, as tested using data originating from
12 different transmitters. We discuss strategies for extension of MST to a much
larger number of transmitters.



Robust Expectation-Maximization Algorithm for DOA Estimation of Acoustic Sources in the Spherical Harmonic Domain

The direction of arrival (DOA) estimation of sound sources has been a popular
signal processing research topic due to its widespread applications. Using
spherical microphone arrays (SMA), DOA estimation can be applied in the
spherical harmonic (SH) domain without any spatial ambiguity. However, the
environment reverberation and noise can degrade the estimation performance. In
this paper, we propose a new expectation maximization (EM) algorithm for
deterministic maximum likelihood (ML) DOA estimation of L sound sources in the
presence of spatially nonuniform noise in the SH domain. Furthermore a new
closed-form Cramer-Rao bound (CRB) for the deterministic ML DOA estimation is
derived for the signal model in the SH domain. The main idea of the proposed
algorithm is considering the general model of the received signal in the SH
domain, we reduce the complexity of the ML estimation by breaking it down into
two steps: expectation and maximization steps. The proposed algorithm reduces
the complexity from 2L-dimensional space to L 2-dimensional space. Simulation
results indicate that the proposed algorithm shows at least an improvement of
6dB in robustness in terms of root mean square error (RMSE). Moreover, the RMSE
of the proposed algorithm is very close to the CRB compared to the recent
methods in reverberant and noisy environments in the large range of signal to
noise ratio.



Mutual Information in Frequency and its Application to Measure Cross-Frequency Coupling in Epilepsy

We define a metric, mutual information in frequency (MI-in-frequency), to
detect and quantify the statistical dependence between different frequency
components in the data, referred to as cross-frequency coupling and apply it to
electrophysiological recordings from the brain to infer cross-frequency
coupling. The current metrics used to quantify the cross-frequency coupling in
neuroscience cannot detect if two frequency components in non-Gaussian brain
recordings are statistically independent or not. Our MI-in-frequency metric,
based on Shannon's mutual information between the Cramer's representation of
stochastic processes, overcomes this shortcoming and can detect statistical
dependence in frequency between non-Gaussian signals. We then describe two
data-driven estimators of MI-in-frequency: one based on kernel density
estimation and the other based on the nearest neighbor algorithm and validate
their performance on simulated data. We then use MI-in-frequency to estimate
mutual information between two data streams that are dependent across time,
without making any parametric model assumptions. Finally, we use the MI-in-
frequency metric to investigate the cross-frequency coupling in seizure onset
zone from electrocorticographic recordings during seizures. The inferred
cross-frequency coupling characteristics are essential to optimize the spatial
and spectral parameters of electrical stimulation based treatments of epilepsy.



Multilingual Speech Recognition With A Single End-To-End Model

Training a conventional automatic speech recognition (ASR) system to support
multiple languages is challenging because the sub-word unit, lexicon and word
inventories are typically language specific. In contrast, sequence-to-sequence
models are well suited for multilingual ASR because they encapsulate an
acoustic, pronunciation and language model jointly in a single network. In this
work we present a single sequence-to-sequence ASR model trained on 9 different
Indian languages, which have very little overlap in their scripts.
Specifically, we take a union of language-specific grapheme sets and train a
grapheme-based sequence-to-sequence model jointly on data from all languages.
We find that this model, which is not explicitly given any information about
language identity, improves recognition performance by 21% relative compared to
analogous sequence-to-sequence models trained on each language individually. By
modifying the model to accept a language identifier as an additional input
feature, we further improve performance by an additional 7% relative and
eliminate confusion between different languages.



Simultaneous Block-Sparse Signal Recovery Using Pattern-Coupled Sparse Bayesian Learning

In this paper, we consider the block-sparse signals recovery problem in the
context of multiple measurement vectors (MMV) with common row sparsity
patterns. We develop a new method for recovery of common row sparsity MMV
signals, where a pattern-coupled hierarchical Gaussian prior model is
introduced to characterize both the block-sparsity of the coefficients and the
statistical dependency between neighboring coefficients of the common row
sparsity MMV signals. Unlike many other methods, the proposed method is able to
automatically capture the block sparse structure of the unknown signal. Our
method is developed using an expectation-maximization (EM) framework.
Simulation results show that our proposed method offers competitive performance
in recovering block-sparse common row sparsity pattern MMV signals.



Performance Analysis of NOMA in Training Based Multiuser MIMO Systems

This paper considers the use of NOMA in multiuser MIMO systems in practical
scenarios where CSI is acquired through pilot signaling. A new NOMA scheme that
uses shared pilots is proposed. Achievable rate analysis is carried out for
different pilot signaling schemes including both uplink and downlink pilots.
The achievable rate performance of the proposed NOMA scheme with shared pilot
within each group is compared with the traditional orthogonal access scheme
with orthogonal pilots. Our proposed scheme is a generalization of the
orthogonal scheme, and can be reduced to the orthogonal scheme when appropriate
power allocation parameters are chosen. Numerical results show that when
downlink CSI is available at the users, our proposed NOMA scheme outperforms
orthogonal schemes. However with more groups of users present in the cell, it
is preferable to use multi-user beamforming in stead of NOMA.



Minimum-Phase HRTF Modeling of Pinna Spectral Notches using Group Delay Decomposition

Accurate reconstruction of HRTFs is important in the development of high
quality binaural sound synthesis systems. Conventionally, minimum phase HRTF
model development for reconstruction of HRTFs has been limited to minimum
phase-pure delay models which ignore the all pass component of the HRTF. In
this paper, a novel method for minimum phase HRTF modelling of Pinna Spectral
Notches (PSNs) using group delay decomposition is proposed. The proposed model
captures the PSNs contributed by both the minimum phase and all pass component
of HRTF thus facilitating an accurate reconstruction of HRTFs. The purely
minimum phase HRTF components and their corresponding spatial angles are first
identified using Fourier Bessel Series method that ensures a continuous
evolution of the PSNs. The minimum phase-pure delay model is used to
reconstruct HRTF for these spatial angles. Subsequently, the spatial angles
which require both the minimum phase and all pass components are modelled using
an all-pass filter cascaded with minimum-phase pure-delay model. Performance of
the proposed model is evaluated by conducting experiments on PSN extraction,
cross coherence analysis, and binaural synthesis. Both objective and subjective
evaluation results are used to indicate the significance of the proposed model
in binaural sound synthesis.



Mandarin tone modeling using recurrent neural networks

We propose an Encoder-Classifier framework to model the Mandarin tones using
recurrent neural networks (RNN). In this framework, extracted frames of
features for tone classification are fed in to the RNN and casted into a fixed
dimensional vector (tone embedding) and then classified into tone types using a
softmax layer along with other auxiliary inputs. We investigate various
configurations that help to improve the model, including pooling, feature
splicing and utilization of syllable-level tone embeddings. Besides, tone
embeddings and durations of the contextual syllables are exploited to
facilitate tone classification. Experimental results on Mandarin tone
classification show the proposed network setups improve tone classification
accuracy. The results indicate that the RNN encoder-classifier based tone model
flexibly accommodates heterogeneous inputs (sequential and segmental) and hence
has the advantages from both the sequential classification tone models and
segmental classification tone models.



Semi-Parallel Deep Neural Networks (SPDNN), Convergence and Generalization

The Semi-Parallel Deep Neural Network (SPDNN) idea is explained in this
article and it has been shown that the convergence of the mixed network is very
close to the best network in the set and the generalization of SPDNN is better
than all the parent networks.



Partial Relaxation Approach: An Eigenvalue-Based DOA Estimator Framework

In this paper, the partial relaxation approach is introduced and applied to
DOA estimation using spectral search. Unlike existing methods like Capon or
MUSIC which can be considered as single source approximations of multi-source
estimation criteria, the proposed approach accounts for the existence of
multiple sources. At each considered direction, the manifold structure of the
remaining interfering signals impinging on the sensor array is relaxed, which
results in closed form estimates for the interference parameters. The
conventional multidimensional optimization problem reduces, thanks to this
relaxation, to a simple spectral search. Following this principle, we propose
estimators based on the Deterministic Maximum Likelihood, Weighted Subspace
Fitting and covariance fitting methods. To calculate the pseudo-spectra
efficiently, an iterative rooting scheme based on the rational function
approximation is applied to the partial relaxation methods. Simulation results
show that the performance of the proposed estimators is superior to the
conventional methods especially in the case of low Signal-to-Noise-Ratio and
low number of snapshots, irrespectively of any specific structure of the sensor
array while maintaining a comparable computational cost as MUSIC.



Comparison of Low Complexity Coherent Receivers for UDWDM-PONs ($\lambda$-to-the-user)

It is predicted that demand in optical access networks will reach multi-Gb/s
per user. However, the limited performance of the direct detection receiver
technology currently used in the optical network units at the customers'
premises restricts data rates/user. Therefore, the concept of coherent-enabled
access networks has attracted attention in recent years, as this technology
offers high receiver sensitivity, inherent frequency selectivity, and linear
field detection enabling the full compensation of linear channel impairments.
However, the complexity of conventional (dual-polarisation digital) coherent
receivers has so far prevented their introduction into access networks. Thus,
to exploit the benefits of coherent technology in the ONUs, low complexity
coherent receivers, suitable for implementation in ONUs, are needed. In this
paper, the recently proposed low complexity coherent (i.e.,
polarisation-independent Alamouti-coding heterodyne) receiver is, for the first
time, compared in terms of its minimum receiver sensitivity with five
previously reported receiver designs, including a detailed discussion on their
advantages and limitations. It is shown that the Alamouti-coding based receiver
approach allows the lowest number of photons per bit (PPB) transmitted (with a
lower bound of 15.5 PPB in an ideal system simulations) whilst requiring the
lowest optical receiver hardware complexity. It also exhibits comparable
complexity to the currently deployed direct-detection receivers, which
typically require >1000 PPB. Finally, a comparison of experimentally achieved
receiver sensitivities and transmission distances using these receivers is
presented. The highest spectral efficiency and longest transmission distance at
the highest bit rate reported using the Alamouti-coding receiver, which is also
the only one, to date, to have been demonstrated in a full system bidirectional
transmission.



Wireless Power Transfer and Data Collection in Wireless Sensor Networks

In a rechargeable wireless sensor network, the data packets are generated by
sensor nodes at a specific data rate, and transmitted to a base station.
Moreover, the base station transfers power to the nodes by using Wireless Power
Transfer (WPT) to extend their battery life. However, inadequately scheduling
WPT and data collection causes some of the nodes to drain their battery and
have their data buffer overflow, while the other nodes waste their harvested
energy, which is more than they need to transmit their packets. In this paper,
we investigate a novel optimal scheduling strategy, called EHMDP, aiming to
minimize data packet loss from a network of sensor nodes in terms of the nodes'
energy consumption and data queue state information. The scheduling problem is
first formulated by a centralized MDP model, assuming that the complete states
of each node are well known by the base station. This presents the upper bound
of the data that can be collected in a rechargeable wireless sensor network.
Next, we relax the assumption of the availability of full state information so
that the data transmission and WPT can be semi-decentralized. The simulation
results show that, in terms of network throughput and packet loss rate, the
proposed algorithm significantly improves the network performance.



Design of graph filters and filterbanks

Basic operations in graph signal processing consist in processing signals
indexed on graphs either by filtering them, to extract specific part out of
them, or by changing their domain of representation, using some transformation
or dictionary more adapted to represent the information contained in them. The
aim of this chapter is to review general concepts for the introduction of
filters and representations of graph signals. We first begin by recalling the
general framework to achieve that, which put the emphasis on introducing some
spectral domain that is relevant for graph signals to define a Graph Fourier
Transform. We show how to introduce a notion of frequency analysis for graph
signals by looking at their variations. Then, we move to the introduction of
graph filters, that are defined like the classical equivalent for 1D signals or
2D images, as linear systems which operate on each frequency of a signal. Some
examples of filters and of their implementations are given. Finally, as
alternate representations of graph signals, we focus on multiscale transforms
that are defined from filters. Continuous multiscale transforms such as
spectral wavelets on graphs are reviewed, as well as the versatile approaches
of filterbanks on graphs. Several variants of graph filterbanks are discussed,
for structured as well as arbitrary graphs, with a focus on the central point
of the choice of the decimation or aggregation operators.



Unsupervised Learning of Semantic Audio Representations

Even in the absence of any explicit semantic annotation, vast collections of
audio recordings provide valuable information for learning the categorical
structure of sounds. We consider several class-agnostic semantic constraints
that apply to unlabeled nonspeech audio: (i) noise and translations in time do
not change the underlying sound category, (ii) a mixture of two sound events
inherits the categories of the constituents, and (iii) the categories of events
in close temporal proximity are likely to be the same or related. Without
labels to ground them, these constraints are incompatible with classification
loss functions. However, they may still be leveraged to identify geometric
inequalities needed for triplet loss-based training of convolutional neural
networks. The result is low-dimensional embeddings of the input spectrograms
that recover 41% and 84% of the performance of their fully-supervised
counterparts when applied to downstream query-by-example sound retrieval and
sound event classification tasks, respectively. Moreover, in
limited-supervision settings, our unsupervised embeddings double the
state-of-the-art classification performance.



Underlay Control Signaling for Ultra-Reliable Low-Latency IoT Communications

Future mobile networks not only envision enhancing the traditional link
quality and data rates of mobile broad band (MBB) links, but also development
of new control channels to meet the requirements of delay sensitive use cases.
In particular, the need for ultra-reliable low-latency communications (URLLC)
for many internet of things (IoT) users is greatly emphasized. In this paper,
we present a novel spread spectrum waveform design that we propose for
transmission of control signals to establish URLLC communications. These
control signals are transmitted over the spectral resources that belong to the
MBB communications in the network, but at a level that minimally affects these
data channels. The proposed waveform, although a direct sequence spread
spectrum (DSSS) technique, is designed to take advantage of symbol
synchronization available to the OFDM broad band communications in the network.
This, clearly, allows simple synchronization with the rest of the network. The
proposed DSSS method can transmit single and multiple bits within each OFDM
time frame and can serve many user equipment (UE) nodes simultaneously.



Non-uniform time-scaling of Carnatic music transients

Gamakas are an integral aspect of Carnatic Music, a form of classical music
prevalent in South India. They are used in ragas, which may be seen as melodic
scales and/or a set of characteristic melodic phrases. Gamakas exhibit
continuous pitch variation often spanning several semitones. In this paper, we
study how gamakas scale with tempo and propose a novel approach to change the
tempo of Carnatic music pieces. The music signal is viewed as consisting of
constant-pitch segments and transients. The transients show continuous pitch
variation and we consider their analyses from a theoretical stand-point. We
next observe the non-uniform ratios of time-scaling of constant-pitch segments,
transients and silence in excerpts from nine concert renditions of varnams in
six ragas. The results indicate that the changing tempo of Carnatic music does
not change the duration of transients significantly. We report listening tests
on our algorithm to slow down Carnatic music that is consistent with this
observation.



Learning a Physical Activity Classifier for a Low-power Embedded Wrist-located Device

This article presents and evaluates a novel algorithm for learning a physical
activity classifier for a low-power embedded wrist-located device. The overall
system is designed for real-time execution and it is implemented in the
commercial low-power System-on-Chips nRF51 and nRF52. Results were obtained
using a database composed of 140 users containing more than 340 hours of
labeled raw acceleration data. The final precision achieved for the most
important classes, (Rest, Walk, and Run), was of 96%, 94%, and 99% and it
generalizes to compound activities such as XC skiing or Housework. We conclude
with a benchmarking of the system in terms of memory footprint and power
consumption.



The ACCompanion v0.1: An Expressive Accompaniment System

In this paper we present a preliminary version of the ACCompanion, an
expressive accompaniment system for MIDI input. The system uses a probabilistic
monophonic score follower to track the position of the soloist in the score,
and a linear Gaussian model to compute tempo updates. The expressiveness of the
system is powered by the Basis-Mixer, a state-of-the-art computational model of
expressive music performance. The system allows for expressive dynamics, timing
and articulation.



A Survey on Hardware Implementations of Visual Object Trackers

Visual object tracking is an active topic in the computer vision domain with
applications extending over numerous fields. The main sub-tasks required to
build an object tracker (e.g. object detection, feature extraction and object
tracking) are computation-intensive. In addition, real-time operation of the
tracker is indispensable for almost all of its applications. Therefore,
complete hardware or hardware/software co-design approaches are pursued for
better tracker implementations. This paper presents a literature survey of the
hardware implementations of object trackers over the last two decades. Although
several tracking surveys exist in literature, a survey addressing the hardware
implementations of the different trackers is missing. We believe this survey
would fill the gap and complete the picture with the existing surveys of how to
design an efficient tracker and point out the future directions researchers can
follow in this field. We highlight the lack of hardware implementations for
state-of-the-art tracking algorithms as well as for enhanced classical
algorithms. We also stress the need for measuring the tracking performance of
the hardware-based trackers. Additionally, enough details of the hardware-based
trackers need to be provided to allow reasonable comparison between the
different implementations.



Integrated All-Optical Fast Fourier Transform: Design and Sensitivity Analysis

The fast Fourier transform, FFT, is a useful and prevalent algorithm in
signal processing. It characterizes the spectral components of a signal, or is
used in combination with other operations to perform more complex computations
such as filtering, convolution, and correlation. Digital FFTs are limited in
speed by the necessity of moving charge within logic gates. An analog temporal
FFT in fiber optics has been demonstrated with highest data bandwidth. However,
the implementation with discrete fiber optic FFT components is bulky. Here, we
present and analyze a design of an optical FFT in Silicon photonics and
evaluate its performance with respect to variations in phase and amplitude. We
discuss the impact of the deployed devices on the FFTs transfer function
quality as defined by the transmission output power as a function of frequency,
detuning phase, optical delay, and loss.



End-to-end learning for music audio tagging at scale

The lack of data tends to limit the outcomes of deep learning research,
particularly when dealing with end-to-end learning stacks processing raw data
such as waveforms. In this study, 1.2M tracks annotated with musical labels are
available to train our end-to-end models. This large amount of data allows us
to unrestrictedly explore two different design paradigms for music
auto-tagging: assumption-free models - using waveforms as input with very small
convolutional filters; and models that rely on domain knowledge - log-mel
spectrograms with a convolutional neural network designed to learn timbral and
temporal features. Our work focuses on studying how these two types of deep
architectures perform when datasets of variable size are available for
training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs),
and a private dataset of 1.2M songs. Our experiments suggest that music domain
assumptions are relevant when not enough training data are available, thus
showing how waveform-based models outperform spectrogram-based ones in
large-scale data scenarios.



Structured Optical Receivers for Efficient Deep-Space Communication

We discuss conceptual designs for structured optical receivers that can
alleviate the requirement for high peak-to-average power ratio in
photon-starved optical communication. The basic idea is to transmit sequences
of suitably modulated coherent light pulses whose energy can be concentrated in
a single temporal bin on the receiver side through optical interference. Two
examples of scalable architectures for structured receivers are presented. The
first one, based on active polarization switching, maps Hadamard codewords
composed from the binary phase shift keying (BPSK) constellation onto the
standard pulse position modulation (PPM) format. The second receiver, using
solely passive optical elements, converts phase-polarization patterns of
coherent light pulses into a single pulse preserving a synchronized time of
arrival. Such a conversion enables implementation of a communication protocol
equivalent to the PPM scheme but with distributed optical power provided that
the intersymbol guard-time exceeds the pattern length.



A reconstruction algorithm for electrical capacitance tomography via total variation and l0-norm regularizations using experimental data

Electrical capacitance tomography (ECT) has been investigated in many fields
due to its advantages of being non-invasive and low cost. Sparse algorithms
with l1-norm regularization are used to reduce the smoothing effect and obtain
sharp images, such as total variation (TV)regularization. This paper proposed
for the first time to solve the ECT inverse problem using an l0-norm
regularization algorithm, namely the doubly extrapolated proximal iterative
hard thresholding (DEPIHT) algorithm. The accelerated alternating direction
method of multipliers (AADMM) algorithm, based on the TV regularization, has
been selected to acquire the first point for the DEPIHT algorithm. Experimental
tests were carried out to validate the feasibility of the AADMM-DEPIHT
algorithm,which is compared with the Landweber iteration (LI) and AADMM
algorithms. The results show the AADMM-DEPIHT algorithm has an improvement on
the quality of images and also indicates that the DEPIHT algorithm can be a
suitable candidate for ECT in post-process.



Kramers Kronig PAM transceiver and two-sided polarization-multiplexed Kramers Kronig transceiver

We propose two transceiver schemes based on Kramers Kronig (KK) detection.
One targets low-cost high-throughput applications and uses PAM transmission in
combination with direct detection and digital reconstruction of the optical
phase. This scheme allows digital compensation of chromatic dispersion and
provides a significant improvement in terms of spectral efficiency, compared to
conventional PAM transmission. The second scheme targets high-channel-count
coherent systems with the aim of simplifying the receiver complexity by
reducing the optical components count.



Tensor-Generative Adversarial Network with Two-dimensional Sparse Coding: Application to Real-time Indoor Localization

Localization technology is important for the development of indoor
location-based services (LBS). Global Positioning System (GPS) becomes invalid
in indoor environments due to the non-line-of-sight issue, so it is urgent to
develop a real-time high-accuracy localization approach for smartphones.
However, accurate localization is challenging due to issues such as real-time
response requirements, limited fingerprint samples and mobile device storage.
To address these problems, we propose a novel deep learning architecture:
Tensor-Generative Adversarial Network (TGAN).
  We first introduce a transform-based 3D tensor to model fingerprint samples.
Instead of those passive methods that construct a fingerprint database as a
prior, our model applies artificial neural network with deep learning to train
network classifiers and then gives out estimations. Then we propose a novel
tensor-based super-resolution scheme using the generative adversarial network
(GAN) that adopts sparse coding as the generator network and a residual
learning network as the discriminator. Further, we analyze the performance of
tensor-GAN and implement a trace-based localization experiment, which achieves
better performance. Compared to existing methods for smartphones indoor
positioning, that are energy-consuming and high demands on devices, TGAN can
give out an improved solution in localization accuracy, response time and
implementation complexity.



Sparse Randomized Kaczmarz for Support Recovery of Jointly Sparse Corrupted Multiple Measurement Vectors

While single measurement vector (SMV) models have been widely studied in
signal processing, there is a surging interest in addressing the multiple
measurement vectors (MMV) problem. In the MMV setting, more than one
measurement vector is available and the multiple signals to be recovered share
some commonalities such as a common support. Applications in which MMV is a
naturally occurring phenomenon include online streaming, medical imaging, and
video recovery. This work presents a stochastic iterative algorithm for the
support recovery of jointly sparse corrupted MMV. We present a variant of the
Sparse Randomized Kaczmarz algorithm for corrupted MMV and compare our proposed
method with an existing Kaczmarz type algorithm for MMV problems. We also
showcase the usefulness of our approach in the online (streaming) setting and
provide empirical evidence that suggests the robustness of the proposed method
to the distribution of the corruption and the number of corruptions occurring.



Realtime Profiling of Fine-Grained Air Quality Index Distribution using UAV Sensing

Given significant air pollution problems, air quality index (AQI) monitoring
has recently received increasing attention. In this paper, we design a mobile
AQI monitoring system boarded on unmanned-aerial-vehicles (UAVs), called ARMS,
to efficiently build fine-grained AQI maps in realtime. Specifically, we first
propose the Gaussian plume model on basis of the neural network (GPM-NN), to
physically characterize the particle dispersion in the air. Based on GPM-NN, we
propose a battery efficient and adaptive monitoring algorithm to monitor AQI at
the selected locations and construct an accurate AQI map with the sensed data.
The proposed adaptive monitoring algorithm is evaluated in two typical
scenarios, a two-dimensional open space like a roadside park, and a
three-dimensional space like a courtyard inside a building. Experimental
results demonstrate that our system can provide higher prediction accuracy of
AQI with GPM-NN than other existing models, while greatly reducing the power
consumption with the adaptive monitoring algorithm.



High-Speed Gate Driver Using GaN HEMTs for 20-MHz Hard Switching of SiC MOSFETs

In this paper, we investigated a gate driver using a GaN HEMT push-pull
configuration for the high-frequency hard switching of a SiC power MOSFET. Low
on-resistance and low input capacitance of GaN HEMTs are suitable for a
high-frequency gate driver from the logic level, and robustness of SiC MOSFET
with high avalanche capability is suitable for a valve transistor in power
converters. Our proposed gate driver consists of digital isolators,
complementary Si MOSFETs, and GaN HEMTs. The GaN HEMT push-pull stage has a
high driving capability owing to its superior switching characteristics, and
complementary Si MOSFETs can enhance the control signal from the digital
isolator. We investigated limiting factors of the switching frequency of the
proposed gate driver by focusing on each circuit component and proposed an
improved driving configuration for the gate driver. As a result, 20-MHz hard
switching of a SiC MOSFET was achieved using the improved gate driver with GaN
HEMTs.



Detection of Beat-to-Beat Intervals from Wrist Photoplethysmography in Patients with Sinus Rhythm and Atrial Fibrillation after Surgery

Wrist photoplethysmography (PPG) allows unobtrusive monitoring of the heart
rate (HR). PPG is affected by the capillary blood perfusion and the pumping
function of the heart, which generally deteriorate with age and due to presence
of cardiac arrhythmia. The performance of wrist PPG in monitoring beat-to-beat
HR in older patients with arrhythmia has not been reported earlier. We
monitored PPG from wrist in 18 patients recovering from surgery in the post
anesthesia care unit, and evaluated the inter-beat interval (IBI) detection
accuracy against ECG based R-to-R intervals (RRI). Nine subjects had sinus
rhythm (SR, 68.0y$\pm$10.2y, 6 males) and nine subjects had atrial fibrillation
(AF, 71.3y$\pm$7.8y, 4 males) during the recording. For the SR group, 99.44% of
the beats were correctly identified, 2.39% extra beats were detected, and the
mean absolute error (MAE) was 7.34 ms. For the AF group, 97.49% of the
heartbeats were correctly identified, 2.26% extra beats were detected, and the
MAE was 14.31 ms. IBI from the PPG were hence in close agreement with the ECG
reference in both groups. The results suggest that wrist PPG provides a
comfortable alternative to ECG and can be used for long-term monitoring and
screening of AF episodes.



Location-Aided Coordinated Analog Precoding for Uplink Multi-User Millimeter Wave Systems

Millimeter wave (mmWave) communication is expected to play an important role
in next generation cellular networks, aiming to cope with the bandwidth
shortage affecting conventional wireless carriers. Using side-information has
been proposed as a potential approach to accelerate beam selection in mmWave
massive MIMO (m-MIMO) communications. However, in practice, such information is
not error-free, leading to performance degradation. In the multi-user case, a
wrong beam choice might result in irreducible inter-user interference at the
base station (BS) side. In this paper, we consider location-aided precoder
design in a mmWave uplink scenario with multiple users (UEs). Assuming the
existence of direct device-to-device (D2D) links, we propose a decentralized
coordination mechanism for robust fast beam selection. The algorithm allows for
improved treatment of interference at the BS side and in turn leads to greater
spectral efficiencies.



A joint separation-classification model for sound event detection of weakly labelled data

Source separation (SS) aims to separate individual sources from an audio
recording. Sound event detection (SED) aims to detect sound events from an
audio recording. We propose a joint separation-classification (JSC) model
trained only on weakly labelled audio data, that is, only the tags of an audio
recording are known but the time of the events are unknown. First, we propose a
separation mapping from the time-frequency (T-F) representation of an audio to
the T-F segmentation masks of the audio events. Second, a classification
mapping is built from each T-F segmentation mask to the presence probability of
each audio event. In the source separation stage, sources of audio events and
time of sound events can be obtained from the T-F segmentation masks. The
proposed method achieves an equal error rate (EER) of 0.14 in SED,
outperforming deep neural network baseline of 0.29. Source separation SDR of
8.08 dB is obtained by using global weighted rank pooling (GWRP) as probability
mapping, outperforming the global max pooling (GMP) based probability mapping
giving SDR at 0.03 dB. Source code of our work is published.



Asynchronous Channel Training in Multi-Cell Massive MIMO

Pilot contamination has been regarded as the main bottleneck in time division
duplexing (TDD) multi-cell massive multiple-input multiple-output (MIMO)
systems. The pilot contamination problem cannot be addressed with large-scale
antenna arrays. We provide a novel asynchronous channel training scheme to
obtain precise channel matrices without the cooperation of base stations. The
scheme takes advantage of sampling diversity by inducing intentional timing
mismatch. Then, the linear minimum mean square error (LMMSE) estimator and the
zero-forcing (ZF) estimator are designed. Moreover, we derive the minimum
square error (MSE) upper bound of the ZF estimator. In addition, we propose the
equally-divided delay scheme which under certain conditions is the optimal
solution to minimize the MSE of the ZF estimator employing the identity matrix
as pilot matrix. We calculate the uplink achievable rate using maximum ratio
combining (MRC) to compare asynchronous and synchronous channel training
schemes. Finally, simulation results demonstrate that the asynchronous channel
estimation scheme can greatly reduce the harmful effect of pilot contamination.



Cellular Offloading via Downlink Cache Placement

In this paper, the downlink file transmission within a finite lifetime is
optimized with the assistance of wireless cache nodes. Specifically, the number
of requests within the lifetime of one file is modeled as a Poisson point
process. The base station multicasts files to downlink users and the selected
the cache nodes, so that the cache nodes can help to forward the files in the
next file request. Thus we formulate the downlink transmission as a Markov
decision process with random number of stages, where transmission power and
time on each transmission are the control policy. Due to random number of file
transmissions, we first proposed a revised Bellman's equation, where the
optimal control policy can be derived. In order to address the prohibitively
huge state space, we also introduce a low-complexity sub-optimal solution based
on an linear approximation of the value function. The approximated value
function can be calculated analytically, so that conventional numerical value
iteration can be eliminated. Moreover, the gap between the approximated value
function and the real value function is bounded analytically. It is shown by
simulation that, with the approximated MDP approach, the proposed algorithm can
significantly reduce the resource consumption at the base station.



Crafting Adversarial Examples For Speech Paralinguistics Applications

Computational paralinguistic analysis is increasingly being used in a wide
range of cyber applications, including security-sensitive applications such as
speaker verification, deceptive speech detection, and medical diagnostics.
While state-of-the-art machine learning techniques, such as deep neural
networks, can provide robust and accurate speech analysis, they are susceptible
to adversarial attacks. In this work, we propose an end-to-end scheme to
generate adversarial examples for computational paralinguistic applications by
perturbing directly the raw waveform of an audio recording rather than specific
acoustic features. Our experiments show that the proposed adversarial
perturbation can lead to a significant performance drop of state-of-the-art
deep neural networks, while only minimally impairing the audio quality.



Picasso, Matisse, or a Fake? Automated Analysis of Drawings at the Stroke Level for Attribution and Authentication

This paper proposes a computational approach for analysis of strokes in line
drawings by artists. We aim at developing an AI methodology that facilitates
attribution of drawings of unknown authors in a way that is not easy to be
deceived by forged art. The methodology used is based on quantifying the
characteristics of individual strokes in drawings. We propose a novel algorithm
for segmenting individual strokes. We designed and compared different
hand-crafted and learned features for the task of quantifying stroke
characteristics. We also propose and compare different classification methods
at the drawing level. We experimented with a dataset of 300 digitized drawings
with over 80 thousands strokes. The collection mainly consisted of drawings of
Pablo Picasso, Henry Matisse, and Egon Schiele, besides a small number of
representative works of other artists. The experiments shows that the proposed
methodology can classify individual strokes with accuracy 70%-90%, and
aggregate over drawings with accuracy above 80%, while being robust to be
deceived by fakes (with accuracy 100% for detecting fakes in most settings).



Hydra: An Accelerator for Real-Time Edge-Aware Permeability Filtering in 65nm CMOS

Many modern video processing pipelines rely on edge-aware (EA) filtering
methods. However, recent high-quality methods are challenging to run in
real-time on embedded hardware due to their computational load. To this end, we
propose an area-efficient and real-time capable hardware implementation of a
high quality EA method. In particular, we focus on the recently proposed
permeability filter (PF) that delivers promising quality and performance in the
domains of HDR tone mapping, disparity and optical flow estimation. We present
an efficient hardware accelerator that implements a tiled variant of the PF
with low on-chip memory requirements and a significantly reduced external
memory bandwidth (6.4x w.r.t. the non-tiled PF). The design has been taped out
in 65 nm CMOS technology, is able to filter 720p grayscale video at 24.8 Hz and
achieves a high compute density of 6.7 GFLOPS/mm2 (12x higher than embedded
GPUs when scaled to the same technology node). The low area and bandwidth
requirements make the accelerator highly suitable for integration into SoCs
where silicon area budget is constrained and external memory is typically a
heavily contended resource.



Performance of Source transmit Antenna selection for MIMO cooperative communication System Based DF protocol: Symbol Error Rate and Diversity order

In this work, we study the performance of a single relay Multiple Input
Multiple Output (MIMO) cooperative communication system based on Decode and
Forward (DF) relaying protocol for two strategies using transmit antenna
selection at the source. The first strategy uses one antenna between the relay
and the destination, and the second strategy uses Space Time Block Coding
(STBC). All channels follow the Rayleigh fading distribution. We derive the
expression and Upper Bound for Symbol Error Rate (SER) for M-ary Phase Shift
Keying (M-PSK), and the diversity order for both strategies. The analytical
results show that the second strategy performs better than the first one for
the same diversity order and the same Rate R.



Forced Oscillation Source Location via Multivariate Time Series Classification

Precisely locating low-frequency oscillation sources is the prerequisite of
suppressing sustained oscillation, which is an essential guarantee for the
secure and stable operation of power grids. Using synchrophasor measurements, a
machine learning method is proposed to locate the source of forced oscillation
in power systems. Rotor angle and active power of each power plant are utilized
to construct multivariate time series (MTS). Applying Mahalanobis distance
metric and dynamic time warping, the distance between MTS with different phases
or lengths can be appropriately measured. The obtained distance metric,
representing characteristics during the transient phase of forced oscillation
under different disturbance sources, is used for offline classifier training
and online matching to locate the disturbance source. Simulation results using
the four-machine two-area system and IEEE 39-bus system indicate that the
proposed location method can identify the power system forced oscillation
source online with high accuracy.



Beyond Trans-dimensional RJMCMC: Application to Impulsive Data Modeling

Reversible jump Markov chain Monte Carlo (RJMCMC) is a Bayesian model
estimation method which has been used for trans-dimensional sampling. In this
study, we propose utilization of RJMCMC beyond trans-dimensional sampling. This
new interpretation, which we call trans-space RJMCMC, reveals the undiscovered
potential of RJMCMC by exploiting the original formulation to explore spaces of
different classes or structures. This provides flexibility in using different
types of candidate classes in the combined model space such as spaces of linear
and nonlinear models or of various distribution families. As an application for
the proposed method, we have performed a special case of trans-space sampling,
namely trans-distributional RJMCMC in impulsive data modeling. In many areas
such as seismology, radar, image, using Gaussian models is a common practice
due to analytical ease. However, many noise processes do not follow a Gaussian
character and generally exhibit events too impulsive to be successfully
described by the Gaussian model. We test the proposed method to choose between
various impulsive distribution families to model both synthetically generated
noise processes and real-life measurements on power line communications (PLC)
impulsive noises and 2-D discrete wavelet transform (2-D DWT) coefficients.



How Long Will My Phone Battery Last?

Mobile devices are only as useful as their battery lasts. Unfortunately, the
operation and life of a mobile device's battery degrade over time and usage.
The state-of-health (SoH) of batteries quantifies their degradation, but mobile
devices are unable to support its accurate estimation -- despite its importance
-- due mainly to their limited hardware and dynamic usage patterns, causing
various problems such as unexpected device shutoffs or even fire/explosion. To
remedy this lack of support, we design, implement and evaluate V-Health, a
low-cost user-level SoH estimation service for mobile devices based only on
their battery voltage, which is commonly available on all commodity mobile
devices. V-Health also enables four novel use-cases that improve mobile users'
experience from different perspectives. The design of V-Health is inspired by
our empirical finding that the relaxing voltages of a device battery
fingerprint its SoH, and is steered by extensive measurements with 15 batteries
used for various commodity mobile devices, such as Nexus 6P, Galaxy S3, iPhone
6 Plus, etc. These measurements consist of 13,377 battery
discharging/charging/resting cycles and have been conducted over 72 months
cumulatively. V-Health has been evaluated via both laboratory experiments and
field tests over 4-6 months, showing <5% error in SoH estimation.



Human Exposure to RF Fields in 5G Downlink

While cellular communications in millimeter wave (mmW) bands have been
attracting significant research interest, their potential harmful impacts on
human health are not as significantly studied. Prior research on human exposure
to radio frequency (RF) fields in a cellular communications system has been
focused on uplink only due to the closer physical contact of a transmitter to a
human body. However, this paper claims the necessity of thorough investigation
on human exposure to downlink RF fields, as cellular systems deployed in mmW
bands will entail (i) deployment of more transmitters due to smaller cell size
and (ii) higher concentration of RF energy using a highly directional antenna.
In this paper, we present human RF exposure levels in downlink of a Fifth
Generation Wireless Systems (5G). Our results show that 5G downlink RF fields
generate significantly higher power density (PD) and specific absorption rate
(SAR) than a current cellular system. This paper also shows that SAR should
also be taken into account for determining human RF exposure in the mmW
downlink.



Tracking Multiple Vehicles Using a Variational Radar Model

High-resolution radar sensors are able to resolve multiple detections per
object and therefore provide valuable information for vehicle environment
perception. For instance, multiple detections allow to infer the size of an
object or to more precisely measure the object's motion. Yet, the increased
amount of data raises the demands on tracking modules: measurement models that
are able to process multiple detections for an object are necessary and
measurement-to-object associations become more complex. This paper presents a
new variational radar model for tracking vehicles using radar detections and
demonstrates how this model can be incorporated into a Random-Finite-Set-based
multi-object filter. The measurement model is learned from actual data using
variational Gaussian mixtures and avoids excessive manual engineering. In
combination with the multiobject tracker, the entire process chain from the raw
measurements to the resulting tracks is formulated probabilistically. The
presented approach is evaluated on experimental data and it is demonstrated
that the data-driven measurement model outperforms a manually designed model.



Sparse Bayesian Learning for DOA Estimation in Heteroscedastic Noise

The paper considers direction of arrival (DOA) estimation from long-term
observations in a noisy environment. In such an environment the noise source
might evolve, causing the stationary models to fail. Therefore a
heteroscedastic Gaussian noise model is introduced where the variance can vary
across observations and sensors. The source amplitudes are assumed independent
zero-mean complex Gaussian distributed with unknown variances (i.e. the source
powers), inspiring stochastic maximum likelihood DOA estimation. The DOAs of
plane waves are estimated from multi-snapshot sensor array data using sparse
Bayesian learning (SBL) where the noise is estimated across both sensors and
snapshots. This SBL approach is more flexible and performs better than
high-resolution methods since they cannot estimate the heteroscedastic noise
process. An alternative to SBL is simple data normalization, whereby only the
phase across the array is utilized. Simulations demonstrate that taking the
heteroscedastic noise into account improves DOA estimation.



Interpolation and Extrapolation of Toeplitz Matrices via Optimal Mass Transport

In this work, we propose a novel method for quantifying distances between
Toeplitz structured covariance matrices. By exploiting the spectral
representation of Toeplitz matrices, the proposed distance measure is defined
based on an optimal mass transport problem in the spectral domain. This may
then be interpreted in the covariance domain, suggesting a natural way of
interpolating and extrapolating Toeplitz matrices, such that the positive
semi-definiteness and the Toeplitz structure of these matrices are preserved.
The proposed distance measure is also shown to be contractive with respect to
both additive and multiplicative noise, and thereby allows for a quantification
of the decreased distance between signals when these are corrupted by noise.
Finally, we illustrate how this approach can be used for several applications
in signal processing. In particular, we consider interpolation and
extrapolation of Toeplitz matrices, as well as clustering problems and tracking
of slowly varying stochastic processes.



Estimation of Angles of Arrival Through Superresolution -- A Soft Recovery Approach for General Antenna Geometries

The estimation of direction of arrivals with help of $TV$-minimization is
studied. Contrary to prior work in this direction, which has only considered
certain antenna placement designs, we consider general antenna geometries.
Applying the soft-recovery framework, we are able to derive a theoretic
guarantee for a certain direction of arrival to be approximately recovered. We
discuss the impact of the recovery guarantee for a few concrete antenna
designs. Additionally, numerical simulations supporting the findings of the
theoretical part are performed.



Deep Within-Class Covariance Analysis for Robust Audio Representation Learning

Convolutional Neural Networks (CNNs) can learn effective features, though
have been shown to suffer from a performance drop when the distribution of the
data changes from training to test data. In this paper we analyze the internal
representations of CNNs and observe that the representations of unseen data in
each class, spread more (with higher variance) in the embedding space of the
CNN compared to representations of the training data. More importantly, this
difference is more extreme if the unseen data comes from a shifted
distribution. Based on this observation, we objectively evaluate the degree of
representation's variance in each class via eigenvalue decomposition on the
within-class covariance of the internal representations of CNNs and observe the
same behaviour. This can be problematic as larger variances might lead to
mis-classification if the sample crosses the decision boundary of its class. We
apply nearest neighbor classification on the representations and empirically
show that the embeddings with the high variance actually have significantly
worse KNN classification performances, although this could not be foreseen from
their end-to-end classification results. To tackle this problem, we propose
Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that
significantly reduces the within-class covariance of a DNN's representation,
improving performance on unseen test data from a shifted distribution. We
empirically evaluate DWCCA on two datasets for Acoustic Scene Classification
(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA
significantly improve the network's internal representation, it also increases
the end-to-end classification accuracy, especially when the test set exhibits a
distribution shift. By adding DWCCA to a VGG network, we achieve around 6
percentage points improvement in the case of a distribution mismatch.



Weakly Supervised Audio Source Separation via Spectrum Energy Preserved Wasserstein Learning

Separating audio mixtures into individual instrument tracks has been a long
standing challenging task. We introduce a novel weakly supervised audio source
separation approach based on deep adversarial learning. Specifically, our loss
function adopts the Wasserstein distance which directly measures the
distribution distance between the separated sources and the real sources for
each individual source. Moreover, a global regularization term is added to
fulfill the spectrum energy preservation property regardless separation. Unlike
state-of-the-art weakly supervised models which often involve deliberately
devised constraints or careful model selection, our approach need little prior
model specification on the data, and can be straightforwardly learned in an
end-to-end fashion. We show that the proposed method performs competitively on
public benchmark against state-of-the-art weakly supervised methods.



Financial Time Series Prediction Using Deep Learning

In this work we present a data-driven end-to-end Deep Learning approach for
time series prediction, applied to financial time series. A Deep Learning
scheme is derived to predict the temporal trends of stocks and ETFs in NYSE or
NASDAQ. Our approach is based on a neural network (NN) that is applied to raw
financial data inputs, and is trained to predict the temporal trends of stocks
and ETFs. In order to handle commission-based trading, we derive an investment
strategy that utilizes the probabilistic outputs of the NN, and optimizes the
average return. The proposed scheme is shown to provide statistically
significant accurate predictions of financial market trends, and the investment
strategy is shown to be profitable under this challenging setup. The
performance compares favorably with contemporary benchmarks along two-years of
back-testing.



Power System Transient Stability Assessment Using Couple Machines Method

Analyzing the stability of the power system by using a few machines is
promising for transient stability assessment. A hybrid direct-time-domain
method that is fully based on the thinking of partial energy function is
proposed in this paper. During post-fault period, a pair of machines with high
rotor speed difference is defined as couple machines, and the stability
analysis of the system is transformed into that of several pairs of couple
machines. Based on the prediction of power-angle curve of couple machines
within a sampling window after fault clearing, the proposed method avoids the
definition of Center of Inertia (COI) and it can also evaluate the stability
margin of the system by using the predicted power-angle curve. Simulation
results demonstrate its effectiveness in transient stability assessment.



Sensor Selection and Random Field Reconstruction for Robust and Cost-effective Heterogeneous Weather Sensor Networks for the Developing World

We address the two fundamental problems of spatial field reconstruction and
sensor selection in heterogeneous sensor networks: (i) how to efficiently
perform spatial field reconstruction based on measurements obtained
simultaneously from networks with both high and low quality sensors; and (ii)
how to perform query based sensor set selection with predictive MSE performance
guarantee. For the first problem, we developed a low complexity algorithm based
on the spatial best linear unbiased estimator (S-BLUE). Next, building on the
S-BLUE, we address the second problem, and develop an efficient algorithm for
query based sensor set selection with performance guarantee. Our algorithm is
based on the Cross Entropy method which solves the combinatorial optimization
problem in an efficient manner.



Deep Networks tag the location of bird vocalisations on audio spectrograms

This work focuses on reliable detection and segmentation of bird
vocalizations as recorded in the open field. Acoustic detection of avian sounds
can be used for the automatized monitoring of multiple bird taxa and querying
in long-term recordings for species of interest. These tasks are tackled in
this work, by suggesting two approaches: A) First, DenseNets are applied to
weekly labeled data to infer the attention map of the dataset (i.e. Salience
and CAM). We push further this idea by directing attention maps to the YOLO v2
Deepnet-based, detection framework to localize bird vocalizations. B) A deep
autoencoder, namely the U-net, maps the audio spectrogram of bird vocalizations
to its corresponding binary mask that encircles the spectral blobs of
vocalizations while suppressing other audio sources. We focus solely on
procedures requiring minimum human attendance, suitable to scan massive volumes
of data, in order to analyze them, evaluate insights and hypotheses and
identify patterns of bird activity. Hopefully, this approach will be valuable
to researchers, conservation practitioners, and decision makers that need to
design policies on biodiversity issues.



Automatic detection of alarm sounds in a noisy hospital environment using model and non-model based approaches

In the noisy acoustic environment of a Neonatal Intensive Care Unit (NICU)
there is a variety of alarms, which are frequently triggered by the biomedical
equipment. In this paper different approaches for automatic detection of those
sound alarms are presented and compared: 1) a non-model-based approach that
employs signal processing techniques; 2) a model-based approach based on neural
networks; and 3) an approach that combines both non-model and model-based
approaches. The performance of the developed detection systems that follow each
of those approaches is assessed, analysed and compared both at the frame level
and at the event level by using an audio database recorded in a real-world
hospital environment.



Dynamic Multi-Arm Bandit Game Based Multi-Agents Spectrum Sharing Strategy Design

For a wireless avionics communication system, a Multi-arm bandit game is
mathematically formulated, which includes channel states, strategies, and
rewards. The simple case includes only two agents sharing the spectrum which is
fully studied in terms of maximizing the cumulative reward over a finite time
horizon. An Upper Confidence Bound (UCB) algorithm is used to achieve the
optimal solutions for the stochastic Multi-Arm Bandit (MAB) problem. Also, the
MAB problem can also be solved from the Markov game framework perspective.
Meanwhile, Thompson Sampling (TS) is also used as benchmark to evaluate the
proposed approach performance. Numerical results are also provided regarding
minimizing the expectation of the regret and choosing the best parameter for
the upper confidence bound.



A Novel Method of Bolt Detection Based on Variational Modal Decomposition

The pull test is a destructive detection method, and it can t measure the
actual length of the bolt. As such, ultrasonic echo is one of the most
important non-destructive testing methods for bolt quality detection. In this
paper, the variance modal decomposition method is introduced into the bolt
detection signal analysis. Based on the morphological filtering and the VMD
method, the VMD combined morphological filtering principle is established into
the bolt detection signal analysis method. MF-VMD was used in order to analyze
the simulation vibration signal and the actual bolt detection signal. The
results showed that the MF-VMD is able to effectively separate the intrinsic
mode function, even when under the background of strong interference. Compared
with the conventional VMD method, the proposed method is able to remove the
noise interference. The intrinsic mode function of the field detection signal
can be effectively identified by the reflection of the signal at the bottom of
the bolt.



Multilayer Nonlinear Processing for Information Privacy in Sensor Networks

A sensor network wishes to transmit information to a fusion center to allow
it to detect a public hypothesis, but at the same time prevent it from
inferring a private hypothesis. We propose a multilayer nonlinear processing
procedure at each sensor to distort the sensor's data before it is sent to the
fusion center. In our proposed framework, sensors are grouped into clusters,
and each sensor first applies a nonlinear fusion function on the information it
receives from sensors in the same cluster and in a previous layer. A linear
weighting matrix is then used to distort the information it sends to sensors in
the next layer. We adopt a nonparametric approach and develop a modified mirror
descent algorithm to optimize the weighting matrices so as to ensure that the
regularized empirical risk of detecting the private hypothesis is above a given
privacy threshold, while minimizing the regularized empirical risk of detecting
the public hypothesis. Experiments on empirical datasets demonstrate that our
approach is able to achieve a good trade-off between the error rates of the
public and private hypothesis.



Blind Source Separation Using Mixtures of Alpha-Stable Distributions

We propose a new blind source separation algorithm based on mixtures of
alpha-stable distributions. Complex symmetric alpha-stable distributions have
been recently showed to better model audio signals in the time-frequency domain
than classical Gaussian distributions thanks to their larger dynamic range.
However, inference of these models is notoriously hard to perform because their
probability density functions do not have a closed-form expression in general.
Here, we introduce a novel method for estimating mixture of alpha-stable
distributions based on characteristic function matching. We apply this to the
blind estimation of binary masks in individual frequency bands from
multichannel convolutive audio mixes. We show that the proposed method yields
better separation performance than Gaussian-based binary-masking methods.



Audio-to-score alignment of piano music using RNN-based automatic music transcription

We propose a framework for audio-to-score alignment on piano performance that
employs automatic music transcription (AMT) using neural networks. Even though
the AMT result may contain some errors, the note prediction output can be
regarded as a learned feature representation that is directly comparable to
MIDI note or chroma representation. To this end, we employ two recurrent neural
networks that work as the AMT-based feature extractors to the alignment
algorithm. One predicts the presence of 88 notes or 12 chroma in frame-level
and the other detects note onsets in 12 chroma. We combine the two types of
learned features for the audio-to-score alignment. For comparability, we apply
dynamic time warping as an alignment algorithm without any additional
post-processing. We evaluate the proposed framework on the MAPS dataset and
compare it to previous work. The result shows that the alignment framework with
the learned features significantly improves the accuracy, achieving less than
10 ms in mean onset error.



Phonemic and Graphemic Multilingual CTC Based Speech Recognition

Training automatic speech recognition (ASR) systems requires large amounts of
data in the target language in order to achieve good performance. Whereas large
training corpora are readily available for languages like English, there exists
a long tail of languages which do suffer from a lack of resources. One method
to handle data sparsity is to use data from additional source languages and
build a multilingual system. Recently, ASR systems based on recurrent neural
networks (RNNs) trained with connectionist temporal classification (CTC) have
gained substantial research interest. In this work, we extended our previous
approach towards training CTC-based systems multilingually. Our systems feature
a global phone set, based on the joint phone sets of each source language. We
evaluated the use of different language combinations as well as the addition of
Language Feature Vectors (LFVs). As contrastive experiment, we built systems
based on graphemes as well. Systems having a multilingual phone set are known
to suffer in performance compared to their monolingual counterparts. With our
proposed approach, we could reduce the gap between these mono- and multilingual
setups, using either graphemes or phonemes.



Multilingual Adaptation of RNN Based ASR Systems

In this work, we focus on multilingual systems based on recurrent neural
networks (RNNs), trained using the Connectionist Temporal Classification (CTC)
loss function. Using a multilingual set of acoustic units poses difficulties.
To address this issue, we proposed Language Feature Vectors (LFVs) to train
language adaptive multilingual systems. Language adaptation, in contrast to
speaker adaptation, needs to be applied not only on the feature level, but also
to deeper layers of the network. In this work, we therefore extended our
previous approach by introducing a novel technique which we call "modulation".
Based on this method, we modulated the hidden layers of RNNs using LFVs. We
evaluated this approach in both full and low resource conditions, as well as
for grapheme and phone based systems. Lower error rates throughout the
different conditions could be achieved by the use of the modulation.



An Extended Kalman Filter Enhanced Hilbert-Huang Transform in Oscillation Detection

Hilbert-Huang transform (HHT) has drawn great attention in power system
analysis due to its capability to deal with dynamic signal and provide
instantaneous characteristics such as frequency, damping, and amplitudes.
However, its shortcomings, including mode mixing and end effects, are as
significant as its advantages. A preliminary result of an extended Kalman
filter (EKF) method to enhance HHT and hopefully to overcome these
disadvantages is presented in this paper. The proposal first removes dynamic DC
components in signals using empirical mode decomposition. Then an EKF model is
applied to extract instant coefficients. Numerical results using simulated and
real-world low-frequency oscillation data suggest the proposal can help to
overcome the mode mixing and end effects with a properly chosen number of
modes.



Orbital-angular-momentum mode-group multiplexed transmission over a graded-index ring-core fiber based on receive diversity and maximal ratio combining

An orbital-angular-momentum (OAM) mode-group multiplexing (MGM) scheme based
on a graded-index ring-core fiber (GIRCF) is proposed, in which a single-input
two-output (or receive diversity) architecture is designed for each MG channel
and simple digital signal processing (DSP) is utilized to adaptively resist the
mode partition noise resulting from random intra-group mode crosstalk. There is
no need of complex multiple-input multiple-output (MIMO) equalization in this
scheme. Furthermore, the signal-to-noise ratio (SNR) of the received signals
can be improved if a simple maximal ratio combining (MRC) technique is employed
on the receiver side to efficiently take advantage of the diversity gain of
receiver. Intensity-modulated direct-detection (IM-DD) systems transmitting
three OAM mode groups with total 100-Gb/s discrete multi-tone (DMT) signals
over a 1-km GIRCF and two OAM mode groups with total 40-Gb/s DMT signals over
an 18-km GIRCF are experimentally demonstrated, respectively, to confirm the
feasibility of our proposed OAM-MGM scheme.



Person Recognition using Smartphones' Accelerometer Data

Smartphones have become quite pervasive in various aspects of our daily
lives. They have become important links to a host of important data and
applications, which if compromised, can lead to disastrous results. Due to
this, today's smartphones are equipped with multiple layers of authentication
modules. However, there still lies the need for a viable and unobtrusive layer
of security which can perform the task of user authentication using resources
which are cost-efficient and widely available on smartphones. In this work, we
propose a method to recognize users using data from a phone's embedded
accelerometer sensors. Features encapsulating information from both time and
frequency domains are extracted from walking data samples, and are used to
build a Random Forest ensemble classification model. Based on the experimental
results, the resultant model delivers an accuracy of 0.9679 and Area under
Curve (AUC) of 0.9822.



Invariances and Data Augmentation for Supervised Music Transcription

This paper explores a variety of models for frame-based music transcription,
with an emphasis on the methods needed to reach state-of-the-art on human
recordings. The translation-invariant network discussed in this paper, which
combines a traditional filterbank with a convolutional neural network, was the
top-performing model in the 2017 MIREX Multiple Fundamental Frequency
Estimation evaluation. This class of models shares parameters in the
log-frequency domain, which exploits the frequency invariance of music to
reduce the number of model parameters and avoid overfitting to the training
data. All models in this paper were trained with supervision by labeled data
from the MusicNet dataset, augmented by random label-preserving pitch-shift
transformations.



A unified decision making framework for supply and demand management in microgrid networks

This paper considers two important problems -- on the supply-side and
demand-side respectively and studies both in a unified framework. On the supply
side, we study the problem of energy sharing among microgrids with the goal of
maximizing profit obtained from selling power while at the same time not
deviating much from the customer demand. On the other hand, under shortage of
power, this problem becomes one of deciding the amount of power to be bought
with dynamically varying prices. On the demand side, we consider the problem of
optimally scheduling the time-adjustable demand - i.e., of loads with flexible
time windows in which they can be scheduled. While previous works have treated
these two problems in isolation, we combine these problems together and provide
a unified Markov decision process (MDP) framework for these problems. We then
apply the Q-learning algorithm, a popular model-free reinforcement learning
technique, to obtain the optimal policy. Through simulations, we show that the
policy obtained by solving our MDP model provides more profit to the
microgrids.



Application of Machine Learning for Channel based Message Authentication in Mission Critical Machine Type Communication

The design of robust wireless communication systems for industrial
applications such as closed loop control processes has been considered manifold
recently. Additionally, the ongoing advances in the area of connected mobility
have similar or even higher requirements regarding system reliability and
availability. Beside unfulfilled reliability requirements, the availability of
a system can further be reduced, if it is under attack in the sense of
violation of information security goals such as data authenticity or integrity.
In order to guarantee the safe operation of an application, a system has at
least to be able to detect these attacks. Though there are numerous techniques
in the sense of conventional cryptography in order to achieve that goal, these
are not always suited for the requirements of the applications mentioned due to
resource inefficiency. In the present work, we show how the goal of message
authenticity based on physical layer security (PHYSEC) can be achieved. The
main idea for such techniques is to exploit user specific characteristics of
the wireless channel, especially in spatial domain. Additionally, we show the
performance of our machine learning based approach and compare it with other
existing approaches.



Robust massive MIMO Equilization for mmWave systems with low resolution ADCs

Leveraging the available millimeter wave spectrum will be important for 5G.
In this work, we investigate the performance of digital beamforming with low
resolution ADCs based on link level simulations including channel estimation,
MIMO equalization and channel decoding. We consider the recently agreed 3GPP NR
type 1 OFDM reference signals. The comparison shows sequential DCD outperforms
MMSE-based MIMO equalization both in terms of detection performance and
complexity. We also show that the DCD based algorithm is more robust to channel
estimation errors. In contrast to the common believe we also show that the
complexity of MMSE equalization for a massive MIMO system is not dominated by
the matrix inversion but by the computation of the Gram matrix.



Optimal Tuning of Two-Dimensional Keyboards

We give a new analysis of a tuning problem in music theory, pertaining
specifically to the approximation of harmonics on a two-dimensional keyboard.
We formulate the question as a linear programming problem on families of
constraints and provide exact solutions for many new keyboard dimensions. We
also show that an optimal tuning for harmonic approximation can be obtained for
any keyboard of given width, provided sufficiently many rows of octaves.



Unified Approach to Convex Robust Distributed Control given Arbitrary Information Structures

We consider the problem of computing optimal linear control policies for
linear systems in finite-horizon. The states and the inputs are required to
remain inside pre-specified safety sets at all times despite unknown
disturbances. In this technical note, we focus on the requirement that the
control policy is distributed, in the sense that it can only be based on
partial information about the history of the outputs. It is well-known that
when a condition denoted as Quadratic Invariance (QI) holds, the optimal
distributed control policy can be computed in a tractable way. Our goal is to
unify and generalize the class of information structures over which quadratic
invariance is equivalent to a test over finitely many binary matrices. The test
we propose certifies convexity of the output-feedback distributed control
problem in finite-horizon given any arbitrarily defined information structure,
including the case of time varying communication networks and forgetting
mechanisms. Furthermore, the framework we consider allows for including
polytopic constraints on the states and the inputs in a natural way, without
affecting convexity.



Automatic Conflict Detection in Police Body-Worn Audio

Automatic conflict detection has grown in relevance with the advent of
body-worn technology, but existing metrics such as turn-taking and overlap are
poor indicators of conflict in police-public interactions. Moreover, standard
techniques to compute them fall short when applied to such diversified and
noisy contexts. We develop a pipeline catered to this task combining adaptive
noise removal, non-speech filtering and new measures of conflict based on the
repetition and intensity of phrases in speech. We demonstrate the effectiveness
of our approach on body-worn audio data collected by the Los Angeles Police
Department.



Linear system security -- detection and correction of adversarial attacks in the noise-free case

We address the problem of attack detection and attack correction for
multi-output discrete-time linear time-invariant systems under sensor attack.
More specifically, we focus on the situation where adversarial attack signals
are added to some of the system's output signals. A 'security index' is defined
to characterize the vulnerability of a system against such sensor attacks.
Methods to compute the security index are presented as are algorithms to detect
and correct for sensor attacks. The results are illustrated by examples
involving multiple sensors.



Human and Machine Speaker Recognition Based on Short Trivial Events

Trivial events are ubiquitous in human to human conversations, e.g., cough,
laugh and sniff. Compared to regular speech, these trivial events are usually
short and unclear, thus generally regarded as not speaker discriminative and so
are largely ignored by present speaker recognition research. However, these
trivial events are highly valuable in some particular circumstances such as
forensic examination, as they are less subjected to intentional change, so can
be used to discover the genuine speaker from disguised speech. In this paper,
we collect a trivial event speech database that involves 75 speakers and 6
types of events, and report preliminary speaker recognition results on this
database, by both human listeners and machines. Particularly, the deep feature
learning technique recently proposed by our group is utilized to analyze and
recognize the trivial events, which leads to acceptable equal error rates
(EERs) despite the extremely short durations (0.2-0.5 seconds) of these events.
Comparing different types of events, 'hmm' seems more speaker discriminative.



Emotional End-to-End Neural Speech Synthesizer

In this paper, we introduce an emotional speech synthesizer based on the
recent end-to-end neural model, named Tacotron. Despite its benefits, we found
that the original Tacotron suffers from the exposure bias problem and
irregularity of the attention alignment. Later, we address the problem by
utilization of context vector and residual connection at recurrent neural
networks (RNNs). Our experiments showed that the model could successfully train
and generate speech for given emotion labels.



Statistical Approaches for Initial Access in mmWave 5G Systems

mmWave communication systems overcome high attenuation by using multiple
antennas at both the transmitter and the receiver to perform beamforming. Upon
entrance of a user equipment (UE) into a cell a scanning procedure must be
performed by the base station in order to find the UE, in what is known as
initial access (IA) procedure. In this paper we start from the observation that
UEs are more likely to enter from some directions than from others, as they
typically move along streets, while other movements are impossible due to the
presence of obstacles. Moreover, users are entering with a given time
statistics, for example described by inter-arrival times. In this context we
propose scanning strategies for IA that take into account the entrance
statistics. In particular, we propose two approaches: a memory-less random
illumination (MLRI) algorithm and a statistic and memory-based illumination
(SMBI) algorithm. The MLRI algorithm scans a random sector in each slot, based
on the statistics of sector entrance, without memory. The SMBI algorithm
instead scans sectors in a deterministic sequence selected according to the
statistics of sector entrance and time of entrance, and taking into account the
fact that the user has not yet been discovered (thus including memory). We
assess the performance of the proposed methods in terms of average discovery
time.



Sound Event Detection in Synthetic Audio: Analysis of the DCASE 2016 Task Results

As part of the 2016 public evaluation challenge on Detection and
Classification of Acoustic Scenes and Events (DCASE 2016), the second task
focused on evaluating sound event detection systems using synthetic mixtures of
office sounds. This task, which follows the `Event Detection - Office
Synthetic' task of DCASE 2013, studies the behaviour of tested algorithms when
facing controlled levels of audio complexity with respect to background noise
and polyphony/density, with the added benefit of a very accurate ground truth.
This paper presents the task formulation, evaluation metrics, submitted
systems, and provides a statistical analysis of the results achieved, with
respect to various aspects of the evaluation dataset.



Pitch and timbre discrimination at wave-to-spike transition in the cochlea

A new definition of musical pitch is proposed. A Finite-Difference Time
Domain (FDTM) model of the cochlea is used to calculate spike trains caused by
tone complexes and by a recorded classical guitar tone. All harmonic tone
complexes, musical notes, show a narrow-band Interspike Interval (ISI) pattern
at the respective fundamental frequency of the tone complex. Still this
fundamental frequency is not only present at the bark band holding the
respective best frequency of this fundamental frequency, but rather at all bark
bands driven by the tone complex partials. This is caused by drop-outs in the
basically regular, periodic spike train in the respective bands. These
drop-outs are caused by the energy distribution in the wave form, where time
spans of low energy are not able to drive spikes. The presence of the
fundamental periodicity in all bark bands can be interpreted as pitch. Contrary
to pitch, timbre is represented as a wide distribution of different ISIs over
bark bands. The definition of pitch is shown to also works with residue
pitches. The spike drop-outs in times of low energy of the wave form also cause
undertones, integer multiple subdivisions in periodicity, but in no case
overtones can appear. This might explain the musical minor scale, which was
proposed to be caused by undertones already in 1880 by Hugo Riemann, still
until now without knowledge about any physical realization of such undertones.



A Stochastic Hybrid Framework for Driver Behavior Modeling Based on Hierarchical Dirichlet Process

Scalability is one of the major issues for real-world Vehicle-to-Vehicle
network realization. To tackle this challenge, a stochastic hybrid modeling
framework based on a non-parametric Bayesian inference method, i.e.,
hierarchical Dirichlet process (HDP), is investigated in this paper. This
framework is able to jointly model driver/vehicle behavior through forecasting
the vehicle dynamical time-series. This modeling framework could be merged with
the notion of model-based information networking, which is recently proposed in
the vehicular literature, to overcome the scalability challenges in dense
vehicular networks via broadcasting the behavioral models instead of raw
information dissemination. This modeling approach has been applied on several
scenarios from the realistic Safety Pilot Model Deployment (SPMD) driving data
set and the results show a higher performance of this model in comparison with
the zero-hold method as the baseline.



Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition

We investigate the effectiveness of generative adversarial networks (GANs)
for speech enhancement, in the context of improving noise robustness of
automatic speech recognition (ASR) systems. Prior work demonstrates that GANs
can effectively suppress additive noise in raw waveform speech signals,
improving perceptual quality metrics; however this technique was not justified
in the context of ASR. In this work, we conduct a detailed study to measure the
effectiveness of GANs in enhancing speech contaminated by both additive and
reverberant noise. Motivated by recent advances in image processing, we propose
operating GANs on log-Mel filterbank spectra instead of waveforms, which
requires less computation and is more robust to reverberant noise. While GAN
enhancement improves the performance of a clean-trained ASR system on noisy
speech, it falls short of the performance achieved by conventional multi-style
training (MTR). By appending the GAN-enhanced features to the noisy inputs and
retraining, we achieve a 7% WER improvement relative to the MTR system.



Crowd Counting Through Walls Using WiFi

Counting the number of people inside a building, from outside and without
entering the building, is crucial for many applications. In this paper, we are
interested in counting the total number of people walking inside a building (or
in general behind walls), using readily-deployable WiFi transceivers that are
installed outside the building, and only based on WiFi RSSI measurements. The
key observation of the paper is that the inter-event times, corresponding to
the dip events of the received signal, are fairly robust to the attenuation
through walls (for instance as compared to the exact dip values). We then
propose a methodology that can extract the total number of people from the
inter-event times. More specifically, we first show how to characterize the
wireless received power measurements as a superposition of renewal-type
processes. By borrowing theories from the renewal-process literature, we then
show how the probability mass function of the inter-event times carries vital
information on the number of people. We validate our framework with 44
experiments in five different areas on our campus (3 classrooms, a conference
room, and a hallway), using only one WiFi transmitter and receiver installed
outside of the building, and for up to and including 20 people. Our experiments
further include areas with different wall materials, such as concrete, plaster,
and wood, to validate the robustness of the proposed approach. Overall, our
results show that our approach can estimate the total number of people behind
the walls with a high accuracy while minimizing the need for prior
calibrations.



Passive Crowd Speed Estimation in Adjacent Regions With Minimal WiFi Sensing

In this paper, we propose a methodology for estimating the crowd speed using
WiFi devices without relying on people to carry any device. Our approach not
only enables speed estimation in the region where WiFi links are, but also in
the adjacent possibly WiFi-free regions. More specifically, we use a pair of
WiFi links in one region, whose RSSI measurements are then used to estimate the
crowd speed, not only in this region, but also in adjacent WiFi-free regions.
We first prove how the cross-correlation and the probability of crossing the
two links implicitly carry key information about the pedestrian speeds and
develop a mathematical model to relate them to pedestrian speeds. We then
validate our approach with 108 experiments, in both indoor and outdoor, where
up to 10 people walk in two adjacent areas, with variety of speeds per region,
showing that our framework can accurately estimate these speeds with only a
pair of WiFi links in one region. For instance, the NMSE over all experiments
is 0.18. We also evaluate our framework in a museum-type setting and estimate
the popularity of different exhibits. We finally run experiments in an aisle in
Costco, estimating key attributes of buyers' behaviors.



Enhanced Array Aperture using Higher Order Statistics for DoA Estimation

Recently, the higher order statistics (HOS) and sparsity based array are most
talked about techniques to estimate the Direction of Arrival (DoA). They not
only provide enhanced Degree of Freedom (DoF) to handle underdetermined cases
but also improve the estimation accuracy of the system. To achieve high
accuracy and more number of DoF with limited number of sensors, here we have
proposed a method based on the fourth order statistics. The aperture of virtual
array becomes O(16N^4) using N physical sensors. Proposed method can be
extended to the HOS which increases the DoF by many folds. Numeric simulation
validates these claims that the proposed method increases the resolution
capacity as well as maximize the DoF among all the earlier proposed method.



Physical Layer Authentication for Mission Critical Machine Type Communication using Gaussian Mixture Model based Clustering

The application of Mission Critical Machine Type Communication (MC-MTC) in
wireless systems is currently a hot research topic. Wireless systems are
considered to provide numerous advantages over wired systems in e.g. industrial
applications such as closed loop control. However, due to the broadcast nature
of the wireless channel, such systems are prone to a wide range of cyber
attacks. These range from passive eavesdropping attacks to active attacks like
data manipulation or masquerade attacks. Therefore it is necessary to provide
reliable and efficient security mechanisms. Some of the most important security
issues in such a system are to ensure integrity as well as authenticity of
exchanged messages over the air between communicating devices. In the present
work, an approach on how to achieve this goal in MC-MTC systems based on
Physical Layer Security (PHYSEC) is presented. A new method that clusters
channel estimates of different transmitters based on a Gaussian Mixture Model
is applied for that purpose. Further, an experimental proof-of-concept
evaluation is given and we compare the performance of our approach with a mean
square error based detection method.



Exploiting Occlusion in Non-Line-of-Sight Active Imaging

Active non-line-of-sight imaging systems are of growing interest for diverse
applications. The most commonly proposed approaches to date rely on exploiting
time-resolved measurements, i.e., measuring the time it takes for short light
pulses to transit the scene. This typically requires expensive, specialized,
ultrafast lasers and detectors that must be carefully calibrated. We develop an
alternative approach that exploits the valuable role that natural occluders in
a scene play in enabling accurate and practical image formation in such
settings without such hardware complexity. In particular, we demonstrate that
the presence of occluders in the hidden scene can obviate the need for
collecting time-resolved measurements, and develop an accompanying analysis for
such systems and their generalizations. Ultimately, the results suggest the
potential to develop increasingly sophisticated future systems that are able to
identify and exploit diverse structural features of the environment to
reconstruct scenes hidden from view.



Speech Dereverberation with Context-aware Recurrent Neural Networks

In this paper, we propose a model to perform speech dereverberation by
estimating its spectral magnitude from the reverberant counterpart. Our models
are capable of extracting features that take into account both short and
long-term dependencies in the signal through a convolutional encoder (which
extracts features from a short, bounded context of frames) and a recurrent
neural network for extracting long-term information. Our model outperforms a
recently proposed model that uses different context information depending on
the reverberation time, without requiring any sort of additional input,
yielding improvements of up to 0.4 on PESQ, 0.3 on STOI, and 1.0 on POLQA
relative to reverberant speech. We also show our model is able to generalize to
real room impulse responses even when only trained with simulated room impulse
responses, different speakers, and high reverberation times. Lastly, listening
tests show the proposed method outperforming benchmark models in reduction of
perceived reverberation.



Simultaneous identification of linear building dynamic model and disturbance using sparsity-promoting optimization

We propose a method that simultaneously identifies a linear time-invariant
model of a building's temperature dynamics and a transformed version of the
unmeasured disturbance affecting the building. Our method uses
l1-regularization to encourage the identified disturbance to be approximately
sparse, which is motivated by the slowly-varying nature of occupancy that
determines the disturbance. The proposed method involves solving a convex
optimization problem that guarantees the identified black-box model possess
known properties of the plant, especially input-output stability and positive
DC gains. These features enable one to use the method as part of a
self-learning control system in which the model of the building is updated
periodically without requiring human intervention. Results from the application
of the method on data from a simulated and real building are provided.



A Double Joint Bayesian Approach for J-Vector Based Text-dependent Speaker Verification

J-vector has been proved to be very effective in text-dependent speaker
verification with short-duration speech. However, the current state-of-the-art
back-end classifiers, e.g. joint Bayesian model, cannot make full use of such
deep features. In this paper, we generalize the standard joint Bayesian
approach to model the multi-faceted information in the j-vector explicitly and
jointly. In our generalization, the j-vector was modeled as a result derived by
a generative Double Joint Bayesian (DoJoBa) model, which contains several kinds
of latent variables. With DoJoBa, we are able to explicitly build a model that
can combine multiple heterogeneous information from the j-vectors. In
verification step, we calculated the likelihood to describe whether the two
j-vectors having consistent labels or not. On the public RSR2015 data corpus,
the experimental results showed that our approach can achieve 0.02\% EER and
0.02\% EER for impostor wrong and impostor correct cases respectively.



Digital Nonlinearity Compensation in High-Capacity Optical Fibre Communication Systems: Performance and Optimisation

Meeting the ever-growing information rate demands has become of utmost
importance for optical communication systems. However, it has proven to be a
challenging task due to the presence of Kerr effects, which have largely been
regarded as a major bottleneck for enhancing the achievable information rates
in modern optical communications. In this work, the optimisation and
performance of digital nonlinearity compensation are discussed for maximising
the achievable information rates in spectrally-efficient optical fibre
communication systems. It is found that, for any given target information rate,
there exists a trade-off between modulation format and compensated bandwidth to
reduce the computational complexity requirement of digital nonlinearity
compensation.



FDD Massive MIMO Channel Estimation with Arbitrary 2D-Array Geometry

This paper addresses the problem of downlink channel estimation in
frequency-division duplexing (FDD) massive multiple-input multiple-output
(MIMO) systems. The existing methods usually exploit hidden sparsity under a
discrete Fourier transform (DFT) basis to estimate the cdownlink channel.
However, there are at least two shortcomings of these DFT-based methods: 1)
they are applicable to uniform linear arrays (ULAs) only, since the DFT basis
requires a special structure of ULAs, and 2) they always suffer from a
performance loss due to the leakage of energy over some DFT bins. To deal with
the above shortcomings, we introduce an off-grid model for downlink channel
sparse representation with arbitrary 2D-array antenna geometry, and propose an
efficient sparse Bayesian learning (SBL) approach for the sparse channel
recovery and off-grid refinement. The main idea of the proposed off-grid method
is to consider the sampled grid points as adjustable parameters. Utilizing an
in-exact block majorization-minimization (MM) algorithm, the grid points are
refined iteratively to minimize the off-grid gap. Finally, we further extend
the solution to uplink-aided channel estimation by exploiting the angular
reciprocity between downlink and uplink channels, which brings enhanced
recovery performance.



Spatial Mode Diversity for Robust Free-Space Optical Communications

Free-space communication links are severely affected by atmospheric
turbulence, which causes degradation in the transmitted signal. One of the most
common solutions to overcome this is to exploit diversity. In this approach,
information is sent in parallel using two or more transmitters that are
spatially separated, with each beam therefore experiencing different
atmospheric turbulence, lowering the probability of a receive error. In this
work we propose and experimentally demonstrate a generalization of diversity
based on spatial modes of light, which we have termed $\textit{modal
diversity}$. We remove the need for a physical separation of the transmitters
by exploiting the fact that spatial modes of light experience different
perturbations, even when travelling along the same path. For this
proof-of-principle we selected modes from the Hermite-Gaussian and
Laguerre-Gaussian basis sets and demonstrate an improvement in Bit Error Rate
by up to 54\%. We outline that modal diversity enables physically compact and
longer distance free space optical links without increasing the total transmit
power.



Reconstruction of the External Stimuli from Brain Signals

Despite the rapid advances in Brain-computer Interfacing (BCI) and continuous
effort to improve the accuracy of brain decoding systems, the urge for the
systems to reconstruct the experiences of the users has been widely
acknowledged. This urge has been investigated by some researchers during the
past years in terms of reconstruction of the naturalistic images, abstract
images, video and audio. In this study, we try to tackle this issue by
regressing the stimuli spectrogram using the spectrogram analysis of the brain
signals. The results of our regression-based method suggest the feasibility of
such reconstructions using the neuroimaging techniques that are appropriate for
out-of-lab scenarios.



High Resolution FDMA MIMO Radar

Traditional multiple input multiple output radars, which transmit orthogonal
coded waveforms, suffer from range-azimuth resolution trade-off. In this work,
we adopt a frequency division multiple access (FDMA) approach that breaks this
conflict. We combine narrow individual bandwidth for high azimuth resolution
and large overall total bandwidth for high range resolution. We process all
channels jointly to overcome the FDMA range resolution limitation to a single
bandwidth, and address range-azimuth coupling using a random array
configuration.



A Joint Combiner and Bit Allocation Design for Massive MIMO Using Genetic Algorithm

In this paper, we derive a closed-form expression for the combiner of a
multiple-input-multiple-output (MIMO) receiver equipped with a
minimum-mean-square-error (MMSE) estimator. We propose using
variable-bit-resolution analog-to- digital converters (ADC) across radio
frequency (RF) paths. The combiner designed is a function of the quantization
errors across each RF path. Using very low bit resolution ADCs (1-2bits) is a
popular approach with massive MIMO receiver architectures to mitigate large
power demands. We show that for certain channel conditions, adopting unequal
bit resolution ADCs (e.g., between 1 and 4 bits) on different RF chains, along
with the proposed combiner, improves the performance of the MIMO receiver in
the Mean Squared Error (MSE) sense. The variable-bit-resolution ADCs is still
within the power constraint of using equal bit resolution ADCs on all paths
(e.g., 2-bits). We propose a genetic algorithm in conjunction with the derived
combiner to arrive at an optimal ADC bit allocation framework with significant
reduction in computational complexity.



Separake: Source Separation with a Little Help From Echoes

It is commonly believed that multipath hurts various audio processing
algorithms. At odds with this belief, we show that multipath in fact helps
sound source separation, even with very simple propagation models. Unlike most
existing methods, we neither ignore the room impulse responses, nor we attempt
to estimate them fully. We rather assume that we know the positions of a few
virtual microphones generated by echoes and we show how this gives us enough
spatial diversity to get a performance boost over the anechoic case. We show
improvements for two standard algorithms---one that uses only magnitudes of the
transfer functions, and one that also uses the phases. Concretely, we show that
multichannel non-negative matrix factorization aided with a small number of
echoes beats the vanilla variant of the same algorithm, and that with magnitude
information only, echoes enable separation where it was previously impossible.



WAKE: Wavelet Decomposition Coupled with Adaptive Kalman Filtering for Pathological Tremor Extraction

Pathological Hand Tremor (PHT) is among common symptoms of several
neurological movement disorders, which can significantly degrade quality of
life of affected individuals. Beside pharmaceutical and surgical therapies,
mechatronic technologies have been utilized to control PHTs. Most of these
technologies function based on estimation, extraction, and characterization of
tremor movement signals. Real-time extraction of tremor signal is of paramount
importance because of its application in assistive and rehabilitative devices.
In this paper, we propose a novel on-line adaptive method which can adjust the
hyper-parameters of the filter to the variable characteristics of the tremor.
The proposed "WAKE: Wavelet decomposition coupled with Adaptive Kalman
filtering technique for pathological tremor Extraction, referred to as the WAKE
framework" is composed of a new adaptive Kalman filter and a wavelet transform
core to provide indirect prediction of the tremor, one sample ahead of time, to
be used for its suppression. In this paper, the design, implementation and
evaluation of WAKE are given. The performance is evaluated based on three
different datasets, the first one is a synthetic dataset, developed in this
work, that simulates hand tremor under ten different conditions. The second and
third ones are real datasets recorded from patients with PHTs. The results
obtained from the proposed WAKE framework demonstrate significant improvements
in the estimation accuracy in comparison with two well regarded techniques in
the literature.



A Figurative Identification for Superposed OAM Modes in FSO Systems

We demonstrate that a complete projection in Hilbert Space figuratively
describes a superposed state, introducing a new scale to qualify an FSO system.
Measurement simulation of superposed OAM beam through this projection scheme is
given.



Random Access in Massive MIMO by Exploiting Timing Offsets and Excess Antennas

Massive MIMO systems, where base stations are equipped with hundreds of
antennas, are an attractive way to handle the rapid growth of data traffic. As
the number of user equipments (UEs) increases, the initial access and handover
in contemporary networks will be flooded by user collisions. In this paper, a
random access protocol is proposed that resolves collisions and performs timing
estimation by simply utilizing the large number of antennas envisioned in
Massive MIMO networks. UEs entering the network perform spreading in both time
and frequency domains, and their timing offsets are estimated at the base
station in closed-form using a subspace decomposition approach. This
information is used to compute channel estimates that are subsequently employed
by the base station to communicate with the detected UEs. The favorable
propagation conditions of Massive MIMO suppress interference among UEs whereas
the inherent timing misalignments improve the detection capabilities of the
protocol. Numerical results are used to validate the performance of the
proposed procedure in cellular networks under uncorrelated and correlated
fading channels. With $2.5\times10^3$ UEs that may simultaneously become active
with probability 1\% and a total of $16$ frequency-time codes (in a given
random access block), it turns out that, with $100$ antennas, the proposed
procedure successfully detects a given UE with probability 75\% while providing
reliable timing estimates.



Piecewise Stationary Modeling of Random Processes Over Graphs With an Application to Traffic Prediction

Stationarity is a key assumption in many statistical models for random
processes. With recent developments in the field of graph signal processing,
the conventional notion of wide-sense stationarity has been extended to random
processes defined on the vertices of graphs. It has been shown that well-known
spectral graph kernel methods assume that the underlying random process over a
graph is stationary. While many approaches have been proposed, both in machine
learning and signal processing literature, to model stationary random processes
over graphs, they are too restrictive to characterize real-world datasets as
most of them are non-stationary processes. In this paper, to well-characterize
a non-stationary process over graph, we propose a novel model and a
computationally efficient algorithm that partitions a large graph into disjoint
clusters such that the process is stationary on each of the clusters but
independent across clusters. We evaluate our model for traffic prediction on a
large-scale dataset of fine-grained highway travel times in the Dallas--Fort
Worth area. The accuracy of our method is very close to the state-of-the-art
graph based deep learning methods while the computational complexity of our
model is substantially smaller.



Wrist Sensor Fusion Enables Robust Gait Quantification Across Walking Scenarios

Quantifying step abundance via single wrist-worn accelerometers is a common
approach for encouraging active lifestyle and tracking disease status.
Nonetheless, step counting accuracy can be hampered by fluctuations in walking
pace or demeanor. Here, we assess whether the use of various sensor fusion
techniques, each combining bilateral wrist accelerometer data, may increase
step count robustness. By collecting data from 27 healthy subjects, we find
that high-level step fusion leads to substantially improved accuracy across
diverse walking scenarios. Gait cycle analysis illustrates that wrist devices
can recurrently detect steps proximal to toe-off events. Collectively, our
study suggests that dual-wrist sensor fusion may enable robust gait
quantification in free-living environments.



Hello Edge: Keyword Spotting on Microcontrollers

Keyword spotting (KWS) is a critical component for enabling speech based user
interactions on smart devices. It requires real-time response and high accuracy
for good user experience. Recently, neural networks have become an attractive
choice for KWS architecture because of their superior accuracy compared to
traditional speech processing algorithms. Due to its always-on nature, KWS
application has highly constrained power budget and typically runs on tiny
microcontrollers with limited memory and compute capability. The design of
neural network architecture for KWS must consider these constraints. In this
work, we perform neural network architecture evaluation and exploration for
running KWS on resource-constrained microcontrollers. We train various neural
network architectures for keyword spotting published in literature to compare
their accuracy and memory/compute requirements. We show that it is possible to
optimize these neural network architectures to fit within the memory and
compute constraints of microcontrollers without sacrificing accuracy. We
further explore the depthwise separable convolutional neural network (DS-CNN)
and compare it against other neural network architectures. DS-CNN achieves an
accuracy of 95.4%, which is ~10% higher than the DNN model with similar number
of parameters.



MEC-aware Cell Association for 5G Heterogeneous Networks

The need for efficient use of network resources is continuously increasing
with the grow of traffic demand, however, current mobile systems have been
planned and deployed so far with the mere aim of enhancing radio coverage and
capacity. Unfortunately, this approach is not sustainable anymore, as 5G
communication systems will have to cope with huge amounts of traffic,
heterogeneous in terms of latency among other Qualityof- Service (QoS)
requirements. Moreover, the advent of Multiaccess Edge Computing (MEC) brings
up the need to more efficiently plan and dimension network deployment by means
of jointly exploiting the available radio and processing resources. From this
standpoint, advanced cell association of users can play a key role for 5G
systems. Focusing on a Heterogeneous Network (HetNet), this paper proposes a
comparison between state-of-the-art (i.e., radio-only) and MEC-aware cell
association rules, taking the scenario of task offloading in the Uplink (UL) as
an example. Numerical evaluations show that the proposed cell association rule
provides nearly 60% latency reduction, as compared to its standard,
radio-exclusive counterpart.



A Full Duplex Transceiver with Reduced Hardware Complexity

For future wireless communication systems, full duplex is seen as a possible
solution to the ever present spectrum shortage. The key aspect to enable
In-Band Full Duplex (IBFD) is sufficient cancellation of the unavoidable
Self-Interference (SI). In this work we evaluate the performance of a low
complexity IBFD transceiver, including the required analog and digital
interference cancellation techniques. The Radio Frequency Self- Interference
Canceler (RFSIC) is based on the isolation of a circulator in combination with
a vector modulator regenerating the interference signal, to destructively
combine it with the received signal. On the digital side, a Digital
Self-Interference Cancellation (DSIC) algorithm based on non-linear adaptive
filtering is used. With the simplified analog front-end of a Software Defined
Radio (SDR) platform, SI cancellation of 90 dB is achieved with the presence of
a received signal.



Speech recognition for medical conversations

In this work we explored building automatic speech recognition models for
transcribing doctor patient conversation. We collected a large scale dataset of
clinical conversations ($14,000$ hr), designed the task to represent the real
word scenario, and explored several alignment approaches to iteratively improve
data quality. We explored both CTC and LAS systems for building speech
recognition models. The LAS was more resilient to noisy data and CTC required
more data clean up. A detailed analysis is provided for understanding the
performance for clinical tasks. Our analysis showed the speech recognition
models performed well on important medical utterances, while errors occurred in
causal conversations. Overall we believe the resulting models can provide
reasonable quality in practice.



Design of Sampling Set for Bandlimited Graph Signal Estimation

It is of particular interest to reconstruct or estimate bandlimited graph
signals, which are smoothly varying signals defined over graphs, from partial
noisy measurements. However, choosing an optimal subset of nodes to sample is
NP-hard. We formularize the problem as the experimental design of a linear
regression model if we allow multiple measurements on a single node. By
relaxing it to a convex optimization problem, we get the proportion of sample
for each node given the budget of total sample size. Then, we use a
probabilistic quantization to get the number of each node to be sampled.
Moreover, we analyze how the sample size influences whether our object function
is well-defined by perturbation analysis. Finally, we demonstrate the
performance of the proposed approach through various numerical experiments.



On the Signal Processing Operations in LIGO signals

This article analyzes the data for the five gravitational wave (GW) events
detected in Hanford(H1), Livingston(L1) and Virgo(V1) detectors by the LIGO
collaboration. It is shown that GW170814, GW170817, GW151226 and GW170104 are
very weak signals whose amplitude does not rise significantly during the GW
event, and they are indistinguishable from non-stationary detector noise. LIGO
software implements cross-correlation funcion(CCF) of H1/L1 signals with the
template reference signal, in frequency domain, in a matched filter, using 32
second windows. It is shown that this matched filter misfires with high SNR/CCF
peaks, even for very low-amplitude, short bursts of sine wave signals and
additive white gaussian noise(AWGN), all the time. It is shown that this
erratic behaviour of the matched filter, is due to the error in signal
processing operations, such as lack of cyclic prefix necessary to account for
circular convolution. It is also shown that normalized CCF method implemented
in time domain using short windows, does not have false CCF peaks for sine wave
and noise bursts. It is shown that the normalized CCF for GW151226 and
GW170104, when correlating H1/L1 and template, is indistinguishable from
correlating detector noise and the template. It is also shown that the
normalized CCF for GW151226 and GW170104, when correlating H1/L1 and template,
is indistinguishable from correlating H1/L1 and bogus chirp templates which are
frequency modulated(FM) waveforms which differ significantly from ideal
templates. Similar results are shown with LIGO matched filter, which misfires
with high Signal to Noise Ratio(SNR) for bogus chirp templates.



Identification of potential Music Information Retrieval technologies for computer-aided jingju singing training

Music Information Retrieval (MIR) technologies have been proven useful in
assisting western classical singing training. Jingju (also known as Beijing or
Peking opera) singing is different from western singing in terms of most of the
perceptual dimensions, and the trainees are taught by using mouth/heart method.
In this paper, we first present the training method used in the professional
jingju training classroom scenario and show the potential benefits of
introducing the MIR technologies into the training process. The main part of
this paper dedicates to identify the potential MIR technologies for jingju
singing training. To this intent, we answer the question: how the jingju
singing tutors and trainees value the importance of each jingju musical
dimension-intonation, rhythm, loudness, tone quality and pronunciation? This is
done by (i) classifying the classroom singing practices, tutor's verbal
feedbacks into these 5 dimensions, (ii) surveying the trainees. Then, with the
help of the music signal analysis, a finer inspection on the classroom practice
recording examples reveals the detailed elements in the training process.
Finally, based on the above analysis, several potential MIR technologies are
identified and would be useful for the jingju singing training.



A unified algorithm framework for quality control of sensor data for behavioural clinimetric testing

The use of smartphone and wearable sensing technology for objective,
non-invasive and remote clinimetric testing of symptoms has considerable
potential. However, the clinimetric accuracy achievable with such technology is
highly reliant on separating the useful from irrelevant or confounded sensor
data. Monitoring patient symptoms using digital sensors outside of controlled,
clinical lab settings creates a variety of practical challenges, such as
unavoidable and unexpected user behaviours. These behaviours often violate the
assumptions of clinimetric testing protocols, where these protocols are
designed to probe for specific symptoms. Such violations are frequent outside
the lab, and can affect the accuracy of the subsequent data analysis and
scientific conclusions. At the same time, curating sensor data by hand after
the collection process is inherently subjective, laborious and error-prone. To
address these problems, we report on a unified algorithmic framework for
automated sensor data quality control, which can identify those parts of the
sensor data which are sufficiently reliable for further analysis. Algorithms
which are special cases of this framework for different sensor data types (e.g.
accelerometer, digital audio) detect the extent to which the sensor data
adheres to the assumptions of the test protocol for a variety of clinimetric
tests. The approach is general enough to be applied to a large set of
clinimetric tests and we demonstrate its performance on walking, balance and
voice smartphone-based tests, designed to monitor the symptoms of Parkinson's
disease.



The Lower Bound Error for polynomial NARMAX using an Arbitrary Number of Natural Interval Extensions

The polynomial NARMAX (Nonlinear AutoRegressive Moving Average model with
eXogenous input) is a model that represents the dynamics of physical systems.
This polynomial contains information from the past of the inputs and outputs of
the process, that is, it is a recursive model. In digital computers this
generates the propagation of the rounding error. Our procedure is based on the
estimation of the maximum value of the lower bound error considering an
arbitrary number of pseudo-orbits produced from different natural interval
extensions, and a posterior Lyapunov exponent calculation. We applied
successfully our technique for two identified models of the systems: sine map
and Duffing-Ueda oscillator



Massive MIMO for Drone Communications: Case Studies and Future Directions

Unmanned aerial vehicles (UAVs), also known as drones, are proliferating.
Applications, such as surveillance, disaster management, and drone racing,
place high requirements on the communication with the drones in terms of
throughput, reliability, and latency. The existing wireless technologies,
notably Wi-Fi, that are currently used for drone connectivity are limited to
short ranges and low-mobility situations. New, scalable technology is needed to
meet future demands on long connectivity ranges, support for fast-moving
drones, and the possibility to simultaneously communicate with entire swarms of
drones. Massive multiple-input and multiple-output (MIMO), the main technology
component of emerging 5G standards, has the potential to meet these
requirements.



JamBot: Music Theory Aware Chord Based Generation of Polyphonic Music with LSTMs

We propose a novel approach for the generation of polyphonic music based on
LSTMs. We generate music in two steps. First, a chord LSTM predicts a chord
progression based on a chord embedding. A second LSTM then generates polyphonic
music from the predicted chord progression. The generated music sounds pleasing
and harmonic, with only few dissonant notes. It has clear long-term structure
that is similar to what a musician would play during a jam session. We show
that our approach is sensible from a music theory perspective by evaluating the
learned chord embeddings. Surprisingly, our simple model managed to extract the
circle of fifths, an important tool in music theory, from the dataset.



Incorporating Numerical Uncertainties for Validation of Nonlinear Models

This paper proposes a model validation method that incorporates error due to
numerical procedures. Two identified models for Sine Map and Duffing-Ueda
Circuit systems have been investigated. The indexes RMSE and MAPE have been
applied. We have shown that after some few iterates, it is possible to notice
some significative difference between index provided in the literature. This
difference has been computed in around 34%.



Reflection-Aware Sound Source Localization

We present a novel, reflection-aware method for 3D sound localization in
indoor environments. Unlike prior approaches, which are mainly based on
continuous sound signals from a stationary source, our formulation is designed
to localize the position instantaneously from signals within a single frame. We
consider direct sound and indirect sound signals that reach the microphones
after reflecting off surfaces such as ceilings or walls. We then generate and
trace direct and reflected acoustic paths using inverse acoustic ray tracing
and utilize these paths with Monte Carlo localization to estimate a 3D sound
source position. We have implemented our method on a robot with a cube-shaped
microphone array and tested it against different settings with continuous and
intermittent sound signals with a stationary or a mobile source. Across
different settings, our approach can localize the sound with an average
distance error of 0.8m tested in a room of 7m by 7m area with 3m height,
including a mobile and non-line-of-sight sound source. We also reveal that the
modeling of indirect rays increases the localization accuracy by 40% compared
to only using direct acoustic rays.



Efficient and fast algorithms to generate holograms for optical tweezers

We discuss and compare three algorithms for generating holograms: simple
rounding, Floyd-Steinberg error diffusion dithering, and mixed region amplitude
freedom (MRAF). The methods are optimised for producing large arrays of tightly
focused optical tweezers for trapping particles. The algorithms are compared in
terms of their speed, efficiency, and accuracy, for periodic arrangements of
traps; an arrangement of particular interest in the field of quantum computing.
We simulate the image formation using each of a binary amplitude modulating
digital mirror device (DMD) and a phase modulating spatial light modulator
(PSLM) as the display element. While a DMD allows for fast frame rates, the
slower PSLM is more efficient and provides higher accuracy with a
quasi-continuous variation of phase. We discuss the relative merits of each
algorithm for use with both a DMD and a PSLM, allowing one to choose the ideal
approach depending on the circumstances.



Multichannel Speech Separation and Enhancement Using the Convolutive Transfer Function

This paper addresses the problem of speech separation and enhancement from
multichannel convolutive and noisy mixtures, \emph{assuming known mixing
filters}. We propose to perform the speech separation and enhancement task in
the short-time Fourier transform domain, using the convolutive transfer
function (CTF) approximation. Compared to time-domain filters, CTF has much
less taps, consequently it has less near-common zeros among channels and less
computational complexity. The work proposes three speech-source recovery
methods, namely: i) the multichannel inverse filtering method, i.e. the
multiple input/output inverse theorem (MINT), is exploited in the CTF domain,
and for the multi-source case, ii) a beamforming-like multichannel inverse
filtering method applying single source MINT and using power minimization,
which is suitable whenever the source CTFs are not all known, and iii) a
constrained Lasso method, where the sources are recovered by minimizing the
$\ell_1$-norm to impose their spectral sparsity, with the constraint that the
$\ell_2$-norm fitting cost, between the microphone signals and the mixing model
involving the unknown source signals, is less than a tolerance. The noise can
be reduced by setting a tolerance onto the noise power. Experiments under
various acoustic conditions are carried out to evaluate the three proposed
methods. The comparison between them as well as with the baseline methods is
presented.



Kullback-Leibler Principal Component for Tensors is not NP-hard

We study the problem of nonnegative rank-one approximation of a nonnegative
tensor, and show that the globally optimal solution that minimizes the
generalized Kullback-Leibler divergence can be efficiently obtained, i.e., it
is not NP-hard. This result works for arbitrary nonnegative tensors with an
arbitrary number of modes (including two, i.e., matrices). We derive a
closed-form expression for the KL principal component, which is easy to compute
and has an intuitive probabilistic interpretation. For generalized KL
approximation with higher ranks, the problem is for the first time shown to be
equivalent to multinomial latent variable modeling, and an iterative algorithm
is derived that resembles the expectation-maximization algorithm. On the Iris
dataset, we showcase how the derived results help us learn the model in an
\emph{unsupervised} manner, and obtain strikingly close performance to that
from supervised methods.



Unsupervised Adaptation with Domain Separation Networks for Robust Speech Recognition

Unsupervised domain adaptation of speech signal aims at adapting a
well-trained source-domain acoustic model to the unlabeled data from target
domain. This can be achieved by adversarial training of deep neural network
(DNN) acoustic models to learn an intermediate deep representation that is both
senone-discriminative and domain-invariant. Specifically, the DNN is trained to
jointly optimize the primary task of senone classification and the secondary
task of domain classification with adversarial objective functions. In this
work, instead of only focusing on learning a domain-invariant feature (i.e. the
shared component between domains), we also characterize the difference between
the source and target domain distributions by explicitly modeling the private
component of each domain through a private component extractor DNN. The private
component is trained to be orthogonal with the shared component and thus
implicitly increases the degree of domain-invariance of the shared component. A
reconstructor DNN is used to reconstruct the original speech feature from the
private and shared components as a regularization. This domain separation
framework is applied to the unsupervised environment adaptation task and
achieved 11.08% relative WER reduction from the gradient reversal layer
training, a representative adversarial training method, for automatic speech
recognition on CHiME-3 dataset.



Deep Long Short-Term Memory Adaptive Beamforming Networks For Multichannel Robust Speech Recognition

Far-field speech recognition in noisy and reverberant conditions remains a
challenging problem despite recent deep learning breakthroughs. This problem is
commonly addressed by acquiring a speech signal from multiple microphones and
performing beamforming over them. In this paper, we propose to use a recurrent
neural network with long short-term memory (LSTM) architecture to adaptively
estimate real-time beamforming filter coefficients to cope with non-stationary
environmental noise and dynamic nature of source and microphones positions
which results in a set of timevarying room impulse responses. The LSTM adaptive
beamformer is jointly trained with a deep LSTM acoustic model to predict senone
labels. Further, we use hidden units in the deep LSTM acoustic model to assist
in predicting the beamforming filter coefficients. The proposed system achieves
7.97% absolute gain over baseline systems with no beamforming on CHiME-3 real
evaluation set.



Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio

We propose using cascaded classifiers for a keyword spotting (KWS) task on
narrow-band (NB), 8kHz audio acquired in non-IID environments --- a more
challenging task than most state-of-the-art KWS systems face. We present a
model that incorporates Deep Neural Networks (DNNs), cascading,
multiple-feature representations, and multiple-instance learning. The cascaded
classifiers handle the task's class imbalance and reduce power consumption on
computationally-constrained devices via early termination. The KWS system
achieves a false negative rate of 6% at an hourly false positive rate of 0.75



Turbo EP-based Equalization: a Filter-Type Implementation

This manuscript has been submitted to Transactions on Communications on
September 7, 2017; revised on January 10, 2018 and March 27, 2018; and accepted
on April 25, 2018
  We propose a novel filter-type equalizer to improve the solution of the
linear minimum-mean squared-error (LMMSE) turbo equalizer, with computational
complexity constrained to be quadratic in the filter length. When high-order
modulations and/or large memory channels are used the optimal BCJR equalizer is
unavailable, due to its computational complexity. In this scenario, the
filter-type LMMSE turbo equalization exhibits a good performance compared to
other approximations. In this paper, we show that this solution can be
significantly improved by using expectation propagation (EP) in the estimation
of the a posteriori probabilities. First, it yields a more accurate estimation
of the extrinsic distribution to be sent to the channel decoder. Second,
compared to other solutions based on EP the computational complexity of the
proposed solution is constrained to be quadratic in the length of the finite
impulse response (FIR). In addition, we review previous EP-based turbo
equalization implementations. Instead of considering default uniform priors we
exploit the outputs of the decoder. Some simulation results are included to
show that this new EP-based filter remarkably outperforms the turbo approach of
previous versions of the EP algorithm and also improves the LMMSE solution,
with and without turbo equalization.



Optimal Power Control in Decentralized Gaussian Multiple Access Channels

We consider the decentralized power optimization problem for Gaussian
fast-fading multiple access channel (MAC) so that the average sum-throughput is
maximized. In our MAC setup, each transmitter has access to only its own fading
coefficient or channel state information (CSI) while the receiver has full CSI
available at all instants. Unlike centralized MAC (full CSIT MAC) where the
optimal powers are known explicitly, the analytical solution for optimal
decentralized powers does not seem feasible. In this letter, we specialize
alternating-maximization (AM) method for numerically computing the optimal
powers and ergodic capacity of the decentralized MAC for general fading
statistics and average power constraints. For illustration, we apply our AM
method to compute the capacity of MAC channels with fading distributions such
as Rayleigh, Rician etc.



Riemannian tangent space mapping and elastic net regularization for cost-effective EEG markers of brain atrophy in Alzheimer's disease

The diagnosis of Alzheimer's disease (AD) in routine clinical practice is
most commonly based on subjective clinical interpretations. Quantitative
electroencephalography (QEEG) measures have been shown to reflect
neurodegenerative processes in AD and might qualify as affordable and thereby
widely available markers to facilitate the objectivization of AD assessment.
Here, we present a novel framework combining Riemannian tangent space mapping
and elastic net regression for the development of brain atrophy markers. While
most AD QEEG studies are based on small sample sizes and psychological test
scores as outcome measures, here we train and test our models using data of one
of the largest prospective EEG AD trials ever conducted, including MRI
biomarkers of brain atrophy.



Traffic Allocation for Low-Latency Multi-Hop Networks with Buffers

For millimeter-wave (mm-wave) buffer-aided tandem networks consisting of
relay nodes and multiple channels per hop, we consider two traffic allocation
schemes, namely local allocation and global allocation, and investigate the
end-to-end latency of a file transfer. We formulate the problem for generic
multi-hop queuing systems and subsequently derive closed-form expressions of
the end-to-end latency. We quantify the advantages of the global allocation
scheme relative to its local allocation counterpart, and we conduct an
asymptotic analysis on the performance gain when the number of channels in each
hop increases to infinity. The traffic allocations and the analytical delay
performance are validated through simulations. Furthermore, taking a specific
two-hop mm-wave network as an example, we derive lower bounds on the average
end-to-end latency, where Nakagami-$m$ fading is considered. Numerical results
demonstrate that, compared to the local allocation scheme, the advantage of
global allocation grows as the number of relay nodes increases, at the expense
of higher complexity that linearly increases with the number of relay nodes. It
is also demonstrated that a proper deployment of relay nodes in a linear
mm-wave network plays an important role in reducing the average end-to-end
latency, and the average latency decays as the mm-wave channels become more
deterministic. These findings provide insights for designing multi-hop mm-wave
networks with low end-to-end latency.



Data-Driven Nonparametric Existence and Association Problems

We investigate two closely related nonparametric hypothesis testing problems.
In the first problem (i.e., the existence problem), we test whether a testing
data stream is generated by one of a set of composite distributions. In the
second problem (i.e., the association problem), we test which one of the
multiple distributions generates a testing data stream. We assume that some
distributions in the set are unknown with only training sequences generated by
the corresponding distributions are available. For both problems, we construct
the generalized likelihood (GL) tests, and characterize the error exponents of
the maximum error probabilities. For the existence problem, we show that the
error exponent is mainly captured by the Chernoff information between the set
of composite distributions and alternative distributions. For the association
problem, we show that the error exponent is captured by the minimum Chernoff
information between each pair of distributions as well as the KL divergences
between the approximated distributions (via training sequences) and the true
distributions. We also show that the ratio between the lengths of training and
testing sequences plays an important role in determining the error decay rate.



Detection Theory for Union of Subspaces

The focus of this paper is on detection theory for union of subspaces (UoS).
To this end, generalized likelihood ratio tests (GLRTs) are presented for
detection of signals conforming to the UoS model and detection of the
corresponding "active" subspace. One of the main contributions of this paper is
bounds on the performances of these GLRTs in terms of geometry of subspaces
under various assumptions on the observation noise. The insights obtained
through geometrical interpretation of the GLRTs are also validated through
extensive numerical experiments on both synthetic and real-world data.



Message-Passing Receiver Design for Joint Channel Estimation and Data Decoding in Uplink Grant-Free SCMA Systems

The conventional grant-based network relies on the handshaking between base
station and active users to achieve dynamic multi-user scheduling, which may
cost large signaling overheads as well as system latency. To address those
problems, the grant-free receiver design is considered in this paper based on
sparse code multiple access (SCMA), one of the promising air interface
technologies for 5G wireless networks. With the presence of unknown multipath
fading, the proposed receiver blindly performs joint channel estimation and
data decoding without knowing the user activity in the network. Based on the
framework of belief propagation (BP), we formulate a messagepassing receiver
for uplink SCMA that performs joint estimation iteratively. However, the direct
application of BP for the multivariable detection problem is complex. Motivated
by the idea of approximate inference, we use expectation propagation to project
the intractable distributions into Gaussian families such that a linear
complexity decoder is obtained.Simulation results show that the proposed
receiver can detect active users in the network with a high accuracy and can
achieve an improved bit-error-rate performance compared with existing methods.



Singing voice correction using canonical time warping

Expressive singing voice correction is an appealing but challenging problem.
A robust time-warping algorithm which synchronizes two singing recordings can
provide a promising solution. We thereby propose to address the problem by
canonical time warping (CTW) which aligns amateur singing recordings to
professional ones. A new pitch contour is generated given the alignment
information, and a pitch-corrected singing is synthesized back through the
vocoder. The objective evaluation shows that CTW is robust against
pitch-shifting and time-stretching effects, and the subjective test
demonstrates that CTW prevails the other methods including DTW and the
commercial auto-tuning software. Finally, we demonstrate the applicability of
the proposed method in a practical, real-world scenario.



Bias-Compensated Normalized Maximum Correntropy Criterion Algorithm for System Identification with Noisy Input

This paper proposed a bias-compensated normalized maximum correntropy
criterion (BCNMCC) algorithm charactered by its low steady-state misalignment
for system identification with noisy input in an impulsive output noise
environment. The normalized maximum correntropy criterion (NMCC) is derived
from a correntropy based cost function, which is rather robust with respect to
impulsive noises. To deal with the noisy input, we introduce a bias-compensated
vector (BCV) to the NMCC algorithm, and then an unbiasedness criterion and some
reasonable assumptions are used to compute the BCV. Taking advantage of the
BCV, the bias caused by the input noise can be effectively suppressed. System
identification simulation results demonstrate that the proposed BCNMCC
algorithm can outperform other related algorithms with noisy input especially
in an impulsive output noise environment.



Visual Speech Enhancement

When video is shot in noisy environment, the voice of a speaker seen in the
video can be enhanced using the visible mouth movements, reducing background
noise. While most existing methods use audio-only inputs, improved performance
is obtained with our visual speech enhancement, based on an audio-visual neural
network. We include in the training data videos to which we added the voice of
the target speaker as background noise. Since the audio input is not sufficient
to separate the voice of a speaker from his own voice, the trained model better
exploits the visual input and generalizes well to different noise types. The
proposed model outperforms prior audio visual methods on two public lipreading
datasets. It is also the first to be demonstrated on a dataset not designed for
lipreading, such as the weekly addresses of Barack Obama.



Planar additive bases for rectangles

We study a generalization of additive bases into a planar setting. A planar
additive basis is a set of non-negative integer pairs whose vector sumset
covers a given rectangle. Such bases find applications in active sensor arrays
used in, for example, radar and medical imaging. The problem of minimizing the
basis cardinality has not been addressed before.
  We propose two algorithms for finding the minimal bases of small rectangles:
one in the setting where the basis elements can be anywhere in the rectangle,
and another in the restricted setting, where the elements are confined to the
lower left quadrant. We present numerical results from such searches, including
the minimal cardinalities for all rectangles up to $[0,11]\times[0,11]$, and up
to $[0,46]\times[0,46]$ in the restricted setting. We also prove asymptotic
upper and lower bounds on the minimal basis cardinality for large rectangles.



A Dictionary Approach to Identifying Transient RFI

As radio telescopes become more sensitive, the damaging effects of radio
frequency interference (RFI) become more apparent. Near radio telescope arrays,
RFI sources are often easily removed or replaced; the challenge lies in
identifying them. Transient (impulsive) RFI is particularly difficult to
identify. We propose a novel dictionary-based approach to transient RFI
identification. RFI events are treated as sequences of sub-events, drawn from
particular labelled classes. We demonstrate an automated method of extracting
and labelling sub-events using a dataset of transient RFI. A dictionary of
labels may be used in conjunction with hidden Markov models to identify the
sources of RFI events reliably. We attain improved classification accuracy over
traditional approaches such as SVMs or a na\"ive kNN classifier. Finally, we
investigate why transient RFI is difficult to classify. We show that cluster
separation in the principal components domain is influenced by the mains supply
phase for certain sources.



Spectrum Efficient MIMO-FBMC System using Filter Output Truncation

Due to the use of an appropriately designed pulse shaping prototype filter,
filter bank multicarrier (FBMC) system can achieve low out of band (OoB)
emissions and is also robust to the channel and synchronization errors.
However, it comes at a cost of long filter tails which may reduce the spectral
efficiency significantly when the block size is small. Filter output truncation
(FOT) can reduce the overhead by discarding the filter tails but may also
significantly destroy the orthogonality of FBMC system, by introducing inter
carrier interference (ICI) and inter symbol interference (ISI) terms in the
received signal. As a result, the signal to interference ratio (SIR) is
degraded. In addition, the presence of intrinsic interference terms in FBMC
also proves to be an obstacle in combining multiple input multiple output
(MIMO) with FBMC. In this paper, we present a theoretical analysis on the
effect of FOT in an MIMO-FBMC system. First, we derive the matrix model of
MIMO-FBMC system which is subsequently used to analyze the impact of finite
filter length and FOT on the system performance. The analysis reveals that FOT
can avoid the overhead in time domain but also introduces extra interference in
the received symbols. To combat the interference terms, we then propose a
compensation algorithm that considers odd and even overlapping factors as two
separate cases, where the signals are interfered by the truncation in different
ways. The general form of the compensation algorithm can compensate all the
symbols in a MIMO-FBMC block and can improve the SIR values of each symbol for
better detection at the receiver. It is also shown that the proposed algorithm
requires no overhead and can still achieve a comparable BER performance to the
case with no filter truncation.



Complex-Valued Symbol Transmissions in Filter Bank Multicarrier Systems using Filter Deconvolution

Transmission of complex-valued symbols using filter bank multicarrier systems
has been an issue due to the self-interference between the transmitted symbols
both in the time and frequency domain (so-called intrinsic interference). In
this paper, we propose a novel low-complexity interference-free filter bank
multicarrier system with QAM modulation (FBMC/QAM) using filter deconvolution.
The proposed method is based on inversion of the prototype filters which
completely removes the intrinsic interference at the receiver and allows the
use of complex-valued signaling. The interference terms in FBMC/QAM with and
without the proposed system are analyzed and compared in terms of mean square
error (MSE). It is shown with theoretical and simulation results that the
proposed method cancels the intrinsic interference and improves the output
signal to interference plus noise ratio (SINR) at the expense of slight
enhancement of residual interferences caused by multipath channel. The
complexity of the proposed system is also analyzed along with performance
evaluation in an asynchronous multiservice scenario. It is shown that the
proposed FBMC/QAM system with filter deconvolution outperforms the conventional
OFDM system.



Sparse and Low-Rank Matrix Decomposition for Automatic Target Detection in Hyperspectral Imagery

Given a target prior information, our goal is to propose a method for
automatically separating targets of interests from the background in
hyperspectral imagery. More precisely, we regard the given hyperspectral image
(HSI) as being made up of the sum of low-rank background HSI and a sparse
target HSI that contains the targets based on a pre-learned target dictionary
constructed from some online spectral libraries. Based on the proposed method,
two strategies are briefly outlined and evaluated to realize the target
detection on both synthetic and real experiments.



Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music Retrieval

Little research focuses on cross-modal correlation learning where temporal
structures of different data modalities such as audio and lyrics are taken into
account. Stemming from the characteristic of temporal structures of music in
nature, we are motivated to learn the deep sequential correlation between audio
and lyrics. In this work, we propose a deep cross-modal correlation learning
architecture involving two-branch deep neural networks for audio modality and
text modality (lyrics). Different modality data are converted to the same
canonical space where inter modal canonical correlation analysis is utilized as
an objective function to calculate the similarity of temporal structures. This
is the first study on understanding the correlation between language and music
audio through deep architectures for learning the paired temporal correlation
of audio and lyrics. Pre-trained Doc2vec model followed by fully-connected
layers (fully-connected deep neural network) is used to represent lyrics. Two
significant contributions are made in the audio branch, as follows: i)
pre-trained CNN followed by fully-connected layers is investigated for
representing music audio. ii) We further suggest an end-to-end architecture
that simultaneously trains convolutional layers and fully-connected layers to
better learn temporal structures of music audio. Particularly, our end-to-end
deep architecture contains two properties: simultaneously implementing feature
learning and cross-modal correlation learning, and learning joint
representation by considering temporal structures. Experimental results, using
audio to retrieve lyrics or using lyrics to retrieve audio, verify the
effectiveness of the proposed deep correlation learning architectures in
cross-modal music retrieval.



Map-based Millimeter-Wave Channel Models: An Overview, Hybrid Modeling, Data, and Learning

Compared to the current wireless communication systems, millimeter wave
(mm-Wave) promises a wide range of spectrum. As viable alternatives to existing
mm-Wave channel models, various map-based channel models with different
modeling methods have been widely discussed. Map-based channel models are based
on a ray-tracing algorithm and include realistic channel parameters in a given
map. Such parameters enable researchers to accurately evaluate novel
technologies in the mm-Wave range. Diverse map-based modeling methods result in
different modeling objectives, including the characteristics of channel
parameters and different complexities of the modeling procedure. This article
outlines an overview of map-based mm-Wave channel models and proposes a concept
of how they can be utilized to integrate a hardware testbed/sounder with a
software testbed/sounder. In addition, we categorize map-based channel
parameters and provide guidelines for hybrid modeling. Next, we share the
measurement data and the map-based channel parameters with the public. Lastly,
we evaluate a machine learning-based beam selection algorithm through the
shared database. We expect that the offered guidelines and the shared database
will enable researchers to readily design a map-based channel model.



The Expected Achievable Distortion of Two-User Decentralized Interference Channels

This paper concerns the transmission of two independent Gaussian sources over
a two-user decentralized interference channel, assuming that the transmitters
are unaware of the instantaneous CSIs. The availability of the channel state
information at receivers (CSIR) is considered in two scenarios of perfect and
imperfect CSIR. In the imperfect CSIR case, we consider a more practical
assumption of having an MMSE estimation of the channel gain at the receivers.
In this case, minimizing the expected achievable distortion associated with
each link is considered. Due to the absence of CSI at the transmitters, the
Gaussian sources are encoded in a successively refinable manner and the
resulting code words are transmitted over the channel using a multi-layer
coding technique. Accordingly, the optimal power assignment between code layers
leading to the least expected achievable distortion, under a mean-square error
criterion is derived for both, the perfect and imperfect CSIR scenarios.
Finally, some numerical examples are provided and it is demonstrated that the
proposed method results in better performance as compared with the conventional
single-layer approach, termed as outage approach.



Low Complexity Symbol-Level Design for Linear Precoding Systems

The practical utilization of the symbol-level precoding in MIMO systems is
challenging since the implementation of the sophisticated optimization
algorithms must be done with reasonable computational resources. In the real
implementation of MIMO precoding systems, the processing time for each set of
symbols is a crucial parameter, especially in the high-throughput mode. In this
work, a symbol-level optimization algorithm with reduced complexity is devised.
Performance of a symbol-level precoder is shown to improve in terms of the
processing times per set of symbols.



Entropy-based Generating Markov Partitions for Complex Systems

Finding the correct encoding for a generic dynamical system's trajectory is a
complicated task: the symbolic sequence needs to preserve the invariant
properties from the system's trajectory. In theory, the solution to this
problem is found when a Generating Markov Partition (GMP) is obtained, which is
only defined once the unstable and stable manifolds are known with infinite
precision and for all times. However, these manifolds usually form highly
convoluted Euclidean sets, are \emph{a priori} unknown, and, as it happens in
any real-world experiment, measurements are made with finite resolution and
over a finite time-span. The task gets even more complicated if the system is a
network composed of interacting dynamical units, namely, a high-dimensional
complex system. Here, we tackle this task and solve it by defining a method to
approximately construct GMPs for any complex system's finite-resolution and
finite-time trajectory. We critically test our method on networks of coupled
maps, encoding their trajectories into symbolic sequences. We show that these
sequences are optimal because they minimise the information loss and also any
spurious information added. Consequently, our method allows us to approximately
calculate the invariant probability measures of complex systems from observed
data. Thus, we can efficiently define complexity measures that are applicable
to a wide range of complex phenomena, such as the characterisation of brain
activity from EEG signals measured at different brain regions or the
characterisation of climate variability from temperature anomalies measured at
different Earth regions.



Filter Design for Autoregressive Moving Average Graph Filters

In the field of signal processing on graphs, graph filters play a crucial
role in processing the spectrum of graph signals. This paper proposes two
different strategies for designing autoregressive moving average (ARMA) graph
filters on both directed and undirected graphs. The first approach is inspired
by Prony's method, which considers a modified error between the modeled and the
desired frequency response. The second technique is based on an iterative
approach, which finds the filter coefficients by iteratively minimizing the
true error (instead of the modified error) between the modeled and the desired
frequency response. The performance of the proposed algorithms is evaluated and
compared with finite impulse response (FIR) graph filters, on both synthetic
and real data. The obtained results show that ARMA filters outperform FIR
filters in terms of approximation accuracy and they are suitable for graph
signal interpolation, compression and prediction.



An Extremely Flexible, Energy, and Spectral Effective Green PHY-MAC for Profitable Ubiquitous Rural and Remote 5G/B5G IoT/M2M Communications

In this paper, the fundamental throughput limits and extremums of the
invariant criteria of energy, power and spectral efficiency of the physical
layer (PHY) and medium access control (MAC) sublayer are proved. The invariant
criteria are constructed relying on Shannon m-ary digital channel capacity
which a rich palette of the technically interpreted physical and control
parameters consider. Therefore, the invariant criteria as very suitable for
research and design of the fifth generation (5G) communications extremely
performance problems are found. The smart distributed control techniques which
able implements on-the-fly the limits close and invariant criterion
optimization or trade-off is proposed. Such smart control techniques represent
a key disruptive technology meet the 5G and Beyond 5G network challenges.



Real-Time Capable Micro-Doppler Signature Decomposition of Walking Human Limbs

Unique micro-Doppler signature ($\boldsymbol{\mu}$-D) of a human body motion
can be analyzed as the superposition of different body parts
$\boldsymbol{\mu}$-D signatures. Extraction of human limbs $\boldsymbol{\mu}$-D
signatures in real-time can be used to detect, classify and track human motion
especially for safety application. In this paper, two methods are combined to
simulate $\boldsymbol{\mu}$-D signatures of a walking human. Furthermore, a
novel limbs $\mu$-D signature time independent decomposition feasibility study
is presented based on features as $\mu$-D signatures and range profiles also
known as micro-Range ($\mu$-R). Walking human body parts can be divided into
four classes (base, arms, legs, feet) and a decision tree classifier is used.
Validation is done and the classifier is able to decompose $\mu$-D signatures
of limbs from a walking human signature on real-time basis.



Micro-Doppler Based Human-Robot Classification Using Ensemble and Deep Learning Approaches

Radar sensors can be used for analyzing the induced frequency shifts due to
micro-motions in both range and velocity dimensions identified as micro-Doppler
($\boldsymbol{\mu}$-D) and micro-Range ($\boldsymbol{\mu}$-R), respectively.
Different moving targets will have unique $\boldsymbol{\mu}$-D and
$\boldsymbol{\mu}$-R signatures that can be used for target classification.
Such classification can be used in numerous fields, such as gait recognition,
safety and surveillance. In this paper, a 25 GHz FMCW Single-Input
Single-Output (SISO) radar is used in industrial safety for real-time
human-robot identification. Due to the real-time constraint, joint
Range-Doppler (R-D) maps are directly analyzed for our classification problem.
Furthermore, a comparison between the conventional classical learning
approaches with handcrafted extracted features, ensemble classifiers and deep
learning approaches is presented. For ensemble classifiers, restructured range
and velocity profiles are passed directly to ensemble trees, such as gradient
boosting and random forest without feature extraction. Finally, a Deep
Convolutional Neural Network (DCNN) is used and raw R-D images are directly fed
into the constructed network. DCNN shows a superior performance of 99\%
accuracy in identifying humans from robots on a single R-D map.



Power-Based Real-Time Respiration Monitoring Using FMCW Radar

Non-contact vital sign detection is a required application nowadays in many
fields as patient monitoring and static human detection. Within the last
decade, radar has been introduced as a smart and convenient sensor for
non-contact respiration monitoring. Radar sensors are considered suitable for
such application for its capability to work through obstacles and in harsh
environmental conditions. FMCW radar has been introduced as a powerful tool in
this field for its capability of detecting both the breathing target position
and his chest micro-motions induced due to breathing. Most of the presented
techniques for using the radar for respiration detection is based on bandpass
filtering or wavelet transforms on the required harmonics in either the range
or Doppler dimension. However, both techniques affect the real-time capability
of the monitoring and work on limited distances and aspect angles. A
recognizable fluctuation effect is observed in the received range spectrum
overtime due to respiration chest movements. The proposed technique in this
paper is based on detecting and processing the power changes in real-time over
different aspect angles and distances. Two radar modules working on different
carrier frequency bands, bandwidths and output power levels were tested and
compared.



Stairs Detection for Enhancing Wheelchair Capabilities Based on Radar Sensors

Powered wheelchair users encounter barriers to their mobility everyday.
Entering a building with non barrier-free areas can massively impact the user
mobility related activities. There are a few commercial devices and some
experimental that can climb stairs using for instance adaptive wheels with
joints or caterpillar drive. These systems rely on the use for sensing and
control. For safe automated obstacle crossing, a robust and environment
invariant detection of the surrounding is necessary. Radar may prove to be a
suitable sensor for its capability to handle harsh outdoor environmental
conditions. In this paper, we introduce a mirror based two dimensional
Frequency-Modulated Continuous-Wave (FMCW) radar scanner for stair detection. A
radar image based stair dimensioning approach is presented and tested under
laboratory and realistic conditions.



Multivariate Copula Spatial Dependency in One Bit Compressed Sensing

In this letter, the problem of sparse signal reconstruction from one bit
compressed sensing measurements is investigated. To solve the problem, a
variational Bayes framework with a new statistical multivariate model is used.
The dependency of the wavelet decomposition coefficients is modeled with a
multivariate Gaussian copula. This model can separate marginal structure of
coefficients from their intra scale dependency. In particular, the drawable
Gaussian vine copula multivariate double Lomax model is suggested. The
reconstructed signal is derived by variational Bayes algorithm which can
calculate closed forms for posterior of all unknown parameters and sparse
signal. Numerical results illustrate the effectiveness of the proposed model
and algorithm compared with the competing approaches in the literature.



Assessment of sound spatialisation algorithms for sonic rendering with headsets

Given an input sound signal and a target virtual sound source, sound
spatialisation algorithms manipulate the signal so that a listener perceives it
as though it were emitted from the target source. There exist several
established spatialisation approaches that deliver satisfactory results when
loudspeakers are used to playback the manipulated signal. As headphones have a
number of desirable characteristics over loudspeakers, such as portability,
isolation from the surrounding environment, cost and ease of use, it is
interesting to explore how a sense of acoustic space can be conveyed through
them. This article first surveys traditional spatialisation approaches intended
for loudspeakers, and then reviews them with regard to their adaptability to
headphones.



Chaos suppression of Lorenz Systems by means on the average of rounding modes

This work deals with chaos suppression based on average of the rounded modes
to negative and positive infinite. The present procedure acts to reduce the
rounding errors. It was observed that when the method proposed in this paper is
applied to the chaotic Lorenz's system, it exhibits a periodic behaviour,
characterized by a limit cycle and negative largest Lyapunov exponent. We
tested our approach using three discretization schemes based on Runge-Kutta
method



Consensus State Gram Matrix Estimation for Stochastic Switching Networks from Spectral Distribution Moments

Reaching distributed average consensus quickly and accurately over a network
through iterative dynamics represents an important task in numerous distributed
applications. Suitably designed filters applied to the state values can
significantly improve the convergence rate. For constant networks, these
filters can be viewed in terms of graph signal processing as polynomials in a
single matrix, the consensus iteration matrix, with filter response evaluated
at its eigenvalues. For random, time-varying networks, filter design becomes
more complicated, involving eigendecompositions of sums and products of random,
time-varying iteration matrices. This paper focuses on deriving an estimate for
the Gram matrix of error in the state vectors over a filtering window for
large-scale, stationary, switching random networks. The result depends on the
moments of the empirical spectral distribution, which can be estimated through
Monte-Carlo simulation. This work then defines a quadratic objective function
to minimize the expected consensus estimate error norm. Simulation results
provide support for the approximation.



Inference of Spatio-Temporal Functions over Graphs via Multi-Kernel Kriged Kalman Filtering

Inference of space-time varying signals on graphs emerges naturally in a
plethora of network science related applications. A frequently encountered
challenge pertains to reconstructing such dynamic processes, given their values
over a subset of vertices and time instants. The present paper develops a
graph-aware kernel-based kriged Kalman filter that accounts for the
spatio-temporal variations, and offers efficient online reconstruction, even
for dynamically evolving network topologies. The kernel-based learning
framework bypasses the need for statistical information by capitalizing on the
smoothness that graph signals exhibit with respect to the underlying graph. To
address the challenge of selecting the appropriate kernel, the proposed filter
is combined with a multi-kernel selection module. Such a data-driven method
selects a kernel attuned to the signal dynamics on-the-fly within the linear
span of a pre-selected dictionary. The novel multi-kernel learning algorithm
exploits the eigenstructure of Laplacian kernel matrices to reduce
computational complexity. Numerical tests with synthetic and real data
demonstrate the superior reconstruction performance of the novel approach
relative to state-of-the-art alternatives.



BER Performance Analysis of Coarse Quantized Uplink Massive MIMO

Having lower quantization resolution, has been introduced in the literature,
as a solution to reduce the power consumption of massive MIMO and millimeter
wave MIMO systems. In this paper, we analyze bit error rate (BER) performance
of quantized uplink massive MIMO employing a few-bit resolution ADCs.
Considering Zero-Forcing (ZF) detection, we derive a closed-form quantized
signal-to-interference-plus-noise ratio (SINR) to achieve an analytical BER
approximation for coarse quantized M-QAM massive MIMO systems, by using a
linear quantization model. The proposed expression is a function of
quantization resolution in bits. We further numerically investigate the effects
of different quantization levels, from 1-bit to 4-bits, on the BER of three
modulation types of QPSK, 16-QAM, and 64-QAM. Uniform and non-uniform
quantizers are employed in our simulation.
  Monte Carlo simulation results reveal that our approximate formula gives a
tight upper bound for the BER performance of $b$-bit resolution quantized
systems using non-uniform quantizers, whereas the use of uniform quantizers
cause a lower performance for the same systems. We also found a small BER
performance degradation in coarse quantized systems, for example 2-3 bits QPSK
and 3-4 bits 16-QAM, compared to the full-precision (unquantized) case.
However, this performance degradation can be compensated by increasing the
number of antennas at the BS.



An adaptive software defined radio design based on a standard space telecommunication radio system API

Software defined radio (SDR) has become a popular tool for the implementation
and testing for communications performance. The advantage of the SDR approach
includes: a re-configurable design, adaptive response to changing conditions,
efficient development, and highly versatile implementation. In order to
understand the benefits of SDR, the space telecommunication radio system (STRS)
was proposed by NASA Glenn research center (GRC) along with the standard
application program interface (API) structure. Each component of the system
uses a well-defined API to communicate with other components. The benefit of
standard API is to relax the platform limitation of each component for addition
options. For example, the waveform generating process can support a field
programmable gate array (FPGA), personal computer (PC), or an embedded system.
As long as the API defines the requirements, the generated waveform selection
will work with the complete system. In this paper, we demonstrate the design
and development of adaptive SDR following the STRS and standard API protocol.
We introduce step by step the SDR testbed system including the controlling
graphic user interface (GUI), database, GNU radio hardware control, and
universal software radio peripheral (USRP) tranceiving front end. In addition,
a performance evaluation in shown on the effectiveness of the SDR approach for
space telecommunication.



Design of LDPC Codes for the Unequal Power Two-User Gaussian Multiple Access Channel

In this work, we describe an LDPC code design framework for the unequal power
two-user Gaussian multiple access channel using EXIT charts. We show that the
sum-rate of the LDPC codes designed using our approach can get close to the
maximal sum-rate of the two-user Gaussian multiple access channel. Moreover, we
provide numerical simulation results that demonstrate the excellent
finite-length performance of the designed LDPC codes.



Compressive Sensing of Color Images Using Nonlocal Higher Order Dictionary

This paper addresses an ill-posed problem of recovering a color image from
its compressively sensed measurement data. Differently from the typical 1D
vector-based approach of the state-of-the-art methods, we exploit the nonlocal
similarities inherently existing in images by treating each patch of a color
image as a 3D tensor consisting of not only horizontal and vertical but also
spectral dimensions. A group of nonlocal similar patches form a 4D tensor for
which a nonlocal higher order dictionary is learned via higher order singular
value decomposition. The multiple sub-dictionaries contained in the higher
order dictionary decorrelate the group in each corresponding dimension, thus
help the detail of color images to be reconstructed better. Furthermore, we
promote sparsity of the final solution using a sparsity regularization based on
a weight tensor. It can distinguish those coefficients of the sparse
representation generated by the higher order dictionary which are expected to
have large magnitude from the others in the optimization. Accordingly, in the
iterative solution, it acts like a weighting process which is designed by
approximating the minimum mean squared error filter for more faithful recovery.
Experimental results confirm improvement by the proposed method over the
state-of-the-art ones.



Design and Implementation of a LTE-WiFi Aggregation System based on SDR

With explosive growth of the mobile Internet access and the popularization of
wireless local area network (WLAN) access points (APs), wireless fidelity
(WiFi) offloading is considered as an important supplementary technique to
reduce the load of cellular infrastructure and enhance quality of network
service. In this paper, we design and implement the framework of LTEWiFi
aggregation (LWA) which is in line with the 3rd Generation Partnership Project
(3GPP) released standard recently. In the LWA system, the process of data
offloading is different from that of WiFi interworking on the cellular core
network. WLAN APs directly connect to eNodeBs (eNBs), and the offloading is
realized in Packet Data Convergence Protocol layer (PDCP). Thus, this
architecture makes full use of existed WLAN APs to improve the performance of
indoor cellular network. Besides, benefiting from the flow control between LTE
and WiFi, eNB schedule can be more flexible and efficient. Next, it is
implemented based on open source OpenAirInterface (OAI) software defined radio
(SDR) platform, where a simple and practical reordering method is proposed and
carried out. Experimental results demonstrate that our design works stably with
the WiFi-offloading and reordering functions.



RSS Models for Respiration Rate Monitoring

Received signal strength based respiration rate monitoring is emerging as an
alternative non-contact technology. These systems make use of the radio
measurements of short-range commodity wireless devices, which vary due to the
inhalation and exhalation motion of a person. The success of respiration rate
estimation using such measurements depends on the signal-to-noise ratio, which
alters with properties of the person and with the measurement system. To date,
no model has been presented that allows evaluation of different deployments or
system configurations for successful breathing rate estimation. In this paper,
a received signal strength model for respiration rate monitoring is introduced.
It is shown that measurements in linear and logarithmic scale have the same
functional form, and the same estimation techniques can be used in both cases.
The implications of the model are validated under varying signal-to-noise ratio
conditions using the performances of three estimators: batch frequency
estimator, recursive Bayesian estimator, and model-based estimator. The results
are in coherence with the findings, and they imply that different estimators
are advantageous in different signal-to-noise ratio regimes.



Realistic multi-microphone data simulation for distant speech recognition

The availability of realistic simulated corpora is of key importance for the
future progress of distant speech recognition technology. The reliability,
flexibility and low computational cost of a data simulation process may
ultimately allow researchers to train, tune and test different techniques in a
variety of acoustic scenarios, avoiding the laborious effort of directly
recording real data from the targeted environment.
  In the last decade, several simulated corpora have been released to the
research community, including the data-sets distributed in the context of
projects and international challenges, such as CHiME and REVERB. These efforts
were extremely useful to derive baselines and common evaluation frameworks for
comparison purposes. At the same time, in many cases they highlighted the need
of a better coherence between real and simulated conditions.
  In this paper, we examine this issue and we describe our approach to the
generation of realistic corpora in a domestic context. Experimental validation,
conducted in a multi-microphone scenario, shows that a comparable performance
trend can be observed with both real and simulated data across different
recognition frameworks, acoustic models, as well as multi-microphone processing
techniques.



Analysis of Dual-Hop AF Relay Systems in Mixed RF and FSO Links

We analysis the performances for the dual-hop fixed gain amplify-and-forward
(AF) relay systems operating over mixed radio-frequency (RF) and free-space
optical (FSO) links. The RF link is subject to Rician fading and the FSO link
experiences Gamma-Gamma turbulence fading. We derive the closed-form
expressions for the outage probability and the average symbol error rate in
terms of the Meijer's G function.All analytical results are corroborated by
simulation results and the effects of fading parameters on the system
performances are also studied.



Diffusion Maps Kalman Filter for a Class of Systems with Gradient Flows

In this paper, we propose a non-parametric method for state estimation of
high-dimensional nonlinear stochastic dynamical systems, which evolve according
to gradient flows with isotropic diffusion. We combine diffusion maps, a
manifold learning technique, with a linear Kalman filter and with concepts from
Koopman operator theory. More concretely, using diffusion maps, we construct
data-driven virtual state coordinates, which linearize the system model. Based
on these coordinates, we devise a data-driven framework for state estimation
using the Kalman filter. We demonstrate the strengths of our method with
respect to both parametric and non-parametric algorithms in three tracking
problems. In particular, applying the approach to actual recordings of
hippocampal neural activity in rodents directly yields a representation of the
position of the animals. We show that the proposed method outperforms competing
non-parametric algorithms in the examined stochastic problem formulations.
Additionally, we obtain results comparable to classical parametric algorithms,
which, in contrast to our method, are equipped with model knowledge.



Method for estimating modulation transfer function from sample images

The modulation transfer function (MTF) represents the frequency domain
response of imaging modalities. Here, we report a method for estimating the MTF
from sample images. Test images were generated from a number of images,
including those taken with an electron microscope and with an observation
satellite. These original images were convolved with point spread functions
(PSFs) including those of circular apertures. The resultant test images were
subjected to a Fourier transformation. The logarithm of the squared norm of the
Fourier transform was plotted against the squared distance from the origin.
Linear correlations were observed in the logarithmic plots, indicating that the
PSF of the test images can be approximated with a Gaussian. The MTF was then
calculated from the Gaussian-approximated PSF. The obtained MTF closely
coincided with the MTF predicted from the original PSF. The MTF of an x-ray
microtomographic section of a fly brain was also estimated with this method.
The obtained MTF showed good agreement with the MTF determined from an edge
profile of an aluminum test object. We suggest that this approach is an
alternative way of estimating the MTF, independently of the image type.



Multilingual Training and Cross-lingual Adaptation on CTC-based Acoustic Model

Multilingual models for Automatic Speech Recognition (ASR) are attractive as
they have been shown to benefit from more training data, and better lend
themselves to adaptation to under-resourced languages. However, initialisation
from monolingual context-dependent models leads to an explosion of
context-dependent states. Connectionist Temporal Classification (CTC) is a
potential solution to this as it performs well with monophone labels.
  We investigate multilingual CTC in the context of adaptation and
regularisation techniques that have been shown to be beneficial in more
conventional contexts. The multilingual model is trained to model a universal
International Phonetic Alphabet (IPA)-based phone set using the CTC loss
function. Learning Hidden Unit Contribution (LHUC) is investigated to perform
language adaptive training. In addition, dropout during cross-lingual
adaptation is also studied and tested in order to mitigate the overfitting
problem.
  Experiments show that the performance of the universal phoneme-based CTC
system can be improved by applying LHUC and it is extensible to new phonemes
during cross-lingual adaptation. Updating all the parameters shows consistent
improvement on limited data. Applying dropout during adaptation can further
improve the system and achieve competitive performance with Deep Neural Network
/ Hidden Markov Model (DNN/HMM) systems on limited data.



WSNet: Compact and Efficient Networks Through Weight Sampling

We present a new approach and a novel architecture, termed WSNet, for
learning compact and efficient deep neural networks. Existing approaches
conventionally learn full model parameters independently and then compress them
via ad hoc processing such as model pruning or filter factorization.
Alternatively, WSNet proposes learning model parameters by sampling from a
compact set of learnable parameters, which naturally enforces {parameter
sharing} throughout the learning process. We demonstrate that such a novel
weight sampling approach (and induced WSNet) promotes both weights and
computation sharing favorably. By employing this method, we can more
efficiently learn much smaller networks with competitive performance compared
to baseline networks with equal numbers of convolution filters. Specifically,
we consider learning compact and efficient 1D convolutional neural networks for
audio classification. Extensive experiments on multiple audio classification
datasets verify the effectiveness of WSNet. Combined with weight quantization,
the resulted models are up to 180 times smaller and theoretically up to 16
times faster than the well-established baselines, without noticeable
performance drop.



On the Constructing Bifurcation Diagram of the Quadratic Map With Floating-Point Arithmetic

This paper presents an analysis on the effects of floating-point arithmetic
on the constructing bifurcation diagram of the quadratic map. More precisely,
we are interested in showing the dependence of initial conditions to obtain
some specific features of the diagram. With this study, it was possible to
observe that when there is a restriction regarding the initial condition, the
results present aspects with significant differences of the ones found in the
literature regarding the behaviour of the map, consequently there is a
considerable modification in its bifurcation diagram. We show that these
difference are related to floating-point arithmetic



Machine Learning based Intelligent Cognitive Network using Fog Computing

In this paper, a Cognitive Radio Network (CRN) based on artificial
intelligence is proposed to distribute the limited radio spectrum resources
more efficiently. The CRN framework can analyze the time-sensitive signal data
close to the signal source using fog computing with different types of machine
learning techniques. Depending on the computational capabilities of the fog
nodes, different features and machine learning techniques are chosen to
optimize spectrum allocation. Also, the computing nodes send the periodic
signal summary which is much smaller than the original signal to the cloud so
that the overall system spectrum source allocation strategies are dynamically
updated. Applying fog computing, the system is more adaptive to the local
environment and robust to spectrum changes. As most of the signal data is
processed at the fog level, it further strengthens the system security by
reducing the communication burden of the communications network.



Source Localisation Using Binary Measurements

This paper considers the problem of localising a stationary signal source
using a team of mobile agents which only take binary measurements. Background
false detection rates and missed detection probabilities are incorporated into
the framework. A Bayesian estimation algorithm that discretises the search
environment is employed, and analytical convergence and consistency results for
this are derived. Fisher Information is then used as a metric for the design of
optimal agent geometries. Knowledge of the probability of detection as a
function of the source and agent locations is assumed in the analysis, with
special attention given to range-dependent functions. The behaviour of the
algorithm under inexact knowledge of the probability of detection is also
analysed. Finally, simulation results are presented to demonstrate the
effectiveness of the algorithm.



Exploiting Nontrivial Connectivity for Automatic Speech Recognition

Nontrivial connectivity has allowed the training of very deep networks by
addressing the problem of vanishing gradients and offering a more efficient
method of reusing parameters. In this paper we make a comparison between
residual networks, densely-connected networks and highway networks on an image
classification task. Next, we show that these methodologies can easily be
deployed into automatic speech recognition and provide significant improvements
to existing models.



Learning from Between-class Examples for Deep Sound Recognition

Deep learning methods have achieved high performance in sound recognition
tasks. Deciding how to feed the training data is important for further
performance improvement. We propose a novel learning method for deep sound
recognition: Between-Class learning (BC learning). Our strategy is to learn a
discriminative feature space by recognizing the between-class sounds as
between-class sounds. We generate between-class sounds by mixing two sounds
belonging to different classes with a random ratio. We then input the mixed
sound to the model and train the model to output the mixing ratio. The
advantages of BC learning are not limited only to the increase in variation of
the training data; BC learning leads to an enlargement of Fisher's criterion in
the feature space and a regularization of the positional relationship among the
feature distributions of the classes. The experimental results show that BC
learning improves the performance on various sound recognition networks,
datasets, and data augmentation schemes, in which BC learning proves to be
always beneficial. Furthermore, we construct a new deep sound recognition
network (EnvNet-v2) and train it with BC learning. As a result, we achieved a
performance surpasses the human level.



Kernel-based Inference of Functions over Graphs

The study of networks has witnessed an explosive growth over the past decades
with several ground-breaking methods introduced. A particularly interesting --
and prevalent in several fields of study -- problem is that of inferring a
function defined over the nodes of a network. This work presents a versatile
kernel-based framework for tackling this inference problem that naturally
subsumes and generalizes the reconstruction approaches put forth recently by
the signal processing on graphs community. Both the static and the dynamic
settings are considered along with effective modeling approaches for addressing
real-world problems. The herein analytical discussion is complemented by a set
of numerical examples, which showcase the effectiveness of the presented
techniques, as well as their merits related to state-of-the-art methods.



Power Control and Scheduling In Low SNR Region In The Uplink of Two Cell Networks

In this paper we investigate the sub-channel assignment and power control to
maximize the total sum rate in the uplink of two-cell network. It is assumed
that there are some sub-channels in each cell which should be allocated among
some users. Also, each user is subjected to a power constraint. The underlying
problem is a non-convex mixed integer non-linear optimization problem which
does not have a trivial solution. To solve the problem, having fixed the
consumed power of each user, and assuming low co-channel interference region,
the sub-channel allocation problem is reformulate into a more mathematically
tractable problem which is shown can be tackled through the so-called Hungarian
algorithm. Then, the consumed power of each user is reformulated as a quadratic
fractional problem which can be numerically derived. Numerical results
demonstrate the superiority of the proposed method in low SNR region as
compared to existing works addressed in the literature



Edge Computing and Dynamic Vision Sensing for Low Delay Access to Visual Medical Information

A new method is proposed to decrease the transmission delay of visual and
non-visual medical records by using edge computing and Dynamic Vision Sensing
(DVS) technologies. The simulation results show that the proposed scheme can
decrease the transmission delay by 89.15% to 93.23%. The maximum number of
patients who can be served by edge devices is analysed.



Practical Synchronization Waveform for Massive Machine-Type-Communications

A general synchronization waveform type that is insensitive to frequency
error for massive Internet-of-Things (mIoT) is derived in "Robust
synchronization waveform design for massive low-power IoT" (IEEE Trans. on
Wireless Commun., vol. 16, no. 11, pp. 7551-7559, Nov. 2017). Detailed issues
such as (1) how to obtain the frequency error information from the waveform;
(2) how to refine the timing estimation; and (3) how to adjust the waveform
parameter to meet various spectral requirements are not addressed in the paper,
and yet are of great importance for a practical application. These issues are
discussed in this comment.



Proactive Caching for Energy-Efficiency in Wireless Networks: A Markov Decision Process Approach

Content caching in wireless networks provides a substantial opportunity to
trade off low cost memory storage with energy consumption, yet finding the
optimal causal policy with low computational complexity remains a challenge.
This paper models the Joint Pushing and Caching (JPC) problem as a Markov
Decision Process (MDP) and provides a solution to determine the optimal
randomized policy. A novel approach to decouple the influence from buffer
occupancy and user requests is proposed to turn the high-dimensional
optimization problem into three low-dimensional ones. Furthermore, a
non-iterative algorithm to solve one of the sub-problems is presented,
exploiting a structural property we found as \textit{generalized monotonicity},
and hence significantly reduces the computational complexity. The result
attains close performance in comparison with theoretical bounds from
non-practical policies, while benefiting from higher time efficiency than the
unadapted MDP solution.



Performance Analysis of Massive MIMO with Low-Resolution ADCs

The uplink performance of massive multiple-input-multiple-output (MIMO)
systems where the base stations (BS) employ low-resolution analog-to-digital
converters (ADCs) is analyzed. A high performance MMSE receiver that takes both
additive white Gaussian noise (AWGN) and quantization noise into consideration
is designed, which works well for both cases of uniform resolution ADCs and
non-uniform resolution ADCs. With the proposed MMSE receiver, we then employ
the random matrix theory to derive the asymptotic equivalent of the uplink
spectral efficiency (SE) of the system. Numerical results show the tightness of
the asymptotic equivalent of the uplink SE, and massive MIMO with
low-resolution ADCs can still achieve the satisfying uplink SE by increasing
the number of antennas at BS.



RoboJam: A Musical Mixture Density Network for Collaborative Touchscreen Interaction

RoboJam is a machine-learning system for generating music that assists users
of a touchscreen music app by performing responses to their short
improvisations. This system uses a recurrent artificial neural network to
generate sequences of touchscreen interactions and absolute timings, rather
than high-level musical notes. To accomplish this, RoboJam's network uses a
mixture density layer to predict appropriate touch interaction locations in
space and time. In this paper, we describe the design and implementation of
RoboJam's network and how it has been integrated into a touchscreen music app.
A preliminary evaluation analyses the system in terms of training, musical
generation and user interaction.



$L_2$-Box Optimization for Green Cloud-RAN via Network Adaptation

In this paper, we propose a reformulation for the Mixed Integer Programming
(MIP) problem into an exact and continuous model through using the $\ell_2$-box
technique to recast the binary constraints into a box with an $\ell_2$ sphere
constraint. The reformulated problem can be tackled by a dual ascent algorithm
combined with a Majorization-Minimization (MM) method for the subproblems to
solve the network power consumption problem of the Cloud Radio Access Network
(Cloud-RAN), and which leads to solving a sequence of Difference of Convex (DC)
subproblems handled by an inexact MM algorithm. After obtaining the final
solution, we use it as the initial result of the bi-section Group Sparse
Beamforming (GSBF) algorithm to promote the group-sparsity of beamformers,
rather than using the weighted $\ell_1 / \ell_2$-norm. Simulation results
indicate that the new method outperforms the bi-section GSBF algorithm by
achieving smaller network power consumption, especially in sparser cases, i.e.,
Cloud-RANs with a lot of Remote Radio Heads (RRHs) but fewer users.



Parametric Investigation Of Different Modulation Techniques On Free Space Optical Systems

Free Space Optics systems (FSO) is one of the evolving wireless technologies.
FSO is the only technology with highest data rates in wireless mode of
operation but it suffers from bad weather conditions. In this work, analysis is
carried out on FSO system having certain parameters constant using different
modulation formats (i.e. RZ, NRZ, MDRZ, MODB and CSRZ). Impact of data rate,
link range, input power and attenuation factor has been computed. Weather
conditions are supposed to be nearly clear and suitable for FSO communication
while taking attenuation factor up to 10dB/Km. Q-factor, received signal power
and BER is calculated in all scenarios for obtaining an estimate of system
performance. Results have shown that NRZ & RZ formats are in the lead until now
with highest Q values.



Modeling and Performance Analysis of 10 Gbps Inter-Satellite-Link (ISL) In Inter-Satellite Optical-Wireless Communication (IsOWC) System between LEO and GEO Satellites

Free space optical communication has merged the aspects of fiber optics and
the wireless communication which are the most conquered and controlled
telecommunication technologies. Most of the features of free space optics (FSO)
are interrelated to fiber optics but the difference between them is
transmission medium, which is glass in case of fiber-optics and air/vacuum in
case of FSO. In the near future, communication between LEO & GEO satellites
with each other which are orbiting the Earth will be done by using
inter-satellite optical wireless communication (IsOWC) systems. IsOWC systems
is the most significant application of the FSO and it will be installed in the
space in the near future because of its low input power, no licensing by ITU,
low cost, light weight, small size of the telescopes and very high data rates
as compared to the radio frequency (RF) satellite systems. In this research
article, IsOWC system is designed between LEO and GEO satellites by using
optisystem which is not stated in past examined research works. Inter-satellite
link is established between satellites which are separated by the distance of
40,000 Km at the bit rate of 10 Gbps.



Effect of Weather Conditions on FSO link based in Islamabad

Free space optics (FSO) is a field of curiosity and importance for the
scientists because of its numerous applications and advantages like low cost
FSO systems, easy deployment, high data rate, secure FSO links and license free
bands. Very high bandwidth FSO link can be effectively established between the
skyscrapers of the Islamabad Pakistan for the purpose high capacity
applications in these skyscrapers. FSO links are badly affected by the weather
conditions especially rain and fog because of high attenuation factors.
optisystem is used to study the effect of rain and fog on the performance of
FSO links.



The Tropospheric Scintillation Prediction based on measured data for earth-to-satellite link for Bangladeshi climatic condition

The performance of earth-to-satellite link largely depends on various
environmental factors like rain, fog, cloud, and atmospheric effects like
ionospheric and tropospheric scintillation. In this paper, the tropospheric
scintillation of Bangladesh, a subtropical country, is predicted based on
measured climatic parameters, like relative humidity, temperature. In this
prediction, ITU scintillation model are used. Four major cities, named Dhaka,
Chittagong, Rajshahi and Sylhet, of Bangladesh are selected for prediction of
scintillation. From the simulation result, Rajshahi is found to be the most
badly affected region by the scintillation fade depth (SFD), which is followed
by Chittagong and the SFD is minimum in Dhaka and Sylhet. The difference in
SFDs among the considered cities does not vary heavily. It is also found that
the SFD varies from 3 dB to 13 dB depending on the frequency in used. Moreover,
higher scintillation is found in rainy season of Bangladesh. During this
period, the scintillation becomes double of the average figure.



Now Playing: Continuous low-power music recognition

Existing music recognition applications require a connection to a server that
performs the actual recognition. In this paper we present a low-power music
recognizer that runs entirely on a mobile device and automatically recognizes
music without user interaction. To reduce battery consumption, a small music
detector runs continuously on the mobile device's DSP chip and wakes up the
main application processor only when it is confident that music is present.
Once woken, the recognizer on the application processor is provided with a few
seconds of audio which is fingerprinted and compared to the stored fingerprints
in the on-device fingerprint database of tens of thousands of songs. Our
presented system, Now Playing, has a daily battery usage of less than 1% on
average, respects user privacy by running entirely on-device and can passively
recognize a wide range of music.



HoME: a Household Multimodal Environment

We introduce HoME: a Household Multimodal Environment for artificial agents
to learn from vision, audio, semantics, physics, and interaction with objects
and other agents, all within a realistic context. HoME integrates over 45,000
diverse 3D house layouts based on the SUNCG dataset, a scale which may
facilitate learning, generalization, and transfer. HoME is an open-source,
OpenAI Gym-compatible platform extensible to tasks in reinforcement learning,
language grounding, sound-based navigation, robotics, multi-agent learning, and
more. We hope HoME better enables artificial agents to learn as humans do: in
an interactive, multimodal, and richly contextualized setting.



Meeting of Mobile Nodes Based on RSS Measurements in Wireless Ad Hoc Networks

In this work, we address a completely novel case which is the meeting problem
of mobile nodes (players) in a wireless ad hoc network. We assume that the only
information players have is the received signal strength (RSS) measurements of
other players and there is no centralized communication. Each node has a
different frequency band and they try to meet with each other in a
decentralized manner. We consider the case where players are allowed to move in
four orthogonal directions in a grid. Then, we use multi-armed bandit approach
to give rewards in the end of every move that players do using RSS
measurements. Taking into account that we have an adversarial setting, we
propose different algorithms to solve the meeting problem. We start with
considering two players and then extend the work to multiplayer by considering
multiplayer problem as multiple two player problems. Throughout the paper, we
construct mathematical bounds on meeting times and related concepts and
evaluate the simulation results in the end.



Pressure Loss and Sound Generated In a Miniature Pig Airway Tree Model

Background: Pulmonary auscultation is a common tool for diagnosing various
respiratory diseases. Previous studies have documented many details of
pulmonary sounds in humans. However, information on sound generation and
pressure loss inside animal airways is scarce. Since the morphology of animal
airways can be significantly different from human, the characteristics of
pulmonary sounds and pressure loss inside animal airways can be different.
Objective: The objective of this study is to investigate the sound and static
pressure loss measured at the trachea of a miniature pig airway tree model
based on the geometric details extracted from physical measurements. Methods:
In the current study, static pressure loss and sound generation measured in the
trachea was documented at different flow rates of a miniature pig airway tree.
Results: Results showed that the static pressure and the amplitude of the
recorded sound at the trachea increased as the flow rate increased. The
dominant frequency was found to be around 1840-1870 Hz for flow rates of
0.2-0.55 lit/s. Conclusion: The results suggested that the dominant frequency
of the measured sounds remained similar for flow rates from 0.20 to 0.55 lit/s.
Further investigation is needed to study sound generation under different inlet
flow and pulsatile flow conditions.



Analysis of Seismocardiographic Signals Using Polynomial Chirplet Transform and Smoothed Pseudo Wigner-Ville Distribution

Seismocardiographic (SCG) signals are chest surface vibrations induced by
cardiac activity. These signals may offer a method for diagnosing and
monitoring heart function. Successful classification of SCG signals in health
and disease depends on accurate signal characterization and feature extraction.
One approach of determining signal features is to estimate its time-frequency
characteristics. In this regard, four different time-frequency distribution
(TFD) approaches were used including short-time Fourier transform (STFT),
polynomial chirplet transform (PCT), Wigner-Ville distribution (WVD), and
smoothed pseudo Wigner-Ville distribution (SPWVD). Synthetic SCG signals with
known time-frequency properties were generated and used to evaluate the
accuracy of the different TFDs in extracting SCG spectral characteristics.
Using different TFDs, the instantaneous frequency (IF) of each synthetic signal
was determined and the error (NRMSE) in estimating IF was calculated. STFT had
lower NRMSE than WVD for synthetic signals considered. PCT and SPWVD were,
however, more accurate IF estimators especially for the signal with
time-varying frequencies. PCT and SPWVD also provided better discrimination
between signal frequency components. Therefore, the results of this study
suggest that PCT and SPWVD would be more reliable methods for estimating IF of
SCG signals. Analysis of actual SCG signals showed that these signals had
multiple spectral components with slightly time-varying frequencies. More
studies are needed to investigate SCG spectral properties for healthy subjects
as well as patients with different cardiac conditions.



Grouping Similar Seismocardiographic Signals Using Respiratory Information

Seismocardiography (SCG) offers a potential non-invasive method for cardiac
monitoring. Quantification of the effects of different physiological conditions
on SCG can lead to enhanced understanding of SCG genesis, and may explain how
some cardiac pathologies may affect SCG morphology. In this study, the effect
of the respiration on the SCG signal morphology is investigated. SCG, ECG, and
respiratory flow rate signals were measured simultaneously in 7 healthy
subjects. Results showed that SCG events tended to have two slightly different
morphologies. The respiratory flow rate and lung volume information were used
to group the SCG events into inspiratory/expiratory groups or low/high
lung-volume groups, respectively. Although respiratory flow information could
separate similar SCG events into two different groups, the lung volume
information provided better grouping of similar SCGs. This suggests that
variations in SCG morphology may be due, at least in part, to changes in the
intrathoracic pressure or heart location since those parameters correlates more
with lung volume than respiratory flow. Categorizing SCG events into different
groups containing similar events allows more accurate estimation of SCG
features, and better signal characterization, and classification.



Stream Attention for far-field multi-microphone ASR

A stream attention framework has been applied to the posterior probabilities
of the deep neural network (DNN) to improve the far-field automatic speech
recognition (ASR) performance in the multi-microphone configuration. The stream
attention scheme has been realized through an attention vector, which is
derived by predicting the ASR performance from the phoneme posterior
distribution of individual microphone stream, focusing the recognizer's
attention to more reliable microphones. Investigation on the various ASR
performance measures has been carried out using the real recorded dataset.
Experiments results show that the proposed framework has yielded substantial
improvements in word error rate (WER).



Memory-constrained Vectorization and Scheduling of Dataflow Graphs for Hybrid CPU-GPU Platforms

The increasing use of heterogeneous embedded systems with multi-core CPUs and
Graphics Processing Units (GPUs) presents important challenges in effectively
exploiting pipeline, task and data-level parallelism to meet throughput
requirements of digital signal processing (DSP) applications. Moreover, in the
presence of system-level memory constraints, hand optimization of code to
satisfy these requirements is inefficient and error-prone, and can therefore,
greatly slow down development time or result in highly underutilized processing
resources. In this paper, we present vectorization and scheduling methods to
effectively exploit multiple forms of parallelism for throughput optimization
on hybrid CPU-GPU platforms, while conforming to system-level memory
constraints. The methods operate on synchronous dataflow representations, which
are widely used in the design of embedded systems for signal and information
processing. We show that our novel methods can significantly improve system
throughput compared to previous vectorization and scheduling approaches under
the same memory constraints. In addition, we present a practical case-study of
applying our methods to significantly improve the throughput of an orthogonal
frequency division multiplexing (OFDM) receiver system for wireless
communications.



Symbol Error Rate Performance of Box-relaxation Decoders in Massive MIMO

The maximum-likelihood (ML) decoder for symbol detection in large
multiple-input multiple-output wireless communication systems is typically
computationally prohibitive. In this paper, we study a popular and practical
alternative, namely the Box-relaxation optimization (BRO) decoder, which is a
natural convex relaxation of the ML. For iid real Gaussian channels with
additive Gaussian noise, we obtain exact asymptotic expressions for the symbol
error rate (SER) of the BRO. The formulas are particularly simple, they yield
useful insights, and they allow accurate comparisons to the matched-filter
bound (MFB) and to the zero-forcing decoder. For BPSK signals the SER
performance of the BRO is within 3dB of the MFB for square systems, and it
approaches the MFB as the number of receive antennas grows large compared to
the number of transmit antennas. Our analysis further characterizes the
empirical density function of the solution of the BRO, and shows that error
events for any fixed number of symbols are asymptotically independent. The
fundamental tool behind the analysis is the convex Gaussian min-max theorem.



Properties on n-dimensional convolution for image deconvolution

Convolution system is linear and time invariant, and can describe the optical
imaging process. Based on convolution system, many deconvolution techniques
have been developed for optical image analysis, such as boosting the space
resolution of optical images, image denoising, image enhancement and so on.
Here, we gave properties on N-dimensional convolution. By using these
properties, we proposed image deconvolution method. This method uses a series
of convolution operations to deconvolute image. We demonstrated that the method
has the similar deconvolution results to the state-of-art method. The core
calculation of the proposed method is image convolution, and thus our method
can easily be integrated into GPU mode for large-scale image deconvolution.



A modeling and algorithmic framework for (non)social (co)sparse audio restoration

We propose a unified modeling and algorithmic framework for audio restoration
problem. It encompasses analysis sparse priors as well as more classical
synthesis sparse priors, and regular sparsity as well as various forms of
structured sparsity embodied by shrinkage operators (such as social shrinkage).
The versatility of the framework is illustrated on two restoration scenarios:
denoising, and declipping. Extensive experimental results on these scenarios
highlight both the speedups of 20% or even more offered by the analysis sparse
prior, and the substantial declipping quality that is achievable with both the
social and the plain flavor. While both flavors overall exhibit similar
performance, their detailed comparison displays distinct trends depending
whether declipping or denoising is considered.



Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks

We propose a parallel-data-free voice-conversion (VC) method that can learn a
mapping from source to target speech without relying on parallel data. The
proposed method is general purpose, high quality, and parallel-data free and
works without any extra data, modules, or alignment procedure. It also avoids
over-smoothing, which occurs in many conventional statistical model-based VC
methods. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial
network (CycleGAN) with gated convolutional neural networks (CNNs) and an
identity-mapping loss. A CycleGAN learns forward and inverse mappings
simultaneously using adversarial and cycle-consistency losses. This makes it
possible to find an optimal pseudo pair from unpaired data. Furthermore, the
adversarial loss contributes to reducing over-smoothing of the converted
feature sequence. We configure a CycleGAN with gated CNNs and train it with an
identity-mapping loss. This allows the mapping function to capture sequential
and hierarchical structures while preserving linguistic information. We
evaluated our method on a parallel-data-free VC task. An objective evaluation
showed that the converted feature sequence was near natural in terms of global
variance and modulation spectra. A subjective evaluation showed that the
quality of the converted speech was comparable to that obtained with a Gaussian
mixture model-based method under advantageous conditions with parallel and
twice the amount of data.



Raga Identification using Repetitive Note Patterns from prescriptive notations of Carnatic Music

Carnatic music, a form of Indian Art Music, has relied on an oral tradition
for transferring knowledge across several generations. Over the last two
hundred years, the use of prescriptive notations has been adopted for learning,
sight-playing and sight-singing. Prescriptive notations offer generic
guidelines for a raga rendition and do not include information about the
ornamentations or the gamakas, which are considered to be critical for
characterizing a raga. In this paper, we show that prescriptive notations
contain raga attributes and can reliably identify a raga of Carnatic music from
its octave-folded prescriptive notations. We restrict the notations to 7 notes
and suppress the finer note position information. A dictionary based approach
captures the statistics of repetitive note patterns within a raga notation. The
proposed stochastic models of repetitive note patterns (or SMRNP in short)
obtained from raga notations of known compositions, outperforms the state of
the art melody based raga identification technique on an equivalent melodic
data corresponding to the same compositions. This in turn shows that for
Carnatic music, the note transitions and movements have a greater role in
defining the raga structure than the exact note positions.



MR image reconstruction using deep density priors

Algorithms for Magnetic Resonance (MR) image reconstruction from undersampled
measurements exploit prior information to compensate for missing k-space data.
Deep learning (DL) provides a powerful framework for extracting such
information from existing image datasets, through learning, and then using it
for reconstruction. Leveraging this, recent methods employed DL to learn
mappings from undersampled to fully sampled images using paired datasets,
including undersampled and corresponding fully sampled images, integrating
prior knowledge implicitly. In this article, we propose an alternative approach
that learns the probability distribution of fully sampled MR images using
unsupervised DL, specifically Variational Autoencoders (VAE), and use this as
an explicit prior term in reconstruction, completely decoupling the encoding
operation from the prior. The resulting reconstruction algorithm enjoys a
powerful image prior to compensate for missing k-space data without requiring
paired datasets for training nor being prone to associated sensitivities, such
as deviations in undersampling patterns used in training and test time or coil
settings. We evaluated the proposed method with T1 weighted images from a
publicly available dataset, multi-coil complex images acquired from healthy
volunteers (N=8) and images with white matter lesions. The proposed algorithm,
using the VAE prior, produced visually high quality reconstructions and
achieved low RMSE values, outperforming most of the alternative methods on the
same dataset. On multi-coil complex data, the algorithm yielded accurate
magnitude and phase reconstruction results. In the experiments on images with
white matter lesions, the method faithfully reconstructed the lesions.
  Keywords: Reconstruction, MRI, prior probability, machine learning, deep
learning, unsupervised learning, density estimation



Low-Complexity Statistically Robust Precoder/Detector Computation for Massive MIMO Systems

Massive MIMO is a variant of multiuser MIMO in which the number of antennas
at the base station (BS) $M$ is very large and typically much larger than the
number of served users (data streams) $K$. Recent research has illustrated the
system-level advantages of such a system and in particular the beneficial
effect of increasing the number of antennas $M$. These benefits, however, come
at the cost of dramatic increase in hardware and computational complexity. This
is partly due to the fact that the BS needs to compute suitable beamforming
vectors in order to coherently transmit/receive data to/from each user, where
the resulting complexity grows proportionally to the number of antennas $M$ and
the number of served users $K$. Recently, different algorithms based on tools
from random matrix theory in the asymptotic regime of $M,K \to \infty$ with
$\frac{K}{M} \to \rho \in (0,1)$ have been proposed to reduce such complexity.
The underlying assumption in all these techniques, however, is that the exact
statistics (covariance matrix) of the channel vectors of the users is a priori
known. This is far from being realistic, especially that in the high-dim regime
of $M\to \infty$, estimation of the underlying covariance matrices is well
known to be a very challenging problem.
  In this paper, we propose a novel technique for designing beamforming vectors
in a massive MIMO system. Our method is based on the randomized Kaczmarz
algorithm and does not require knowledge of the statistics of the users channel
vectors. We analyze the performance of our proposed algorithm theoretically and
compare its performance with that of other competitive techniques via numerical
simulations. Our results indicate that our proposed technique has a comparable
performance while it does not require the knowledge of the statistics of the
users channel vectors.



FPS-SFT: A Multi-dimensional Sparse Fourier Transform Based on the Fourier Projection-slice Theorem

We propose a multi-dimensional (M-D) sparse Fourier transform inspired by the
idea of the Fourier projection-slice theorem, called FPS-SFT. FPS-SFT extracts
samples along lines (1-dimensional slices from an M-D data cube), which are
parameterized by random slopes and offsets. The discrete Fourier transform
(DFT) along those lines represents projections of M-D DFT of the M-D data onto
those lines. The M-D sinusoids that are contained in the signal can be
reconstructed from the DFT along lines with a low sample and computational
complexity provided that the signal is sparse in the frequency domain and the
lines are appropriately designed. The performance of FPS-SFT is demonstrated
both theoretically and numerically. A sparse image reconstruction application
is illustrated, which shows the capability of the FPS-SFT in solving practical
problems.



Mean-Square Performance Analysis of Noise-Robust Normalized Subband Adaptive Filter Algorithm

This paper studies the statistical models of the noise-robust normalized
subband adaptive filter (NR-NSAF) algorithm in the mean and mean square
deviation senses involving transient-state and steady-state behavior by
resorting to the method of the vectorization operation and the Kronecker
product. The analysis method does not require the Gaussian input signal.
Moreover, the proposed analysis removes the paraunitary assumption imposed on
the analysis filter banks as in the existing analyses of subband adaptive
algorithms. Simulation results in various conditions demonstrate the
effectiveness of our theoretical analysis. For a special form of the algorithm,
the proposed steady-state expression is also better accurate than the previous
analysis.



Scenario-based Economic Dispatch with Uncertain Demand Response

This paper introduces a new computational framework to account for
uncertainties in day-ahead electricity market clearing process in the presence
of demand response providers. A central challenge when dealing with many demand
response providers is the uncertainty of its realization. In this paper, a new
economic dispatch framework that is based on the recent theoretical development
of the scenario approach is introduced. By removing samples from a finite
uncertainty set, this approach improves dispatch performance while guaranteeing
a quantifiable risk level with respect to the probability of violating the
constraints. The theoretical bound on the level of risk is shown to be a
function of the number of scenarios removed. This is appealing to the system
operator for the following reasons: (1) the improvement of performance comes at
the cost of a quantifiable level of violation probability in the constraints;
(2) the violation upper bound does not depend on the probability distribution
assumption of the uncertainty in demand response. Numerical simulations on (1)
3-bus and (2) IEEE 14-bus system (3) IEEE 118-bus system suggest that this
approach could be a promising alternative in future electricity markets with
multiple demand response providers.



Spectrum Sensing under Spectrum Misuse Behaviors: A Multi-Hypothesis Test Perspective

Spectrum misuse behaviors, brought either by illegitimate access or by rogue
power emission, endanger the legitimate communication and deteriorate the
spectrum usage environment. In this paper, our aim is to detect whether the
spectrum band is occupied, and if it is occupied, recognize whether the misuse
behavior exists. One vital challenge is that the legitimate spectrum
exploitation and misuse behaviors coexist and the illegitimate user may act in
an intermittent and fast-changing manner, which brings about much uncertainty
for spectrum sensing. To tackle it, we firstly formulate the spectrum sensing
problems under illegitimate access and rogue power emission as a uniform
ternary hypothesis test. Then, we develop a novel test criterion, named the
generalized multi-hypothesis N-P criterion. Following the criterion, we derive
two test rules based on the generalized likelihood ratio test and the R-test,
respectively, whose asymptotic performances are analyzed and an upper bound is
also given. Furthermore, a cooperative spectrum sensing scheme is designed
based on the global N-P criterion to further improve the detection
performances. In addition, extensive simulations are provided to verify the
proposed schemes' performance under various parameter configurations.



Technical Report: A New Decision-Theory-Based Framework for Echo Canceler Control

A control logic has a central role in many echo cancellation systems for
optimizing the performance of adaptive filters while estimating the echo path.
For reliable control, accurate double-talk (DT) and channel change (CC)
detectors are usually incorporated to the echo canceler. This work expands the
usual detection strategy to define a classification problem characterizing four
possible states of the echo canceler operation. The new formulation allow the
use of decision theory to continuously control the transitions among the
different modes of operation. The classification rule reduces to a low cost
statistics for which it is possible to determine the probability of error under
all hypotheses, allowing the classification performance to be accessed
analytically. Monte Carlo simulations using synthetic and real data illustrate
the reliability of the proposed method.



Deep Neural Networks for Multiple Speaker Detection and Localization

We propose to use neural networks for simultaneous detection and localization
of multiple sound sources in human-robot interaction. In contrast to
conventional signal processing techniques, neural network-based sound source
localization methods require fewer strong assumptions about the environment.
Previous neural network-based methods have been focusing on localizing a single
sound source, which do not extend to multiple sources in terms of detection and
localization. In this paper, we thus propose a likelihood-based encoding of the
network output, which naturally allows the detection of an arbitrary number of
sources. In addition, we investigate the use of sub-band cross-correlation
information as features for better localization in sound mixtures, as well as
three different network architectures based on different motivations.
Experiments on real data recorded from a robot show that our proposed methods
significantly outperform the popular spatial spectrum-based approaches.



On the Combined Effect of Directional Antennas and Imperfect Spectrum Sensing upon Ergodic Capacity of Cognitive Radio Systems

We consider a cognitive radio system, consisting of a primary transmitter
(PUtx), a primary receiver (PUrx), a secondary transmitter (SUtx), and a
secondary receiver (SUrx). The secondary users (SUs) are equipped with
steerable directional antennas. We assume the SUs and primary users (PUs)
coexist and the SUtx knows the geometry of network. We find the ergodic
capacity of the channel between SUtx and SUrx , and study how spectrum sensing
errors affect the capacity. In our system, the SUtx first senses the spectrum
and then transmits data at two power levels, according to the result of
sensing. The optimal SUtx transmit power levels and the optimal directions of
SUtx transmit antenna and SUrx receive antenna are obtained by maximizing the
ergodic capacity, subject to average transmit power and average interference
power constraints. To study the effect of fading channel, we considered three
scenarios: 1) when SUtx knows fading channels between SUtx and PUrx, PUtx and
SUrx, SUtx and SUrx, 2) when SUtx knows only the channel between SUtx and SUrx,
and statistics of the other two channels, and, 3) when SUtx only knows the
statistics of these three fading channels. For each scenario, we explore the
optimal SUtx transmit power levels and the optimal directions of SUtx and SUrx
antennas, such that the ergodic capacity is maximized, while the power
constraints are satisfied.



Sensor Selection and Power Allocation via Maximizing Bayesian Fisher Information for Distributed Vector Estimation

In this paper we study the problem of distributed estimation of a Gaussian
vector with linear observation model in a wireless sensor network (WSN)
consisting of K sensors that transmit their modulated quantized observations
over orthogonal erroneous wireless channels (subject to fading and noise) to a
fusion center, which estimates the unknown vector. Due to limited network
transmit power, only a subset of sensors can be active at each task period.
Here, we formulate the problem of sensor selection and transmit power
allocation that maximizes the trace of Bayesian Fisher Information Matrix (FIM)
under network transmit power constraint, and propose three algorithms to solve
it. Simulation results demonstrate the superiority of these algorithms compared
to the algorithm that uniformly allocates power among all sensors.



Fundamental Limits on Data Acquisition: Trade-offs between Sample Complexity and Query Difficulty

We consider query-based data acquisition and the corresponding information
recovery problem, where the goal is to recover $k$ binary variables
(information bits) from parity measurements of those variables. The queries and
the corresponding parity measurements are designed using the encoding rule of
Fountain codes. By using Fountain codes, we can design potentially limitless
number of queries, and corresponding parity measurements, and guarantee that
the original $k$ information bits can be recovered with high probability from
any sufficiently large set of measurements of size $n$. In the query design,
the average number of information bits that is associated with one parity
measurement is called query difficulty ($\bar{d}$) and the minimum number of
measurements required to recover the $k$ information bits for a fixed $\bar{d}$
is called sample complexity ($n$). We analyze the fundamental trade-offs
between the query difficulty and the sample complexity, and show that the
sample complexity of $n=c\max\{k,(k\log k)/\bar{d}\}$ for some constant $c>0$
is necessary and sufficient to recover $k$ information bits with high
probability as $k\to\infty$.



Audio Cover Song Identification using Convolutional Neural Network

In this paper, we propose a new approach to cover song identification using a
CNN (convolutional neural network). Most previous studies extract the feature
vectors that characterize the cover song relation from a pair of songs and used
it to compute the (dis)similarity between the two songs. Based on the
observation that there is a meaningful pattern between cover songs and that
this can be learned, we have reformulated the cover song identification problem
in a machine learning framework. To do this, we first build the CNN using as an
input a cross-similarity matrix generated from a pair of songs. We then
construct the data set composed of cover song pairs and non-cover song pairs,
which are used as positive and negative training samples, respectively. The
trained CNN outputs the probability of being in the cover song relation given a
cross-similarity matrix generated from any two pieces of music and identifies
the cover song by ranking on the probability. Experimental results show that
the proposed algorithm achieves performance better than or comparable to the
state-of-the-art.



Speaker identification from the sound of the human breath

This paper examines the speaker identification potential of breath sounds in
continuous speech. Speech is largely produced during exhalation. In order to
replenish air in the lungs, speakers must periodically inhale. When inhalation
occurs in the midst of continuous speech, it is generally through the mouth.
Intra-speech breathing behavior has been the subject of much study, including
the patterns, cadence, and variations in energy levels. However, an often
ignored characteristic is the {\em sound} produced during the inhalation phase
of this cycle. Intra-speech inhalation is rapid and energetic, performed with
open mouth and glottis, effectively exposing the entire vocal tract to enable
maximum intake of air. This results in vocal tract resonances evoked by
turbulence that are characteristic of the speaker's speech-producing apparatus.
Consequently, the sounds of inhalation are expected to carry information about
the speaker's identity. Moreover, unlike other spoken sounds which are subject
to active control, inhalation sounds are generally more natural and less
affected by voluntary influences. The goal of this paper is to demonstrate that
breath sounds are indeed bio-signatures that can be used to identify speakers.
We show that these sounds by themselves can yield remarkably accurate speaker
recognition with appropriate feature representations and classification
frameworks.



Rapid point-of-care Hemoglobin measurement through low-cost optics and Convolutional Neural Network based validation

A low-cost, robust, and simple mechanism to measure hemoglobin would play a
critical role in the modern health infrastructure. Consistent sample
acquisition has been a long-standing technical hurdle for photometer-based
portable hemoglobin detectors which rely on micro cuvettes and dry chemistry.
Any particulates (e.g. intact red blood cells (RBCs), microbubbles, etc.) in a
cuvette's sensing area drastically impact optical absorption profile, and
commercial hemoglobinometers lack the ability to automatically detect faulty
samples. We present the ground-up development of a portable, low-cost and open
platform with equivalent accuracy to medical-grade devices, with the addition
of CNN-based image processing for rapid sample viability prechecks. The
developed platform has demonstrated precision to the nearest $0.18[g/dL]$ of
hemoglobin, an R^2 = 0.945 correlation to hemoglobin absorption curves reported
in literature, and a 97% detection accuracy of poorly-prepared samples. We see
the developed hemoglobin device/ML platform having massive implications in
rural medicine, and consider it an excellent springboard for robust deep
learning optical spectroscopy: a currently untapped source of data for
detection of countless analytes.



Tensors, Learning, and 'Kolmogorov Extension' for Finite-alphabet Random Vectors

Estimating the joint probability mass function (PMF) of a set of random
variables lies at the heart of statistical learning and signal processing.
Without structural assumptions, such as modeling the variables as a Markov
chain, tree, or other graphical model, joint PMF estimation is often considered
mission impossible - the number of unknowns grows exponentially with the number
of variables. But who gives us the structural model? Is there a generic,
`non-parametric' way to control joint PMF complexity without relying on a
priori structural assumptions regarding the underlying probability model? Is it
possible to discover the operational structure without biasing the analysis up
front? What if we only observe random subsets of the variables, can we still
reliably estimate the joint PMF of all? This paper shows, perhaps surprisingly,
that if the joint PMF of any three variables can be estimated, then the joint
PMF of all the variables can be provably recovered under relatively mild
conditions. The result is reminiscent of Kolmogorov's extension theorem -
consistent specification of lower-dimensional distributions induces a unique
probability measure for the entire process. The difference is that for
processes of limited complexity (rank of the high-dimensional PMF) it is
possible to obtain complete characterization from only three-dimensional
distributions. In fact not all three-dimensional PMFs are needed; and under
more stringent conditions even two-dimensional will do. Exploiting multilinear
algebra, this paper proves that such higher-dimensional PMF completion can be
guaranteed - several pertinent identifiability results are derived. It also
provides a practical and efficient algorithm to carry out the recovery task.
Judiciously designed simulations and real-data experiments on movie
recommendation and data classification are presented to showcase the
effectiveness of the approach.



Micro Hand Gesture Recognition System Using Ultrasonic Active Sensing

In this paper, we propose a micro hand gesture recognition system and methods
using ultrasonic active sensing. This system uses micro dynamic hand gestures
for recognition to achieve human-computer interaction (HCI). The implemented
system, called hand-ultrasonic gesture (HUG), consists of ultrasonic active
sensing, pulsed radar signal processing, and time-sequence pattern recognition
by machine learning. We adopt lower frequency (300 kHz) ultrasonic active
sensing to obtain high resolution range-Doppler image features. Using high
quality sequential range-Doppler features, we propose a state-transition-based
hidden Markov model for gesture recognition. This method achieves a recognition
accuracy of nearly 90\% by using symbolized range-Doppler features and
significantly reduces the computational complexity and power consumption.
Furthermore, to achieve higher classification accuracy, we utilize an
end-to-end neural network model and obtain a recognition accuracy of 96.32\%.
In addition to offline analysis, a real-time prototype is released to verify
our method's potential for application in the real world.



Utilizing Domain Knowledge in End-to-End Audio Processing

End-to-end neural network based approaches to audio modelling are generally
outperformed by models trained on high-level data representations. In this
paper we present preliminary work that shows the feasibility of training the
first layers of a deep convolutional neural network (CNN) model to learn the
commonly-used log-scaled mel-spectrogram transformation. Secondly, we
demonstrate that upon initializing the first layers of an end-to-end CNN
classifier with the learned transformation, convergence and performance on the
ESC-50 environmental sound classification dataset are similar to a CNN-based
model trained on the highly pre-processed log-scaled mel-spectrogram features.



Fast-SSC-Flip Decoding of Polar Codes

Polar codes are widely considered as one of the most exciting recent
discoveries in channel coding. For short to moderate block lengths, their
error-correction performance under list decoding can outperform that of other
modern error-correcting codes. However, high-speed list-based decoders with
moderate complexity are challenging to implement. Successive-cancellation
(SC)-flip decoding was shown to be capable of a competitive error-correction
performance compared to that of list decoding with a small list size, at a
fraction of the complexity, but suffers from a variable execution time and a
higher worst-case latency. In this work, we show how to modify the
state-of-the-art high-speed SC decoding algorithm to incorporate the SC-flip
ideas. The algorithmic improvements are presented as well as average
execution-time results tailored to a hardware implementation. The results show
that the proposed fast-SSC-flip algorithm has a decoding speed close to an
order of magnitude better than the previous works while retaining a comparable
error-correction performance.



WiSpeed: A Statistical Electromagnetic Approach for Device-Free Indoor Speed Estimation

Due to the severe multipath effect, no satisfactory device-free methods have
ever been found for indoor speed estimation problem, especially in
non-line-of-sight scenarios, where the direct path between the source and
observer is blocked. In this paper, we present WiSpeed, a universal
low-complexity indoor speed estimation system leveraging radio signals, such as
commercial WiFi, LTE, 5G, etc., which can work in both device-free and
device-based situations. By exploiting the statistical theory of
electromagnetic waves, we establish a link between the autocorrelation function
of the physical layer channel state information and the speed of a moving
object, which lays the foundation of WiSpeed. WiSpeed differs from the other
schemes requiring strong line-of-sight conditions between the source and
observer in that it embraces the rich-scattering environment typical for
indoors to facilitate highly accurate speed estimation. Moreover, as a
calibration-free system, WiSpeed saves the users' efforts from large-scale
training and fine-tuning of system parameters. In addition, WiSpeed could
extract the stride length as well as detect abnormal activities such as falling
down, a major threat to seniors that leads to a large number of fatalities
every year. Extensive experiments show that WiSpeed achieves a mean absolute
percentage error of 4.85% for device-free human walking speed estimation and
4.62% for device-based speed estimation, and a detection rate of 95% without
false alarms for fall detection.



Polar Coding for the Large Hadron Collider: Challenges in Code Concatenation

In this work, we present a concatenated repetition-polar coding scheme that
is aimed at applications requiring highly unbalanced unequal bit-error
protection, such as the Beam Interlock System of the Large Hadron Collider at
CERN. Even though this concatenation scheme is simple, it reveals significant
challenges that may be encountered when designing a concatenated scheme that
uses a polar code as an inner code, such as error correlation and unusual
decision log-likelihood ratio distributions. We explain and analyze these
challenges and we propose two ways to overcome them.



Estimating Target-Object Shape Using Location-Unknown Mobile Fixed-Direction Distance Sensors

This paper proposes a method of estimating a target-object shape, the
location of which is unknown, through the use of location-unknown mobile
distance sensors. The direction of the sensor is fixed from the moving
direction. Typically, mobile sensors are mounted on vehicles. Each sensor
continuously measures the distance from it to the target object. The estimation
method does not require any positioning function, anchor-location information,
or additional mechanisms to obtain side information such as angle of arrival of
signal. Under the assumption of a polygon target object, each edge length and
vertex angle and their combinations are estimated to completely estimate the
shape of the target object.



Observing and Tracking Bandlimited Graph Processes

One of the most crucial challenges in graph signal processing is the sampling
of bandlimited graph signals, i.e., signals that are sparse in a well-defined
graph Fourier domain. So far, the prior art is mostly focused on (sub)sampling
single snapshots of graph signals ignoring their evolution over time. However,
time can bring forth new insights, since many real signals like sensor
measurements, biological, financial, and network signals in general, have
intrinsic correlations in both domains. In this work, {we fill this lacuna} by
jointly considering the graph-time nature of graph signals, named \emph{graph
processes} for two main tasks: \emph{i)} observability of graph processes; and
\emph{ii)} tracking of graph processes via Kalman filtering; both from a
(possibly time-varying) subset of nodes. A detailed mathematical analysis
ratifies the proposed methods and provides insights into the role played by the
different actors, such as the graph topology, the process bandwidth, and the
sampling strategy. Moreover, (sub)optimal sampling strategies that jointly
exploit the nature of the graph structure and graph process are proposed.
Several numerical tests on both synthetic and real data validate our
theoretical findings and illustrate the performance of the proposed methods in
coping with time-varying graph signals.



Graph Signal Processing: Overview, Challenges and Applications

Research in Graph Signal Processing (GSP) aims to develop tools for
processing data defined on irregular graph domains. In this paper we first
provide an overview of core ideas in GSP and their connection to conventional
digital signal processing. We then summarize recent developments in developing
basic GSP tools, including methods for sampling, filtering or graph learning.
Next, we review progress in several application areas using GSP, including
processing and analysis of sensor network data, biological data, and
applications to image processing and machine learning. We finish by providing a
brief historical perspective to highlight how concepts recently developed in
GSP build on top of prior research in other areas.



Visual Features for Context-Aware Speech Recognition

Automatic transcriptions of consumer-generated multi-media content such as
"Youtube" videos still exhibit high word error rates. Such data typically
occupies a very broad domain, has been recorded in challenging conditions, with
cheap hardware and a focus on the visual modality, and may have been
post-processed or edited. In this paper, we extend our earlier work on adapting
the acoustic model of a DNN-based speech recognition system to an RNN language
model and show how both can be adapted to the objects and scenes that can be
automatically detected in the video. We are working on a corpus of "how-to"
videos from the web, and the idea is that an object that can be seen ("car"),
or a scene that is being detected ("kitchen") can be used to condition both
models on the "context" of the recording, thereby reducing perplexity and
improving transcription. We achieve good improvements in both cases and compare
and analyze the respective reductions in word error rate. We expect that our
results can be used for any type of speech processing in which "context"
information is available, for example in robotics, man-machine interaction, or
when indexing large audio-visual archives, and should ultimately help to bring
together the "video-to-text" and "speech-to-text" communities.



Note on improvement precision of recursive function simulation in floating point standard

An improvement on precision of recursive function simulation in IEEE floating
point standard is presented. It is shown that the average of rounding towards
negative infinite and rounding towards positive infinite yields a better result
than the usual standard rounding to the nearest in the simulation of recursive
functions. In general, the method improves one digit of precision and it has
also been useful to avoid divergence from a correct stationary regime in the
logistic map. Numerical studies are presented to illustrate the method.



Performance Improvement of Time-Balance Radar Schedulers Through Decision Policies (Extended Version)

The resource management of a phase array system capable of multiple target
tracking and surveillance is critical for the realization of its full
potential. Present work aims to improve the performance of an existing method,
time-balance scheduling, by establishing an analogy with a well-known
stochastic control problem, the machine replacement problem. With the suggested
policy, the scheduler can adapt to the operational scenario without a
significant sacrifice from the practicality of the time-balance schedulers.
More specifically, the numerical experiments indicate that the schedulers
directed with the suggested policy can successfully trade the unnecessary track
updates, say of non-maneuvering targets, with the updates of targets with
deteriorating tracks, say of rapidly maneuvering targets, yielding an overall
improvement in the tracking performance.



Distributed Topology Design for Network Coding Deployed Large-scale Sensor Networks

In this paper, we propose a solution to the distributed topology formation
problem for large-scale sensor networks with multi-source multicast flows. The
proposed solution is based on game-theoretic approaches in conjunction with
network coding. The proposed algorithm requires significantly low computational
complexity, while it is known as NP-hard to find an optimal topology for
network coding deployed multi-source multicast flows. In particular, we
formulate the problem of distributed network topology formation as a network
formation game by considering the nodes in the network as players that can take
actions for making outgoing links. The proposed solution decomposes the
original game that consists of multiple players and multicast flows into
independent link formation games played by only two players with a unicast
flow. We also show that the proposed algorithm is guaranteed to determine at
least one stable topology. Our simulation results confirm that the
computational complexity of the proposed solution is low enough for practical
deployment in large-scale networks.



Network Coding Based Evolutionary Network Formation for Dynamic Wireless Networks

In this paper, we aim to find a robust network formation strategy that can
adaptively evolve the network topology against network dynamics in a
distributed manner. We consider a network coding deployed wireless ad hoc
network where source nodes are connected to terminal nodes with the help of
intermediate nodes. We show that mixing operations in network coding can induce
packet anonymity that allows the inter-connections in a network to be
decoupled. This enables each intermediate node to consider complex network
inter-connections as a node-environment interaction such that the Markov
decision process (MDP) can be employed at each intermediate node. The optimal
policy that can be obtained by solving the MDP provides each node with optimal
amount of changes in transmission range given network dynamics (e.g., the
number of nodes in the range and channel condition). Hence, the network can be
adaptively and optimally evolved by responding to the network dynamics. The
proposed strategy is used to maximize long-term utility, which is achieved by
considering both current network conditions and future network dynamics. We
define the utility of an action to include network throughput gain and the cost
of transmission power. We show that the resulting network of the proposed
strategy eventually converges to stationary networks, which maintain the states
of the nodes. Moreover, we propose to determine initial transmission ranges and
initial network topology that can expedite the convergence of the proposed
algorithm. Our simulation results confirm that the proposed strategy builds a
network which adaptively changes its topology in the presence of network
dynamics. Moreover, the proposed strategy outperforms existing strategies in
terms of system goodput and successful connectivity ratio.



Coded Caching in a Multi-Server System with Random Topology

Cache-aided content delivery is studied in a multi-server system with $P$
servers and $K$ users, each equipped with a local cache memory. In the delivery
phase, each user connects randomly to any $\rho$ out of $P$ servers. Thanks to
the availability of multiple servers, which model small base stations with
limited storage capacity, user demands can be satisfied with reduced storage
capacity at each server and reduced delivery rate per server; however, this
also leads to reduced multicasting opportunities compared to a single server
serving all the users simultaneously. A joint storage and proactive caching
scheme is proposed, which exploits coded storage across the servers, uncoded
cache placement at the users, and coded delivery. The delivery \textit{latency}
is studied for both \textit{successive} and \textit{simultaneous} transmission
from the servers. It is shown that, with successive transmission the achievable
average delivery latency is comparable to that achieved by a single server,
while the gap between the two depends on $\rho$, the available redundancy
across servers, and can be reduced by increasing the storage capacity at the
SBSs.



Evaluation of Alzheimer's Disease by Analysis of MR Images using Multilayer Perceptrons and Kohonen SOM Classifiers as an Alternative to the ADC Maps

Alzheimer's disease is the most common cause of dementia, yet hard to
diagnose precisely without invasive techniques, particularly at the onset of
the disease. This work approaches image analysis and classification of
synthetic multispectral images composed by diffusion-weighted magnetic
resonance (MR) cerebral images for the evaluation of cerebrospinal fluid area
and measuring the advance of Alzheimer's disease. A clinical 1.5 T MR imaging
system was used to acquire all images presented. The classification methods are
based on multilayer perceptrons and Kohonen Self-Organized Map classifiers. We
assume the classes of interest can be separated by hyperquadrics. Therefore, a
2-degree polynomial network is used to classify the original image, generating
the ground truth image. The classification results are used to improve the
usual analysis of the apparent diffusion coefficient map.



Reconstruction of Electrical Impedance Tomography Using Fish School Search, Non-Blind Search, and Genetic Algorithm

Electrical Impedance Tomography (EIT) is a noninvasive imaging technique that
does not use ionizing radiation, with application both in environmental
sciences and in health. Image reconstruction is performed by solving an inverse
problem and ill-posed. Evolutionary Computation and Swarm Intelligence have
become a source of methods for solving inverse problems. Fish School Search
(FSS) is a promising search and optimization method, based on the dynamics of
schools of fish. In this article the authors present a method for
reconstruction of EIT images based on FSS and Non-Blind Search (NBS). The
method was evaluated using numerical phantoms consisting of electrical
conductivity images with subjects in the center, between the center and the
edge and on the edge of a circular section, with meshes of 415 finite elements.
The authors performed 20 simulations for each configuration. Results showed
that both FSS and FSS-NBS were able to converge faster than genetic algorithms.



Femtosecond CDMA Using Dielectric Metasurfaces: Design Procedure and Challenges

Inspired by the ever-increasing demand for higher data transmission rates and
the tremendous attention toward all-optical signal processing based on
miniaturized nanophotonics, in this paper, for the first time, we investigate
the integrable design of coherent ultrashort light pulse code-division
multiple-access (CDMA) technique, also known as femtosecond CDMA, using
all-dielectric metasurfaces (MSs). In this technique, the data bits are firstly
modulated using ultrashort femtosecond optical pulses generated by mode-locked
lasers, and then by employing a unique phase metamask for each data stream, in
order to provide the multiple access capability, the optical signals are
spectrally encoded. This procedure spreads the optical signal in the temporal
domain and generates low-intensity pseudo-noise bursts through random phase
coding leading to minimized multiple access interference. This paper
comprehensively presents the principles and design approach to realize
fundamental components of a typical femtosecond CDMA encoder, including the
grating, lens, and phase mask, by employing high-contrast CMOS-compatible MSs.
By controlling the interference between the provided Mie and Fabry-Perot
resonance modes, we tailor the spectral and spatial responses of the impinging
light locally and independently. Accordingly, we design a MS-based grating with
the highest possible refracted angle and, in the meantime, the maximized
efficiency which results in a reasonable diameter for the subsequent lens.
Moreover, to design our MS-based lens commensurate with the spot size and
distance requirements of the pursuant phase mask, we leverage a new
optimization method which splits the lens structure into central and peripheral
parts, and then design the peripheral part using a collection of gratings
converging the impinging at the subsequent phase mask.



EDIZ: An Error Diffusion Image Zooming Scheme

Interpolation based image zooming methods provide a high execution speed and
low computational complexity. However, the quality of the zoomed images is
unsatisfactory in many cases. The main challenge of super- resolution methods
is to create new details to the image. This paper proposes a new algorithm to
create new details using a zoom-out-zoom-in strategy. This strategy permits
reducing blurring effects by adding the estimated error to the final image.
Experimental results for natural images confirm the algorithm's ability to
create visually pleasing results.



Raw Waveform-based Audio Classification Using Sample-level CNN Architectures

Music, speech, and acoustic scene sound are often handled separately in the
audio domain because of their different signal characteristics. However, as the
image domain grows rapidly by versatile image classification models, it is
necessary to study extensible classification models in the audio domain as
well. In this study, we approach this problem using two types of sample-level
deep convolutional neural networks that take raw waveforms as input and uses
filters with small granularity. One is a basic model that consists of
convolution and pooling layers. The other is an improved model that
additionally has residual connections, squeeze-and-excitation modules and
multi-level concatenation. We show that the sample-level models reach
state-of-the-art performance levels for the three different categories of
sound. Also, we visualize the filters along layers and compare the
characteristics of learned filters.



Transient Stability Assessment Using Individual Machine Equal Area Criterion Part I: Unity Principle

Analyzing system trajectory from the perspective of individual machines
provides a distinctive angle to analyze the transient stability of power
systems. This two-paper series propose a direct-time-domain method that is
based on the individual-machine equal area criterion. In the first paper, by
examining the mapping between the trajectory and power-vs-angle curve of an
individual machine, the stability property to characterize a critical machine
is clarified. The mapping between the system trajectory and individual-machine
equal area criterion is established. Furthermore, a unity principle between the
individual-machine stability and system stability is proposed. It is proved
that the instability of the system can be confirmed by finding any one unstable
critical machine, thence, the transient stability of a multimachine system can
be monitored in an individual-machine way in transient stability assessment.



Transient Stability Assessment Using Individual Machine Equal Area Criterion Part II: Stability Margin

In the second part of this two-paper series, the stability margin of a
critical machine and that of the system are first proposed, and then the
concept of non-global stability margin is illustrated. Based on the crucial
statuses of the leading unstable critical machine and the most severely
disturbed critical machine, the critical stability of the system from the
perspective of an individual machine is analyzed. In the end of this paper,
comparisons between the proposed method and classic global methods are
demonstrated.



FMI Compliant Approach to Investigate the Impact of Communication to Islanded Microgrid Secondary Control

In multi-master islanded microgrids, the inverter controllers need to share
the signals and to coordinate, in either centralized or distributed way, in
order to operate properly and to assure a good functionality of the grid. The
central controller is used in centralized strategy. In distributed control,
Multi-agent system (MAS) is considered to be a suitable solution for
coordination of such system. However the latency and disturbance of the network
may disturb the communication from central controller to local controllers or
among agents or and negatively influence the grid operation. As a consequence,
communication aspects need to be properly addressed during the control design
and assessment. In this paper, we propose a holistic approach with
co-simulation using Functional Mockup Interface (FMI) standard to validate the
microgrid control system taking into account the communication network. A
use-case of islanded microgrid frequency secondary control with MAS under
consensus algorithm is implemented to demonstrate the impact of communication
and to illustrate the proposed holistic approach.



A text-independent speaker verification model: A comparative analysis

The most pressing challenge in the field of voice biometrics is selecting the
most efficient technique of speaker recognition. Every individual's voice is
peculiar, factors like physical differences in vocal organs, accent and
pronunciation contributes to the problem's complexity. In this paper, we
explore the various methods available in each block in the process of speaker
recognition with the objective to identify best of techniques that could be
used to get precise results. We study the results on text independent corpora.
We use MFCC (Melfrequency cepstral coefficient), LPCC (linear predictive
cepstral coefficient) and PLP (perceptual linear prediction) algorithms for
feature extraction, PCA (Principal Component Analysis) and tSNE for
dimensionality reduction and SVM (Support Vector Machine), feed forward,
nearest neighbor and decision tree algorithms for classification block in
speaker recognition system and comparatively analyze each block to determine
the best technique



Chord Generation from Symbolic Melody Using BLSTM Networks

Generating a chord progression from a monophonic melody is a challenging
problem because a chord progression requires a series of layered notes played
simultaneously. This paper presents a novel method of generating chord
sequences from a symbolic melody using bidirectional long short-term memory
(BLSTM) networks trained on a lead sheet database. To this end, a group of
feature vectors composed of 12 semitones is extracted from the notes in each
bar of monophonic melodies. In order to ensure that the data shares uniform key
and duration characteristics, the key and the time signatures of the vectors
are normalized. The BLSTM networks then learn from the data to incorporate the
temporal dependencies to produce a chord progression. Both quantitative and
qualitative evaluations are conducted by comparing the proposed method with the
conventional HMM and DNN-HMM based approaches. Proposed model achieves 23.8%
and 11.4% performance increase from the other models, respectively. User
studies further confirm that the chord sequences generated by the proposed
method are preferred by listeners.



A rigorous evaluation of the intermittence in the logistic map using lower bound error

This article investigates the maximum time of simulation in which the
phenomenon of the intermittence can be observed with numerical confidence in
discrete maps. Interval analysis and the lower error limit were used. As a
result, it was observed that the reliability of the intermittency is dependent
on the initial condition. Four numerical examples show the efficiency of the
proposal.



Ultrasonic Tissue Reflectivity Function Estimation Using Correlation Constrained Multichannel FLMS Algorithm with Missing RF Data

Poor resolution of ultrasound images due to convolution of the tissue
reflectivity function (TRF) with the system point spread function (PSF) is a
major issue in medical ultrasound imaging. In this paper, we propose a
correlation constrained missing-data estimation based blind multichannel
frequency- domain least-mean-squares (md-bMCFLMS) algorithm to undo the effect
of PSF on the ultrasound radio-frequency (RF) data. In the first step, a
block-based MCFLMS (bMCFLMS) algorithm is proposed to estimate the TRFs and the
PSF which are used in the second step to estimate the missing data. This
missing data is used in the md-bMCFLMS algorithm to construct a modified cost
function for further improvement of the image resolution. To account for the
nonstationarity of the PSF, unlike the blocking approach described in the
literature, we introduce a time-efficient blocking method in this paper. The
blocking approach described here uses a block position independent fixed size
matrix and can be implemented parallely. The bMCFLMS algorithm, however, shows
misconvergence due to both channel noise and propagation of TRF estimation
error from the previous blocks. This phe- nomenon is more intense in the case
of md-bMCFLMS algorithm because of increased estimation error. To address this
problem, a novel constraint based on the correlation between the measured RF
data and estimated TRF is proposed in this paper. The efficacy of our proposed
blind deconvolution algorithm is measured using simulation phantom,
experimental phantom and in-vivo data.



Synchronization on the accuracy of chaotic oscillators simulations

Numerical problems are considered on general synchronization of chaotic
oscillators, through the evaluation of the Lower Bound Error index on two case
studies: a Lorenz system unidirectionally coupled to a Duffing system and a
Duffing system unidirectionally coupled to a Rossler system. It was possible to
observe, in each case, that the behavior of the slave's LBE curve tends to
follow the behavior of the master's as the value of the coupling constant is
increased up to a certain value, and thus, that synchronization can affect
numerical calculations.



Sparse recovery of undersampled intensity patterns for coherent diffraction imaging at high X-ray energies

Coherent X-ray photons with energies higher than 50 keV offer new
possibilities for imaging nanoscale lattice distortions in bulk crystalline
materials using Bragg peak phase retrieval methods. However, the compression of
reciprocal space at high energies typically results in poorly resolved fringes
on an area detector, rendering the diffraction data unsuitable for the
three-dimensional reconstruction of compact crystals. To address this problem,
we propose a method by which to recover fine fringe detail in the scattered
intensity. This recovery is achieved in two steps: multiple undersampled
measurements are made by in-plane sub-pixel motion of the area detector, then
this data set is passed to a sparsity-based numerical solver that recovers
fringe detail suitable for standard Bragg coherent diffraction imaging (BCDI)
reconstruction methods of compact single crystals. The key insight of this
paper is that sparsity in a BCDI data set can be enforced by recognising that
the signal in the detector, though poorly resolved, is band-limited. This
requires fewer in-plane detector translations for complete signal recovery,
while adhering to information theory limits. We use simulated BCDI data sets to
demonstrate the approach, outline our sparse recovery strategy, and comment on
future opportunities.



Study of Robust Distributed Beamforming Based on Cross-Correlation and Subspace Projection Techniques

In this work, we present a novel robust distributed beamforming (RDB)
approach to mitigate the effects of channel errors on wireless networks
equipped with relays based on the exploitation of the cross-correlation between
the received data from the relays at the destination and the system output. The
proposed RDB method, denoted cross-correlation and subspace projection (CCSP)
RDB, considers a total relay transmit power constraint in the system and the
objective of maximizing the output signal-to-interference-plus-noise ratio
(SINR). The relay nodes are equipped with an amplify-and-forward (AF) protocol
and we assume that the channel state information (CSI) is imperfectly known at
the relays and there is no direct link between the sources and the destination.
The CCSP does not require any costly optimization procedure and simulations
show an excellent performance as compared to previously reported algorithms.



Wavenet based low rate speech coding

Traditional parametric coding of speech facilitates low rate but provides
poor reconstruction quality because of the inadequacy of the model used. We
describe how a WaveNet generative speech model can be used to generate high
quality speech from the bit stream of a standard parametric coder operating at
2.4 kb/s. We compare this parametric coder with a waveform coder based on the
same generative model and show that approximating the signal waveform incurs a
large rate penalty. Our experiments confirm the high performance of the WaveNet
based coder and show that the speech produced by the system is able to
additionally perform implicit bandwidth extension and does not significantly
impair recognition of the original speaker for the human listener, even when
that speaker has not been used during the training of the generative model.



RF-Based Direction Finding of UAVs Using DNN

This paper presents a sparse denoising autoencoder (SDAE)-based deep neural
network (DNN) for the direction finding (DF) of small unmanned aerial vehicles
(UAVs). It is motivated by the practical challenges associated with classical
DF algorithms such as MUSIC and ESPRIT. The proposed DF scheme is practical and
low-complex in the sense that a phase synchronization mechanism, an antenna
calibration mechanism, and the analytical model of the antenna radiation
pattern are not essential. Also, the proposed DF method can be implemented
using a single-channel RF receiver. The paper validates the proposed method
experimentally as well.



State Estimation in Power Distribution Systems Based on Ensemble Kalman Filtering

State estimation in power distribution systems is a key component for
increased reliability and optimal system performance. Well understood in
transmission systems, state estimation is now an area of active research in
distribution networks. While several snapshot-based approaches have been used
to solve this problem, few solutions have been proposed in a dynamic framework.
In this paper, a Past-Aware State Estimation (PASE) method is proposed for
distribution systems that takes previous estimates into account to improve the
accuracy of the current one, using an Ensemble Kalman Filter. Fewer phasor
measurements units (PMU) are needed to achieve the same estimation error target
than snapshot-based methods. Contrary to current methods, the proposed solution
does not embed power flow equations into the estimator. A theoretical
formulation is presented to compute a priori the advantages of the proposed
method vis-a-vis the state-of-the-art. The proposed approach is validated
considering the 33-bus distribution system and using power consumption traces
from real households.



Precision Scaling of Neural Networks for Efficient Audio Processing

While deep neural networks have shown powerful performance in many audio
applications, their large computation and memory demand has been a challenge
for real-time processing. In this paper, we study the impact of scaling the
precision of neural networks on the performance of two common audio processing
tasks, namely, voice-activity detection and single-channel speech enhancement.
We determine the optimal pair of weight/neuron bit precision by exploring its
impact on both the performance and processing time. Through experiments
conducted with real user data, we demonstrate that deep neural networks that
use lower bit precision significantly reduce the processing time (up to 30x).
However, their performance impact is low (< 3.14%) only in the case of
classification tasks such as those present in voice activity detection.



Learning to Fuse Music Genres with Generative Adversarial Dual Learning

FusionGAN is a novel genre fusion framework for music generation that
integrates the strengths of generative adversarial networks and dual learning.
In particular, the proposed method offers a dual learning extension that can
effectively integrate the styles of the given domains. To efficiently quantify
the difference among diverse domains and avoid the vanishing gradient issue,
FusionGAN provides a Wasserstein based metric to approximate the distance
between the target domain and the existing domains. Adopting the Wasserstein
distance, a new domain is created by combining the patterns of the existing
domains using adversarial learning. Experimental results on public music
datasets demonstrated that our approach could effectively merge two genres.



Gridless Two-dimensional DOA Estimation With L-shaped Array Based on the Cross-covariance Matrix

The atomic norm minimization (ANM) has been successfully incorporated into
the two-dimensional (2-D) direction-of-arrival (DOA) estimation problem for
super-resolution. However, its computational workload might be unaffordable
when the number of snapshots is large. In this paper, we propose two gridless
methods for 2-D DOA estimation with L-shaped array based on the atomic norm to
improve the computational efficiency. Firstly, by exploiting the
cross-covariance matrix an ANM-based model has been proposed. We then prove
that this model can be efficiently solved as a semi-definite programming (SDP).
Secondly, a modified model has been presented to improve the estimation
accuracy. It is shown that our proposed methods can be applied to both uniform
and sparse L-shaped arrays and do not require any knowledge of the number of
sources. Furthermore, since our methods greatly reduce the model size as
compared to the conventional ANM method, and thus are much more efficient.
Simulations results are provided to demonstrate the advantage of our methods.



Multi-Dialect Speech Recognition With A Single Sequence-To-Sequence Model

Sequence-to-sequence models provide a simple and elegant solution for
building speech recognition systems by folding separate components of a typical
system, namely acoustic (AM), pronunciation (PM) and language (LM) models into
a single neural network. In this work, we look at one such sequence-to-sequence
model, namely listen, attend and spell (LAS), and explore the possibility of
training a single model to serve different English dialects, which simplifies
the process of training multi-dialect systems without the need for separate AM,
PM and LMs for each dialect. We show that simply pooling the data from all
dialects into one LAS model falls behind the performance of a model fine-tuned
on each dialect. We then look at incorporating dialect-specific information
into the model, both by modifying the training targets by inserting the dialect
symbol at the end of the original grapheme sequence and also feeding a 1-hot
representation of the dialect information into all layers of the model.
Experimental results on seven English dialects show that our proposed system is
effective in modeling dialect variations within a single LAS model,
outperforming a LAS model trained individually on each of the seven dialects by
3.1 ~ 16.5% relative.



Fuzzy-Based Dialectical Non-Supervised Image Classification and Clustering

The materialist dialectical method is a philosophical investigative method to
analyze aspects of reality. These aspects are viewed as complex processes
composed by basic units named poles, which interact with each other. Dialectics
has experienced considerable progress in the 19th century, with Hegel's
dialectics and, in the 20th century, with the works of Marx, Engels, and
Gramsci, in Philosophy and Economics. The movement of poles through their
contradictions is viewed as a dynamic process with intertwined phases of
evolution and revolutionary crisis. In order to build a computational process
based on dialectics, the interaction between poles can be modeled using fuzzy
membership functions. Based on this assumption, we introduce the Objective
Dialectical Classifier (ODC), a non-supervised map for classification based on
materialist dialectics and designed as an extension of fuzzy c-means
classifier. As a case study, we used ODC to classify 181 magnetic resonance
synthetic multispectral images composed by proton density, $T_1$- and
$T_2$-weighted synthetic brain images. Comparing ODC to k-means, fuzzy c-means,
and Kohonen's self-organized maps, concerning with image fidelity indexes as
estimatives of quantization distortion, we proved that ODC can reach almost the
same quantization performance as optimal non-supervised classifiers like
Kohonen's self-organized maps.



Triagem virtual de imagens de imuno-histoqu\'imica usando redes neurais artificiais e espectro de padr\~oes

The importance of organizing medical images according to their nature,
application and relevance is increasing. Furhermore, a previous selection of
medical images can be useful to accelerate the task of analysis by
pathologists. Herein this work we propose an image classifier to integrate a
CBIR (Content-Based Image Retrieval) selection system. This classifier is based
on pattern spectra and neural networks. Feature selection is performed using
pattern spectra and principal component analysis, whilst image classification
is based on multilayer perceptrons and a composition of self-organizing maps
and learning vector quantization. These methods were applied for content
selection of immunohistochemical images of placenta and newdeads lungs. Results
demonstrated that this approach can reach reasonable classification
performance.



Avalia\c{c}\~ao do m\'etodo dial\'etico na quantiza\c{c}\~ao de imagens multiespectrais

The unsupervised classification has a very important role in the analysis of
multispectral images, given its ability to assist the extraction of a priori
knowledge of images. Algorithms like k-means and fuzzy c-means has long been
used in this task. Computational Intelligence has proven to be an important
field to assist in building classifiers optimized according to the quality of
the grouping of classes and the evaluation of the quality of vector
quantization. Several studies have shown that Philosophy, especially the
Dialectical Method, has served as an important inspiration for the construction
of new computational methods. This paper presents an evaluation of four methods
based on the Dialectics: the Objective Dialectical Classifier and the
Dialectical Optimization Method adapted to build a version of k-means with
optimal quality indices; each of them is presented in two versions: a canonical
version and another version obtained by applying the Principle of Maximum
Entropy. These methods were compared to k-means, fuzzy c-means and Kohonen's
self-organizing maps. The results showed that the methods based on Dialectics
are robust to noise, and quantization can achieve results as good as those
obtained with the Kohonen map, considered an optimal quantizer.



Dialectical Multispectral Classification of Diffusion-Weighted Magnetic Resonance Images as an Alternative to Apparent Diffusion Coefficients Maps to Perform Anatomical Analysis

Multispectral image analysis is a relatively promising field of research with
applications in several areas, such as medical imaging and satellite
monitoring. A considerable number of current methods of analysis are based on
parametric statistics. Alternatively, some methods in Computational
Intelligence are inspired by biology and other sciences. Here we claim that
Philosophy can be also considered as a source of inspiration. This work
proposes the Objective Dialectical Method (ODM): a method for classification
based on the Philosophy of Praxis. ODM is instrumental in assembling evolvable
mathematical tools to analyze multispectral images. In the case study described
in this paper, multispectral images are composed of diffusion-weighted (DW)
magnetic resonance (MR) images. The results are compared to ground-truth images
produced by polynomial networks using a morphological similarity index. The
classification results are used to improve the usual analysis of the apparent
diffusion coefficient map. Such results proved that gray and white matter can
be distinguished in DW-MR multispectral analysis and, consequently, DW-MR
images can also be used to furnish anatomical information.



Avalia\c{c}\~ao da doen\c{c}a de Alzheimer pela an\'alise multiespectral de imagens DW-MR por redes RBF como alternativa aos mapas ADC

Alzheimer's disease is the most common cause of dementia, yet difficult to
accurately diagnose without the use of invasive techniques, particularly at the
beginning of the disease. This work addresses the classification and analysis
of multispectral synthetic images composed by diffusion-weighted magnetic
resonance brain volumes for evaluation of the area of cerebrospinal fluid and
its correlation with the progression of Alzheimer's disease. A 1.5 T MR imaging
system was used to acquire all the images presented. The classification methods
are based on multilayer perceptrons and classifiers of radial basis function
networks. It is assumed that the classes of interest can be separated by
hyperquadrics. A polynomial network of degree 2 is used to classify the
original volumes, generating a ground-truth volume. The classification results
are used to improve the usual analysis by the map of apparent diffusion
coefficients.



A Tensor Completion Approach for Efficient and Robust Fingerprint-based Indoor Localization

The localization technology is important for the development of indoor
location-based services (LBS). The radio frequency (RF) fingerprint-based
localization is one of the most promising approaches. However, it is
challenging to apply this localization to real-world environments since it is
time-consuming and labor-intensive to construct a fingerprint database as a
prior for localization. Another challenge is that the presence of anomaly
readings in the fingerprints reduces the localization accuracy. To address
these two challenges, we propose an efficient and robust indoor localization
approach. First, we model the fingerprint database as a 3-D tensor, which
represents the relationships between fingerprints, locations and indices of
access points. Second, we introduce a tensor decomposition model for robust
fingerprint data recovery, which decomposes a partial observation tensor as the
superposition of a low-rank tensor and a spare anomaly tensor. Third, we
exploit the alternating direction method of multipliers (ADMM) to solve the
convex optimization problem of tensor-nuclear-norm completion for the anomaly
case. Finally, we verify the proposed approach on a ground truth data set
collected in an office building with size 80m times 20m. Experiment results
show that to achieve a same error rate 4%, the sampling rate of our approach is
only 10%, while it is 60% for the state-of-the-art approach. Moreover, the
proposed approach leads to a more accurate localization (nearly 20%, 0.6m
improvement) over the compared approach.



Multi-speaker Recognition in Cocktail Party Problem

This paper proposes an original statistical decision theory to accomplish a
multi-speaker recognition task in cocktail party problem. This theory relies on
an assumption that the varied frequencies of speakers obey Gaussian
distribution and the relationship of their voiceprints can be represented by
Euclidean distance vectors. This paper uses Mel-Frequency Cepstral Coefficients
to extract the feature of a voice in judging whether a speaker is included in a
multi-speaker environment and distinguish who the speaker should be. Finally, a
thirteen-dimension constellation drawing is established by mapping from
Manhattan distances of speakers in order to take a thorough consideration about
gross influential factors.



Design Automation for Binarized Neural Networks: A Quantum Leap Opportunity?

Design automation in general, and in particular logic synthesis, can play a
key role in enabling the design of application-specific Binarized Neural
Networks (BNN). This paper presents the hardware design and synthesis of a
purely combinational BNN for ultra-low power near-sensor processing. We
leverage the major opportunities raised by BNN models, which consist mostly of
logical bit-wise operations and integer counting and comparisons, for pushing
ultra-low power deep learning circuits close to the sensor and coupling it with
binarized mixed-signal image sensor data. We analyze area, power and energy
metrics of BNNs synthesized as combinational networks. Our synthesis results in
GlobalFoundries 22nm SOI technology shows a silicon area of 2.61mm2 for
implementing a combinational BNN with 32x32 binary input sensor receptive field
and weight parameters fixed at design time. This is 2.2x smaller than a
synthesized network with re-configurable parameters. With respect to other
comparable techniques for deep learning near-sensor processing, our approach
features a 10x higher energy efficiency.



State-of-the-art Speech Recognition With Sequence-to-Sequence Models

Attention-based encoder-decoder architectures such as Listen, Attend, and
Spell (LAS), subsume the acoustic, pronunciation and language model components
of a traditional automatic speech recognition (ASR) system into a single neural
network. In previous work, we have shown that such architectures are comparable
to state-of-theart ASR systems on dictation tasks, but it was not clear if such
architectures would be practical for more challenging tasks such as voice
search. In this work, we explore a variety of structural and optimization
improvements to our LAS model which significantly improve performance. On the
structural side, we show that word piece models can be used instead of
graphemes. We also introduce a multi-head attention architecture, which offers
improvements over the commonly-used single-head attention. On the optimization
side, we explore synchronous training, scheduled sampling, label smoothing, and
minimum word error rate optimization, which are all shown to improve accuracy.
We present results with a unidirectional LSTM encoder for streaming
recognition. On a 12, 500 hour voice search task, we find that the proposed
changes improve the WER from 9.2% to 5.6%, while the best conventional system
achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to
5% for the conventional system.



Improving the Performance of Online Neural Transducer Models

Having a sequence-to-sequence model which can operate in an online fashion is
important for streaming applications such as Voice Search. Neural transducer is
a streaming sequence-to-sequence model, but has shown a significant degradation
in performance compared to non-streaming models such as Listen, Attend and
Spell (LAS). In this paper, we present various improvements to NT.
Specifically, we look at increasing the window over which NT computes
attention, mainly by looking backwards in time so the model still remains
online. In addition, we explore initializing a NT model from a LAS-trained
model so that it is guided with a better alignment. Finally, we explore
including stronger language models such as using wordpiece models, and applying
an external LM during the beam search. On a Voice Search task, we find with
these improvements we can get NT to match the performance of LAS.



Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models

Sequence-to-sequence models, such as attention-based models in automatic
speech recognition (ASR), are typically trained to optimize the cross-entropy
criterion which corresponds to improving the log-likelihood of the data.
However, system performance is usually measured in terms of word error rate
(WER), not log-likelihood. Traditional ASR systems benefit from discriminative
sequence training which optimizes criteria such as the state-level minimum
Bayes risk (sMBR) which are more closely related to WER. In the present work,
we explore techniques to train attention-based models to directly minimize
expected word error rate. We consider two loss functions which approximate the
expected number of word errors: either by sampling from the model, or by using
N-best lists of decoded hypotheses, which we find to be more effective than the
sampling-based method. In experimental evaluations, we find that the proposed
training procedure improves performance by up to 8.2% relative to the baseline
system. This allows us to train grapheme-based, uni-directional attention-based
models which match the performance of a traditional, state-of-the-art,
discriminative sequence-trained system on a mobile voice-search task.



No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models

For decades, context-dependent phonemes have been the dominant sub-word unit
for conventional acoustic modeling systems. This status quo has begun to be
challenged recently by end-to-end models which seek to combine acoustic,
pronunciation, and language model components into a single neural network. Such
systems, which typically predict graphemes or words, simplify the recognition
process since they remove the need for a separate expert-curated pronunciation
lexicon to map from phoneme-based units to words. However, there has been
little previous work comparing phoneme-based versus grapheme-based sub-word
units in the end-to-end modeling framework, to determine whether the gains from
such approaches are primarily due to the new probabilistic model, or from the
joint learning of the various components with grapheme-based units.
  In this work, we conduct detailed experiments which are aimed at quantifying
the value of phoneme-based pronunciation lexica in the context of end-to-end
models. We examine phoneme-based end-to-end models, which are contrasted
against grapheme-based ones on a large vocabulary English Voice-search task,
where we find that graphemes do indeed outperform phonemes. We also compare
grapheme and phoneme-based approaches on a multi-dialect English task, which
once again confirm the superiority of graphemes, greatly simplifying the system
for recognizing multiple dialects.



Geolocation with FDOA Measurements via Polynomial Systems and RANSAC

The problem of geolocation of a transmitter via time difference of arrival
(TDOA) and frequency difference of arrival (FDOA) is given as a system of
polynomial equations. This allows for the use of homotopy continuation-based
methods from numerical algebraic geometry. A novel geolocation algorithm
employs numerical algebraic geometry techniques in conjunction with the random
sample consensus (RANSAC) method. This is all developed and demonstrated in the
setting of only FDOA measurements, without loss of generality. Additionally,
the problem formulation as polynomial systems immediately provides lower bounds
on the number of receivers or measurements required for the solution set to
consist of only isolated points.



An analysis of incorporating an external language model into a sequence-to-sequence model

Attention-based sequence-to-sequence models for automatic speech recognition
jointly train an acoustic model, language model, and alignment mechanism. Thus,
the language model component is only trained on transcribed audio-text pairs.
This leads to the use of shallow fusion with an external language model at
inference time. Shallow fusion refers to log-linear interpolation with a
separately trained language model at each step of the beam search. In this
work, we investigate the behavior of shallow fusion across a range of
conditions: different types of language models, different decoding units, and
different tasks. On Google Voice Search, we demonstrate that the use of shallow
fusion with a neural LM with wordpieces yields a 9.1% relative word error rate
reduction (WERR) over our competitive attention-based sequence-to-sequence
model, obviating the need for second-pass rescoring.



Enabling Early Audio Event Detection with Neural Networks

This paper presents a methodology for early detection of audio events from
audio streams. Early detection is the ability to infer an ongoing event during
its initial stage. The proposed system consists of a novel inference step
coupled with dual parallel tailored-loss deep neural networks (DNNs). The DNNs
share a similar architecture except for their loss functions, i.e. weighted
loss and multitask loss, which are designed to efficiently cope with issues
common to audio event detection. The inference step is newly introduced to make
use of the network outputs for recognizing ongoing events. The monotonicity of
the detection function is required for reliable early detection, and will also
be proved. Experiments on the ITC-Irst database show that the proposed system
achieves state-of-the-art detection performance. Furthermore, even partial
events are sufficient to achieve good performance similar to that obtained when
an entire event is observed, enabling early event detection.



Knowledge-Aided Kaczmarz and LMS Algorithms

The least mean squares (LMS) filter is often derived via the Wiener filter
solution. For a system identification scenario, such a derivation makes it hard
to incorporate prior information on the system's impulse response. We present
an alternative way based on the maximum a posteriori solution, which allows
developing a Knowledge-Aided Kaczmarz algorithm. Based on this Knowledge-Aided
Kaczmarz we formulate a Knowledge-Aided LMS filter. Both algorithms allow
incorporating the prior mean and covariance matrix on the parameter to be
estimated. The algorithms use this prior information in addition to the
measurement information in the gradient for the iterative update of their
estimates. We analyze the convergence of the algorithms and show simulation
results on their performance. As expected, reliable prior information allows
improving the performance of the algorithms for low signal-to-noise (SNR)
scenarios. The results show that the presented algorithms can nearly achieve
the optimal maximum a posteriori (MAP) performance.



Ptychography intensity interferometry imaging

Intensity interferometry (II) exploits the second-order correlation to
acquire the spatial frequency information of an object, which has been used to
observe distant stars since 1950s. However, due to unreliability of employed
imaging reconstruction algorithms, II can only image simple and sparse objects
such as double stars. We here develop a method that overcomes this
unreliability problem and enables imaging complex objects by combing II and a
ptychography iterative algorithm. Different from previous ptychography
iterative-type algorithms that work only for diffractive objects using
coherence light sources, our method obtains the objects spatial spectrum from
the second-order correlation of intensity fluctuation by using an incoherent
source, which therefore largely simplifies the imaging process. Furthermore, by
introducing loose supports in the ptychography algorithm, a high-quality image
can be recovered without knowing the precise size and position of the scanning
illumination, which is a strong requirement for traditional ptychography
iterative algorithm.



Towards a Green and Self-Powered Internet of Things Using Piezoelectric Energy Harvesting

Internet of things (IoT) is a revolutionizing technology which aims to create
an ecosystem of connected objects and embedded devices and provide ubiquitous
connectivity between trillions of not only smart devices but also simple
sensors and actuators. Although recent advancements in miniaturization of
devices with higher computational capabilities and ultra-low power
communication technologies have enabled the vast deployment of sensors and
actuators everywhere, such an evolution calls for fundamental changes in
hardware design, software, network architecture, data analytic, data storage
and power sources. A large portion of IoT devices cannot be powered by
batteries only anymore, as they will be installed in hard to reach areas and
regular battery replacement and maintenance are infeasible. A viable solution
is to scavenge and harvest energy from environment and then provide enough
energy to the devices to perform their operations. This will significantly
increase the device life time and eliminate the need for the battery as an
energy source. This survey aims at providing a comprehensive study on energy
harvesting techniques as alternative and promising solutions to power IoT
devices. We present the main design challenges of IoT devices in terms of
energy and power and provide design considerations for a successful
implementations of self-powered IoT devices. We then specifically focus on
piezoelectric energy harvesting and RF energy harvesting as most promising
solutions to power IoT devices and present the main challenges and research
directions. We also shed light on the security challenges of energy harvesting
enabled IoT systems and green big data.



Beam Alignment and Tracking for Autonomous Vehicular Communication using IEEE 802.11ad-based Radar

Mobility scenarios involving short contact times pose a challenge for high
bandwidth data transfer between autonomous vehicles and roadside base stations
(BS). Millimeter wave bands are a viable solution as they offer enormous
bandwidth in the 60GHz band with several Gbps data transfer rates. However,
beamforming is used as a default mode in this band, which requires accurate and
continuous alignment under relative motion. We propose a method in which an
off-the-shelf IEEE 802.11ad WiFi router is configured to serve as the BS as
well as a radar exploiting special structure of 802.11ad preamble. We embed the
radar functionality within standards-compliant operations that do not modify
the core structure of the frames beyond what is defined by the 802.11ad
protocol. This not only reduces the beam training time, but also ensures
scalability with increasing vehicular traffic because radar allows accurate
ranging of up to 0.1m at distances up to 200m. We further analyze the ensuing
cost-benefit trade-off between the time allotted to the proposed in-band radar
and communication modes. Our results reveal 83% reduction on the overhead
incurred during the beam training achieved for a specific simulated vehicular
scenario over the classical 802.11ad operation.



On Musical Onset Detection via the S-Transform

Musical onset detection is a key component in any beat tracking system.
Existing onset detection methods are based on temporal/spectral analysis, or
methods that integrate temporal and spectral information together with
statistical estimation and machine learning models. In this paper, we propose a
method to localize onset components in music by using the S-transform, and
thus, the method is purely based on temporal/spectral data. Unlike the other
methods based on temporal/spectral data, which usually rely short time Fourier
transform (STFT), our method enables effective isolation of crucial frequency
subbands due to the frequency dependent resolution of S-transform. Moreover,
numerical results show, even with less computationally intensive steps, the
proposed method can closely resemble the performance of more resource intensive
statistical estimation based approaches.



How consistent is my model with the data? Information-Theoretic Model Check

The choice of model class is fundamental in statistical learning and system
identification, no matter whether the class is derived from physical principles
or is a generic black-box. We develop a method to evaluate the specified model
class by assessing its capability of reproducing data that is similar to the
observed data record. This model check is based on the information-theoretic
properties of models viewed as data generators and is applicable to e.g.
sequential data and nonlinear dynamical models. The method can be understood as
a specific two-sided posterior predictive test. We apply the
information-theoretic model check to both synthetic and real data and compare
it with a classical whiteness test.



An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios

With the increasing imaging and processing capabilities of today's mobile
devices, user authentication using iris biometrics has become feasible.
However, as the acquisition conditions become more unconstrained and as image
quality is typically lower than dedicated iris acquisition systems, the
accurate segmentation of iris regions is crucial for these devices. In this
work, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is
proposed to perform the iris segmentation task for lower-quality iris images.
The network design process is explained in detail, and the resulting network is
trained and tuned using several large public iris datasets. A set of methods to
generate and augment suitable lower quality iris images from the high-quality
public databases are provided. The network is trained on Near InfraRed (NIR)
images initially and later tuned on additional datasets derived from visible
images. Comprehensive inter-database comparisons are provided together with
results from a selection of experiments detailing the effects of different
tunings of the network. Finally, the proposed model is compared with
SegNet-basic, and a near-optimal tuning of the network is compared to a
selection of other state-of-art iris segmentation algorithms. The results show
very promising performance from the optimized Deep Neural Networks design when
compared with state-of-art techniques applied to the same lower quality
datasets.



Representations of Sound in Deep Learning of Audio Features from Music

The work of a single musician, group or composer can vary widely in terms of
musical style. Indeed, different stylistic elements, from performance medium
and rhythm to harmony and texture, are typically exploited and developed across
an artist's lifetime. Yet, there is often a discernable character to the work
of, for instance, individual composers at the perceptual level - an experienced
listener can often pick up on subtle clues in the music to identify the
composer or performer. Here we suggest that a convolutional network may learn
these subtle clues or features given an appropriate representation of the
music. In this paper, we apply a deep convolutional neural network to a large
audio dataset and empirically evaluate its performance on audio classification
tasks. Our trained network demonstrates accurate performance on such
classification tasks when presented with 5 s examples of music obtained by
simple transformations of the raw audio waveform. A particularly interesting
example is the spectral representation of music obtained by application of a
logarithmically spaced filter bank, mirroring the early stages of auditory
signal transduction in mammals. The most successful representation of music to
facilitate discrimination was obtained via a random matrix transform (RMT).
Networks based on logarithmic filter banks and RMT were able to correctly guess
the one composer out of 31 possibilities in 68 and 84 percent of cases
respectively.



Blind Multiclass Ensemble Classification

The rising interest in pattern recognition and data analytics has spurred the
development of innovative machine learning algorithms and tools. However, as
each algorithm has its strengths and limitations, one is motivated to
judiciously fuse multiple algorithms in order to find the "best" performing
one, for a given dataset. Ensemble learning aims at such high-performance
meta-algorithm, by combining the outputs from multiple algorithms. The present
work introduces a blind scheme for learning from ensembles of classifiers,
using a moment matching method that leverages joint tensor and matrix
factorization. Blind refers to the combiner who has no knowledge of the
ground-truth labels that each classifier has been trained on. A rigorous
performance analysis is derived and the proposed scheme is evaluated on
synthetic and real datasets.



Uncertainty Principle and Sparse Reconstruction in Pairs of Orthonormal Rational Function Bases

Most rational systems can be described in terms of orthonormal basis
functions. This paper considers the reconstruction of a sparse coefficient
vector for a rational transfer function under a pair of orthonormal rational
function bases and from a limited number of linear frequency-domain
measurements. We prove the uncertainty principle concerning pairs of
compressible representation of orthonormal rational functions in the infinite
dimensional function space. The uniqueness of compressible representation using
such pairs is provided as a direct consequence of uncertainty principle. The
bound of the number of measurements which guarantees the replacement of 1_0
optimization searching for the unique sparse reconstruction by 1_1 optimization
using random sampling on the unit circle with high probability is provided as
well.



Reconstruction of Brain Activity from EEG/MEG Using MV-PURE Framework

We consider the problem of reconstruction of brain activity from
electroencephalography (EEG) or magnetoencephalography (MEG) using spatial
filtering (beamforming). We propose spatial filters which are based on the
minimum-variance pseudo-unbiased reduced-rank estimation (MV-PURE) framework.
They come in two flavours, depending whether the EEG/MEG forward model
considers explicitly "interfering activity", understood as brain's electrical
activity originating from brain areas other than regions of interest which is
recorded at EEG/MEG sensors as a signal correlated with activity of interest.
In both cases, the proposed filters are equipped with a rank-selection
criterion minimizing the mean-square-error (MSE) of the filter output.
Therefore, we consider them as novel nontrivial generalizations of well-known
linearly constrained minimum-variance (LCMV) and nulling filters. The proposed
filters have equally wide area of applications, which include in particular
evaluation of directed connectivity measures based on the reconstructed
activity of sources of interest, considered in this paper as a sample
application. Moreover, in order to facilitate reproducibility of our research,
we provide (jointly with this paper) comprehensive simulation framework that
allows for estimation of error of signal reconstruction for a number of spatial
filters applied to MEG or EEG signals. Based on this framework, chief
properties of proposed filters are verified in a set of detailed simulations.



Effect of the image resolution on the statistical descriptors of heterogeneous media

The characterization and reconstruction of heterogeneous materials, such as
porous media and electrode materials, involve the application of image
processing methods to data acquired by microscopy techniques. In this study, we
present a theoretical analysis of the effects of the image size reduction, due
to a gradual decimation of the original image. Three different decimation
procedures were implemented and their consequences on the discrete correlation
functions and the coarseness are reported and analyzed. A normalization for
each of the correlation functions has been performed. When the loss of
statistical information has not been significant for a decimated image, its
normalized correlation function is forecast by the trend of the original image.
In contrast, when the decimated image does not represent the statistical
evidence of the original one, the normalized correlation function diverts from
the reference function. Moreover, the equally weighted sum of the average of
the squared differences leads to a definition of an overall error. During the
first stages of the gradual decimation, the error remains relatively small and
independent of the decimation procedure. Above a threshold defined by the
correlation length of the reference function, the error becomes a function of
the number of decimation steps. At this stage, some statistical information is
lost and the error becomes dependent of the decimation procedure. These results
may help us to restrict the amount of information that one can afford to lose
during a decimation process, in order to reduce the computational and memory
cost, when one aims to diminish the time consumed by a characterization or
reconstruction technique, yet maintaining the statistical quality of the
digitized sample.



Music Transcription by Deep Learning with Data and "Artificial Semantic" Augmentation

In this progress paper the previous results of the single note recognition by
deep learning are presented. The several ways for data augmentation and
"artificial semantic" augmentation are proposed to enhance efficiency of deep
learning approaches for monophonic and polyphonic note recognition by increase
of dimensions of training data, their lossless and lossy transformations.



Performance of Analog Nonlinear Filtering for Impulsive Noise Mitigation in OFDM-based PLC Systems

Asynchronous and cyclostationary impulsive noise can severely impact the
bit-error-rate (BER) of OFDM-based powerline communication systems. In this
paper, we analyze an adaptive nonlinear analog front end filter that mitigates
various types of impulsive noise without detrimental effects such as
self-interference and out-of-band power leakage caused by other nonlinear
approaches like clipping and blanking. Our proposed Adaptive Nonlinear
Differential Limiter (ANDL) is constructed from a linear analog filter by
applying a feedback-based nonlinearity, controlled by a single resolution
parameter. We present a simple practical method to find the value of this
resolution parameter that ensures the mitigation of impulsive without impacting
the desired OFDM signal. Unlike many prior approaches for impulsive noise
mitigation that assume a statistical noise model, ANDL is blind to the exact
nature of the noise distribution, and is designed to be fully compatible with
existing linear front end filters. We demonstrate the potency of ANDL by
simulating the OFDM-based narrowband PLC compliant with the IEEE standards. We
show that the proposed ANDL outperforms other approaches in reducing the BER in
impulsive noise environments.



Open Orchestration Cloud Radio Access Network (OOCRAN) Testbed

The Cloud radio access network (C-RAN) offers a revolutionary approach to
cellular network deployment, management and evolution. Advances in
software-defined radio (SDR) and networking technology, moreover, enable
delivering software-defined everything through the Cloud. Resources will be
pooled and dynamically allocated leveraging abstraction, virtualization, and
consolidation techniques; processes will be automated using common application
programming interfaces; and network functions and services will be
programmatically provided through an orchestrator. OOCRAN, oocran.dynu.com, is
a software framework that is based on the NFV MANO architecture proposed by
ETSI. It provides an orchestration layer for the entire wireless
infrastructure, including hardware, software, spectrum, fronthaul and backhaul.
OOCRAN extends existing NFV management frameworks by incorporating the radio
communications layers and their management dependencies. The wireless
infrastructure provider can then dynamically provision virtualized wireless
networks to wireless service providers. The testbed's physical infrastructure
is built around a computing cluster that executes open-source SDR libraries and
connects to SDR-based remote radio heads. We demonstrate the operation of
OOCRAN and discuss the temporal implications of dynamic LTE small cell network
deployments.



Noise Level Estimation for Overcomplete Dictionary Learning Based on Tight Asymptotic Bounds

In this letter, we address the problem of estimating Gaussian noise level
from the trained dictionaries in update stage. We first provide rigorous
statistical analysis on the eigenvalue distributions of a sample covariance
matrix. Then we propose an interval-bounded estimator for noise variance in
high dimensional setting. To this end, an effective estimation method for noise
level is devised based on the boundness and asymptotic behavior of noise
eigenvalue spectrum. The estimation performance of our method has been
guaranteed both theoretically and empirically. The analysis and experiment
results have demonstrated that the proposed algorithm can reliably infer true
noise levels, and outperforms the relevant existing methods.



Exploiting WiFi Channel State Information for Residential Healthcare Informatics

Detection and interpretation of human activities have emerged as a
challenging healthcare problem in areas such as assisted living and remote
monitoring. Besides traditional approaches that rely on wearable devices and
camera systems, WiFi based technologies are evolving as a promising solution
for indoor monitoring and activity recognition. This is, in part, due to the
pervasive nature of WiFi in residential settings such as homes and care
facilities, and unobtrusive nature of WiFi based sensing. Advanced signal
processing techniques can accurately extract WiFi channel status information
(CSI) using commercial off-the-shelf (COTS) devices or bespoke hardware. This
includes phase variations, frequency shifts and signal levels. In this paper,
we describe the healthcare application of Doppler shifts in the WiFi CSI,
caused by human activities which take place in the signal coverage area. The
technique is shown to recognize different types of human activities and
behaviour and be very suitable for applications in healthcare. Three
experimental case studies are presented to illustrate the capabilities of WiFi
CSI Doppler sensing in assisted living and residential care environments. We
also discuss the potential opportunities and practical challenges for
real-world scenarios.



Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models

In this paper, we describe how to efficiently implement an acoustic room
simulator to generate large-scale simulated data for training deep neural
networks. Even though Google Room Simulator in [1] was shown to be quite
effective in reducing the Word Error Rates (WERs) for far-field applications by
generating simulated far-field training sets, it requires a very large number
of Fast Fourier Transforms (FFTs) of large size. Room Simulator in [1] used
approximately 80 percent of Central Processing Unit (CPU) usage in our CPU +
Graphics Processing Unit (GPU) training architecture [2]. In this work, we
implement an efficient OverLap Addition (OLA) based filtering using the
open-source FFTW3 library. Further, we investigate the effects of the Room
Impulse Response (RIR) lengths. Experimentally, we conclude that we can cut the
tail portions of RIRs whose power is less than 20 dB below the maximum power
without sacrificing the speech recognition accuracy. However, we observe that
cutting RIR tail more than this threshold harms the speech recognition accuracy
for rerecorded test sets. Using these approaches, we were able to reduce CPU
usage for the room simulator portion down to 9.69 percent in CPU/GPU training
architecture. Profiling result shows that we obtain 22.4 times speed-up on a
single machine and 37.3 times speed up on Google's distributed training
infrastructure.



Wireless Energy Beamforming Using Signal Strength Feedback

Multiple antenna techniques, that allow energy beamforming, have been looked
upon as a possible candidate for increasing the efficiency of the transfer
process between the energy transmitter (ET) and the energy receiver (ER) in
wireless energy transfer. This paper introduces a novel scheme that facilitates
energy beamforming by utilizing Received Signal Strength Indicator (RSSI)
values to estimate the channel. Firstly, in the training stage, the ET will
transmit sequentially using each beamforming vector in a codebook, which is
pre-defined using a Cramer-Rao lower bound analysis. The RSSI value
corresponding to each beamforming vector is fed back to the ET, and these
values are used to estimate the channel through a maximum likelihood analysis.
The results that are obtained are remarkably simple, requires minimal
processing, and can be easily implemented. Also, the results are general and
hold for all well known fading models. The paper also validates the analytical
results numerically, as well as experimentally, and it is shown that the
proposed method achieves impressive results in wireless energy transfer.



The organization of a three-manual keyboard for 53-tone tempered and other tempered systems

The aim is to explore new opportunities of the pitch organization of the
musical scale. Specifically, a numerical comparison of the different musical
temperaments among themselves in the degree of approximation of the Pythagorean
scale is provided, and thus it numerically substantiates the thesis that the
53-tone tempered system is the most advanced among possible others. We present
numerical data on the approximation of overtones from first twenty by steps of
the 53-tone temperament. Here were proposed some schemes of the three-manual
keyboard for the implementation of 53-tone temperament, which are also
implemented at the same time for 12 -, 17 -, 24 -, 29 - and 41-sounding system.
If there are technical means then these schemes can be used to play music in
any temperaments, based on said number of steps.



Prodorshok I: A Bengali Isolated Speech Dataset for Voice-Based Assistive Technologies - A comparative analysis of the effects of data augmentation on HMM-GMM and DNN classifiers

Prodorshok I is a Bengali isolated word dataset tailored to help create
speaker-independent, voice-command driven automated speech recognition (ASR)
based assistive technologies to help improve human-computer interaction (HCI).
This paper presents the results of an objective analysis that was undertaken
using a subset of words from Prodorshok I to assess its reliability in ASR
systems that utilize Hidden Markov Models (HMM) with Gaussian emissions and
Deep Neural Networks (DNN). The results show that simple data augmentation
involving a small pitch shift can make surprisingly tangible improvements to
accuracy levels in speech recognition.



A Cascade Architecture for Keyword Spotting on Mobile Devices

We present a cascade architecture for keyword spotting with speaker
verification on mobile devices. By pairing a small computational footprint with
specialized digital signal processing (DSP) chips, we are able to achieve low
power consumption while continuously listening for a keyword.



Cascaded Reconstruction Network for Compressive image sensing

The theory of compressed sensing (CS) has been successfully applied to image
compression in the past few years, whose traditional iterative reconstruction
algorithm is time-consuming. However, it has been reported deep learning-based
CS reconstruction algorithms could greatly reduce the computational complexity.
In this paper, we propose two efficient structures of cascaded reconstruction
networks corresponding to two different sampling methods in CS process. The
first reconstruction network is a compatibly sampling reconstruction network
(CSRNet), which recovers an image from its compressively sensed measurement
sampled by a traditional random matrix. In CSRNet, deep reconstruction network
module obtains an initial image with acceptable quality, which can be further
improved by residual network module based on convolutional neural network. The
second reconstruction network is adaptively sampling reconstruction network
(ASRNet), by matching automatically sampling module with corresponding residual
reconstruction module. The experimental results have shown that the proposed
two reconstruction networks outperform several state-of-the-art compressive
sensing reconstruction algorithms. Meanwhile, the proposed ASRNet can achieve
more than 1 dB gain, as compared with the CSRNet.



Identifying the Mislabeled Training Samples of ECG Signals using Machine Learning

The classification accuracy of electrocardiogram signal is often affected by
diverse factors in which mislabeled training samples issue is one of the most
influential problems. In order to mitigate this negative effect, the method of
cross validation is introduced to identify the mislabeled samples. The method
utilizes the cooperative advantages of different classifiers to act as a filter
for the training samples. The filter removes the mislabeled training samples
and retains the correctly labeled ones with the help of 10-fold cross
validation. Consequently, a new training set is provided to the final
classifiers to acquire higher classification accuracies. Finally, we
numerically show the effectiveness of the proposed method with the MIT-BIH
arrhythmia database.



Age Minimization in Energy Harvesting Communications: Energy-Controlled Delays

We consider an energy harvesting source that is collecting measurements from
a physical phenomenon and sending updates to a destination within a
communication session time. Updates incur transmission delays that are function
of the energy used in their transmission. The more transmission energy used per
update, the faster it reaches the destination. The goal is to transmit updates
in a timely manner, namely, such that the total age of information is minimized
by the end of the communication session, subject to energy causality
constraints. We consider two variations of this problem. In the first setting,
the source controls the number of measurement updates, their transmission
times, and the amounts of energy used in their transmission (which govern their
delays, or service times, incurred). In the second setting, measurement updates
externally arrive over time, and therefore the number of updates becomes fixed,
at the expense of adding data causality constraints to the problem. We
characterize age-minimal policies in the two settings, and discuss the
relationship of the age of information metric to other metrics used in the
energy harvesting literature.



Learning Based Segmentation of CT Brain Images: Application to Post-Operative Hydrocephalic Scans

Objective: Hydrocephalus is a medical condition in which there is an abnormal
accumulation of cerebrospinal fluid (CSF) in the brain. Segmentation of brain
imagery into brain tissue and CSF (before and after surgery, i.e. pre-op vs.
postop) plays a crucial role in evaluating surgical treatment. Segmentation of
pre-op images is often a relatively straightforward problem and has been well
researched. However, segmenting post-operative (post-op) computational
tomographic (CT)-scans becomes more challenging due to distorted anatomy and
subdural hematoma collections pressing on the brain. Most intensity and feature
based segmentation methods fail to separate subdurals from brain and CSF as
subdural geometry varies greatly across different patients and their intensity
varies with time. We combat this problem by a learning approach that treats
segmentation as supervised classification at the pixel level, i.e. a training
set of CT scans with labeled pixel identities is employed. Methods: Our
contributions include: 1.) a dictionary learning framework that learns class
(segment) specific dictionaries that can efficiently represent test samples
from the same class while poorly represent corresponding samples from other
classes, 2.) quantification of associated computation and memory footprint, and
3.) a customized training and test procedure for segmenting post-op
hydrocephalic CT images. Results: Experiments performed on infant CT brain
images acquired from the CURE Children's Hospital of Uganda reveal the success
of our method against the state-of-the-art alternatives. We also demonstrate
that the proposed algorithm is computationally less burdensome and exhibits a
graceful degradation against number of training samples, enhancing its
deployment potential.



Multi-Speaker Localization Using Convolutional Neural Network Trained with Noise

The problem of multi-speaker localization is formulated as a multi-class
multi-label classification problem, which is solved using a convolutional
neural network (CNN) based source localization method. Utilizing the common
assumption of disjoint speaker activities, we propose a novel method to train
the CNN using synthesized noise signals. The proposed localization method is
evaluated for two speakers and compared to a well-known steered response power
method.



A Novel RF Energy Harvesting Module Integrated on a Single Substrate

This paper presents the RF energy harvesting module (RECTENNA). The working
range of this module includes multiple bands i.e. GSM, ISM, WLAN, and UWB band.
To enhance the capturing RF power capability an array arrangement of coplanar
monopole antenna has been proposed. Wilkinson power combiner has also been
implemented to combine the powers of this antenna array. The RF DC converter
circuit having seven stages has also been integrated with this structure. This
module produces the DC voltage of 1.8V with respect to +40dB RF input. It is
the unique module because it has no need of port connectors. The impedance
matching of antenna and converter has been fulfilled by incorporating the
passive component at the combiners branch. The value of this passive component
is kept equal to the existing value of impedance at input port of converter
circuit.



Music Generation by Deep Learning - Challenges and Directions

In addition to traditional tasks such as prediction, classification and
translation, deep learning is receiving growing attention as an approach for
music generation, as witnessed by recent research groups such as Magenta at
Google and CTRL (Creator Technology Research Lab) at Spotify. The motivation is
in using the capacity of deep learning architectures and training techniques to
automatically learn musical styles from arbitrary musical corpora and then to
generate samples from the estimated distribution. However, a direct application
of deep learning to generate content rapidly reaches limits as the generated
content tends to mimic the training set without exhibiting true creativity.
Moreover, deep learning architectures do not offer direct ways for controlling
generation (e.g., imposing some tonality or other arbitrary constraints).
Furthermore, deep learning architectures alone are autistic automata which
generate music autonomously without human user interaction, far from the
objective of interactively assisting musicians to compose and refine music.
Issues such as: control, structure, creativity and interactivity are the focus
of our analysis. In this paper, we select some limitations of a direct
application of deep learning to music generation, analyze why the issues are
not fulfilled and how to address them by possible approaches. Various examples
of recent systems are cited as examples of promising directions.



auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks

auDeep is a Python toolkit for deep unsupervised representation learning from
acoustic data. It is based on a recurrent sequence to sequence autoencoder
approach which can learn representations of time series data by taking into
account their temporal dynamics. We provide an extensive command line interface
in addition to a Python API for users and developers, both of which are
comprehensively documented and publicly available at
https://github.com/auDeep/auDeep. Experimental results indicate that auDeep
features are competitive with state-of-the art audio classification.



Maximum-Likelihood Power-Distortion Monitoring for GNSS Signal Authentication

We propose an extension to the so-called PD detector. The PD detector jointly
monitors received power and correlation profile distortion to detect the
presence of GNSS carry-off-type spoofing, jamming, or multipath. We show that
classification performance can be significantly improved by replacing the PD
detector's symmetric-difference-based distortion measurement with one based on
the post-fit residuals of the maximum-likelihood estimate of a single-signal
correlation function model. We call the improved technique the PD-ML detector.
In direct comparison with the PD detector, the PD-ML detector exhibits improved
classification accuracy when tested against an extensive library of recorded
field data. In particular, it is (1) significantly more accurate at
distinguishing a spoofing attack from a jamming attack, (2) better at
distinguishing multipath-afflicted data from interference-free data, and (3)
less likely to issue a false alarm by classifying multipath as spoofing. The
PD-ML detector achieves this improved performance at the expense of additional
computational complexity.



Analysis and Optimization of Aperture Design in Computational Imaging

There is growing interest in the use of coded aperture imaging systems for a
variety of applications. Using an analysis framework based on mutual
information, we examine the fundamental limits of such systems---and the
associated optimum aperture coding---under simple but meaningful propagation
and sensor models. Among other results, we show that when thermal noise
dominates, spectrally-flat masks, which have 50% transmissivity, are optimal,
but that when shot noise dominates, randomly generated masks with lower
transmissivity offer greater performance. We also provide comparisons to
classical pinhole cameras.



Classification vs. Regression in Supervised Learning for Single Channel Speaker Count Estimation

The task of estimating the maximum number of concurrent speakers from single
channel mixtures is important for various audio-based applications, such as
blind source separation, speaker diarisation, audio surveillance or auditory
scene classification. Building upon powerful machine learning methodology, we
develop a Deep Neural Network (DNN) that estimates a speaker count. While DNNs
efficiently map input representations to output targets, it remains unclear how
to best handle the network output to infer integer source count estimates, as a
discrete count estimate can either be tackled as a regression or a
classification problem. In this paper, we investigate this important design
decision and also address complementary parameter choices such as the input
representation. We evaluate a state-of-the-art DNN audio model based on a
Bi-directional Long Short-Term Memory network architecture for speaker count
estimations. Through experimental evaluations aimed at identifying the best
overall strategy for the task and show results for five seconds speech segments
in mixtures of up to ten speakers.



Online Nonlinear Estimation via Iterative L2-Space Projections: Reproducing Kernel of Subspace

We propose a novel online learning paradigm for nonlinear-function estimation
tasks based on the iterative projections in the L2 space with probability
measure reflecting the stochastic property of input signals. The proposed
learning algorithm exploits the reproducing kernel of the so-called dictionary
subspace, based on the fact that any finite-dimensional space of functions has
a reproducing kernel characterized by the Gram matrix. The L2-space geometry
provides the best decorrelation property in principle. The proposed learning
paradigm is significantly different from the conventional kernel-based learning
paradigm in two senses: (i) the whole space is not a reproducing kernel Hilbert
space and (ii) the minimum mean squared error estimator gives the best
approximation of the desired nonlinear function in the dictionary subspace. It
preserves efficiency in computing the inner product as well as in updating the
Gram matrix when the dictionary grows. Monotone approximation, asymptotic
optimality, and convergence of the proposed algorithm are analyzed based on the
variable-metric version of adaptive projected subgradient method. Numerical
examples show the efficacy of the proposed algorithm for real data over a
variety of methods including the extended Kalman filter and many batch
machine-learning methods such as the multilayer perceptron.



Over the Air Deep Learning Based Radio Signal Classification

We conduct an in depth study on the performance of deep learning based radio
signal classification for radio communications signals. We consider a rigorous
baseline method using higher order moments and strong boosted gradient tree
classification and compare performance between the two approaches across a
range of configurations and channel impairments. We consider the effects of
carrier frequency offset, symbol rate, and multi-path fading in simulation and
conduct over-the-air measurement of radio classification performance in the lab
using software radios and compare performance and training strategies for both.
Finally we conclude with a discussion of remaining problems, and design
considerations for using such techniques.



Cellular V2X in Unlicensed Spectrum: Harmonious Coexistence with VANET in 5G systems

With the increasing demand for vehicular data transmission, limited dedicated
cellular spectrum becomes a bottleneck to satisfy the requirements of all
cellular vehicle-to-everything (V2X) users. To address this issue, unlicensed
spectrum is considered to serve as the complement to support cellular V2X
users. In this paper, we study the coexistence problem of cellular V2X users
and vehicular ad-hoc network~(VANET) users over the unlicensed spectrum. To
facilitate the coexistence, we design an energy sensing based spectrum sharing
scheme, where cellular V2X users are able to access the unlicensed channels
fairly while reducing the data transmission collisions between cellular V2X and
VANET users. In order to maximize the number of active cellular V2X users, we
formulate the scheduling and resource allocation problem as a two-sided
many-to-many matching with peer effects. We then propose a dynamic
vehicle-resource matching algorithm (DV-RMA) and present the analytical results
on the convergence time and computational complexity. Simulation results show
that the proposed algorithm outperforms existing approaches in terms of the
performance of cellular V2X system when the unlicensed spectrum is utilized.



Learning Spontaneity to Improve Emotion Recognition In Speech

We investigate the effect and usefulness of spontaneity (i.e. whether a given
speech is spontaneous or not) in speech in the context of emotion recognition.
We hypothesize that emotional content in speech is interrelated with its
spontaneity, and use spontaneity classification as an auxiliary task to the
problem of emotion recognition. We propose two supervised learning settings
that utilize spontaneity to improve speech emotion recognition: a hierarchical
model that performs spontaneity detection before performing emotion
recognition, and a multitask learning model that jointly learns to recognize
both spontaneity and emotion. Through various experiments on the well known
IEMOCAP database, we show that by using spontaneity detection as an additional
task, significant improvement can be achieved over emotion recognition systems
that are unaware of spontaneity. We achieve state-of-the-art emotion
recognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming
several relevant and competitive baselines.



Localization of multilayer networks by the optimized single-layer rewiring

We study localization properties of principal eigenvector (PEV) of multilayer
networks. Starting with a multilayer network corresponding to a delocalized
PEV, we rewire the network edges using an optimization technique such that the
PEV of the rewired multilayer network becomes more localized. The framework
allows us to scrutinize structural and spectral properties of the networks at
various localization points during the rewiring process. We show that rewiring
only one-layer is enough to attain a multilayer network having a highly
localized PEV. Our investigation reveals that a single edge rewiring of the
optimized multilayer network can lead to the complete delocalization of a
highly localized PEV. This sensitivity in the localization behavior of PEV is
accompanied by a pair of almost degenerate eigenvalues. This observation opens
an avenue to gain a deeper insight into the origin of PEV localization of
networks. Furthermore, analysis of multilayer networks constructed using
real-world social and biological data show that the localization properties of
these real-world multilayer networks are in good agreement with the simulation
results for the model multilayer network. The study is relevant to applications
that require understanding propagation of perturbation in multilayer networks.



Multidimensional Data Tensor Sensing for RF Tomographic Imaging

Radio-frequency (RF) tomographic imaging is a promising technique for
inferring multi-dimensional physical space by processing RF signals traversed
across a region of interest. However, conventional RF tomography schemes are
generally based on vector compressed sensing, which ignores the geometric
structures of the target spaces and leads to low recovery precision. The
recently proposed transform-based tensor model is more appropriate for sensory
data processing, as it helps exploit the geometric structures of the
three-dimensional target and improve the recovery precision. In this paper, we
propose a novel tensor sensing approach that achieves highly accurate
estimation for real-world three-dimensional spaces. First, we use the
transform-based tensor model to formulate a tensor sensing problem, and propose
a fast alternating minimization algorithm called Alt-Min. Secondly, we drive an
algorithm which is optimized to reduce memory and computation requirements.
Finally, we present evaluation of our Alt-Min approach using IKEA 3D data and
demonstrate significant improvement in recovery error and convergence speed
compared to prior tensor-based compressed sensing.



Towards Deep Modeling of Music Semantics using EEG Regularizers

Modeling of music audio semantics has been previously tackled through
learning of mappings from audio data to high-level tags or latent unsupervised
spaces. The resulting semantic spaces are theoretically limited, either because
the chosen high-level tags do not cover all of music semantics or because audio
data itself is not enough to determine music semantics. In this paper, we
propose a generic framework for semantics modeling that focuses on the
perception of the listener, through EEG data, in addition to audio data. We
implement this framework using a novel end-to-end 2-view Neural Network (NN)
architecture and a Deep Canonical Correlation Analysis (DCCA) loss function
that forces the semantic embedding spaces of both views to be maximally
correlated. We also detail how the EEG dataset was collected and use it to
train our proposed model. We evaluate the learned semantic space in a transfer
learning context, by using it as an audio feature extractor in an independent
dataset and proxy task: music audio-lyrics cross-modal retrieval. We show that
our embedding model outperforms Spotify features and performs comparably to a
state-of-the-art embedding model that was trained on 700 times more data. We
further discuss improvements to the model that are likely to improve its
performance.



ICT Convergence in Internet of Things - The Birth of Smart Factories (A Technical Note)

Over the past decade, most factories across developed parts of the world
employ a varying amount of the manufacturing technologies including autonomous
robots, RFID (radio frequency identification) technology, NCs (numerically
controlled machines), wireless sensor networks embedded with specialized
computerized softwares for sophisticated product designs, engineering analysis,
and remote control of machinery, etc. The ultimate aim of these all dramatic
developments in manufacturing sector is thus to achieve aspects such as shorter
innovation / product life cycles and raising overall productivity via
efficiently handling complex interactions among the various stages (functions,
departments) of a production line. The notion, Factory of the Future, is an
unpredictable heaven of efficaciousness, wherein, issues such as the flaws and
downtime would be issues of the long forgotten age. This technical note thus
provides an overview of this awesome revolution waiting to be soon realized in
the manufacturing sector.



A Data Driven Approach for Resting-state EEG signal Classification of Schizophrenia with Control Participants using Random Matrix Theory

Resting state electroencephalogram (EEG) abnormalities in clinically
high-risk individuals (CHR), clinically stable first-episode patients with
schizophrenia (FES), healthy controls (HC) suggest alterations in neural
oscillatory activity. However, few studies directly compare these anomalies
among each types. Therefore, this study investigated whether these
electrophysiological characteristics differentiate clinical populations from
one another, and from non-psychiatric controls. To address this question,
resting EEG power and coherence were assessed in 40 clinically high-risk
individuals (CHR), 40 first-episode patients with schizophrenia (FES), and 40
healthy controls (HC). These findings suggest that resting EEG can be a
sensitive measure for differentiating between clinical disorders.This paper
proposes a novel data-driven supervised learning method to obtain
identification of the patients mental status in schizophrenia research.
According to Marchenko-Pastur Law, the distribution of the eigenvalues of EEG
data is divided into signal subspace and noise subspace. A test statistic named
LES that embodies the characteristics of all eigenvalues is adopted. different
classifier and different feature(LES test function) are selected for
experiments, we have shown that using von Neumann Entropy as LES test function
combine with SVM classifier could obtain the best average classification
accuracy during three classification among HC, FES and CHR of Schizophrenia
group with EEG signal. It is worth noting that the result of LES feature
extraction with the highest classification accuracy is around 90% in two
classification(HC compare with FES) and around 70% in three classification.
Where the classification accuracy higher than 70% could be used to assist
clinical diagnosis.



60 GHz Blockage Study Using Phased Arrays

The millimeter wave (mmWave) frequencies offer the potential for enormous
capacity wireless links. However, designing robust communication systems at
these frequencies requires that we understand the channel dynamics over both
time and space: mmWave signals are extremely vulnerable to blocking and the
channel can thus rapidly appear and disappear with small movement of obstacles
and reflectors. In rich scattering environments, different paths may experience
different blocking trajectories and understanding these multi-path blocking
dynamics is essential for developing and assessing beamforming and
beam-tracking algorithms. This paper presents the design and experimental
results of a novel measurement system which uses phased arrays to perform
mmWave dynamic channel measurements. Specifically, human blockage and its
effects across multiple paths are investigated with only several microseconds
between successive measurements. From these measurements we develop a modeling
technique which uses low-rank tensor factorization to separate the available
paths so that their joint statistics can be understood.



DOA and Polarization Estimation for Non-Circular Signals in 3-D Millimeter Wave Polarized Massive MIMO Systems

In this paper, an algorithm of multiple signal classification (MUSIC) is
proposed for two-dimensional (2-D) direction of- arrival (DOA) and polarization
estimation of non-circular signal in three-dimensional (3-D) millimeter wave
polarized largescale/ massive multiple-input-multiple-output (MIMO) systems.
The traditional MUSIC-based algorithms can estimate either the DOA and
polarization for circular signal or the DOA for non-circular signal by using
spectrum search. By contrast, in the proposed algorithm only the DOA estimation
needs spectrum search, and the polarization estimation has a closedform
expression. First, a novel dimension-reduced MUSIC (DRMUSIC) is proposed for
DOA and polarization estimation of circular signal with low computational
complexity. Next, based on the quaternion theory, a novel algorithm named
quaternion non-circular MUSIC (QNC-MUSIC) is proposed for parameter estimation
of non-circular signal with high estimation accuracy. Then based on the DOA
estimation result using QNC-MUSIC, the polarization estimation of non-circular
signal is acquired by using the closed-form expression of the polarization
estimation in DR-MUSIC. In addition, the computational complexity analysis
shows that compared with the conventional DOA and polarization estimation
algorithms, our proposed QNC-MUSIC and DRMUSIC have much lower computational
complexity, especially when the source number is large. The stochastic
Cramer-Rao Bound (CRB) for the estimation of the 2-D DOA and polarization
parameters of the non-circular signals is derived as well. Finally, numerical
examples are provided to demonstrate that the proposed algorithms can improve
the parameter estimation performance when the large-scale/massive MIMO systems
are employed.



A Novel Approach for Effective Learning in Low Resourced Scenarios

Deep learning based discriminative methods, being the state-of-the-art
machine learning techniques, are ill-suited for learning from lower amounts of
data. In this paper, we propose a novel framework, called simultaneous two
sample learning (s2sL), to effectively learn the class discriminative
characteristics, even from very low amount of data. In s2sL, more than one
sample (here, two samples) are simultaneously considered to both, train and
test the classifier. We demonstrate our approach for speech/music
discrimination and emotion classification through experiments. Further, we also
show the effectiveness of s2sL approach for classification in low-resource
scenario, and for imbalanced data.



Improved Target Acquisition Rates with Feedback Codes

This paper considers the problem of acquiring an unknown target location
(among a finite number of locations) via a sequence of measurements, where each
measurement consists of simultaneously probing a group of locations. The
resulting observation consists of a sum of an indicator of the target's
presence in the probed region, and a zero mean Gaussian noise term whose
variance is a function of the measurement vector. An equivalence between the
target acquisition problem and channel coding over a binary input additive
white Gaussian noise (BAWGN) channel with state and feedback is established.
Utilizing this information theoretic perspective, a two-stage adaptive target
search strategy based on the sorted Posterior Matching channel coding strategy
is proposed. Furthermore, using information theoretic converses, the
fundamental limits on the target acquisition rate for adaptive and non-adaptive
strategies are characterized. As a corollary to the non-asymptotic upper bound
of the expected number of measurements under the proposed two-stage strategy,
and to non-asymptotic lower bound of the expected number of measurements for
optimal non-adaptive search strategy, a lower bound on the adaptivity gain is
obtained. The adaptivity gain is further investigated in different asymptotic
regimes of interest.



Low Rank Matrix Recovery for Joint Array Self-Calibration and Sparse Model DoA Estimation

In this work, combined calibration and DoA estimation is approached as an
extension of the formulation for the Single Measurement Vector (SMV) model of
self-calibration to the Multiple Measurement Model (MMV) case. By taking
advantage of multiple snapshots, a modified nuclear norm minimization problem
is proposed to recover a low-rank larger dimension matrix. We also give the
definition of a linear operator for the MMV model, and give its corresponding
matrix representation to generate a variant of a convex optimization problem.
In order to mitigate the computational complexity of the approach, singular
value decomposition (SVD) is applied to reduce the problem size. The
performance of the proposed methods are demonstrated by numerical simulations.



Blind Estimation Algorithms for I/Q Imbalance in Direct Down-conversion Receivers

As known, receivers with in-phase and quadrature phase (I/Q) down conversion,
especially direct-conversion architectures, always suffer from I/Q imbalance.
I/Q imbalance is caused by amplitude and phase mismatch between I/Q paths. The
performance degradation resulting from I/Q imbalance can not be mitigated with
simply higher signal to noise ratio (SNR). Thus, I/Q imbalance compensation in
the digital domain is critical. There are two main contributions in this paper.
Firstly, we proposed a blind estimation algorithm for I/Q imbalance parameters
based on joint first and second order statistics (FSS) which has lower
complexity than conventional Gaussian maximum likelihood estimation (GMLE).
This can be used for further processing such as equalization in the presence of
receiver IQ imbalance. In addition, we find out the reason of the error floor
in conventional I/Q imbalance compensation method based on the conjugate signal
model (CSM). The proposed joint first order statistics and conjugate signal
model (FSCSM) compensation algorithm can reach the ideal bit error rate (BER)
performance.



Deep Learning for Distant Speech Recognition

Deep learning is an emerging technology that is considered one of the most
promising directions for reaching higher levels of artificial intelligence.
Among the other achievements, building computers that understand speech
represents a crucial leap towards intelligent machines. Despite the great
efforts of the past decades, however, a natural and robust human-machine speech
interaction still appears to be out of reach, especially when users interact
with a distant microphone in noisy and reverberant environments. The latter
disturbances severely hamper the intelligibility of a speech signal, making
Distant Speech Recognition (DSR) one of the major open challenges in the field.
  This thesis addresses the latter scenario and proposes some novel techniques,
architectures, and algorithms to improve the robustness of distant-talking
acoustic models. We first elaborate on methodologies for realistic data
contamination, with a particular emphasis on DNN training with simulated data.
We then investigate on approaches for better exploiting speech contexts,
proposing some original methodologies for both feed-forward and recurrent
neural networks. Lastly, inspired by the idea that cooperation across different
DNNs could be the key for counteracting the harmful effects of noise and
reverberation, we propose a novel deep learning paradigm called network of deep
neural networks. The analysis of the original concepts were based on extensive
experimental validations conducted on both real and simulated data, considering
different corpora, microphone configurations, environments, noisy conditions,
and ASR tasks.



"Zero-Shot" Super-Resolution using Deep Internal Learning

Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance
in the past few years. However, being supervised, these SR methods are
restricted to specific training data, where the acquisition of the
low-resolution (LR) images from their high-resolution (HR) counterparts is
predetermined (e.g., bicubic downscaling), without any distracting artifacts
(e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images,
however, rarely obey these restrictions, resulting in poor SR results by SotA
(State of the Art) methods. In this paper we introduce "Zero-Shot" SR, which
exploits the power of Deep Learning, but does not rely on prior training. We
exploit the internal recurrence of information inside a single image, and train
a small image-specific CNN at test time, on examples extracted solely from the
input image itself. As such, it can adapt itself to different settings per
image. This allows to perform SR of real old photos, noisy images, biological
data, and other images where the acquisition process is unknown or non-ideal.
On such images, our method outperforms SotA CNN-based SR methods, as well as
previous unsupervised SR methods. To the best of our knowledge, this is the
first unsupervised CNN-based SR method.



Towards the 1G of Mobile Power Network: RF, Signal and System Designs to Make Smart Objects Autonomous

This article reviews some recent promising approaches to make mobile power
closer to reality. In contrast with articles commonly published by the
microwave community and the communication/signal processing community that
separately emphasize RF, circuit and antenna solutions for WPT on one hand and
communications, signal and system designs for WPT on the other hand, this
review article uniquely bridges RF, signal and system designs in order to bring
those communities closer to each other and get a better understanding of the
fundamental building blocks of an efficient WPT network architecture. We start
by reviewing the engineering requirements and design challenges of making
mobile power a reality. We then review the state-of-the-art in a wide range of
areas spanning sensors and devices, RF design for wireless power and wireless
communications. We identify their limitations and make critical observations
before providing some fresh new look and promising avenues on signal and system
designs for WPT.



Learning Sequential Channel Selection for Interference Alignment using Reconfigurable Antennas

In recent years, machine learning techniques have been explored to support,
enhance or augment wireless systems especially at the physical layer of the
protocol stack. Traditional ML based approach or optimization is often not
suitable due to algorithmic complexity, reliance on existing training data
and/or due to distributed setting. In this paper, we formulate a reconfigurable
antenna based channel selection problem for interference alignment in a
multi-user wireless network as a learning problem. More specifically, we
propose that by using sequential learning, an effective channel or combination
of channels can be selected in order to enhance interference alignment using
reconfigurable antennas. We first formulate the channel selection as a
multi-armed problem that aims to optimize the sum rate of the network. We show
that by using an adaptive sequential learning policy, each node in the network
can learn to select optimal channels without requiring full and instantaneous
CSI for all the available antenna states. We conduct performance analysis of
our technique for a MIMO interference channel using a conventional IA scheme
and quantify the benefits of pattern diversity and learning channel selection.



Adaptive antenna system by ESP32-PICO-D4 and its application to web radio system

Adaptive antenna technique has an important role in the IoT environment in
order to establish reliable and stable wireless communication in high
congestion situation. Even if knowing antenna characteristics in advance,
electromagnetic wave propagation in the non-line-of-sight environment is very
complex and unpredictable, therefore, the adjustment the antenna radiation for
the optimum signal reception is important for the better wireless link. This
article presents a simple but effective adaptive antenna system for Wi-Fi
utilizing the function of a highly integrated component, ESP32-PICO-D4. This
chip is a system-in-chip containing all components for Wi-Fi and Bluetooth
application except for antenna. Together with SP3T RF switch and dielectric
antennas and high-resolution audio DAC, completed web-radio system is made in
the size of 50 x 50 mm.



Language and Noise Transfer in Speech Enhancement Generative Adversarial Network

Speech enhancement deep learning systems usually require large amounts of
training data to operate in broad conditions or real applications. This makes
the adaptability of those systems into new, low resource environments an
important topic. In this work, we present the results of adapting a speech
enhancement generative adversarial network by finetuning the generator with
small amounts of data. We investigate the minimum requirements to obtain a
stable behavior in terms of several objective metrics in two very different
languages: Catalan and Korean. We also study the variability of test
performance to unseen noise as a function of the amount of different types of
noise available for training. Results show that adapting a pre-trained English
model with 10 min of data already achieves a comparable performance to having
two orders of magnitude more data. They also demonstrate the relative stability
in test performance with respect to the number of training noise types.



Graph Transform Optimization with Application to Image Compression

In this paper, we propose a new graph-based transform and illustrate its
potential application to signal compression. Our approach relies on the careful
design of a graph that optimizes the overall rate-distortion performance
through an effective graph-based transform. We introduce a novel graph
estimation algorithm, which uncovers the connectivities between the graph
signal values by taking into consideration the coding of both the signal and
the graph topology in rate-distortion terms. In particular, we introduce a
novel coding solution for the graph by treating the edge weights as another
graph signal that lies on the dual graph. Then, the cost of the graph
description is introduced in the optimization problem by minimizing the
sparsity of the coefficients of its graph Fourier transform (GFT) on the dual
graph. In this way, we obtain a convex optimization problem whose solution
defines an efficient transform coding strategy. The proposed technique is a
general framework that can be applied to different types of signals, and we
show two possible application fields, namely natural image coding and piecewise
smooth image coding. The experimental results show that the proposed
graph-based transform outperforms classical fixed transforms such as DCT for
both natural and piecewise smooth images. In the case of depth map coding, the
obtained results are even comparable to the state-of-the-art graph-based coding
method, that are specifically designed for depth map images.



Temporal Analog Optical Computing using an On-Chip Fully Reconfigurable Photonic Signal Processor

This paper introduces the concept of on-chip temporal optical computing,
based on dispersive Fourier transform and suitably designed modulation module,
to perform mathematical operations of interest, such as differentiation,
integration, or convolution in time domain. The desired mathematical operation
is performed as signal propagates through a fully reconfigurable on-chip
photonic signal processor. Although a few number of photonic temporal signal
processors have been introduced recently, they are usually bulky or they suffer
from limited reconfigurability which is of great importance to implement
large-scale general-purpose photonic signal processors. To address these
limitations, this paper demonstrates a fully reconfigurable photonic integrated
signal processing system. As the key point, the reconfigurability is achieved
by taking advantages of dispersive Fourier transformation, linearly chirp
modulation using four wave mixing, and applying the desired arbitrary transfer
function through a cascaded Mach-Zehnder modulator and phase modulator. Our
demonstration reveals an operation time of $200~ps$ with high resolution of
$300~fs$. To have an on-chip photonic signal processor, a broadband photonic
crystal waveguide with an extremely large group-velocity dispersion of $2.81
\times {10^{6}}~\frac{{{ps^2}}}{km}$ is utilized. Numerical simulations of the
proposed structure reveal a great potential for chip-scale fully reconfigurable
all-optical signal processing through a bandwidth of $400~GHz$.



WiBall: A Time-Reversal Focusing Ball Method for Indoor Tracking

With the development of the Internet of Things technology, indoor tracking
has become a popular application nowadays, but most existing solutions can only
work in line-of-sight scenarios, or require regular re-calibration. In this
paper, we propose WiBall, an accurate and calibration-free indoor tracking
system that can work well in non-line-of-sight based on radio signals. WiBall
leverages a stationary and location-independent property of the time-reversal
focusing effect of radio signals for highly accurate moving distance
estimation. Together with the direction estimation based on inertial
measurement unit and location correction using the constraints from the
floorplan, WiBall is shown to be able to track a moving object with
decimeter-level accuracy in different environments. Since WiBall can
accommodate a large number of users with only a single pair of devices, it is
low-cost and easily scalable, and can be a promising candidate for future
indoor tracking applications.



Objects that Sound

In this paper our objectives are, first, networks that can embed audio and
visual inputs into a common space that is suitable for cross-modal retrieval;
and second, a network that can localize the object that sounds in an image,
given the audio signal. We achieve both these objectives by training from
unlabelled video using only audio-visual correspondence (AVC) as the objective
function. This is a form of cross-modal self-supervision from video.
  To this end, we design new network architectures that can be trained for
cross-modal retrieval and localizing the sound source in an image, by using the
AVC task. We make the following contributions: (i) show that audio and visual
embeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and
between-mode retrieval; (ii) explore various architectures for the AVC task,
including those for the visual stream that ingest a single image, or multiple
images, or a single image and multi-frame optical flow; (iii) show that the
semantic object that sounds within an image can be localized (using only the
sound, no motion or flow information); and (iv) give a cautionary tale on how
to avoid undesirable shortcuts in the data preparation.



SNR Wall for Cooperative Spectrum Sensing Using Generalized Energy Detector

Cognitive radio (CR) is a promising scheme to improve the spectrum
utilization. Spectrum sensing (SS) is one of the main tasks of CR. Cooperative
spectrum sensing (CSS) is used in CR to improve detection capability. Due to
its simplicity and low complexity, sensing based on energy detection known as
conventional energy detection (CED) is widely adopted. CED can be generalized
by changing the squaring operation of the amplitude of received samples by an
arbitrary positive power p which is referred to as the generalized energy
detector (GED). The performance of GED degrades when there exists noise
uncertainty (NU). In this paper, we investigate the performance of CSS by
considering the noise NU when all the secondary users (SUs) employ GED. We
derive the signal to noise ratio (SNR) wall for CSS for both hard and soft
decision combining. All the derived expressions are validated using Monte Carlo
(MC) simulations.



Bilinear residual Neural Network for the identification and forecasting of dynamical systems

Due to the increasing availability of large-scale observation and simulation
datasets, data-driven representations arise as efficient and relevant
computation representations of dynamical systems for a wide range of
applications, where model-driven models based on ordinary differential equation
remain the state-of-the-art approaches. In this work, we investigate neural
networks (NN) as physically-sound data-driven representations of such systems.
Reinterpreting Runge-Kutta methods as graphical models, we consider a residual
NN architecture and introduce bilinear layers to embed non-linearities which
are intrinsic features of dynamical systems. From numerical experiments for
classic dynamical systems, we demonstrate the relevance of the proposed
NN-based architecture both in terms of forecasting performance and model
identification.



Joint model-based recognition and localization of overlapped acoustic events using a set of distributed small microphone arrays

In the analysis of acoustic scenes, often the occurring sounds have to be
detected in time, recognized, and localized in space. Usually, each of these
tasks is done separately. In this paper, a model-based approach to jointly
carry them out for the case of multiple simultaneous sources is presented and
tested. The recognized event classes and their respective room positions are
obtained with a single system that maximizes the combination of a large set of
scores, each one resulting from a different acoustic event model and a
different beamformer output signal, which comes from one of several
arbitrarily-located small microphone arrays. By using a two-step method, the
experimental work for a specific scenario consisting of meeting-room acoustic
events, either isolated or overlapped with speech, is reported. Tests carried
out with two datasets show the advantage of the proposed approach with respect
to some usual techniques, and that the inclusion of estimated priors brings a
further performance improvement.



Improving End-to-End Speech Recognition with Policy Learning

Connectionist temporal classification (CTC) is widely used for maximum
likelihood learning in end-to-end speech recognition models. However, there is
usually a disparity between the negative maximum likelihood and the performance
metric used in speech recognition, e.g., word error rate (WER). This results in
a mismatch between the objective function and metric during training. We show
that the above problem can be mitigated by jointly training with maximum
likelihood and policy gradient. In particular, with policy learning we are able
to directly optimize on the (otherwise non-differentiable) performance metric.
We show that joint training improves relative performance by 4% to 13% for our
end-to-end model as compared to the same model learned through maximum
likelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and
5.42% and 14.70% on Librispeech test-clean and test-other set, respectively.



Improved Regularization Techniques for End-to-End Speech Recognition

Regularization is important for end-to-end speech models, since the models
are highly flexible and easy to overfit. Data augmentation and dropout has been
important for improving end-to-end models in other domains. However, they are
relatively under explored for end-to-end speech models. Therefore, we
investigate the effectiveness of both methods for end-to-end trainable, deep
speech recognition models. We augment audio data through random perturbations
of tempo, pitch, volume, temporal alignment, and adding random noise.We further
investigate the effect of dropout when applied to the inputs of all layers of
the network. We show that the combination of data augmentation and dropout give
a relative performance improvement on both Wall Street Journal (WSJ) and
LibriSpeech dataset of over 20%. Our model performance is also competitive with
other end-to-end speech models on both datasets.



Detection and classification of masses in mammographic images in a multi-kernel approach

According to the World Health Organization, breast cancer is the main cause
of cancer death among adult women in the world. Although breast cancer occurs
indiscriminately in countries with several degrees of social and economic
development, among developing and underdevelopment countries mortality rates
are still high, due to low availability of early detection technologies. From
the clinical point of view, mammography is still the most effective diagnostic
technology, given the wide diffusion of the use and interpretation of these
images. Herein this work we propose a method to detect and classify
mammographic lesions using the regions of interest of images. Our proposal
consists in decomposing each image using multi-resolution wavelets. Zernike
moments are extracted from each wavelet component. Using this approach we can
combine both texture and shape features, which can be applied both to the
detection and classification of mammary lesions. We used 355 images of fatty
breast tissue of IRMA database, with 233 normal instances (no lesion), 72
benign, and 83 malignant cases. Classification was performed by using SVM and
ELM networks with modified kernels, in order to optimize accuracy rates,
reaching 94.11%. Considering both accuracy rates and training times, we defined
the ration between average percentage accuracy and average training time in a
reverse order. Our proposal was 50 times higher than the ratio obtained using
the best method of the state-of-the-art. As our proposed model can combine high
accuracy rate with low learning time, whenever a new data is received, our work
will be able to save a lot of time, hours, in learning process in relation to
the best method of the state-of-the-art.



Y-net: 3D intracranial artery segmentation using a convolutional autoencoder

Automated segmentation of intracranial arteries on magnetic resonance
angiography (MRA) allows for quantification of cerebrovascular features, which
provides tools for understanding aging and pathophysiological adaptations of
the cerebrovascular system. Using a convolutional autoencoder (CAE) for
segmentation is promising as it takes advantage of the autoencoder structure in
effective noise reduction and feature extraction by representing high
dimensional information with low dimensional latent variables. In this report,
an optimized CAE model (Y-net) was trained to learn a 3D segmentation model of
intracranial arteries from 49 cases of MRA data. The trained model was shown to
perform better than the three traditional segmentation methods in both binary
classification and visual evaluation.



Analysis of supervised and semi-supervised GrowCut applied to segmentation of masses in mammography images

Breast cancer is already one of the most common form of cancer worldwide.
Mammography image analysis is still the most effective diagnostic method to
promote the early detection of breast cancer. Accurately segmenting tumors in
digital mammography images is important to improve diagnosis capabilities of
health specialists and avoid misdiagnosis. In this work, we evaluate the
feasibility of applying GrowCut to segment regions of tumor and we propose two
GrowCut semi-supervised versions. All the analysis was performed by evaluating
the application of segmentation techniques to a set of images obtained from the
Mini-MIAS mammography image database. GrowCut segmentation was compared to
Region Growing, Active Contours, Random Walks and Graph Cut techniques.
Experiments showed that GrowCut, when compared to the other techniques, was
able to acquire better results for the metrics analyzed. Moreover, the proposed
semi-supervised versions of GrowCut was proved to have a clinically
satisfactory quality of segmentation.



Improvement of Resting-state EEG Analysis Process with Spectrum Weight-Voting based on LES

EEG is a non-invasive technique for recording brain bioelectric activity,
which has potential applications in various fields such as human-computer
interaction and neuroscience. However, there are many difficulties in analyzing
EEG data, including its complex composition, low amplitude as well as low
signal-to-noise ratio. Some of the existing methods of analysis are based on
feature extraction and machine learning to differentiate the phase of
schizophrenia that samples belong to. However, medical research requires the
use of machine learning not only to give more accurate classification results,
but also to give the results that can be applied to pathological studies. The
main purpose of this study is to obtain the weight values as the representation
of influence of each frequency band on the classification of schizophrenia
phases on the basis of a more effective classification method using the LES
feature extraction, and then the weight values are processed and applied to
improve the accuracy of machine learning classification. We propose a method
called weight-voting to obtain the weights of sub-bands features by using
results of classification for voting to fit the actual categories of EEG data,
and using weights for reclassification. Through this method, we can first
obtain the influence of each band in distinguishing three schizophrenia phases,
and analyze the effect of band features on the risk of schizophrenia
contributing to the study of psychopathology. Our results show that there is a
high correlation between the change of weight of low gamma band and the
difference between HC, CHR and FES. If the features revised according to
weights are used for reclassification, the accuracy of result will be improved
compared with the original classifier, which confirms the role of the band
weight distribution.



Molecular Signal Modeling of a Partially Counting Absorbing Spherical Receiver

To communicate at the nanoscale, researchers have proposed molecular
communication as an energy-efficient solution. The drawback to this solution is
that the histogram of the molecules' hitting times, which constitute the
molecular signal at the receiver, has a heavy tail. Reducing the effects of
this heavy tail, inter-symbol interference (ISI), has been the focus of most
prior research. In this paper, a novel way of decreasing the ISI by defining a
counting region on the spherical receiver's surface facing towards the
transmitter node is proposed. The beneficial effect comes from the fact that
the molecules received from the back lobe of the receiver are more likely to be
coming through longer paths that contribute to ISI. In order to justify this
idea, the joint distribution of the arrival molecules with respect to angle and
time is derived. Using this distribution, the channel model function is
approximated for the proposed system, i.e., the partially counting absorbing
spherical receiver. After validating the channel model function, the
characteristics of the molecular signal are investigated and improved
performance is presented. Moreover, the optimal counting region in terms of bit
error rate is found analytically.



Tensor-driven extraction of developmental features from varying paediatric EEG datasets

Objective. Consistently changing physiological properties in developing
children's brains challenges new data heavy technologies, like brain-computer
interfaces (BCI). Advancing signal processing methods in such technologies to
be more sensitive to developmental changes could help improve their function
and usability in paediatric populations. Taking advantage of the
multi-dimensional structure of EEG data through tensor analysis offers a
framework to extract relevant developmental features present in paediatric
resting-state EEG datasets. Methods. Three paediatric datasets from varying
developmental states and populations were analyzed using a developed two-step
constrained Parallel Factor (PARAFAC) tensor decomposition. The datasets
included the Muir Maxwell Epilepsy Centre, Children's Hospital Boston-MIT and
the Child Mind Institute, outlining two impaired and one healthy population,
respectively. Within dataset cross-validation used support vector machines
(SVM) for classification of out-of-fold data predicting subject age as a proxy
measure of development. t-distributed Stochastic Neighbour Embedding (t-SNE)
maps complemented classification analysis through visualization of the
high-dimensional feature structures. Main Results. Development-sensitive
features were successfully identified for the developmental conditions of each
dataset. SVM classification accuracy and misclassification costs were improved
significantly for both healthy and impaired paediatric populations. t-SNE maps
revealed suitable tensor factorization was key in extracting developmental
features. Significance. The described methods are a promising tool for
incorporating the unique developmental features present throughout childhood
EEG into new technologies like BCI and its applications.



Beyond Powers of Two: Hexagonal Modulation and Non-Binary Coding for Wireless Communication Systems

Adaptive modulation and coding (AMC) is widely employed in modern wireless
communication systems to improve the transmission efficiency by adjusting the
transmission rate according to the channel conditions. Thus, AMC can provide
very efficient use of channel resources especially over fading channels.
Quadrature Amplitude Modulation (QAM) is an ef- ficient and widely employed
digital modulation technique. It typically employs a rectangular signal
constellation. Therefore the decision regions of the constellation are square
partitions of the two-dimensional signal space. However, it is well known that
hexagons rather than squares provide the most compact regular tiling in two
dimensions. A compact tiling means a dense packing of the constellation points
and thus more energy efficient data transmission. Hexagonal modulation can be
difficult to implement because it does not fit well with the usual power-
of-two symbol sizes employed with binary data. To overcome this problem,
non-binary coding is combined with hexagonal modulation in this paper to
provide a system which is compatible with binary data. The feasibility and
efficiency are evaluated using a software-defined radio (SDR) based prototype.
Extensive simulation results are presented which show that this approach can
provide improved energy efficiency and spectrum utilization in wireless
communication systems.



Towards a Deep Improviser: a prototype deep learning post-tonal free music generator

Two modest-sized symbolic corpora of post-tonal and post-metric keyboard
music have been constructed, one algorithmic, the other improvised. Deep
learning models of each have been trained and largely optimised. Our purpose is
to obtain a model with sufficient generalisation capacity that in response to a
small quantity of separate fresh input seed material, it can generate outputs
that are distinctive, rather than recreative of the learned corpora or the seed
material. This objective has been first assessed statistically, and as judged
by k-sample Anderson-Darling and Cramer tests, has been achieved. Music has
been generated using the approach, and informal judgements place it roughly on
a par with algorithmic and composed music in related forms. Future work will
aim to enhance the model such that it can be evaluated in relation to
expression, meaning and utility in real-time performance.



Indoor Sound Source Localization with Probabilistic Neural Network

It is known that adverse environments such as high reverberation and low
signal-to-noise ratio (SNR) pose a great challenge to indoor sound source
localization. To address this challenge, in this paper, we propose a sound
source localization algorithm based on probabilistic neural network, namely
Generalized cross correlation Classification Algorithm (GCA). Experimental
results for adverse environments with high reverberation time T60 up to 600ms
and low SNR such as -10dB show that, the average azimuth angle error and
elevation angle error by GCA are only 4.6 degrees and 3.1 degrees respectively.
Compared with three recently published algorithms, GCA has increased the
success rate on direction of arrival estimation significantly with good
robustness to environmental changes. These results show that the proposed GCA
can localize accurately and robustly for diverse indoor applications where the
site acoustic features can be studied prior to the localization stage.



Hyperspectral image unmixing with LiDAR data-aided spatial regularization

Spectral unmixing methods incorporating spatial regularizations have
demonstrated increasing interest. Although spatial regularizers which promote
smoothness of the abundance maps have been widely used, they may overly smooth
these maps and, in particular, may not preserve edges present in the
hyperspectral image. Existing unmixing methods usually ignore these edge
structures or use edge information derived from the hyperspectral image itself.
However, this information may be affected by large amounts of noise or
variations in illumination, leading to erroneous spatial information
incorporated into the unmixing procedure. This paper proposes a simple, yet
powerful, spectral unmixing framework which incorporates external data (i.e.
LiDAR data). The LiDAR measurements can be easily exploited to adjust standard
spatial regularizations applied to the unmixing process. The proposed framework
is rigorously evaluated using two simulated datasets and a real hyperspectral
image. It is compared with competing methods that rely on spatial information
derived from a hyperspectral image. The results show that the proposed
framework can provide better abundance estimates and, more specifically, can
significantly improve the abundance estimates for pixels affected by shadows.



Rate-Distributed Spatial Filtering Based Noise Reduction in Wireless Acoustic Sensor Networks

In wireless acoustic sensor networks (WASNs), sensors typically have a
limited energy budget as they are often battery driven. Energy efficiency is
therefore essential to the design of algorithms in WASNs. One way to reduce
energy costs is to only select the sensors which are most informative, a
problem known as {\it sensor selection}. In this way, only sensors that
significantly contribute to the task at hand will be involved. In this work, we
consider a more general approach, which is based on rate-distributed spatial
filtering. Together with the distance over which transmission takes place, bit
rate directly influences the energy consumption. We try to minimize the battery
usage due to transmission, while constraining the noise reduction performance.
This results in an efficient rate allocation strategy, which depends on the
underlying signal statistics, as well as the distance from sensors to a fusion
center (FC). Under the utilization of a linearly constrained minimum variance
(LCMV) beamformer, the problem is derived as a semi-definite program.
Furthermore, we show that rate allocation is more general than sensor
selection, and sensor selection can be seen as a special case of the presented
rate-allocation solution, e.g., the best microphone subset can be determined by
thresholding the rates. Finally, numerical simulations for the application of
estimating several target sources in a WASN demonstrate that the proposed
method outperforms the microphone subset selection based approaches in the
sense of energy usage, and we find that the sensors close to the FC and close
to point sources are allocated with higher rates.



A Joint Code-Frequency Index Modulation for Low-complexity, High Spectral and Energy Efficiency Communications

A relatively simple low complexity multiuser communication system based on
simultaneous code and frequency index modulation (CFIM) is proposed in this
paper. The proposed architecture reduces the emitted energy at the transmitter
as well as the peak-to-average-power ratio (PAPR) of orthogonal
frequency-division multiplexing (OFDM)- based schemes functions without
relegating data rate. In the scheme we introduce here, we implement a joint
code- frequency- index modulation (CFIM) in order to enhance spectral and
energy efficiencies. After introducing and analysing the structure with respect
to latter metrics, we derive closed-form expressions of the bit error rate
(BER) performance over Rayleigh fading channels and we validate the outcome by
simulation results. Simulation are used verify the analyses and show that, in
terms of BER, the proposed CFIM outperforms the existing index modulation (IM)
based systems such as spatial modulation (SM), OFDM-IM and OFDM schemes
significantly. To better exhibit the particularities of the proposed scheme,
PAPR, complexity, spectral efficiency (SE) and energy efficiency (EE) are
thoroughly examined. Results indicate a high SE while ensuring an elevated
reliability compared to the aforementioned systems. In addition, the concept is
extended to synchronous multiuser communication networks, where full
functionality is obtained. With the characteristics demonstrated in this work,
the proposed system would constitute an exceptional nominee for Internet of
Things (IoT) applications where low-complexity, low-power consumption and high
data rate are paramount.



Predicting physiological developments from human gait using smartphone sensor data

Coronary artery disease, heart failure, angina pectoris and diabetes are
among the leading causes of morbidity and mortality over the globe.
Susceptibility to such disorders is compounded by changing lifestyles, poor
dietary routines, aging and obesity. Besides, conventional diagnostics are
limited in their capability to detect such pathologies at an early stage. This
generates demand for automatic recommender systems that could effectively
monitor and predict pathogenic behaviors in the body. To this end, we propose
human gait analysis for predicting two important physiological parameters
associated with different diseases, body mass index and age. Predicting age and
body mass index by actively profiling the gait samples, could be further used
for providing suitable healthcare recommendations. Existing strategies for
predicting age and body mass index, however, necessitate stringent experimental
settings for achieving appropriate performance. For instance, precisely
recorded speech signals were recently used for predicting body mass indices of
different subjects. Similarly, age groups were predicted by recording gait
samples from on-body and wearable sensors. Such specialized methods limit
active and convenient profiling of human age and body mass indices. We address
these issues, by introducing smartphone sensors as a means for recording gait
signals. Using on-board accelerometer and gyroscope helps in developing
easy-to-use and accessible systems for predicting body mass index and age. To
empirically show the effectiveness of our proposed methodology, we collected
gait samples from sixty-three different subjects that were classified in body
mass index and age groups using six well-known machine learning classifiers. We
evaluated our system using two different windowing operations for feature
extraction, namely Gaussian and Square.



Analyzing transient-evoked otoacoustic emissions by concentration of frequency and time

The linear part of transient evoked (TE) otoacoustic emission (OAE) is
thought to be generated via coherent reflection near the characteristic place
of constituent wave components. Because of the tonotopic organization of the
cochlea, high frequency emissions return earlier than low frequencies; however,
due to the random nature of coherent reflection, the instantaneous frequency
(IF) and amplitude envelope of TEOAEs both fluctuate. Multiple reflection
components and synchronized spontaneous emissions can further make it difficult
to extract the IF by linear transforms. In this paper, we propose to model
TEOAEs as a sum of {\em intrinsic mode-type functions} and analyze it by a
{nonlinear-type time-frequency analysis} technique called concentration of
frequency and time (ConceFT). When tested with synthetic OAE signals {with
possibly multiple oscillatory components}, the present method is able to
produce clearly visualized traces of individual components on the
time-frequency plane. Further, when the signal is noisy, the proposed method is
compared with existing linear and bilinear methods in its accuracy for
estimating the fluctuating IF. Results suggest that ConceFT outperforms the
best of these methods in terms of optimal transport distance, reducing the
error by 10 to {21\%} when the signal to noise ratio is 10 dB or below.



On a Parametric Spectral Estimation Problem

We consider an open question posed in \cite{Zhu-Baggio-17} on the uniqueness
of the solution to a parametric spectral estimation problem.



On the Use of a Spectral Glottal Model for the Source-filter Separation of Speech

The estimation of glottal flow from a speech waveform is a key method for
speech analysis and parameterization. Significant research effort has been made
to dissociate the first vocal tract resonance from the glottal formant (the
low-frequency resonance describing the open-phase of the vocal fold vibration).
However few methods cope with estimation of high-frequency spectral tilt to
describe the return-phase of the vocal fold vibration, which is crucial to the
perception of vocal effort. This paper proposes an improved version of the
well-known Iterative Adaptive Inverse Filtering (IAIF) called GFM-IAIF.
GFM-IAIF includes a full spectral model of the glottis that incorporates both
glottal formant and spectral tilt features. Comparisons with the standard IAIF
method show that while GFM-IAIF maintains good performance on vocal tract
removal, it significantly improves the perceptive timbral variations associated
to vocal effort.



Multisensor Poisson Multi-Bernoulli Filter for Joint Target-Sensor State Tracking

In a typical multitarget tracking (MTT) scenario, the sensor state is either
assumed known, or tracking is performed in the sensor's (relative) coordinate
frame. This assumption does not hold when the sensor, e.g., an automotive
radar, is mounted on a vehicle, and the target state should be represented in a
global (absolute) coordinate frame. Then it is important to consider the
uncertain location of the vehicle on which the sensor is mounted for MTT. In
this paper, we present a multisensor low complexity Poisson multi-Bernoulli MTT
filter, which jointly tracks the uncertain vehicle state and target states.
Measurements collected by different sensors mounted on multiple vehicles with
varying location uncertainty are incorporated sequentially based on the arrival
of new sensor measurements. In doing so, targets observed from a sensor mounted
on a well-localized vehicle reduce the state uncertainty of other poorly
localized vehicles, provided that a common non-empty subset of targets is
observed. A low complexity filter is obtained by approximations of the joint
sensor-feature state density minimizing the Kullback-Leibler divergence (KLD).
Results from synthetic as well as experimental measurement data, collected in a
vehicle driving scenario, demonstrate the performance benefits of joint
vehicle-target state tracking.



Analysis-synthesis model learning with shared features: a new framework for histopathological image classification

Automated histopathological image analysis offers exciting opportunities for
the early diagnosis of several medical conditions including cancer. There are
however stiff practical challenges: 1.) discriminative features from such
images for separating diseased vs. healthy classes are not readily apparent,
and 2.) distinct classes, e.g. healthy vs. stages of disease continue to share
several geometric features. We propose a novel Analysis-synthesis model
Learning with Shared Features algorithm (ALSF) for classifying such images more
effectively. In ALSF, a joint analysis and synthesis learning model is
introduced to learn the classifier and the feature extractor at the same time.
In this way, the computation load in patch-level based image classification can
be much reduced. Crucially, we integrate into this framework the learning of a
low rank shared dictionary and a shared analysis operator, which more
accurately represents both similarities and differences in histopathological
images from distinct classes. ALSF is evaluated on two challenging databases:
(1) kidney tissue images provided by the Animal Diagnosis Lab (ADL) at the
Pennsylvania State University and (2) brain tumor images from The Cancer Genome
Atlas (TCGA) database. Experimental results confirm that ALSF can offer
benefits over state of the art alternatives.



COOJA Network Simulator: Exploring the Infinite Possible Ways to Compute the Performance Metrics of IOT Based Smart Devices to Understand the Working of IOT Based Compression & Routing Protocols

This paper demonstrates the scheme regarding Internet of Things (IOT) which
is well thought-out the next generation of Internet. IOT explicitly elaborates
the assimilation of human beings and physical systems, as they can cooperate
with each other so leading towards a sort of encroachment in networking by
interconnecting things together while making use of wireless embedded systems,
said to be the building blocks of IOT, that are capable to be given an IP
address and thus making them part of the global internet. Several essential
approaches that entail in IOT and supports this innovation are being argued in
this paper. 6LoWPAN (IPV6 Low Power Personal Area Networks) is a protocol used
to appropriately and efficiently use IPV6 addresses. Control messages of RPL
routing protocol for low power devices are discussed to understand the working
of RPL protocol. In the end Contiki OS based COOJA Network simulator is used to
demonstrate the working of how these routing and compression protocol works in
real time simulation.



An Efficient Spectral Leakage Filtering for IEEE 802.11af in TV White Space

Orthogonal frequency division multiplexing (OFDM) has been widely adopted for
modern wireless standards and become a key enabling technology for cognitive
radios. However, one of its main drawbacks is significant spectral leakage due
to the accumulation of multiple sinc-shaped subcarriers. In this paper, we
present a novel pulse shaping scheme for efficient spectral leakage suppression
in OFDM based physical layer of IEEE 802.11af standard. With conventional pulse
shaping filters such as a raised-cosine filter, vestigial symmetry can be used
to reduce spectral leakage very effectively. However, these pulse shaping
filters require long guard interval, i.e., cyclic prefix in an OFDM system, to
avoid inter-symbol interference (ISI), resulting in a loss of spectral
efficiency. The proposed pulse shaping method based on asymmetric pulse shaping
achieves better spectral leakage suppression and decreases ISI caused by
filtering as compared to conventional pulse shaping filters.



Music of Brain and Music on Brain: A Novel EEG Sonification approach

Can we hear the sound of our brain? Is there any technique which can enable
us to hear the neuro-electrical impulses originating from the different lobes
of brain? The answer to all these questions is YES. In this paper we present a
novel method with which we can sonify the Electroencephalogram (EEG) data
recorded in rest state as well as under the influence of a simplest acoustical
stimuli - a tanpura drone. The tanpura drone has a very simple yet very complex
acoustic features, which is generally used for creation of an ambiance during a
musical performance. Hence, for this pilot project we chose to study the
correlation between a simple acoustic stimuli (tanpura drone) and sonified EEG
data. Till date, there have been no study which deals with the direct
correlation between a bio-signal and its acoustic counterpart and how that
correlation varies under the influence of different types of stimuli. This is
the first of its kind study which bridges this gap and looks for a direct
correlation between music signal and EEG data using a robust mathematical
microscope called Multifractal Detrended Cross Correlation Analysis (MFDXA).
For this, we took EEG data of 10 participants in 2 min 'rest state' (i.e. with
white noise) and in 2 min 'tanpura drone' (musical stimulus) listening
condition. Next, the EEG signals from different electrodes were sonified and
MFDXA technique was used to assess the degree of correlation (or the cross
correlation coefficient) between tanpura signal and EEG signals. The variation
of {\gamma}x for different lobes during the course of the experiment also
provides major interesting new information. Only music stimuli has the ability
to engage several areas of the brain significantly unlike other stimuli (which
engages specific domains only).



Reconfigurable Digital Channelizer Design Using Factored Markov Decision Processes

In this work, a novel digital channelizer design is developed through the use
of a compact, system-level modeling approach. The model efficiently captures
key properties of a digital channelizer system and its time-varying operation.
The model applies powerful Markov Decision Process (MDP) techniques in new ways
for design optimization of reconfigurable channelization processing. The result
is a promising methodology for design and implementation of digital
channelizers that adapt dynamically to changing use cases and stochastic
environments while optimizing simultaneously for multiple conflicting
performance goals. The method is used to employ an MDP to generate a runtime
reconfiguration policy for a time-varying environment. Through extensive
simulations, the robustness of the adaptation is demonstrated in comparison
with the prior state of the art.



On Using Backpropagation for Speech Texture Generation and Voice Conversion

Inspired by recent work on neural network image generation which rely on
backpropagation towards the network inputs, we present a proof-of-concept
system for speech texture synthesis and voice conversion based on two
mechanisms: approximate inversion of the representation learned by a speech
recognition neural network, and on matching statistics of neuron activations
between different source and target utterances. Similar to image texture
synthesis and neural style transfer, the system works by optimizing a cost
function with respect to the input waveform samples. To this end we use a
differentiable mel-filterbank feature extraction pipeline and train a
convolutional CTC speech recognition network. Our system is able to extract
speaker characteristics from very limited amounts of target speaker data, as
little as a few seconds, and can be used to generate realistic speech babble or
reconstruct an utterance in a different voice.



Music Genre Classification with Paralleling Recurrent Convolutional Neural Network

Deep learning has been demonstrated its effectiveness and efficiency in music
genre classification. However, the existing achievements still have several
shortcomings which impair the performance of this classification task. In this
paper, we propose a hybrid architecture which consists of the paralleling CNN
and Bi-RNN blocks. They focus on spatial features and temporal frame orders
extraction respectively. Then the two outputs are fused into one powerful
representation of musical signals and fed into softmax function for
classification. The paralleling network guarantees the extracting features
robust enough to represent music. Moreover, the experiments prove our proposed
architecture improve the music genre classification performance and the
additional Bi-RNN block is a supplement for CNNs.



GSM-CommSense-based through-the-wall sensing

We have recently proposed a scheme to use the channel equalization blocks of
telecommunication systems to sense changes in an environment. We call this
communication-sensing, CommSense for short. After some initial positive results
we tried to use our global system for mobile communication (GSM) based
CommSense system for a through-the-wall sensing application. As the system was
inherently highly under-determined we used statistical machine learning
techniques to help us sense environmental changes in the behind-the-wall
experiments. We observed that with limited amount of data per GSM frame of 577
{\mu}s a person can be detected across a wall to an accuracy of 77.458% and a
person carrying a weapon can be detected to an accuracy of 95.208%. We present
details of the experiments and the encouraging results that we have obtained in
this article.



Evaluation of PPG Biometrics for Authentication in different states

Amongst all medical biometric traits, Photoplethysmograph (PPG) is the
easiest to acquire. PPG records the blood volume change with just combination
of Light Emitting Diode and Photodiode from any part of the body. With IoT and
smart homes' penetration, PPG recording can easily be integrated with other
vital wearable devices. PPG represents peculiarity of hemodynamics and
cardiovascular system for each individual. This paper presents non-fiducial
method for PPG based biometric authentication. Being a physiological signal,
PPG signal alters with physical/mental stress and time. For robustness, these
variations cannot be ignored. While, most of the previous works focused only on
single session, this paper demonstrates extensive performance evaluation of PPG
biometrics against single session data, different emotions, physical exercise
and time-lapse using Continuous Wavelet Transform (CWT) and Direct Linear
Discriminant Analysis (DLDA). When evaluated on different states and datasets,
equal error rate (EER) of $0.5\%$-$6\%$ was achieved for $45$-$60$s average
training time. Our CWT/DLDA based technique outperformed all other
dimensionality reduction techniques and previous work.



Travel time tomography with adaptive dictionaries

We develop a 2D travel time tomography method which regularizes the inversion
by modeling groups of slowness pixels from discrete slowness maps, called
patches, as sparse linear combinations of atoms from a dictionary. We propose
to use dictionary learning during the inversion to adapt dictionaries to
specific slowness maps. This patch regularization, called the local model, is
integrated into the overall slowness map, called the global model. The local
model considers small-scale variations using a sparsity constraint and the
global model considers larger-scale features constrained using $\ell_2$
regularization. This strategy in a locally-sparse travel time tomography (LST)
approach enables simultaneous modeling of smooth and discontinuous slowness
features. This is in contrast to conventional tomography methods, which
constrain models to be exclusively smooth or discontinuous. We develop a
$\textit{maximum a posteriori}$ formulation for LST and exploit the sparsity of
slowness patches using dictionary learning. The LST approach compares favorably
with smoothness and total variation regularization methods on densely, but
irregularly sampled synthetic slowness maps.



Quickest Attack Detection in Smart Grid Based on Sequential Monte Carlo Filtering

Quick and accurate detection of cyber attack is key to the normal operation
of the smart grid system. In this paper, a joint state estimation and
sequential attack detection method for a given bus with grid frequency drift is
proposed that utilizes the commonly monitored output voltage. In particular,
based on a non-linear state-space model derived from the three-phase sinusoidal
voltage equations, we employ the sequential Monte Carlo (SMC) filtering to
estimate the system state. The output of the SMC filter is fed into a CUSUM
test to detect the attack in a fastest way. Moreover, an adaptive sampling
strategy is proposed to reduce the rate of taking measurements and
communicating with the controller. Extensive simulation results demonstrate
that the proposed method achieves high adaptivity and efficient detection of
various types of attacks in power systems.



Boundary-sensitive Network for Portrait Segmentation

Compared to the general semantic segmentation problem, portrait segmentation
has higher precision requirement on boundary area. However, this problem has
not been well studied in previous works. In this paper, we propose a
boundary-sensitive deep neural network (BSN) for portrait segmentation. BSN
introduces three novel techniques. First, an individual boundary-sensitive
kernel is proposed by dilating the contour line and assigning the boundary
pixels with multi-class labels. Second, a global boundary-sensitive kernel is
employed as a position sensitive prior to further constrain the overall shape
of the segmentation map. Third, we train a boundary-sensitive attribute
classifier jointly with the segmentation network to reinforce the network with
semantic boundary shape information. We have evaluated BSN on the current
largest public portrait segmentation dataset, i.e, the PFCN dataset, as well as
the portrait images collected from other three popular image segmentation
datasets: COCO, COCO-Stuff, and PASCAL VOC. Our method achieves the superior
quantitative and qualitative performance over state-of-the-arts on all the
datasets, especially on the boundary area.



Variational Autoencoders for Learning Latent Representations of Speech Emotion: A Preliminary Study

Learning the latent representation of data in unsupervised fashion is a very
interesting process that provides relevant features for enhancing the
performance of a classifier. For speech emotion recognition tasks, generating
effective features is crucial. Currently, handcrafted features are mostly used
for speech emotion recognition, however, features learned automatically using
deep learning have shown strong success in many problems, especially in image
processing. In particular, deep generative models such as Variational
Autoencoders (VAEs) have gained enormous success for generating features for
natural images. Inspired by this, we propose VAEs for deriving the latent
representation of speech signals and use this representation to classify
emotions. To the best of our knowledge, we are the first to propose VAEs for
speech emotion classification. Evaluations on the IEMOCAP dataset demonstrate
that features learned by VAEs can produce state-of-the-art results for speech
emotion classification.



Texture Object Segmentation Based on Affine Invariant Texture Detection

To solve the issue of segmenting rich texture images, a novel detection
methods based on the affine invariable principle is proposed. Considering the
similarity between the texture areas, we first take the affine transform to get
numerous shapes, and utilize the KLT algorithm to verify the similarity. The
transforms include rotation, proportional transformation and perspective
deformation to cope with a variety of situations. Then we propose an improved
LBP method combining canny edge detection to handle the boundary in the
segmentation process. Moreover, human-computer interaction of this method which
helps splitting the matched texture area from the original images is
user-friendly.



A Low-Cost Robust Distributed Linearly Constrained Beamformer for Wireless Acoustic Sensor Networks with Arbitrary Topology

We propose a new robust distributed linearly constrained beamformer which
utilizes a set of linear equality constraints to reduce the cross power
spectral density matrix to a block-diagonal form. The proposed beamformer has a
convenient objective function for use in arbitrary distributed network
topologies while having identical performance to a centralized implementation.
Moreover, the new optimization problem is robust to relative acoustic transfer
function (RATF) estimation errors and to target activity detection (TAD)
errors. Two variants of the proposed beamformer are presented and evaluated in
the context of multi-microphone speech enhancement in a wireless acoustic
sensor network, and are compared with other state-of-the-art distributed
beamformers in terms of communication costs and robustness to RATF estimation
errors and TAD errors.



Leveraging Native Language Speech for Accent Identification using Deep Siamese Networks

The problem of automatic accent identification is important for several
applications like speaker profiling and recognition as well as for improving
speech recognition systems. The accented nature of speech can be primarily
attributed to the influence of the speaker's native language on the given
speech recording. In this paper, we propose a novel accent identification
system whose training exploits speech in native languages along with the
accented speech. Specifically, we develop a deep Siamese network-based model
which learns the association between accented speech recordings and the native
language speech recordings. The Siamese networks are trained with i-vector
features extracted from the speech recordings using either an unsupervised
Gaussian mixture model (GMM) or a supervised deep neural network (DNN) model.
We perform several accent identification experiments using the CSLU Foreign
Accented English (FAE) corpus. In these experiments, our proposed approach
using deep Siamese networks yield significant relative performance improvements
of 15.4 percent on a 10-class accent identification task, over a baseline
DNN-based classification system that uses GMM i-vectors. Furthermore, we
present a detailed error analysis of the proposed accent identification system.



Overcomplete Frame Thresholding for Acoustic Scene Analysis

In this work, we derive a generic overcomplete frame thresholding scheme
based on risk minimization. Overcomplete frames being favored for analysis
tasks such as classification, regression or anomaly detection, we provide a way
to leverage those optimal representations in real-world applications through
the use of thresholding. We validate the method on a large scale bird activity
detection task via the scattering network architecture performed by means of
continuous wavelets, known for being an adequate dictionary in audio
environments.



RIBBONS: Rapid Inpainting Based on Browsing of Neighborhood Statistics

Image inpainting refers to filling missing places in images using neighboring
pixels. It also has many applications in different tasks of image processing.
Most of these applications enhance the image quality by significant unwanted
changes or even elimination of some existing pixels. These changes require
considerable computational complexities which in turn results in remarkable
processing time. In this paper we propose a fast inpainting algorithm called
RIBBONS based on selection of patches around each missing pixel. This would
accelerate the execution speed and the capability of online frame inpainting in
video. The applied cost-function is a combination of statistical and spatial
features in all neighboring pixels. We evaluate some candidate patches using
the proposed cost function and minimize it to achieve the final patch.
Experimental results show the higher speed of 'Ribbons' in comparison with
previous methods while being comparable in terms of PSNR and SSIM for the
images in MISC dataset.



Sampling and Recovery of Graph Signals

The aim of this chapter is to give an overview of the recent advances related
to sampling and recovery of signals defined over graphs. First, we illustrate
the conditions for perfect recovery of bandlimited graph signals from samples
collected over a selected set of vertexes. Then, we describe some sampling
design criteria proposed in the literature to mitigate the effect of noise and
model mismatching when performing graph signal recovery. Finally, we illustrate
algorithms and optimal sampling strategies for adaptive recovery and tracking
of dynamic graph signals, where both sampling set and signal values are allowed
to vary with time. Numerical simulations carried out over both synthetic and
real data illustrate the potential advantages of graph signal processing
methods for sampling, interpolation, and tracking of signals observed over
irregular domains such as, e.g., technological or biological networks.



Analytic signal in many dimensions

In this work we extend analytic signal theory to the multidimensional case
when oscillations are observed in the $d$ orthogonal directions. First it is
shown how to obtain separate phase-shifted components and how to combine them
into instantaneous amplitude and phases. Second, the proper hypercomplex
analytic signal is defined as holomorphic hypercomplex function on the boundary
of certain upper half-space. Next it is shown that correct phase-shifted
components can be obtained by positive frequency restriction of hypercomplex
Fourier transform. Necessary and sufficient conditions for analytic extension
of the hypercomplex analytic signal into the upper hypercomplex half-space by
means of holomorphic Fourier transform are given by the corresponding
Paley-Wiener theorem. Moreover it is demonstrated that for $d>2$ there is no
corresponding non-commutative hypercomplex Fourier transform (including
Clifford and Cayley-Dickson based) that allows to recover phase-shifted
components correctly.



Audio to Body Dynamics

We present a method that gets as input an audio of violin or piano playing,
and outputs a video of skeleton predictions which are further used to animate
an avatar. The key idea is to create an animation of an avatar that moves their
hands similarly to how a pianist or violinist would do, just from audio. Aiming
for a fully detailed correct arms and fingers motion is a goal, however, it's
not clear if body movement can be predicted from music at all. In this paper,
we present the first result that shows that natural body dynamics can be
predicted at all. We built an LSTM network that is trained on violin and piano
recital videos uploaded to the Internet. The predicted points are applied onto
a rigged avatar to create the animation.



Comparing Radio Propagation Channels Between 28 and 140 GHz Bands in a Shopping Mall

In this paper, we compare the radio propagation channels characteristics
between 28 and 140 GHz bands based on the wideband (several GHz) and
directional channel sounding in a shopping mall environment. The measurements
and data processing are conducted in such a way to meet requirements for a fair
comparison of large- and small- scale channel parameters between the two bands.
Our results reveal that there is high spatial-temporal correlation between 28
and 140 GHz channels, similar numbers of strong multipath components, and only
small variations in the large-scale parameters between the two bands.
Furthermore, when including the weak paths there are higher total numbers of
clusters and paths in 28 GHz as compared to those in 140 GHz bands. With these
similarities, it would be very interesting to investigate the potentials of
using 140 GHz band in the future mobile radio communications.



Eventness: Object Detection on Spectrograms for Temporal Localization of Audio Events

In this paper, we introduce the concept of Eventness for audio event
detection, which can, in part, be thought of as an analogue to Objectness from
computer vision. The key observation behind the eventness concept is that audio
events reveal themselves as 2-dimensional time-frequency patterns with specific
textures and geometric structures in spectrograms. These time-frequency
patterns can then be viewed analogously to objects occurring in natural images
(with the exception that scaling and rotation invariance properties do not
apply). With this key observation in mind, we pose the problem of detecting
monophonic or polyphonic audio events as an equivalent visual object(s)
detection problem under partial occlusion and clutter in spectrograms. We adapt
a state-of-the-art visual object detection model to evaluate the audio event
detection task on publicly available datasets. The proposed network has
comparable results with a state-of-the-art baseline and is more robust on
minority events. Provided large-scale datasets, we hope that our proposed
conceptual model of eventness will be beneficial to the audio signal processing
community towards improving performance of audio event detection.



Multiple Instance Deep Learning for Weakly Supervised Small-Footprint Audio Event Detection

State-of-the-art audio event detection (AED) systems rely on supervised
learning using strongly labeled data. However, this dependence severely limits
scalability to large-scale datasets where fine resolution annotations are too
expensive to obtain. In this paper, we propose a small-footprint multiple
instance learning (MIL) framework for multi-class AED using weakly annotated
labels. The proposed MIL framework uses audio embeddings extracted from a
pre-trained convolutional neural network as input features. We show that by
using audio embeddings the MIL framework can be implemented using a simple DNN
with performance comparable to recurrent neural networks.
  We evaluate our approach by training an audio tagging system using a subset
of AudioSet, which is a large collection of weakly labeled YouTube video
excerpts. Combined with a late-fusion approach, we improve the F1 score of a
baseline audio tagging system by 17%. We show that audio embeddings extracted
by the convolutional neural networks significantly boost the performance of all
MIL models. This framework reduces the model complexity of the AED system and
is suitable for applications where computational resources are limited.



A Light-Weight Multimodal Framework for Improved Environmental Audio Tagging

The lack of strong labels has severely limited the state-of-the-art fully
supervised audio tagging systems to be scaled to larger dataset. Meanwhile,
audio-visual learning models based on unlabeled videos have been successfully
applied to audio tagging, but they are inevitably resource hungry and require a
long time to train. In this work, we propose a light-weight, multimodal
framework for environmental audio tagging. The audio branch of the framework is
a convolutional and recurrent neural network (CRNN) based on multiple instance
learning (MIL). It is trained with the audio tracks of a large collection of
weakly labeled YouTube video excerpts; the video branch uses pretrained
state-of-the-art image recognition networks and word embeddings to extract
information from the video track and to map visual objects to sound events.
Experiments on the audio tagging task of the DCASE 2017 challenge show that the
incorporation of video information improves a strong baseline audio tagging
system by 5.3\% absolute in terms of $F_1$ score. The entire system can be
trained within 6~hours on a single GPU, and can be easily carried over to other
audio tasks such as speech sentimental analysis.



Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning Architectures

Objective: A clinical decision support tool that automatically interprets
EEGs can reduce time to diagnosis and enhance real-time applications such as
ICU monitoring. Clinicians have indicated that a sensitivity of 95% with a
specificity below 5% was the minimum requirement for clinical acceptance. We
propose a highperformance classification system based on principles of big data
and machine learning. Methods: A hybrid machine learning system that uses
hidden Markov models (HMM) for sequential decoding and deep learning networks
for postprocessing is proposed. These algorithms were trained and evaluated
using the TUH EEG Corpus, which is the world's largest publicly available
database of clinical EEG data. Results: Our approach delivers a sensitivity
above 90% while maintaining a specificity below 5%. This system detects three
events of clinical interest: (1) spike and/or sharp waves, (2) periodic
lateralized epileptiform discharges, (3) generalized periodic epileptiform
discharges. It also detects three events used to model background noise: (1)
artifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep
learning system can deliver a low false alarm rate on EEG event detection,
making automated analysis a viable option for clinicians. Significance: The TUH
EEG Corpus enables application of highly data consumptive machine learning
algorithms to EEG analysis. Performance is approaching clinical acceptance for
real-time applications.



Deep Architectures for Automated Seizure Detection in Scalp EEGs

Automated seizure detection using clinical electroencephalograms is a
challenging machine learning problem because the multichannel signal often has
an extremely low signal to noise ratio. Events of interest such as seizures are
easily confused with signal artifacts (e.g, eye movements) or benign variants
(e.g., slowing). Commercially available systems suffer from unacceptably high
false alarm rates. Deep learning algorithms that employ high dimensional models
have not previously been effective due to the lack of big data resources. In
this paper, we use the TUH EEG Seizure Corpus to evaluate a variety of hybrid
deep structures including Convolutional Neural Networks and Long Short-Term
Memory Networks. We introduce a novel recurrent convolutional architecture that
delivers 30% sensitivity at 7 false alarms per 24 hours. We have also evaluated
our system on a held-out evaluation set based on the Duke University Seizure
Corpus and demonstrate that performance trends are similar to the TUH EEG
Seizure Corpus. This is a significant finding because the Duke corpus was
collected with different instrumentation and at different hospitals. Our work
shows that deep learning architectures that integrate spatial and temporal
contexts are critical to achieving state of the art performance and will enable
a new generation of clinically-acceptable technology.



OPARC: Optimal and Precise Array Response Control Algorithm -- Part I: Fundamentals

In this paper, the problem of how to optimally and precisely control array
response levels is addressed. By using the concept of the optimal weight vector
from the adaptive array theory and adding virtual interferences one by one, the
change rule of the optimal weight vector is found and a new formulation of the
weight vector update is thus devised. Then, the issue of how to precisely
control the response level of one single direction is investigated. More
specifically, we assign a virtual interference to a direction such that the
response level can be precisely controlled. Moreover, the parameters, such as,
the interference-to-noise ratio (INR), can be figured out according to the
desired level. Additionally, the parameter optimization is carried out to
obtain the maximal array gain. The resulting scheme is called optimal and
precise array response control (OPARC) in this paper. To understand it better,
its properties are given, and its comparison with the existing accurate array
response control ($ {\textrm A}^2\textrm{RC} $) algorithm is provided. Finally,
simulation results are presented to verify the effectiveness and superiority of
the proposed OPARC.



OPARC: Optimal and Precise Array Response Control Algorithm -- Part II: Multi-points and Applications

In this paper, the optimal and precise array response control (OPARC)
algorithm proposed in Part I of this two paper series is extended from single
point to multi-points. Two computationally attractive parameter determination
approaches are provided to maximize the array gain under certain constraints.
In addition, the applications of the multi-point OPARC algorithm to array
signal processing are studied. It is applied to realize array pattern synthesis
(including the general array case and the large array case), multi-constraint
adaptive beamforming and quiescent pattern control, where an innovative concept
of normalized covariance matrix loading (NCL) is proposed. Finally, simulation
results are presented to validate the superiority and effectiveness of the
multi-point OPARC algorithm.



A novel array response control algorithm via oblique projection

This paper presents a novel array response control algorithm and its
application to array pattern synthesis. The proposed algorithm considers how to
flexibly and precisely adjust the array responses at multiple points, on the
basis of one given weight vector. With the principle of adaptive beamforming,
it is shown that the optimal weight vector for array response control can be
equivalently obtained with a different manner, in which a linear transformation
is conducted on the quiescent weight. This new strategy is utilized to realize
multi-point precise array response control from one given weight vector, and it
obtains a closed-form solution. A careful analysis shows that the response
levels at given points can be independently, flexibly and accurately adjusted
by simply varying the parameter vector, and that the uncontrolled region
remains almost unchanged. By applying the proposed algorithm, an effective
pattern synthesis approach is devised. Simulation results are provided to
demonstrate the performance of the proposed algorithm.



Enhanced Radar Imaging Using a Complex-valued Convolutional Neural Network

Convolutional neural networks (CNN) have been successfully employed to tackle
several remote sensing tasks such as image classification and show better
performance than previous techniques. For the radar imaging community, a
natural question is: Can CNN be introduced to radar imaging and enhance its
performance? The presented letter gives an affirmative answer to this question.
We firstly propose a processing framework by which a complex-valued CNN
(CV-CNN) is used to enhance radar imaging. Then we introduce two modifications
to the CV-CNN to adapt it to radar imaging tasks. Subsequently, the method to
generate training data is shown and some implementation details are presented.
Finally, simulations and experiments are carried out, and both results show the
superiority of the proposed method on imaging quality and computational
efficiency.



Objective evaluation metrics for automatic classification of EEG events

The evaluation of machine learning algorithms in biomedical fields for
applications involving sequential data lacks standardization. Common
quantitative scalar evaluation metrics such as sensitivity and specificity can
often be misleading depending on the requirements of the application.
Evaluation metrics must ultimately reflect the needs of users yet be
sufficiently sensitive to guide algorithm development. Feedback from critical
care clinicians who use automated event detection software in clinical
applications has been overwhelmingly emphatic that a low false alarm rate,
typically measured in units of the number of errors per 24 hours, is the single
most important criterion for user acceptance. Though using a single metric is
not often as insightful as examining performance over a range of operating
conditions, there is a need for a single scalar figure of merit. In this paper,
we discuss the deficiencies of existing metrics for a seizure detection task
and propose several new metrics that offer a more balanced view of performance.
We demonstrate these metrics on a seizure detection task based on the TUH EEG
Corpus. We show that two promising metrics are a measure based on a concept
borrowed from the spoken term detection literature, Actual Term-Weighted Value
(ATWV), and a new metric, Time-Aligned Event Scoring (TAES), that accounts for
the temporal alignment of the hypothesis to the reference annotation. We also
demonstrate that state of the art technology based on deep learning, though
impressive in its performance, still needs significant improvement before it
meets very strict user acceptance criteria.



Polyp detection inside the capsule endoscopy: an approach for power consumption reduction

Capsule endoscopy is a novel and non-invasive method for diagnosis, which
assists gastroenterologists to monitor the digestive track. Although this new
technology has many advantages over the conventional endoscopy, there are
weaknesses that limits the usage of this technology. Some weaknesses are due to
using small-size batteries. Radio transmitter consumes the largest portion of
energy; consequently, a simple way to reduce the power consumption is to reduce
the data to be transmitted. Many works are proposed to reduce the amount of
data to be transmitted consist of specific compression methods and reduction in
video resolution and frame rate. We proposed a system inside the capsule for
detecting informative frames and sending these frames instead of several
non-informative frames. In this work, we specifically focused on hardware
friendly algorithm (with capability of parallelism and pipeline) for
implementation of polyp detection. Two features of positive contrast and
customized edges of polyps are exploited to define whether the frame consists
of polyp or not. The proposed method is devoid of complex and iterative
structure to save power and reduce the response time. Experimental results
indicate acceptable rate of detection of our work.



Spectral analysis for nonstationary audio

A new approach for the analysis of nonstationary signals is proposed, with a
focus on audio applications. Following earlier contributions, nonstationarity
is modeled via stationarity-breaking operators acting on Gaussian stationary
random signals. The focus is on time warping and amplitude modulation, and an
approximate maximum-likelihood approach based on suitable approximations in the
wavelet transform domain is developed. This paper provides theoretical analysis
of the approximations, and introduces JEFAS, a corresponding estimation
algorithm. The latter is tested and validated on synthetic as well as real
audio signal.



Communications and Control for Wireless Drone-Based Antenna Array

In this paper, the effective use of multiple quadrotor drones as an aerial
antenna array that provides wireless service to ground users is investigated.
In particular, under the goal of minimizing the airborne service time needed
for communicating with ground users, a novel framework for deploying and
operating a drone-based antenna array system whose elements are single-antenna
drones is proposed. In the considered model, the service time is minimized by
minimizing the wireless transmission time as well as the control time that is
needed for movement and stabilization of the drones. To minimize the
transmission time, first, the antenna array gain is maximized by optimizing the
drone spacing within the array. In this case, using perturbation techniques,
the drone spacing optimization problem is addressed by solving successive,
perturbed convex optimization problems. Then, the optimal locations of the
drones around the array's center are derived such that the transmission time
for the user is minimized. Given the determined optimal locations of drones,
the drones must spend a control time to adjust their positions dynamically so
as to serve multiple users. To minimize this control time of the quadrotor
drones, the speed of rotors is optimally adjusted based on both the
destinations of the drones and external forces (e.g., wind and gravity). In
particular, using bang-bang control theory, the optimal rotors' speeds as well
as the minimum control time are derived in closed-form. Simulation results show
that the proposed approach can significantly reduce the service time to ground
users compared to a fixed-array case in which the same number of drones form a
fixed uniform antenna array. The results also show that, in comparison with the
fixed-array case, the network's spectral efficiency can be improved by 32%
while leveraging the drone antenna array system.



Logarithmic Frequency Scaling and Consistent Frequency Coverage for the Selection of Auditory Filterbank Center Frequencies

This paper provides new insights into the problem of selecting filter center
frequencies for the auditory filterbanks. We propose to use a constant
frequency distance and a consistent frequency coverage as the two metrics that
motivate the logarithmic frequency scaling and a regularized selection of
center frequencies. The frequency scaling and the consistent frequency coverage
have been derived based on a common harmonic speaker signal model. Furthermore,
we have found that the existing linear equivalent rectangular bandwidth (ERB)
function as well as any possible linear ERB approximation can also lead to a
consistent frequency coverage. The results are verified and demonstrated using
the gammatone filterbank.



A PDE-based log-agnostic illumination correction algorithm

This report presents the results of a partial differential equation
(PDE)-based image enhancement algorithm, for dynamic range compression and
illumination correction in the absence of the logarithmic function. The
proposed algorithm combines forward and reverse flows in a PDE-based
formulation. The experimental results are compared with algorithms from the
literature and indicate comparable performance in most cases.



Image Inpainting by Hyperbolic Selection of Pixels for Two Dimensional Bicubic Interpolations

Image inpainting is a restoration process which has numerous applications.
Restoring of scanned old images with scratches, or removing objects in images
are some of inpainting applications. Different approaches have been used for
implementation of inpainting algorithms. Interpolation approaches only consider
one direction for this purpose. In this paper we present a new perspective to
image inpainting. We consider multiple directions and apply both
one-dimensional and two-dimensional bicubic interpolations. Neighboring pixels
are selected in a hyperbolic formation to better preserve corner pixels. We
compare our work with recent inpainting approaches to show our superior
results.



Does the Cross-Talk Between Nonlinear Modes Limit the Performance of NFDM Systems?

We show a non-negligible cross-talk between nonlinear modes in Nonlinear
Frequency-Division Multiplexed system when data is modulated over the nonlinear
Fourier spectrum, both the continuous spectrum and the discrete spectrum, and
transmitted over a lumped amplified fiber link. We evaluate the performance
loss if the cross-talks are neglected.



Directional Modulation Design Based on Crossed-Dipole Arrays for Two Signals With Orthogonal Polarisations

Directional modulation (DM) is a physical layer security technique based on
antenna arrays and so far the polarisation information has not been considered
in its designs. To increase the channel capacity, we consider exploiting the
polarisation information and send two different signals simultaneously at the
same direction, same frequency, but with different polarisations. These two
signals can also be considered as one composite signal using the four
dimensional (4-D) modulation scheme across the two polarisation diversity
channels. In this paper, based on cross-dipole arrays, we formulate the design
to find a set of common weight coefficients to achieve directional modulation
for such a composite signal and examples are provided to verify the
effectiveness of the proposed method.



Experimental Evaluation of the Effects of Antenna Radiation Characteristics on Heart Rate Monitoring Radar Systems

This paper presents an experimental study to evaluate the effects of antenna
radiation parameters on the detection capabilities of a 2.4 GHz Doppler radar
used in non-contact heart rate monitoring systems. Four different types of
patch antennas and array configurations were implemented on both the
transmitter and receiver sides. Extensive experiments using a linear actuator
were performed and several interesting and nontrivial results were reported. It
is shown that using a linearly polarized single patch antenna at the
transmitter and a circularly polarized antenna array at the receiver results in
the highest signal quality and system performance. Proof-of-concept experiments
on human subjects further validated the suggested results. It was also shown
that using the recommended antenna arrangement will boost the heart rate
detection accuracy of the radar by an average of 11%.



Image denoising through bivariate shrinkage function in framelet domain

Denoising of coefficients in a sparse domain (e.g. wavelet) has been
researched extensively because of its simplicity and effectiveness. Literature
mainly has focused on designing the best global threshold. However, this paper
proposes a new denoising method using bivariate shrinkage function in framelet
domain. In the proposed method, maximum aposteriori probability is used for
estimate of the denoised coefficient and non-Gaussian bivariate function is
applied to model the statistics of framelet coefficients. For every framelet
coefficient, there is a corresponding threshold depending on the local
statistics of framelet coefficients. Experimental results show that using
bivariate shrinkage function in framelet domain yields significantly superior
image quality and higher PSNR than some well-known denoising methods.



Random Euler Complex-Valued Nonlinear Filters

Over the last decade, both the neural network and kernel adaptive filter have
successfully been used for nonlinear signal processing. However, they suffer
from high computational cost caused by their complex/growing network
structures. In this paper, we propose two random Euler filters for
complex-valued nonlinear filtering problem, i.e., linear random Euler
complex-valued filter (LRECF) and its widely-linear version (WLRECF), which
possess a simple and fixed network structure. The transient and steady-state
performances are studied in a non-stationary environment. The analytical
minimum mean square error (MSE) and optimum step-size are derived. Finally,
numerical simulations on complex-valued nonlinear system identification and
nonlinear channel equalization are presented to show the effectiveness of the
proposed methods.



Learning audio and image representations with bio-inspired trainable feature extractors

Recent advancements in pattern recognition and signal processing concern the
automatic learning of data representations from labeled training samples.
Typical approaches are based on deep learning and convolutional neural
networks, which require large amount of labeled training samples. In this work,
we propose novel feature extractors that can be used to learn the
representation of single prototype samples in an automatic configuration
process. We employ the proposed feature extractors in applications of audio and
image processing, and show their effectiveness on benchmark data sets.



Exploring Architectures, Data and Units For Streaming End-to-End Speech Recognition with RNN-Transducer

We investigate training end-to-end speech recognition models with the
recurrent neural network transducer (RNN-T): a streaming, all-neural,
sequence-to-sequence architecture which jointly learns acoustic and language
model components from transcribed acoustic data. We explore various model
architectures and demonstrate how the model can be improved further if
additional text or pronunciation data are available. The model consists of an
`encoder', which is initialized from a connectionist temporal
classification-based (CTC) acoustic model, and a `decoder' which is partially
initialized from a recurrent neural network language model trained on text data
alone. The entire neural network is trained with the RNN-T loss and directly
outputs the recognized transcript as a sequence of graphemes, thus performing
end-to-end speech recognition. We find that performance can be improved further
through the use of sub-word units (`wordpieces') which capture longer context
and significantly reduce substitution errors. The best RNN-T system, a
twelve-layer LSTM encoder with a two-layer LSTM decoder trained with 30,000
wordpieces as output targets achieves a word error rate of 8.5\% on
voice-search and 5.2\% on voice-dictation tasks and is comparable to a
state-of-the-art baseline at 8.3\% on voice-search and 5.4\% voice-dictation.



DeepJ: Style-Specific Music Generation

Recent advances in deep neural networks have enabled algorithms to compose
music that is comparable to music composed by humans. However, few algorithms
allow the user to generate music with tunable parameters. The ability to tune
properties of generated music will yield more practical benefits for aiding
artists, filmmakers, and composers in their creative tasks. In this paper, we
introduce DeepJ - an end-to-end generative model that is capable of composing
music conditioned on a specific mixture of composer styles. Our innovations
include methods to learn musical style and music dynamics. We use our model to
demonstrate a simple technique for controlling the style of generated music as
a proof of concept. Evaluation of our model using human raters shows that we
have improved over the Biaxial LSTM approach.



Performance Analysis of a Scalable DC Microgrid Offering Solar Power Based Energy Access and Efficient Control for Domestic Loads

DC microgrids conform to distributed control of renewable energy sources
which ratifies efficacious instantaneous power sharing and sustenance of energy
access among different domestic Power Management Units (PMUs) along with
maintaining stability of the grid voltage. In this paper design metrics and
performance evaluation of a scalable DC microgrid are documented where a
look-up table of generated power of a source converter complies with the
distribution of efficient power sharing phenomenon among a set of two home
PMUs. The source converter is connected with a Photovoltaic panel of 300 W and
uses Perturb and Observation (P&O) method for executing Maximum Power Point
Tracking (MPPT). A boost average DCDC converter topology is used to enhance the
voltage level of the source converter before transmission. The load converter
consists of two parallely connected PMUs each of which is constructed with high
switching frequency based Full Bridge (FB) converter to charge an integrated
Energy Storage System (ESS). In this paper the overall system is modeled and
simulated on MATLAB/Simulink platform with ESSs in the form of Lead Acid
batteries connected to the load side of the FB converter circuits and these
batteries yield to support marginalized power utilities. The behaviour of the
system is tested in different solar insolation levels along with several
battery charging levels of 12 V and 36 V to assess the power efficiency. In
each testbed the efficiency is found to be more than 93% which affirm the
reliability of the framework and a look-up table is generated comprising the
grid and load quantities for effective control of power transmission.



A Review of Noise Cancellation Techniques for Cognitive Radio

One of the fundamental challenges affecting the performance of communication
systems is the undesired impact of noise on a signal. Noise distorts the signal
and originates due to several sources including, system non-linearity and noise
interference from adjacent environment. Conventional communication systems use
filters to cancel noise in a received signal. In the case of cognitive radio
systems, denoising a signal is important during the spectrum sensing period,
and also during communication with other network nodes. Based on our findings,
few surveys are found that only review particular denoising techniques employed
for the spectrum sensing phase of cognitive radio communication. This paper
aims to provide a collective review of denoising techniques that can be applied
to a cognitive radio system during all the phases of cognitive communication
and discusses several works where the denoising techniques are employed. To
establish comprehensive overview, a performance comparison of the discussed
denoising techniques are also provided.



Multiplication with Fourier Optics Simulating 16-bit Modular Multiplication

This paper will describe a simulator developed by the authors to explore the
design of Fourier transform based multiplication using optics. Then it will
demonstrate an application to the problem of constructing an all-optical
modular multiplication circuit. That circuit implements a novel approximate
version of the Montgomery multiplication algorithm that enables the calculation
to be performed entirely in the analog domain. The results will be used to
corroborate the feasibility of scaling the design up to 16-bits without the
need for analog to digital conversions at intermediate steps.



Training Symbol-Based Equalization for Quadrature Duobinary PDM-FTN Systems

A training symbol-based equalization algorithm is proposed for polarization
de-multiplexing in quadrature duobinary (QDB) modulated polarization division
multiplexedfaster-than-Nyquist (FTN) coherent optical systems. The proposed
algorithm is based on the least mean square algorithm, and multiple location
candidates of a symbol are considered in order to make use of the training
symbols with QDB modulation.Results show that an excellent convergence
performance is obtained using the proposed algorithm under different
polarization alignment scenarios. The optical signal-to-noise ratio required to
attain a bit error rate of 2*10-2 is reduced by 1.7 and 1.8 dB using the
proposed algorithm, compared to systems using the constant modulus algorithm
with differential coding for 4-ary quadrature amplitude modulation(4-QAM) and
16-QAM systems with symbol-by-symbol detection, respectively.Furthermore,
comparisons with the Tomlinson-Harashima precoding-based FTN systems illustrate
that QDB is preferable when 4-QAM is utilized.



Robust Tomlinson-Harashima Precoding for Two-Way Relaying

Most of the non-linear transceivers, which are based on Tomlinson Harashima
(TH) precoding and have been proposed in the literature for two-way relay
networks, assume perfect channel state information (CSI). In this paper, we
propose a novel and robust TH precoding scheme for two-way relay networks with
multiple antennas at the transceiver and the relay nodes. We assume imperfect
CSI and that the channel uncertainty is bounded by a spherical region.
Furthermore, we consider the sum of the mean square error as the objective
function, under a limited power constraint for transceiver and relay nodes.
Simulations are provided to evaluate the performance and to validate the
efficiency of the proposed scheme.



Modulation Classification Using Received Signal's Amplitude Distribution for Coherent Receivers

In this letter, we propose a modulation classification algorithm which is
based on the received signal's amplitude for coherent optical receivers. The
proposed algorithm classifies the modulation format from several possible
candidates by differentiating the cumulative distribution function (CDF) curves
of their normalized amplitudes. The candidate with the most similar CDF to the
received signal is selected. The measure of similarity is the minimum average
distance between these CDFs. Five commonly used quadrature amplitude modulation
formats in digital coherent optical systems are employed. Optical back-to-back
experiments and extended simulations are carried out to investigate the
performance of the proposed algorithm. Results show that the proposed algorithm
achieves accurate classification at optical signal-to-noise ratios of interest.
Furthermore, it does not require carrier recovery.



A pairwise discriminative task for speech emotion recognition

I have submitted a new version to arXiv:1910.11174. I forget to choose to
replace the old version, but submitted a new one. It's my mistake.



Sparse Bayesian ARX models with flexible noise distributions

This paper considers the problem of estimating linear dynamic system models
when the observations are corrupted by random disturbances with nonstandard
distributions. The paper is particularly motivated by applications where sensor
imperfections involve significant contribution of outliers or wrap-around
issues resulting in multi-modal distributions such as commonly encountered in
robotics applications. As will be illustrated, these nonstandard measurement
errors can dramatically compromise the effectiveness of standard estimation
methods, while a computational Bayesian approach developed here is demonstrated
to be equally effective as standard methods in standard measurement noise
scenarios, but dramatically more effective in nonstandard measurement noise
distribution scenarios.



Ultra-Reliable and Low-Latency Wireless Communication: Tail, Risk and Scale

Ensuring ultra-reliable and low-latency communication (URLLC) for 5G wireless
networks and beyond is of capital importance and is currently receiving
tremendous attention in academia and industry. At its core, URLLC mandates a
departure from expected utility-based network design approaches, in which
relying on average quantities (e.g., average throughput, average delay and
average response time) is no longer an option but a necessity. Instead, a
principled and scalable framework which takes into account delay, reliability,
packet size, network architecture, and topology (across access, edge, and core)
and decision-making under uncertainty is sorely lacking. The overarching goal
of this article is a first step to fill this void. Towards this vision, after
providing definitions of latency and reliability, we closely examine various
enablers of URLLC and their inherent tradeoffs. Subsequently, we focus our
attention on a plethora of techniques and methodologies pertaining to the
requirements of ultra-reliable and low-latency communication, as well as their
applications through selected use cases. These results provide crisp insights
for the design of low-latency and high-reliable wireless networks.



Extended Target Poisson Multi-Bernoulli Filter

This paper presents a Poisson multi-Bernoulli (PMB) filter for multiple
extended targets estimation. The extended target PMB filter is based on the
Poisson multi-Bernoulli mixture (PMBM) conjugate prior for multiple extended
target filtering and approximates the multi-Bernoulli (MB) mixture in the
posterior density as a single MB. Because both the prediction and the update
preserve the PMB form of the density, the proposed PMB filter is
computationally cheaper than the PMBM filter while maintaining good filtering
performance. Different methods for merging the MB mixture as a single MB are
presented, along with their Gamma Gaussian inverse Wishart implementations. The
performance of the extended target PMB filter is compared to the extended
target PMBM filter and the extended target labelled MB filter in a thorough
simulation study.



Robust Faster-than-Nyquist PDM-mQAM Systems with Tomlinson-Harashima Precoding

A training-based channel estimation algorithm is proposed for the
faster-than-Nyquist PDM-mQAM (m = 4, 16, 64) systems with Tomlinson-Harashima
precoding (THP). This is robust to the convergence failure phenomenon suffered
by the existing algorithm, yet remaining format-transparent. Simulation results
show that the proposed algorithm requires a reduced optical signal-to-noise
ratio (OSNR) to achieve a certain bit error rate (BER) in the presence of
first-order polarization mode dispersion and phase noise introduced by the
laser linewidth.



A semi-supervised fuzzy GrowCut algorithm to segment and classify regions of interest of mammographic images

According to the World Health Organization, breast cancer is the most common
form of cancer in women. It is the second leading cause of death among women
round the world, becoming the most fatal form of cancer. Mammographic image
segmentation is a fundamental task to support image analysis and diagnosis,
taking into account shape analysis of mammary lesions and their borders.
However, mammogram segmentation is a very hard process, once it is highly
dependent on the types of mammary tissues. In this work we present a new
semi-supervised segmentation algorithm based on the modification of the GrowCut
algorithm to perform automatic mammographic image segmentation once a region of
interest is selected by a specialist. In our proposal, we used fuzzy Gaussian
membership functions to modify the evolution rule of the original GrowCut
algorithm, in order to estimate the uncertainty of a pixel being object or
background. The main impact of the proposed method is the significant reduction
of expert effort in the initialization of seed points of GrowCut to perform
accurate segmentation, once it removes the need of selection of background
seeds. We also constructed an automatic point selection process based on the
simulated annealing optimization method, avoiding the need of human
intervention. The proposed approach was qualitatively compared with other
state-of-the-art segmentation techniques, considering the shape of segmented
regions. In order to validate our proposal, we built an image classifier using
a classical multilayer perceptron. We used Zernike moments to extract segmented
image features. This analysis employed 685 mammograms from IRMA breast cancer
database, using fat and fibroid tissues. Results show that the proposed
technique could achieve a classification rate of 91.28\% for fat tissues,
evidencing the feasibility of our approach.



Improvement to the Prediction of Fuel Cost Distributions Using ARIMA Model

Availability of a validated, realistic fuel cost model is a prerequisite to
the development and validation of new optimization methods and control tools.
This paper uses an autoregressive integrated moving average (ARIMA) model with
historical fuel cost data in development of a three-step-ahead fuel cost
distribution prediction. First, the data features of Form EIA-923 are explored
and the natural gas fuel costs of Texas generating facilities are used to
develop and validate the forecasting algorithm for the Texas example.
Furthermore, the spot price associated with the natural gas hub in Texas is
utilized to enhance the fuel cost prediction. The forecasted data is fit to a
normal distribution and the Kullback-Leibler divergence is employed to evaluate
the difference between the real fuel cost distributions and the estimated
distributions. The comparative evaluation suggests the proposed forecasting
algorithm is effective in general and is worth pursuing further.



Frequency Independent Framework for Synthesis of Programmable Non-reciprocal Networks

Passive and linear nonreciprocal networks at microwave frequencies hold great
promises in enabling new front-end architectures for wireless communication
systems. Their nonreciprocity has been achieved by disrupting the time-reversal
symmetry using various forms of biasing schemes, but only over a limited
frequency range. Here we demonstrate a framework for synthesizing theoretically
frequency-independent multi-port nonreciprocal networks. The framework is
highly expandable, and can have an arbitrary number of ports while
simultaneously sustaining balanced performance and providing unprecedented
programmability of non-reciprocity. A 4-port circulator based on such a
framework is implemented and tested to produce broadband nonreciprocal
performance from 10 MHz to 900 MHz with a temporal switching effort at 23.8
MHz. With the combination of broad bandwidth, low temporal effort, and high
programmability, the framework could inspire new ways of implementing multiple
input multiple output (MIMO) communication systems for 5G.



Neural Style Transfer for Audio Spectograms

There has been fascinating work on creating artistic transformations of
images by Gatys. This was revolutionary in how we can in some sense alter the
'style' of an image while generally preserving its 'content'. In our work, we
present a method for creating new sounds using a similar approach, treating it
as a style-transfer problem, starting from a random-noise input signal and
iteratively using back-propagation to optimize the sound to conform to
filter-outputs from a pre-trained neural architecture of interest.
  For demonstration, we investigate two different tasks, resulting in bandwidth
expansion/compression, and timbral transfer from singing voice to musical
instruments. A feature of our method is that a single architecture can generate
these different audio-style-transfer types using the same set of parameters
which otherwise require different complex hand-tuned diverse signal processing
pipelines.



Discrete FRFT-Based Frame and Frequency Synchronization for Coherent Optical Systems

A joint frame and carrier frequency synchronization algorithm for coherent
optical systems, based on the digital computation of the fractional Fourier
transform (FRFT), is proposed. The algorithm utilizes the characteristics of
energy centralization of chirp signals in the FRFT domain, together with the
time and phase shift properties of the FRFT. Chirp signals are used to
construct a training sequence (TS), and fractional cross-correlation is
employed to define a detection metric for the TS, from which a set of equations
can be obtained. Estimates of both the timing offset and carrier frequency
offset (CFO) are obtained by solving these equations. This TS is later employed
in a phase-dependent decision-directed least-mean square algorithm for adaptive
equalization. Simulation results of a 32-Gbaud coherent polarization division
multiplexed Nyquist system show that the proposed scheme has a wide CFO
estimation range and accurate synchronization performance even in poor optical
signal-to-noise ratio conditions.



Bandwidth-Efficient Synchronization for Fiber Optic Transmission: System Performance Measurements

In this article, we first provide a brief overview of optical transmission
systems and some of their performance specifications. We then present a simple,
robust, and bandwidth-efficient OFDM synchronization method, and carry out
measurements to validate the presented synchronization method with the aid of
an experimental setup.



Robust Frame and Frequency Synchronization Based on Alamouti Coding for RGI-CO-OFDM

We propose an algorithm for carrying out joint frame and frequency
synchronization in reduced-guard-interval coherent optical orthogonal frequency
division multiplexing (RGI-CO-OFDM) systems. The synchronization is achieved by
using the same training symbols (TS) employed for training-aided channel
estimation (TA-CE), thereby avoiding additional training overhead. The proposed
algorithm is designed for polarization division multiplexing (PDM) RGI-CO-OFDM
systems that use the Alamouti-type polarization-time coding for TA-CE. Due to
their optimal TA-CE performance, Golay complementary sequences have been used
as the TS in the proposed algorithm. The frame synchronization is accomplished
by exploiting the cross-correlation between the received TS from the two
orthogonal polarizations. The arrangement of the TS is also used to estimate
the carrier frequency offset. Simulation results of a PDM RGI-CO-OFDM system
operating at 238.1 Gb/s data rate (197.6-Gb/s after coding), with a total
overhead of 9.2% (31.6% after coding), show that the proposed scheme has
accurate synchronization, and is robust to linear fiber impairments.



Joint timing and frequency synchronization based on weighted CAZAC sequences for reduced-guard-interval CO-OFDM systems

A novel joint symbol timing and carrier frequency offset (CFO) estimation
algorithm is proposed for reduced-guard-interval coherent optical orthogonal
frequency-division multiplexing (RGI-CO-OFDM) systems. The proposed algorithm
is based on a constant amplitude zero autocorrelation (CAZAC) sequence weighted
by a pseudo-random noise (PN) sequence. The symbol timing is accomplished by
using only one training symbol of two identical halves, with the weighting
applied to the second half. The special structure of the training symbol is
also utilized in estimating the CFO. The performance of the proposed algorithm
is demonstrated by means of numerical simulations in a 115.8-Gb/s 16-QAM
RGI-CO-OFDM system.



Simple sampling clock synchronisation scheme for reduced-guard-interval coherent optical OFDM systems

A simple data-aided scheme for sampling clock synchronisation in
reduced-guard-interval coherent optical orthogonal frequency division
multiplexing (RGI-CO-OFDM) systems is proposed. In the proposed scheme, the
sampling clock offset (SCO) is estimated by using the training symbols reserved
for channel estimation, thus avoiding extra training overhead. The SCO is then
compensated by resampling, using a time-domain interpolation filter. The
feasibility of the proposed scheme is demonstrated by means of numerical
simulations in a 32-Gbaud 16-QAM dual-polarisation RGI-CO-OFDM system.



Optimal Pilot Symbols Ratio in terms of Spectrum and Energy Efficiency in Uplink CoMP Networks

In wireless networks, Spectrum Efficiency (SE) and Energy Efficiency (EE) can
be affected by the channel estimation that needs to be well designed in
practice. In this paper, considering channel estimation error and non-ideal
backhaul links, we optimize the pilot symbols ratio in terms of SE and EE in
uplink Coordinated Multi-point (CoMP) networks. Modeling the channel estimation
error, we formulate the SE and EE maximization problems by analyzing the system
capacity with imperfect channel estimation. The maximal system capacity in SE
optimization and the minimal transmit power in EE optimization, which both have
the closed-form expressions, are derived by some reasonable approximations to
reduce the complexity of solving complicated equations. Simulations are carried
out to validate the superiority of our scheme, verify the accuracy of our
approximation, and show the effect of pilot symbols ratio.



A Survey of Air-to-Ground Propagation Channel Modeling for Unmanned Aerial Vehicles

In recent years, there has been a dramatic increase in the use of unmanned
aerial vehicles (UAVs), particularly for small UAVs, due to their affordable
prices, ease of availability, and ease of operability. Existing and future
applications of UAVs include remote surveillance and monitoring, relief
operations, package delivery, and communication backhaul infrastructure.
Additionally, UAVs are envisioned as an important component of 5G wireless
technology and beyond. The unique application scenarios for UAVs necessitate
accurate air-to-ground (AG) propagation channel models for designing and
evaluating UAV communication links for control/non-payload as well as payload
data transmissions. These AG propagation models have not been investigated in
detail when compared to terrestrial propagation models. In this paper, a
comprehensive survey is provided on available AG channel measurement campaigns,
large and small scale fading channel models, their limitations, and future
research directions for UAV communication scenarios.



Cross-Sensor Iris Recognition: LG4000-to-LG2200 Comparison

Cross-sensor comparison experimental results reported here show that the
procedure defined and simulated during the Cross-Sensor Comparison Competition
2013 by our team for migrating / upgrading LG2200 based to LG4000 based
biometric systems leads to better LG4000-to-LG2200 cross-sensor iris
recognition results than previously reported, both in terms of user comfort and
in terms of system safety. On the other hand, LG2200-to-LG400 migration/upgrade
procedure defined and implemented by us is applicable to solve interoperability
issues between LG2200 based and LG4000 based systems, but also to other pairs
of systems having the same shift in the quality of acquired images.



Tree based classification of tabla strokes

The paper attempts to validate the effectiveness of tree classifiers to
classify tabla strokes especially the ones which are overlapping in nature. It
uses decision tree, ID3 and random forest as classifiers. A custom made data
sets of 650 samples of 13 different tabla strokes were used for experimental
purpose. 31 different features with their mean and variances were extracted for
classification. Three data sets consisting of 21361, 18802 and 19543 instances
respectively were used for the purpose. Validation has been done using measures
like ROC curve and accuracy. The experimental results showed that all the
classifiers showing excellent results with random forest outperforming the
other two. The effectiveness of random forest in classifying strokes which are
overlapping in nature is done by comparing the known results of that with
multi-layer perceptron.



Diffusion Leaky Zero Attracting Least Mean Square Algorithm and Its Performance Analysis

Recently, the leaky diffusion least-mean-square (DLMS) algorithm has obtained
much attention because of its good performance for high input eigenvalue spread
and low signal-to-noise ratio (SNR). However, the leaky DLMS algorithm may
suffer from performance deterioration in the sparse system. To overcome this
drawback, the leaky zero attracting DLMS (LZA-DLMS) algorithm is developed in
this paper, which adds an l1-norm penalty to the cost function to exploit the
property of sparse system. The leaky reweighted zero attracting DLMS
(LRZA-DLMS) algorithm is also put forward, which can improve the estimation
performance in the presence of time-varying sparsity. Instead of using the
l1-norm penalty, in the reweighted version, a log-sum function is employed as
the substitution. Based on the weight error variance relation and several
common assumptions, we analyze the transient behavior of our findings and
determine the stability bound of the step-size. Moreover, we implement the
steady state theoretical analysis for the proposed algorithms. Simulations in
the context of distributed network system identification illustrate that the
proposed schemes outperform various existing algorithms and validate the
accuracy of the theoretical results.



Optimal Vehicle Dimensioning for Multi-Class Autonomous Electric Mobility On-Demand Systems

Autonomous electric mobility on demand (AEMoD) has recently emerged as a
cyber-physical system aiming to bring automation, electrification, and
on-demand services for the future private transportation market. The expected
massive demand for such services and its resulting insufficient charging
time/resources prohibit the use of centralized management and full vehicle
charging. A fog-based multi-class solution for these challenges was recently
suggested, by enabling per-zone management and partial charging for different
classes of AEMoD vehicles. This paper focuses on finding the optimal vehicle
dimensioning for each zone of these systems in order to guarantee a bounded
response time of its vehicles. Using a queuing model representing the
multi-class charging and dispatching processes, we first derive the stability
conditions and the number of system classes to guarantee the response time
bound. Decisions on the proportions of each class vehicles to partially/fully
charge, or directly serve customers are then optimized so as to minimize the
vehicles in-flow to any given zone. Excess waiting times of customers in rare
critical events, such as limited charging resources and/or limited vehicles
availabilities, are also investigated. Results show the merits of our proposed
model compared to other schemes and in usual and critical scenarios.



Root Mean Square Minimum Distance as a Quality Metric for Localization Nanoscopy Images

A localization algorithm in stochastic optical localization nanoscopy plays
an important role in obtaining a high-quality image. A universal and objective
metric is crucial and necessary to evaluate qualities of nanoscopy images and
performances of localization algorithms. In this paper, we propose root mean
square minimum distance (RMSMD) as a quality metric for localization nanoscopy
images. RMSMD measures an average, local, and mutual fitness between two sets
of points. Its properties common to a distance metric as well as unique to
itself are presented. The ambiguity, discontinuity, and inappropriateness of
the metrics of accuracy, precision, recall, and Jaccard index, which are
currently used in the literature, are analyzed. A numerical example
demonstrates the advantages of RMSMD over the four existing metrics that fail
to distinguish qualities of different nanoscopy images in certain conditions.
The unbiased Gaussian estimator that achieves the Fisher information and
Cramer-Rao lower bound (CRLB) of a single data frame is proposed to benchmark
the quality of localization nanoscopy images and the performance of
localization algorithms. The information-achieving estimator is simulated in an
example and the result demonstrates the superior sensitivity of RMSMD over the
other four metrics. As a universal and objective metric, RMSMD can be broadly
employed in various applications to measure the mutual fitness of two sets of
points.



Frame-based Sparse Analysis and Synthesis Signal Representations and Parseval K-SVD

Frames are the foundation of the linear operators used in the decomposition
and reconstruction of signals, such as the discrete Fourier transform, Gabor,
wavelets, and curvelet transforms. The emergence of sparse representation
models has shifted of the emphasis in frame theory toward sparse
l1-minimization problems. In this paper, we apply frame theory to the sparse
representation of signals in which a synthesis dictionary is used for a frame
and an analysis dictionary is used for a dual frame. We sought to formulate a
novel dual frame design in which the sparse vector obtained through the
decomposition of any signal is also the sparse solution representing signals
based on a reconstruction frame. Our findings demonstrate that this type of
dual frame cannot be constructed for over-complete frames, thereby precluding
the use of any linear analysis operator in driving the sparse synthesis
coefficient for signal representation. Nonetheless, the best approximation to
the sparse synthesis solution can be derived from the analysis coefficient
using the canonical dual frame. In this study, we developed a novel dictionary
learning algorithm (called Parseval K-SVD) to learn a tight-frame dictionary.
We then leveraged the analysis and synthesis perspectives of signal
representation with frames to derive optimization formulations for problems
pertaining to image recovery. Our preliminary, results demonstrate that the
images recovered using this approach are correlated to the frame bounds of
dictionaries, thereby demonstrating the importance of using different
dictionaries for different applications.



On the efficiency of computational imaging with structured illumination

A generic computational imaging setup is considered which assumes sequential
illumination of a semi-transparent object by an arbitrary set of structured
illumination patterns. For each incident illumination pattern, all transmitted
light is collected by a photon-counting bucket (single-pixel) detector. The
transmission coefficients measured in this way are then used to reconstruct the
spatial distribution of the object's projected transmission. It is demonstrated
that the squared spatial resolution of such a setup is usually equal to the
ratio of the image area to the number of linearly independent illumination
patterns. If the noise in the measured transmission coefficients is dominated
by photon shot noise, then the ratio of the spatially-averaged squared mean
signal to the spatially-averaged noise variance in the "flat" distribution
reconstructed in the absence of the object, is equal to the average number of
registered photons when the illumination patterns are orthogonal. The
signal-to-noise ratio in a reconstructed transmission distribution is always
lower in the case of non-orthogonal illumination patterns due to spatial
correlations in the measured data. Examples of imaging methods relevant to the
presented analysis include conventional imaging with a pixelated detector,
computational ghost imaging, compressive sensing, super-resolution imaging and
computed tomography.



Predicting Encoded Picture Quality in Two Steps is a Better Way

Full-reference (FR) image quality assessment (IQA) models assume a high
quality "pristine" image as a reference against which to measure perceptual
image quality. In many applications, however, the assumption that the reference
image is of high quality may be untrue, leading to incorrect perceptual quality
predictions. To address this, we propose a new two-step image quality
prediction approach which integrates both no-reference (NR) and full-reference
perceptual quality measurements into the quality prediction process. The
no-reference module accounts for the possibly imperfect quality of the source
(reference) image, while the full-reference component measures the quality
differences between the source image and its possibly further distorted
version. A simple, yet very efficient, multiplication step fuses the two
sources of information into a reliable objective prediction score. We evaluated
our two-step approach on a recently designed subjective image database and
achieved standout performance compared to full-reference approaches, especially
when the reference images were of low quality. The proposed approach is made
publicly available at https://github.com/xiangxuyu/2stepQA



Stochastic Dynamic Pricing for EV Charging Stations with Renewables Integration and Energy Storage

This paper studies the problem of stochastic dynamic pricing and energy
management policy for electric vehicle (EV) charging service providers. In the
presence of renewable energy integration and energy storage system, EV charging
service providers must deal with multiple uncertainties --- charging demand
volatility, inherent intermittency of renewable energy generation, and
wholesale electricity price fluctuation. The motivation behind our work is to
offer guidelines for charging service providers to determine proper charging
prices and manage electricity to balance the competing objectives of improving
profitability, enhancing customer satisfaction, and reducing impact on power
grid in spite of these uncertainties. We propose a new metric to assess the
impact on power grid without solving complete power flow equations. To protect
service providers from severe financial losses, a safeguard of profit is
incorporated in the model. Two algorithms --- stochastic dynamic programming
(SDP) algorithm and greedy algorithm (benchmark algorithm) --- are applied to
derive the pricing and electricity procurement policy. A Pareto front of the
multiobjective optimization is derived. Simulation results show that using SDP
algorithm can achieve up to 7% profit gain over using greedy algorithm.
Additionally, we observe that the charging service provider is able to reshape
spatial-temporal charging demands to reduce the impact on power grid via
pricing signals.



Placement of EV Charging Stations --- Balancing Benefits among Multiple Entities

This paper studies the problem of multi-stage placement of electric vehicle
(EV) charging stations with incremental EV penetration rates. A nested logit
model is employed to analyze the charging preference of the individual consumer
(EV owner), and predict the aggregated charging demand at the charging
stations. The EV charging industry is modeled as an oligopoly where the entire
market is dominated by a few charging service providers (oligopolists). At the
beginning of each planning stage, an optimal placement policy for each service
provider is obtained through analyzing strategic interactions in a Bayesian
game. To derive the optimal placement policy, we consider both the
transportation network graph and the electric power network graph. A simulation
software --- The EV Virtual City 1.0 --- is developed using Java to investigate
the interactions among the consumers (EV owner), the transportation network
graph, the electric power network graph, and the charging stations. Through a
series of experiments using the geographic and demographic data from the city
of San Pedro District of Los Angeles, we show that the charging station
placement is highly consistent with the heatmap of the traffic flow. In
addition, we observe a spatial economic phenomenon that service providers
prefer clustering instead of separation in the EV charging market.



A Consumer Behavior Based Approach to Multi-Stage EV Charging Station Placement

This paper presents a multi-stage approach to the placement of charging
stations under the scenarios of different electric vehicle (EV) penetration
rates. The EV charging market is modeled as the oligopoly. A consumer behavior
based approach is applied to forecast the charging demand of the charging
stations using a nested logit model. The impacts of both the urban road network
and the power grid network on charging station planning are also considered. At
each planning stage, the optimal station placement strategy is derived through
solving a Bayesian game among the service providers. To investigate the
interplay of the travel pattern, the consumer behavior, urban road network,
power grid network, and the charging station placement, a simulation platform
(The EV Virtual City 1.0) is developed using Java on Repast.We conduct a case
study in the San Pedro District of Los Angeles by importing the geographic and
demographic data of that region into the platform. The simulation results
demonstrate a strong consistency between the charging station placement and the
traffic flow of EVs. The results also reveal an interesting phenomenon that
service providers prefer clustering instead of spatial separation in this
oligopoly market.



Multipath interference analysis of IR-UWB systems in indoor office LOS environment

Bit error rate (BER) performance of impulse radio Ultra-Wideband (UWB)
systems in the presence of intrasymbol interference, inter-symbol interference,
multiuser interference and addictive white Gaussian noise (AWGN) is presented
in this paper. By analyzing the indoor office LOS channel model defined by IEEE
802.15.4a Task Group and deducing the variance for intra-symbol interference
(IASI), inter-symbol interference (ISI) and multiuser interference (MUI), the
system BER expression is obtained and verified by MATLAB simulations. Through
comparing the simulation results with and without intra-symbol interference,
the conclusion that intra-symbol interference cannot be neglected is
drawn-moreover, such interference will significantly decrease performance of
UWB based wireless sensor networks (WSN). Then, the BER performance of UWB
systems in multiuser environment is also analyzed and analysis results show
that multiuser interference will further worsen the transmission performance of
UWB systems.



Partial Template Based Receiver in Impulse Radio Ultra-Wideband Communication Systems

For high speed ultra-wideband (UWB) communication systems, the multipath
interference exhibits a primary obstacle to improve transmission performance.
In order to enhance the signal to interference plus noise ratio (SINR) in the
receiver, a partial template receiver is proposed in this paper. Instead of
using the conventional template, the model in this paper adopts a partial
template to demodulate signals. To analyze the performance of the proposed
receiver, bit error rate (BER) formulation of IR-UWB systems in the presence of
multipath interference, multiuser interference and addictive white Gaussian
noise (AWGN) is derived in IEEE 802.15.4a channel models. Simulation results
show that, compared with the conventional correlation receiver, the proposed
receiver can achieve a better BER performance for high Eb/N0, which falls in
the conventional used Eb/N0 range.



Performance Analysis of UWB Based Wireless Sensor Networks in Indoor Office LOS Environment

With the fast development of wireless sensor networks (WSN), more attentions
are paid to high data rate transmission of WSN, and hence, in IEEE 802.15.4a
standard, ultra-wideband (UWB) is introduced as one of the physical layer
technique to support high transmission data rate and precisie locationing
applications. In order to analyze the bit error rate (BER) performance of UWB
based WSN, a system model considering intra-symbol interference (IASI),
inter-symbol interference (ISI), multiuser interference (MUI) and addictive
white Gaussian noise (AWGN) is proposed in this paper, and then verified using
simulation results. Moreover, the pulse waveforms complying with the spectrum
requirement of IEEE 802.15.4a standard are given, and based on such obtained
pulses, the effect of transmission data rate and user number is also shown.
Results show that with the increase of SNR, the intra-symbol interference will
decrease the system performance significantly, and system performance can be
improve by using pulse waveforms with little intra-symbol interference.



Binning based algorithm for Pitch Detection in Hindustani Classical Music

Speech coding forms a crucial element in speech communications. An important
area concerning it lies in feature extraction which can be used for analyzing
Hindustani Classical Music. An important feature in this respect is the
fundamental frequency often referred to as the pitch. In this work, the terms
pitch and its acoustical sensation, the frequency is used interchangeably.
There exists numerous pitch detection algorithms which detect the main/
fundamental frequency in a given musical piece, but we have come up with a
unique algorithm for pitch detection using the binning method as described in
the paper using appropriate bin size. Previous work on this subject throws
light on pitch identification for Hindustani Classical Music. Pitch Class
Distribution has been employed in this work. It can be used to identify pitches
in Hindustani Classical Music which is based on suitable intonations and
swaras. It follows a particular ratio pattern which is a tuning for diatonic
scale proposed by Ptolemy and confirmed by Zarlino is explored in this paper.
We have also given our estimated of these ratios and compared the error with
the above. The error produced by varying the bin size in our algorithm is
investigated and an estimate for an appropriate bin size is suggested and
tested. The binning algorithm thus helps to segregate the important pitches in
a given musical piece.



Realistic Image Degradation with Measured PSF

Training autonomous vehicles requires lots of driving sequences in all
situations\cite{zhao2016}. Typically a simulation environment
(software-in-the-loop, SiL) accompanies real-world test drives to
systematically vary environmental parameters. A missing piece in the optical
model of those SiL simulations is the sharpness, given in linear system theory
by the point-spread function (PSF) of the optical system. We present a novel
numerical model for the PSF of an optical system that can efficiently model
both experimental measurements and lens design simulations of the PSF. The
numerical basis for this model is a non-linear regression of the PSF with an
artificial neural network (ANN). The novelty lies in the portability and the
parameterization of this model, which allows to apply this model in basically
any conceivable optical simulation scenario, e.g. inserting a measured lens
into a computer game to train autonomous vehicles. We present a lens
measurement series, yielding a numerical function for the PSF that depends only
on the parameters defocus, field and azimuth. By convolving existing images and
videos with this PSF model we apply the measured lens as a transfer function,
therefore generating an image as if it were seen with the measured lens itself.
Applications of this method are in any optical scenario, but we focus on the
context of autonomous driving, where quality of the detection algorithms
depends directly on the optical quality of the used camera system. With the
parameterization of the optical model we present a method to validate the
functional and safety limits of camera-based ADAS based on the real, measured
lens actually used in the product.



Cross-modal Embeddings for Video and Audio Retrieval

The increasing amount of online videos brings several opportunities for
training self-supervised neural networks. The creation of large scale datasets
of videos such as the YouTube-8M allows us to deal with this large amount of
data in manageable way. In this work, we find new ways of exploiting this
dataset by taking advantage of the multi-modal information it provides. By
means of a neural network, we are able to create links between audio and visual
documents, by projecting them into a common region of the feature space,
obtaining joint audio-visual embeddings. These links are used to retrieve audio
samples that fit well to a given silent video, and also to retrieve images that
match a given a query audio. The results in terms of Recall@K obtained over a
subset of YouTube-8M videos show the potential of this unsupervised approach
for cross-modal feature learning. We train embeddings for both scales and
assess their quality in a retrieval problem, formulated as using the feature
extracted from one modality to retrieve the most similar videos based on the
features computed in the other modality.



Joint Estimation of Low-Rank Components and Connectivity Graph in High-Dimensional Graph Signals: Application to Brain Imaging

This paper presents a graph signal processing algorithm to uncover the
intrinsic low-rank components and the underlying graph of a high-dimensional,
graph-smooth and grossly-corrupted dataset. In our problem formulation, we
assume that the perturbation on the low-rank components is sparse and the
signal is smooth on the graph. We propose an algorithm to estimate the low-rank
components with the help of the graph and refine the graph with better
estimated low-rank components. We propose to perform the low-rank estimation
and graph refinement jointly so that low-rank estimation can benefit from the
refined graph, and graph refinement can leverage the improved low-rank
estimation. We propose to address the problem with an alternating optimization.
Moreover, we perform a mathematical analysis to understand and quantify the
impact of the inexact graph on the low-rank estimation, justifying our scheme
with graph refinement as an integrated step in estimating low-rank components.
We perform extensive experiments on the proposed algorithm and compare with
state-of-the-art low-rank estimation and graph learning techniques. Our
experiments use synthetic data and real brain imaging (MEG) data that is
recorded when subjects are presented with different categories of visual
stimuli. We observe that our proposed algorithm is competitive in estimating
the low-rank components, adequately capturing the intrinsic task-related
information in the reduced dimensional representation, and leading to better
performance in a classification task. Furthermore, we notice that our estimated
graph indicates compatible brain active regions for visual activity as
neuroscientific findings.



Attacking Speaker Recognition With Deep Generative Models

In this paper we investigate the ability of generative adversarial networks
(GANs) to synthesize spoofing attacks on modern speaker recognition systems. We
first show that samples generated with SampleRNN and WaveNet are unable to fool
a CNN-based speaker recognition system. We propose a modification of the
Wasserstein GAN objective function to make use of data that is real but not
from the class being learned. Our semi-supervised learning method is able to
perform both targeted and untargeted attacks, raising questions related to
security in speaker authentication systems.



The Gaussian Noise Model in the Presence of Inter-channel Stimulated Raman Scattering

A Gaussian noise (GN) model is presented that properly accounts for an
arbitrary frequency dependent signal power profile along the link. This enables
the evaluation of the impact of inter-channel stimulated Raman scattering
(ISRS) on the optical Kerr nonlinearity. Additionally, the frequency dependent
fiber attenuation can be taken into account and transmission systems that use
hybrid amplification schemes can be modeled, where distributed Raman
amplification is partly applied over the optical spectrum. To include the
latter two cases, a set of coupled ordinary differential equations must be
numerically solved in order to obtain the signal power profile yielding a
semi-analytical model. However for lumped amplification and negligible
variation in fiber attenuation, a less complex and fully analytical model is
presented which is referred to as the ISRS GN model. The derived model is exact
to first-order for Gaussian modulated signals and extensively validated by
numerical split-step simulations. A maximum deviation of $0.1$~dB in nonlinear
interference power between simulations and the ISRS GN model is found. The
model is applied to a transmission system that occupies an optical bandwidth of
$10$~THz, representing the entire C+L band. At optimum launch power, changes of
up to $2$~dB in nonlinear interference power due to ISRS are reported.
Furthermore, comparable models published in the literature are benchmarked
against the ISRS GN model.



Electroencephalographic Slowing: A Source of Error in Automatic Seizure Detection

Although a seizure event represents a major deviation from a baseline
electroencephalographic signal, there are features of seizure morphology that
can be seen in non-epileptic portions of the record. A transient decrease in
frequency, referred to as slowing, is a generally abnormal but not necessarily
epileptic EEG variant. Seizure termination is often associated with a period of
slowing between the period of peak amplitude and frequency of the seizure and
the return to baseline. In annotation of seizure events in the TUH EEG Seizure
Corpus, independent slowing events were identified as a major source of false
alarm error. Preliminary results demonstrated the difficulty in automatic
differentiation between seizure events and independent slowing events. The TUH
EEG Slowing database, a subset of the TUH EEG Corpus, was created, and is
introduced here, to aid in the development of a seizure detection tool that can
differentiate between slowing at the end of a seizure and an independent
non-seizure slowing event. The corpus contains 100 10-second samples each of
background, slowing, and seizure events. Preliminary experiments show that 77%
sensitivity can be achieved in seizure detection using models trained on all
three sample types compared to 43% sensitivity with only seizure and background
samples.



Gated Recurrent Networks for Seizure Detection

Recurrent Neural Networks (RNNs) with sophisticated units that implement a
gating mechanism have emerged as powerful technique for modeling sequential
signals such as speech or electroencephalography (EEG). The latter is the focus
on this paper. A significant big data resource, known as the TUH EEG Corpus
(TUEEG), has recently become available for EEG research, creating a unique
opportunity to evaluate these recurrent units on the task of seizure detection.
In this study, we compare two types of recurrent units: long short-term memory
units (LSTM) and gated recurrent units (GRU). These are evaluated using a state
of the art hybrid architecture that integrates Convolutional Neural Networks
(CNNs) with RNNs. We also investigate a variety of initialization methods and
show that initialization is crucial since poorly initialized networks cannot be
trained. Furthermore, we explore regularization of these convolutional gated
recurrent networks to address the problem of overfitting. Our experiments
revealed that convolutional LSTM networks can achieve significantly better
performance than convolutional GRU networks. The convolutional LSTM
architecture with proper initialization and regularization delivers 30%
sensitivity at 6 false alarms per 24 hours.



Optimizing Channel Selection for Seizure Detection

Interpretation of electroencephalogram (EEG) signals can be complicated by
obfuscating artifacts. Artifact detection plays an important role in the
observation and analysis of EEG signals. Spatial information contained in the
placement of the electrodes can be exploited to accurately detect artifacts.
However, when fewer electrodes are used, less spatial information is available,
making it harder to detect artifacts. In this study, we investigate the
performance of a deep learning algorithm, CNN-LSTM, on several channel
configurations. Each configuration was designed to minimize the amount of
spatial information lost compared to a standard 22-channel EEG. Systems using a
reduced number of channels ranging from 8 to 20 achieved sensitivities between
33% and 37% with false alarms in the range of [38, 50] per 24 hours. False
alarms increased dramatically (e.g., over 300 per 24 hours) when the number of
channels was further reduced. Baseline performance of a system that used all 22
channels was 39% sensitivity with 23 false alarms. Since the 22-channel system
was the only system that included referential channels, the rapid increase in
the false alarm rate as the number of channels was reduced underscores the
importance of retaining referential channels for artifact reduction. This
cautionary result is important because one of the biggest differences between
various types of EEGs administered is the type of referential channel used.



An Analysis of Two Common Reference Points for EEGs

Clinical electroencephalographic (EEG) data varies significantly depending on
a number of operational conditions (e.g., the type and placement of electrodes,
the type of electrical grounding used). This investigation explores the
statistical differences present in two different referential montages: Linked
Ear (LE) and Averaged Reference (AR). Each of these accounts for approximately
45% of the data in the TUH EEG Corpus. In this study, we explore the impact
this variability has on machine learning performance. We compare the
statistical properties of features generated using these two montages, and
explore the impact of performance on our standard Hidden Markov Model (HMM)
based classification system. We show that a system trained on LE data
significantly outperforms one trained only on AR data (77.2% vs. 61.4%). We
also demonstrate that performance of a system trained on both data sets is
somewhat compromised (71.4% vs. 77.2%). A statistical analysis of the data
suggests that mean, variance and channel normalization should be considered.
However, cepstral mean subtraction failed to produce an improvement in
performance, suggesting that the impact of these statistical differences is
subtler.



Semi-automated Annotation of Signal Events in Clinical EEG Data

To be effective, state of the art machine learning technology needs large
amounts of annotated data. There are numerous compelling applications in
healthcare that can benefit from high performance automated decision support
systems provided by deep learning technology, but they lack the comprehensive
data resources required to apply sophisticated machine learning models.
Further, for economic reasons, it is very difficult to justify the creation of
large annotated corpora for these applications. Hence, automated annotation
techniques become increasingly important. In this study, we investigated the
effectiveness of using an active learning algorithm to automatically annotate a
large EEG corpus. The algorithm is designed to annotate six types of EEG
events. Two model training schemes, namely threshold-based and volume-based,
are evaluated. In the threshold-based scheme the threshold of confidence scores
is optimized in the initial training iteration, whereas for the volume-based
scheme only a certain amount of data is preserved after each iteration.
Recognition performance is improved 2% absolute and the system is capable of
automatically annotating previously unlabeled data. Given that the
interpretation of clinical EEG data is an exceedingly difficult task, this
study provides some evidence that the proposed method is a viable alternative
to expensive manual annotation.



Improved EEG Event Classification Using Differential Energy

Feature extraction for automatic classification of EEG signals typically
relies on time frequency representations of the signal. Techniques such as
cepstral-based filter banks or wavelets are popular analysis techniques in many
signal processing applications including EEG classification. In this paper, we
present a comparison of a variety of approaches to estimating and
postprocessing features. To further aid in discrimination of periodic signals
from aperiodic signals, we add a differential energy term. We evaluate our
approaches on the TUH EEG Corpus, which is the largest publicly available EEG
corpus and an exceedingly challenging task due to the clinical nature of the
data. We demonstrate that a variant of a standard filter bank-based approach,
coupled with first and second derivatives, provides a substantial reduction in
the overall error rate. The combination of differential energy and derivatives
produces a 24% absolute reduction in the error rate and improves our ability to
discriminate between signal events and background noise. This relatively simple
approach proves to be comparable to other popular feature extraction approaches
such as wavelets, but is much more computationally efficient.



Autonomous Tracking of Intermittent RF Source Using a UAV Swarm

Localization of a radio frequency (RF) transmitter with intermittent
transmissions is considered via a group of unmanned aerial vehicles (UAVs)
equipped with omnidirectional received signal strength (RSS) sensors. This
group embarks on an autonomous patrol to localize and track the target with a
specified accuracy, as quickly as possible. The challenge can be decomposed
into two stages: 1) estimation of the target position given previous
measurements (localization), and 2) planning the future trajectory of the
tracking UAVs to get lower expected localization error given current estimation
(path planning). For each stage we compare two algorithms in terms of
performance and computational load. For the localization stage, we compare a
detection based extended Kalman filter (EKF) and a recursive Bayesian
estimator. For the path planning stage, we compare steepest descent posterior
Cramer-Rao lower bound (CRLB) path planning and a bio-inspired heuristic path
planning. Our results show that the steepest descent path planning outperforms
the bio-inspired path planning by an order of magnitude, and recursive Bayesian
estimator narrowly outperforms detection based EKF.



Improving Short-Term Electricity Price Forecasting Using Day-Ahead LMP with ARIMA Models

Short-term electricity price forecasting has become important for demand side
management and power generation scheduling. Especially as the electricity
market becomes more competitive, a more accurate price prediction than the
day-ahead locational marginal price (DALMP) published by the independent system
operator (ISO) will benefit participants in the market by increasing profit or
improving load demand scheduling. Hence, the main idea of this paper is to use
autoregressive integrated moving average (ARIMA) models to obtain a better LMP
prediction than the DALMP by utilizing the published DALMP, historical
real-time LMP (RTLMP) and other useful information. First, a set of seasonal
ARIMA (SARIMA) models utilizing the DALMP and historical RTLMP are developed
and compared with autoregressive moving average (ARMA) models that use the
differences between DALMP and RTLMP on their forecasting capability. A
generalized autoregressive conditional heteroskedasticity (GARCH) model is
implemented to further improve the forecasting by accounting for the price
volatility. The models are trained and evaluated using real market data in the
Midcontinent Independent System Operator (MISO) region. The evaluation results
indicate that the ARMAX-GARCH model, where an exogenous time series indicates
weekend days, improves the short-term electricity price prediction accuracy and
outperforms the other proposed ARIMA models



Generative Sensing: Transforming Unreliable Sensor Data for Reliable Recognition

This paper introduces a deep learning enabled generative sensing framework
which integrates low-end sensors with computational intelligence to attain a
high recognition accuracy on par with that attained with high-end sensors. The
proposed generative sensing framework aims at transforming low-end, low-quality
sensor data into higher quality sensor data in terms of achieved classification
accuracy. The low-end data can be transformed into higher quality data of the
same modality or into data of another modality. Different from existing methods
for image generation, the proposed framework is based on discriminative models
and targets to maximize the recognition accuracy rather than a similarity
measure. This is achieved through the introduction of selective feature
regeneration in a deep neural network (DNN). The proposed generative sensing
will essentially transform low-quality sensor data into high-quality
information for robust perception. Results are presented to illustrate the
performance of the proposed framework.



DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features

Acoustic scene recordings are represented by different types of handcrafted
or Neural Network-derived features. These features, typically of thousands of
dimensions, are classified in state of the art approaches using kernel
machines, such as the Support Vector Machines (SVM). However, the complexity of
training these methods increases with the dimensionality of these input
features and the size of the dataset. A solution is to map the input features
to a randomized lower-dimensional feature space. The resulting random features
can approximate non-linear kernels with faster linear kernel computation. In
this work, we computed a set of 6,553 input features and used them to compute
random features to approximate three types of kernels, Gaussian, Laplacian and
Cauchy. We compared their performance using an SVM in the context of the DCASE
Task 1 - Acoustic Scene Classification. Experiments show that both, input and
random features outperformed the DCASE baseline by an absolute 4%. Moreover,
the random features reduced the dimensionality of the input by more than three
times with minimal loss of performance and by more than six times and still
outperformed the baseline. Hence, random features could be employed by state of
the art approaches to compute low-storage features and perform faster kernel
computations.



Brain MRI Super Resolution Using 3D Deep Densely Connected Neural Networks

Magnetic resonance image (MRI) in high spatial resolution provides detailed
anatomical information and is often necessary for accurate quantitative
analysis. However, high spatial resolution typically comes at the expense of
longer scan time, less spatial coverage, and lower signal to noise ratio (SNR).
Single Image Super-Resolution (SISR), a technique aimed to restore
high-resolution (HR) details from one single low-resolution (LR) input image,
has been improved dramatically by recent breakthroughs in deep learning. In
this paper, we introduce a new neural network architecture, 3D Densely
Connected Super-Resolution Networks (DCSRN) to restore HR features of
structural brain MR images. Through experiments on a dataset with 1,113
subjects, we demonstrate that our network outperforms bicubic interpolation as
well as other deep learning methods in restoring 4x resolution-reduced images.



Dynamic Pricing and Energy Management Strategy for EV Charging Stations under Uncertainties

This paper presents a dynamic pricing and energy management framework for
electric vehicle (EV) charging service providers. To set the charging prices,
the service providers faces three uncertainties: the volatility of wholesale
electricity price, intermittent renewable energy generation, and
spatial-temporal EV charging demand. The main objective of our work here is to
help charging service providers to improve their total profits while enhancing
customer satisfaction and maintaining power grid stability, taking into account
those uncertainties. We employ a linear regression model to estimate the EV
charging demand at each charging station, and introduce a quantitative measure
for customer satisfaction. Both the greedy algorithm and the dynamic
programming (DP) algorithm are employed to derive the optimal charging prices
and determine how much electricity to be purchased from the wholesale market in
each planning horizon. Simulation results show that DP algorithm achieves an
increased profit (up to 9%) compared to the greedy algorithm (the benchmark
algorithm) under certain scenarios. Additionally, we observe that the
integration of a low-cost energy storage into the system can not only improve
the profit, but also smooth out the charging price fluctuation, protecting the
end customers from the volatile wholesale market.



Open-Access Full-Duplex Wireless in the ORBIT Testbed

In order to support experimentation with full-duplex (FD) wireless, we
recently integrated an open-access FD transceiver in the ORBIT testbed. In this
report, we present the design and implementation of the FD transceiver and
interfaces, and provide examples and guidelines for experimentation. In
particular, an ORBIT node with a National Instruments (NI)/Ettus Research
Universal Software Radio Peripheral (USRP) N210 software-defined radio (SDR)
was equipped with the Columbia FlexICoN Gen-1 customized RF self-interference
(SI) canceller box. The RF canceller box includes an RF SI canceller that is
implemented using discrete components on a printed circuit board (PCB) and
achieves 40dB RF SI cancellation across 5MHz bandwidth. We provide an FD
transceiver baseline program and present two example FD experiments where 90dB
and 85dB overall SI cancellation is achieved for a simple waveform and PSK
modulated signals across both the RF and digital domains. We also discuss
potential FD wireless experiments that can be conducted based on the
implemented open-access FD transceiver and baseline program.



Improved Time of Arrival measurement model for non-convex optimization

The quadratic system provided by the Time of Arrival technique can be solved
analytically or by optimization algorithms. In practice, a combination of both
methods is used. An important problem in quadratic optimization is the possible
convergence to a local minimum, instead of the global minimum. This article
presents an approach how this risk can be significantly reduced. The main idea
of our approach is to transform the local minimum to a saddle point, by
increasing the number of dimensions. In contrast to similar methods such as,
dimension lifting does our problem remains non-convex.



Improved linear direct solution for asynchronous radio network localization (RNL)

In the field of localization the linear least square solution is frequently
used. This solution is compared to nonlinear solvers more effected by noise,
but able to provide a position estimation without the knowledge of any starting
condition. The linear least square solution is able to minimize Gaussian noise
by solving an overdetermined equation with the MoorePenrose pseudoinverse.
Unfortunately this solution fails if it comes to non Gaussian noise. This
publication presents a direct solution which is able to use prefiltered data
for the LPM (RNL) equation. The used input for the linear position estimation
will not be the raw data but over the time filtered data, for this reason this
solution will be called direct solution. It will be shown that the presented
symmetrical direct solution is superior to non symmetrical direct solution and
especially to the not prefiltered linear least square solution.



Assessment of SFSDP Cooperative Localization Algorithm for WLAN Environment

Cooperative localization for indoor WiFi networks have received little
attention thus far. Many cooperative location algorithms exist for Wireless
Sensor Network Applications but their suitability for WiFi based networks has
not been studied. In this paper the performance of the Sparse Finite Semi
Definite Program (SFSDP) has been examined using real measurements data and
under different indoor conditions. Effects of other network parameters such as
varying number of anchors and blind nodes are also included.



Accelerated Cardiac Diffusion Tensor Imaging Using Joint Low-Rank and Sparsity Constraints

Objective: The purpose of this manuscript is to accelerate cardiac diffusion
tensor imaging (CDTI) by integrating low-rankness and compressed sensing.
Methods: Diffusion-weighted images exhibit both transform sparsity and
low-rankness. These properties can jointly be exploited to accelerate CDTI,
especially when a phase map is applied to correct for the phase inconsistency
across diffusion directions, thereby enhancing low-rankness. The proposed
method is evaluated both ex vivo and in vivo, and is compared to methods using
either a low-rank or sparsity constraint alone. Results: Compared to using a
low-rank or sparsity constraint alone, the proposed method preserves more
accurate helix angle features, the transmural continuum across the myocardium
wall, and mean diffusivity at higher acceleration, while yielding significantly
lower bias and higher intraclass correlation coefficient. Conclusion:
Low-rankness and compressed sensing together facilitate acceleration for both
ex vivo and in vivo CDTI, improving reconstruction accuracy compared to
employing either constraint alone. Significance: Compared to previous methods
for accelerating CDTI, the proposed method has the potential to reach higher
acceleration while preserving myofiber architecture features which may allow
more spatial coverage, higher spatial resolution and shorter temporal footprint
in the future.



Mosaicked multispectral image compression based on inter- and intra-band correlation

Multispectral imaging has been utilized in many fields, but the cost of
capturing and storing image data is still high. Single-sensor cameras with
multispectral filter arrays can reduce the cost of capturing images at the
expense of slightly lower image quality. When multispectral filter arrays are
used, conventional multispectral image compression methods can be applied after
interpolation, but the compressed image data after interpolation has some
redundancy because the interpolated data are computed from the captured raw
data. In this paper, we propose an efficient image compression method for
single-sensor multispectral cameras. The proposed method encodes the captured
multispectral data before interpolation. We also propose a new spectral
transform method for the compression of mosaicked multispectral images. This
transform is designed by considering the filter arrangement and the spectral
sensitivities of a multispectral filter array. The experimental results show
that the proposed method achieves a higher peak signal-to-noise ratio at higher
bit rates than a conventional compression method that encodes a multispectral
image after interpolation, e.g., 3-dB gain over conventional compression when
coding at rates of over 0.1 bit/pixel/bands.



Quadrature compressive sampling SAR imaging

This paper presents a quadrature compressive sampling (QuadCS) and associated
fast imaging scheme for synthetic aperture radar (SAR). Different from other
analog-to-information conversions (AIC), QuadCS AICs using independent
spreading signals sample the SAR echoes due to different transmitted pulses.
Then the resulting sensing matrix has lower correlation between any two columns
than that by a fixed spreading signal, and better SAR image can be
reconstructed. With proper setting of the spreading signals in QuadCS, the
sensing matrix has the structures suitable for fast computation of
matrix-vector multiplication operations, which leads to the fast image
reconstruction. The performance of the proposed scheme is assessed using real
SAR image. The reconstructed SAR images with only one-fourth of the Nyquist
data achieve the image quality similar to that of the classical SAR images with
Nyquist samples.



A Linear Solution Method of Generalized Robust Chance Constrained Real-time Dispatch

In this letter, a novel solution method of generalized robust chance
constrained real-time dispatch (GRCC-RTD) considering wind power uncertainty is
proposed. GRCC models are advantageous in dealing with distributional
uncertainty, however, they are difficult to solve because of the complex
ambiguity set. By constructing traceable counterparts of the robust chance
constraints and using the reformulation linearization technique, the model is
equivalently transformed into a deterministic linear programming problem, which
can be solved efficiently by off-the-shelf solvers. Numerical results verify
the effectiveness and efficiency of the approach.



How to Split UL/DL Antennas in Full-Duplex Cellular Networks

To further improve the potential of full-duplex communications, networks may
employ multiple antennas at the base station or user equipment. To this end,
networks that employ current radios usually deal with self-interference and
multi-user interference by beamforming techniques. Although previous works
investigated beamforming design to improve spectral efficiency, the fundamental
question of how to split the antennas at a base station between uplink and
downlink in full-duplex networks has not been investigated rigorously. This
paper addresses this question by posing antenna splitting as a binary nonlinear
optimization problem to minimize the sum mean squared error of the received
data symbols. It is shown that this is an NP-hard problem. This combinatorial
problem is dealt with by equivalent formulations, iterative convex
approximations, and a binary relaxation. The proposed algorithm is guaranteed
to converge to a stationary solution of the relaxed problem with much smaller
complexity than exhaustive search. Numerical results indicate that the proposed
solution is close to the optimal in both high and low self-interference capable
scenarios, while the usually assumed antenna splitting is far from optimal. For
large number of antennas, a simple antenna splitting is close to the proposed
solution. This reveals that the importance of antenna splitting is inversely
proportional with the number of antennas.



Direction of Arrival with One Microphone, a few LEGOs, and Non-Negative Matrix Factorization

Conventional approaches to sound source localization require at least two
microphones. It is known, however, that people with unilateral hearing loss can
also localize sounds. Monaural localization is possible thanks to the
scattering by the head, though it hinges on learning the spectra of the various
sources. We take inspiration from this human ability to propose algorithms for
accurate sound source localization using a single microphone embedded in an
arbitrary scattering structure. The structure modifies the frequency response
of the microphone in a direction-dependent way giving each direction a
signature. While knowing those signatures is sufficient to localize sources of
white noise, localizing speech is much more challenging: it is an ill-posed
inverse problem which we regularize by prior knowledge in the form of learned
non-negative dictionaries. We demonstrate a monaural speech localization
algorithm based on non-negative matrix factorization that does not depend on
sophisticated, designed scatterers. In fact, we show experimental results with
ad hoc scatterers made of LEGO bricks. Even with these rudimentary structures
we can accurately localize arbitrary speakers; that is, we do not need to learn
the dictionary for the particular speaker to be localized. Finally, we discuss
multi-source localization and the related limitations of our approach.



Polar $n$-Complex and $n$-Bicomplex Singular Value Decomposition and Principal Component Pursuit

Informed by recent work on tensor singular value decomposition and circulant
algebra matrices, this paper presents a new theoretical bridge that unifies the
hypercomplex and tensor-based approaches to singular value decomposition and
robust principal component analysis. We begin our work by extending the
principal component pursuit to Olariu's polar $n$-complex numbers as well as
their bicomplex counterparts. In so doing, we have derived the polar
$n$-complex and $n$-bicomplex proximity operators for both the $\ell_1$- and
trace-norm regularizers, which can be used by proximal optimization methods
such as the alternating direction method of multipliers. Experimental results
on two sets of audio data show that our algebraically-informed formulation
outperforms tensor robust principal component analysis. We conclude with the
message that an informed definition of the trace norm can bridge the gap
between the hypercomplex and tensor-based approaches. Our approach can be seen
as a general methodology for generating other principal component pursuit
algorithms with proper algebraic structures.



A Radio Frequency Non-reciprocal Network Based on Switched Low-loss Acoustic Delay Lines

This work demonstrates the first non-reciprocal network based on switched
low-loss acoustic delay lines. A 21 dB non-reciprocal contrast between
insertion loss (IL=6.7 dB) and isolation (28.3 dB) has been achieved over a
fractional bandwidth of 8.8% at a center frequency 155MHz, using a record low
switching frequency of 877.22 kHz. The 4-port circulator is built upon a newly
reported framework by the authors, but using two in-house fabricated low-loss,
wide-band lithium niobate (LiNbO3) delay lines with single-phase unidirectional
transducers (SPUDT) and commercial available switches. Such a system can
potentially lead to future wide-band, low-loss chip-scale nonreciprocal RF
systems with unprecedented programmability.



Informed Group-Sparse Representation for Singing Voice Separation

Singing voice separation attempts to separate the vocal and instrumental
parts of a music recording, which is a fundamental problem in music information
retrieval. Recent work on singing voice separation has shown that the low-rank
representation and informed separation approaches are both able to improve
separation quality. However, low-rank optimizations are computationally
inefficient due to the use of singular value decompositions. Therefore, in this
paper, we propose a new linear-time algorithm called informed group-sparse
representation, and use it to separate the vocals from music using pitch
annotations as side information. Experimental results on the iKala dataset
confirm the efficacy of our approach, suggesting that the music accompaniment
follows a group-sparse structure given a pre-trained instrumental dictionary.
We also show how our work can be easily extended to accommodate multiple
dictionaries using the DSD100 dataset.



Complex and Quaternionic Principal Component Pursuit and Its Application to Audio Separation

Recently, the principal component pursuit has received increasing attention
in signal processing research ranging from source separation to video
surveillance. So far, all existing formulations are real-valued and lack the
concept of phase, which is inherent in inputs such as complex spectrograms or
color images. Thus, in this letter, we extend principal component pursuit to
the complex and quaternionic cases to account for the missing phase
information. Specifically, we present both complex and quaternionic proximity
operators for the $\ell_1$- and trace-norm regularizers. These operators can be
used in conjunction with proximal minimization methods such as the inexact
augmented Lagrange multiplier algorithm. The new algorithms are then applied to
the singing voice separation problem, which aims to separate the singing voice
from the instrumental accompaniment. Results on the iKala and MSD100 datasets
confirmed the usefulness of phase information in principal component pursuit.



Association fairness in Wi-Fi and LTE-U coexistence

In this paper we address the issue of association fairness when Wi-Fi and LTE
unlicensed (LTE-U) coexist on the same channel in the unlicensed 5 GHz band.
Since beacon transmission is the first step in starting the association process
in Wi-Fi, we define association fairness as how fair LTE-U is in allowing Wi-Fi
to start transmitting beacons on a channel that it occupies with a very large
duty cycle. According to the LTE-U specification, if a LTE-U base station
determines that a channel is vacant, it can transmit for up to 20 ms and turn
OFF for only 1 ms, resulting in a duty cycle of 95%. In an area with heavy
spectrum usage, there will be cases when a Wi-Fi access point wishes to share
the same channel, as it does today with Wi-Fi. We study, both theoretically and
experimentally, the effect that such a large LTE-U duty cycle can have on the
association process, specifically Wi-Fi beacon transmission and reception. We
demonstrate via an experimental set-up using National Instrument (NI) USRPs
that a significant percentage of Wi-Fi beacons will either not be transmitted
in a timely fashion or will not be received at the LTE-U BS thus making it
difficult for the LTE-U BS to adapt its duty cycle in response to the Wi-Fi
usage. Our experimental results corroborate our theoretical analysis. We
compare the results with Wi-Fi/Wi-Fi coexistence and demonstrate that
LTE-U/Wi-Fi coexistence is not fair when it comes to initial association since
there is a much larger percentage of beacon errors in the latter case. Hence,
the results in the paper indicate that in order to maintain association
fairness, a LTE-U BS should not transmit at such high duty cycles, even if it
deems the channel to be vacant.



A Deep Generative Adversarial Architecture for Network-Wide Spatial-Temporal Traffic State Estimation

This study proposes a deep generative adversarial architecture (GAA) for
network-wide spatial-temporal traffic state estimation. The GAA is able to
combine traffic flow theory with neural networks and thus improve the accuracy
of traffic state estimation. It consists of two Long Short-Term Memory Neural
Networks (LSTM NNs) which capture correlation in time and space among traffic
flow and traffic density. One of the LSTM NNs, called a discriminative network,
aims to maximize the probability of assigning correct labels to both true
traffic state matrices (i.e., traffic flow and traffic density within a given
spatial-temporal area) and the traffic state matrices generated from the other
neural network. The other LSTM NN, called a generative network, aims to
generate traffic state matrices which maximize the probability that the
discriminative network assigns true labels to them. The two LSTM NNs are
trained simultaneously such that the trained generative network can generate
traffic matrices similar to those in the training data set. Given a traffic
state matrix with missing values, we use back-propagation on three defined loss
functions to map the corrupted matrix to a latent space. The mapping vector is
then passed through the pre-trained generative network to estimate the missing
values of the corrupted matrix. The proposed GAA is compared with the existing
Bayesian network approach on loop detector data collected from Seattle,
Washington and that collected from San Diego, California. Experimental results
indicate that the GAA can achieve higher accuracy in traffic state estimation
than the Bayesian network approach.



Control and Management of Multiple RATs in Wireless Networks: An SDN Approach

Telecom operators are using a variety of Radio Access Technologies (RATs) for
providing services to mobile subscribers. This development has emphasized the
requirement for unified control and management of diverse RATs. Although
multiple RATs co-exist within today's cellular networks, each RAT is controlled
by a set of different entities. This may lead to suboptimal utilization of the
overall network resources. In this article, we review various architectures for
multi-RAT control proposed by both industry and academia. We also propose a
novel SDN based network architecture for end-to-end control and management of
diverse RATs. The architecture is scalable and provides a framework for
improved network performance over the present day architecture and proposals in
existing literature. Our architecture also provides a framework for deployment
of applications in a RAT agnostic fashion. It facilitates network slicing and
enables the provision of Quality of Service (QoS) guarantees to the end user.
We have also developed an evaluation platform based on ns-3 to evaluate the
performance offered by the architecture. Experimental results obtained using
the platform demonstrate the benefits provided by our architecture.



A Centralized SDN Architecture for the 5G Cellular Network

In order to meet the increasing demands of high data rate and low latency
cellular broadband applications, plans are underway to roll out the Fifth
Generation (5G) cellular wireless system by the year 2020. This paper proposes
a novel method for adapting the Third Generation Partnership Project (3GPP)'s
5G architecture to the principles of Software Defined Networking (SDN). We
propose to have centralized network functions in the 5G network core to control
the network, end-to-end. This is achieved by relocating the control
functionality present in the 5G Radio Access Network (RAN) to the network core,
resulting in the conversion of the base station known as the gNB into a pure
data plane node. This brings about a significant reduction in signaling costs
between the RAN and the core network. It also results in improved system
performance. The merits of our proposal have been illustrated by evaluating the
Key Performance Indicators (KPIs) of the 5G network, such as network attach
(registration) time and handover time. We have also demonstrated improvements
in attach time and system throughput due to the use of centralized algorithms
for mobility management with the help of ns-3 simulations.



Distributed Laser Charging: A Wireless Power Transfer Approach

Wireless power transfer (WPT) is a promising solution to provide convenient
and perpetual energy supplies to electronics. Traditional WPT technologies face
the challenge of providing Watt-level power over meter-level distance for
Internet of Things (IoT) and mobile devices, such as sensors, controllers,
smart-phones, laptops, etc.. Distributed laser charging (DLC), a new WPT
alternative, has the potential to solve these problems and enable WPT with the
similar experience as WiFi communications. In this paper, we present a
multi-module DLC system model, in order to illustrate its physical fundamentals
and mathematical formula. This analytical modeling enables the evaluation of
power conversion or transmission for each individual module, considering the
impacts of laser wavelength, transmission attenuation and photovoltaic-cell
(PV-cell) temperature. Based on the linear approximation of
electricity-to-laser and laser-to-electricity power conversion validated by
measurement and simulation, we derive the maximum power transmission efficiency
in closed-form. Thus, we demonstrate the variation of the maximum power
transmission efficiency depending on the supply power at the transmitter, laser
wavelength, transmission distance, and PV-cell temperature. Similar to the
maximization of information transmission capacity in wireless information
transfer (WIT), the maximization of the power transmission efficiency is
equally important in WPT. Therefore, this work not only provides the insight of
DLC in theory, but also offers the guideline of DLC system design in practice.



Identifying the Topology of Undirected Networks from Diffused Non-stationary Graph Signals

We address the problem of inferring an undirected graph from nodal
observations, which are modeled as non-stationary graph signals generated by
local diffusion dynamics that depend on the structure of the unknown network.
Using the so-called graph-shift operator (GSO), which is a matrix
representation of the graph, we first identify the eigenvectors of the shift
matrix from realizations of the diffused signals, and then estimate the
eigenvalues by imposing desirable properties on the graph to be recovered.
Different from the stationary setting where the eigenvectors can be obtained
directly from the covariance matrix of the observations, here we need to
estimate first the unknown diffusion (graph) filter -- a polynomial in the GSO
that preserves the sought eigenbasis. To carry out this initial system
identification step, we exploit different sources of information on the
arbitrarily-correlated input signal driving the diffusion on the graph. We
first explore the simpler case where the observations, the input information,
and the unknown graph filter are linearly related. We then address the case
where the relation is given by a system of matrix quadratic equations, which
arises in pragmatic scenarios where only the second-order statistics of the
inputs are available. While such quadratic filter identification problem boils
down to a non-convex fourth-order polynomial minimization, we discuss
identifiability conditions, propose algorithms to approximate the solution and
analyze their performance. Numerical tests illustrate the effectiveness of the
proposed topology inference algorithms in recovering brain, social, financial
and urban transportation networks using synthetic and real-world signals.



Speech Dereverberation Based on Integrated Deep and Ensemble Learning Algorithm

Reverberation, which is generally caused by sound reflections from walls,
ceilings, and floors, can result in severe performance degradation of acoustic
applications. Due to a complicated combination of attenuation and time-delay
effects, the reverberation property is difficult to characterize, and it
remains a challenging task to effectively retrieve the anechoic speech signals
from reverberation ones. In the present study, we proposed a novel integrated
deep and ensemble learning algorithm (IDEA) for speech dereverberation. The
IDEA consists of offline and online phases. In the offline phase, we train
multiple dereverberation models, each aiming to precisely dereverb speech
signals in a particular acoustic environment; then a unified fusion function is
estimated that aims to integrate the information of multiple dereverberation
models. In the online phase, an input utterance is first processed by each of
the dereverberation models. The outputs of all models are integrated
accordingly to generate the final anechoic signal. We evaluated the IDEA on
designed acoustic environments, including both matched and mismatched
conditions of the training and testing data. Experimental results confirm that
the proposed IDEA outperforms single deep-neural-network-based dereverberation
model with the same model architecture and training data.



Separation of Instrument Sounds using Non-negative Matrix Factorization with Spectral Envelope Constraints

Spectral envelope is one of the most important features that characterize the
timbre of an instrument sound. However, it is difficult to use spectral
information in the framework of conventional spectrogram decomposition methods.
We overcome this problem by suggesting a simple way to provide a constraint on
the spectral envelope calculated by linear prediction. In the first part of
this study, we use a pre-trained spectral envelope of known instruments as the
constraint. Then we apply the same idea to a blind scenario in which the
instruments are unknown. The experimental results reveal that the proposed
method outperforms the conventional methods.



On Partly Overloaded Spreading Sequences with Variable Spreading Factor

Future wireless communications systems are expected to support multi-service
operation, i.e. especially multi-rate as well as multi-level quality of service
(QoS) requirements. This evolution is mainly driven by the success of the
Internet of Things (IoT) and the growing presence of machine type communication
(MTC). Whereas in the last years information in wireless communication systems
was mainly generated or at least requested by humans and was also processed by
humans, we can now see a paradigm shift since so-called machine type
communication is gaining growing importance. Along with these changes we also
encounter changes regarding the quality of service requirements, data rate
requirements, latency constraints, different duty cycles et cetera. The
challenge for new communication systems will therefore be to enable different
user types and their different requirements efficiently. In this paper, we
present partly overloaded spreading sequences, i.e. sequences which are
globally orthogonal and sequences which interfere with a subset of sequences
while being orthogonal to the globally orthogonal sequences. Additionally, we
are able to vary the spreading factor of these sequences, which allows us to
flexibly assign appropriate sequences to different service types or user types
respectively. We propose the use of these sequences for a CDMA channel access
method which is able to flexibly support different traffic types.



On Partially Overlapping Coexistence for Dynamic Spectrum Access in Cognitive Radio

In this paper, we study partially overlapping co-existence scenarios in
cognitive radio environment. We consider an Orthogonal Frequency Division
Multiplexing (OFDM) cognitive system coexisting with a narrow-band (NB) and an
OFDM primary system, respectively. We focus on finding the minimum frequency
separation between the coexisting systems to meet a certain target BER.
Windowing and nulling are used as simple techniques to reduce the OFDM
out-of-band radiations, and, hence decrease the separation. The effect of these
techniques on the OFDM spectral efficiency and PAPR is also studied.



Robust Sparse Fourier Transform Based on The Fourier Projection-Slice Theorem

The state-of-the-art automotive radars employ multidimensional discrete
Fourier transforms (DFT) in order to estimate various target parameters. The
DFT is implemented using the fast Fourier transform (FFT), at sample and
computational complexity of $O(N)$ and $O(N \log N)$, respectively, where $N$
is the number of samples in the signal space. We have recently proposed a
sparse Fourier transform based on the Fourier projection-slice theorem
(FPS-SFT), which applies to multidimensional signals that are sparse in the
frequency domain. FPS-SFT achieves sample complexity of $O(K)$ and
computational complexity of $O(K \log K)$ for a multidimensional, $K$-sparse
signal. While FPS-SFT considers the ideal scenario, i.e., exactly sparse data
that contains on-grid frequencies, in this paper, by extending FPS-SFT into a
robust version (RFPS-SFT), we emphasize on addressing noisy signals that
contain off-grid frequencies; such signals arise from radar applications. This
is achieved by employing a windowing technique and a voting-based frequency
decoding procedure; the former reduces the frequency leakage of the off-grid
frequencies below the noise level to preserve the sparsity of the signal, while
the latter significantly lowers the frequency localization error stemming from
the noise. The performance of the proposed method is demonstrated both
theoretically and numerically.



Cooperative Multi-Agent Reinforcement Learning for Low-Level Wireless Communication

Traditional radio systems are strictly co-designed on the lower levels of the
OSI stack for compatibility and efficiency. Although this has enabled the
success of radio communications, it has also introduced lengthy standardization
processes and imposed static allocation of the radio spectrum. Various
initiatives have been undertaken by the research community to tackle the
problem of artificial spectrum scarcity by both making frequency allocation
more dynamic and building flexible radios to replace the static ones. There is
reason to believe that just as computer vision and control have been overhauled
by the introduction of machine learning, wireless communication can also be
improved by utilizing similar techniques to increase the flexibility of
wireless networks. In this work, we pose the problem of discovering low-level
wireless communication schemes ex-nihilo between two agents in a fully
decentralized fashion as a reinforcement learning problem. Our proposed
approach uses policy gradients to learn an optimal bi-directional communication
scheme and shows surprisingly sophisticated and intelligent learning behavior.
We present the results of extensive experiments and an analysis of the fidelity
of our approach.



Molecular Communications at the Macroscale: A Novel Framework for Modeling Epidemic Spreading and Mitigation

Using the notion of effective distance proposed by Brockmann and Helbing,
complex spatiotemporal processes of epidemic spreading can be reduced to
circular wave propagation patterns with well-defined wavefronts. This hidden
homogeneity of contagion phenomena enables the mapping of virtual mobility
networks to physical propagation channels. Subsequently, we propose a novel
communications-inspired model of epidemic spreading and mitigation by
establishing the one-to-one correspondence between the essential components
comprising information and disease transmissions. The epidemic processes can be
regarded as macroscale molecular communications, in which individuals are
macroscale information molecules carrying messages (epidemiological states). We
then present the notions of normalized ensemble-average prevalence (NEAP) and
prevalence delay profile (PDP) to characterize the relative impact and time
difference of all the spreading paths, which are analogous to the classical
description methods of path loss and power delay profile in communications.
Furthermore, we introduce the metric of root mean square (RMS) delay spread to
measure the distortion of early contagion dynamics caused by multiple infection
transmission routes. In addition, we show how social and medical interventions
can be understood from the perspectives of various communication modules. The
proposed framework provides an intuitive, coherent, and efficient approach for
characterization of the disease outbreaks by applying the deep-rooted
communications theories as the analytical lens.



Modified SI Epidemic Model for Combating Virus Spread in Spatially Correlated Wireless Sensor Networks

In wireless sensor networks (WSNs), main task of each sensor node is to sense
the physical activity (i.e., targets or disaster conditions) and then to report
it to the control center for further process. For this, sensor nodes are
attached with many sensors having ability to measure the environmental
information. Spatial correlation between nodes exists in such wireless sensor
network based on common sensory coverage and then the redundant data
communication is observed. To study virus spreading dynamics in such scenario,
a modified SI epidemic model is derived mathematically by incorporating WSN
parameters such as spatial correlation, node density, sensing range,
transmission range, total sensor nodes etc. The solution for proposed SI model
is also determined to study the dynamics with time. Initially, a small number
of nodes are attacked by viruses and then virus infection propagates through
its neighboring nodes over normal data communication. Since redundant nodes
exists in correlated sensor field, virus spread process could be different with
different sensory coverage. The proposed SI model captures spatial and temporal
dynamics than existing ones which are global. The infection process leads to
network failure. By exploiting spatial correlation between nodes, spread
control scheme is developed to limit the further infection in the network.
Numerical result analysis is provided with comparison for validation.



Graph Spectral Image Processing

Recent advent of graph signal processing (GSP) has spurred intensive studies
of signals that live naturally on irregular data kernels described by graphs
(e.g., social networks, wireless sensor networks). Though a digital image
contains pixels that reside on a regularly sampled 2D grid, if one can design
an appropriate underlying graph connecting pixels with weights that reflect the
image structure, then one can interpret the image (or image patch) as a signal
on a graph, and apply GSP tools for processing and analysis of the signal in
graph spectral domain. In this article, we overview recent graph spectral
techniques in GSP specifically for image / video processing. The topics covered
include image compression, image restoration, image filtering and image
segmentation.



Two High-performance Schemes of Transmit Antenna Selection for Secure Spatial Modulation

In this paper, a secure spatial modulation (SM) system with artificial noise
(AN)-aided is investigated. To achieve higher secrecy rate (SR) in such a
system, two high-performance schemes of transmit antenna selection (TAS),
leakage-based and maximum secrecy rate (Max-SR), are proposed and a generalized
Euclidean distance-optimized antenna selection (EDAS) method is designed. From
simulation results and analysis, the four TAS schemes have an decreasing order:
Max-SR, leakage-based, generalized EDAS, and random (conventional), in terms of
SR performance. However, the proposed Max-SR method requires the exhaustive
search to achieve the optimal SR performance, thus its complexity is extremely
high as the number of antennas tends to medium and large scale. The proposed
leakage-based method approaches the Max-SR method with much lower complexity.
Thus, it achieves a good balance between complexity and SR performance. In
terms of bit error rate (BER), their performances are in an increasing order:
random, leakage-based, Max-SR, and generalized EDAS.



Sending Information Through Status Updates

We consider an energy harvesting transmitter sending status updates regarding
a physical phenomenon it observes to a receiver. Different from the existing
literature, we consider a scenario where the status updates carry information
about an independent message. The transmitter encodes this message into the
timings of the status updates. The receiver needs to extract this encoded
information, as well as update the status of the observed phenomenon. The
timings of the status updates, therefore, determine both the age of information
(AoI) and the message rate (rate). We study the tradeoff between the achievable
message rate and the achievable average AoI. We propose several achievable
schemes and compare their rate-AoI performances.



Downlink Power Allocation for CoMP-NOMA in Multi-Cell Networks

This work considers the problem of dynamic power allocation in the downlink
of multi-cell networks, where each cell utilizes non-orthogonal multiple access
(NOMA)-based resource allocation. Also, coordinated multi-point (CoMP)
transmission is utilized among multiple cells to serve users experiencing
severe inter-cell interference (ICI). More specifically, we consider a two-tier
heterogeneous network (HetNet) consisting of a high-power macro cell underlaid
with multiple low-power small cells each of which uses the same resource block.
Under this {\em CoMP-NOMA framework}, CoMP transmission is applied to a user
experiencing high channel gain with multiple base stations (BSs)/cells, while
NOMA is utilized to schedule CoMP and non-CoMP users over the same transmission
resources, i.e., time, spectrum and space. Different CoMP-NOMA models are
discussed, but focus is primarily on the joint transmission CoMP-NOMA
(JT-CoMP-NOMA) model. For the JT-CoMP-NOMA model, an optimal joint power
allocation problem is formulated and the solution is derived for each CoMP-set
consisting of multiple cooperating BSs (i.e., CoMP BSs). To overcome the
substantial computational complexity of the joint power optimization approach,
we propose a distributed power optimization problem at each cooperating BS
whose optimal solution is independent of the solution of other coordinating
BSs. The validity of the distributed solution for the joint power optimization
problem is provided and numerical performance evaluation is carried out for the
proposed CoMP-NOMA models including JT-CoMP-NOMA and coordinated scheduling
CoMP-NOMA (CS-CoMP-NOMA). The obtained results reveal significant gains in
spectral and energy efficiency in comparison with conventional CoMP-orthogonal
multiple access (CoMP-OMA) systems.



Cellular UAV-to-X Communications: Design and Optimization for Multi-UAV Networks

In this paper, we consider a single-cell cellular network with a number of
cellular users (CUs) and unmanned aerial vehicles (UAVs), in which multiple
UAVs upload their collected data to the base station (BS). Two transmission
modes are considered to support the multi-UAV communications, i.e.,
UAV-to-infrastructure (U2I) and UAV-to-UAV (U2U) communications. Specifically,
the UAV with a high signal to noise ratio (SNR) for the U2I link uploads its
collected data directly to the BS through U2I communication, while the UAV with
a low SNR for the U2I link can transmit data to a nearby UAV through
underlaying U2U communication for the sake of quality of service. We first
propose a cooperative UAV sense-and-send protocol to enable the UAV-to-X
communications, and then formulate the subchannel allocation and UAV speed
optimization problem to maximize the uplink sum-rate. To solve this NP-hard
problem efficiently, we decouple it into three sub-problems: U2I and cellular
user (CU) subchannel allocation, U2U subchannel allocation, and UAV speed
optimization. An iterative subchannel allocation and speed optimization
algorithm (ISASOA) is proposed to solve these sub-problems jointly. Simulation
results show that the proposed ISASOA can upload 10\% more data than the greedy
algorithm.



Seismic signal sparse time-frequency analysis by Lp-quasinorm constraint

Time-frequency analysis has been applied successfully in many fields.
However, the traditional methods, like short time Fourier transform and Cohen
distribution, suffer from the low resolution or the interference of the cross
terms. To solve these issues, we put forward a new sparse time-frequency
analysis model by using the Lp-quasinorm constraint, which is capable of
fitting the sparsity prior knowledge in the frequency domain. In the proposed
model, we regard the short time truncated data as the observation of sparse
representation and design a dictionary matrix, which builds up the relationship
between the short time measurement and the sparse spectrum. Based on the
relationship and the Lp-quasinorm feasible domain, the proposed model is
established. The alternating direction method of multipliers (ADMM) is adopted
to solve the proposed model. Experiments are then conducted on several
theoretical signals and applied to the seismic signal spectrum decomposition,
indicating that the proposed method is able to obtain a higher time-frequency
distribution than state-of-the-art time-frequency methods. Thus, the proposed
method is of great importance to reservoir exploration.



Enabling Quality-Driven Scalable Video Transmission over Multi-User NOMA System

Recently, non-orthogonal multiple access (NOMA) has been proposed to achieve
higher spectral efficiency over conventional orthogonal multiple access.
Although it has the potential to meet increasing demands of video services, it
is still challenging to provide high performance video streaming. In this
research, we investigate, for the first time, a multi-user NOMA system design
for video transmission. Various NOMA systems have been proposed for data
transmission in terms of throughput or reliability. However, the perceived
quality, or the quality-of-experience of users, is more critical for video
transmission. Based on this observation, we design a quality-driven scalable
video transmission framework with cross-layer support for multi-user NOMA. To
enable low complexity multi-user NOMA operations, a novel user grouping
strategy is proposed. The key features in the proposed framework include the
integration of the quality model for encoded video with the physical layer
model for NOMA transmission, and the formulation of multi-user NOMA-based video
transmission as a quality-driven power allocation problem. As the problem is
non-concave, a global optimal algorithm based on the hidden monotonic property
and a suboptimal algorithm with polynomial time complexity are developed.
Simulation results show that the proposed multi-user NOMA system outperforms
existing schemes in various video delivery scenarios.



Unmanned Aerial Vehicle-Aided Communications: Joint Transmit Power and Trajectory Optimization

This letter investigates the transmit power and trajectory optimization
problem for unmanned aerial vehicle (UAV)-aided networks. Different from
majority of the existing studies with fixed communication infrastructure, a
dynamic scenario is considered where a flying UAV provides wireless services
for multiple ground nodes simultaneously. To fully exploit the controllable
channel variations provided by the UAV's mobility, the UAV's transmit power and
trajectory are jointly optimized to maximize the minimum average throughput
within a given time length. For the formulated non-convex optimization with
power budget and trajectory constraints, this letter presents an efficient
joint transmit power and trajectory optimization algorithm. Simulation results
validate the effectiveness of the proposed algorithm and reveal that the
optimized transmit power shows a water-filling characteristic in spatial
domain.



Energy-Efficient Power Loading for OFDM-based Cognitive Radio Systems with Channel Uncertainties

In this paper, we propose a novel algorithm to optimize the energy-efficiency
(EE) of orthogonal frequency division multiplexing-based cognitive radio
systems under channel uncertainties. We formulate an optimization problem that
guarantees a minimum required rate and a specified power budget for the
secondary user (SU), while restricting the interference to primary users (PUs)
in a statistical manner. The optimization problem is non-convex and it is
transformed to an equivalent problem using the concept of fractional
programming. Unlike all related works in the literature, we consider the effect
of imperfect channel-stateinformation (CSI) on the links between the SU
transmitter and receiver pairs and we additionally consider the effect of
limited sensing capabilities of the SU. Since the interference constraints are
met statistically, the SU transmitter does not require perfect CSI feedback
from the PUs receivers. Simulation results sho w that the EE deteriorates as
the channel estimation error increases. Comparisons with relevant works from
the literature show that the interference thresholds at the PUs receivers can
be severely exceeded and the EE is slightly deteriorated if the SU does no t
account for spectrum sensing errors.



Spatial Field Reconstruction and Sensor Selection in Heterogeneous Sensor Networks with Stochastic Energy Harvesting

We address the two fundamental problems of spatial field reconstruction and
sensor selection in het- erogeneous sensor networks. We consider the case where
two types of sensors are deployed: the first consists of expensive, high
quality sensors; and the second, of cheap low quality sensors, which are
activated only if the intensity of the spatial field exceeds a pre-defined
activation threshold (eg. wind sensors). In addition, these sensors are powered
by means of energy harvesting and their time varying energy status impacts on
the accuracy of the measurement that may be obtained. We account for this
phenomenon by encoding the energy harvesting process into the second moment
properties of the additive noise, resulting in a spatial heteroscedastic
process. We then address the following two important problems: (i) how to
efficiently perform spatial field reconstruction based on measurements obtained
simultaneously from both networks; and (ii) how to perform query based sensor
set selection with predictive MSE performance guarantee. We first show that the
resulting predictive posterior distribution, which is key in fusing such
disparate observations, involves solving intractable integrals. To overcome
this problem, we solve the first problem by developing a low complexity
algorithm based on the spatial best linear unbiased estimator (S-BLUE). Next,
building on the S-BLUE, we address the second problem, and develop an efficient
algorithm for query based sensor set selection with performance guarantee. Our
algorithm is based on the Cross Entropy method which solves the combinatorial
optimization problem in an efficient manner. We present a comprehensive study
of the performance gain that can be obtained by augmenting the high-quality
sensors with low-quality sensors using both synthetic and real insurance storm
surge database known as the Extreme Wind Storms Catalogue.



Non Intrusive Load Monitoring in Chaotic Switching Networks

In this work, a non intrusive load disaggregation scheme is proposed. By
using a kernel based nonlinear regression strategy, the switching dynamic of an
electric network, simulated as a set of RLC circuits with chaotic switching, is
approximated using a time series of the total power consumption. The results
suggest that the employed methodology can be useful in the design of efficient
load disaggregation schemes.



UAV Offloading: Spectrum Trading Contract Design for UAV Assisted 5G Networks

Unmanned Aerial Vehicle (UAV) has been recognized as a promising way to
assist future wireless communications due to its high flexibility of deployment
and scheduling. In this paper, we focus on temporarily deployed UAVs that
provide downlink data offloading in some regions under a macro base station
(MBS). Since the manager of the MBS and the operators of the UAVs could be of
different interest groups, we formulate the corresponding spectrum trading
problem by means of contract theory, where the manager of the MBS has to design
an optimal contract to maximize its own revenue. Such contract comprises a set
of bandwidth options and corresponding prices, and each UAV operator only
chooses the most profitable one from all the options in the whole contract. We
analytically derive the optimal pricing strategy based on fixed bandwidth
assignment, and then propose a dynamic programming algorithm to calculate the
optimal bandwidth assignment in polynomial time. By simulations, we compare the
outcome of the MBS optimal contract with that of a social optimal one, and find
that a selfish MBS manager sells less bandwidth to the UAV operators.



Uplink Interference Mitigation Techniques for Coexistence of 5G mmWave Users with Incumbents at 70 and 80 GHz

The millimeter wave spectra at 71-76GHz (70GHz) and 81-86GHz (80GHz) have the
potential to endow fifth-generation new radio (5G-NR) with mobile connectivity
at gigabit rates. However, a pressing issue is the presence of incumbent
systems in these bands, which are primarily point-to-point fixed stations
(FSs). In this paper, we first identify the key properties of incumbents by
parsing databases of existing stations in major cities to devise several
modeling guidelines and characterize their deployment geometry and antenna
specifications. Second, we develop a detailed uplink interference framework to
compute the aggregate interference from outdoor 5G-NR users into FSs. We then
present several case studies in dense populated areas, using actual incumbent
databases and building layouts. Our simulation results demonstrate promising 5G
coexistence at 70GHz and 80GHz as the majority of FSs experience interference
well below the noise floor thanks to the propagation losses in these bands and
the deployment geometry of the incumbent and 5G systems. For the few FSs that
may incur higher interference, we propose several passive interference
mitigation techniques such as angular-based exclusion zones and spatial power
control. Simulation results show that the techniques can effectively protect
FSs, without tangible degradation of the 5G coverage.



Deep Network for Simultaneous Decomposition and Classification in UWB-SAR Imagery

Classifying buried and obscured targets of interest from other natural and
manmade clutter objects in the scene is an important problem for the U.S. Army.
Targets of interest are often represented by signals captured using
low-frequency (UHF to L-band) ultra-wideband (UWB) synthetic aperture radar
(SAR) technology. This technology has been used in various applications,
including ground penetration and sensing-through-the-wall. However, the
technology still faces a significant issues regarding low-resolution SAR
imagery in this particular frequency band, low radar cross sections (RCS),
small objects compared to radar signal wavelengths, and heavy interference. The
classification problem has been firstly, and partially, addressed by sparse
representation-based classification (SRC) method which can extract noise from
signals and exploit the cross-channel information. Despite providing potential
results, SRC-related methods have drawbacks in representing nonlinear relations
and dealing with larger training sets. In this paper, we propose a Simultaneous
Decomposition and Classification Network (SDCN) to alleviate noise inferences
and enhance classification accuracy. The network contains two jointly trained
sub-networks: the decomposition sub-network handles denoising, while the
classification sub-network discriminates targets from confusers. Experimental
results show significant improvements over a network without decomposition and
SRC-related methods.



Automatic Classification of Music Genre using Masked Conditional Neural Networks

Neural network based architectures used for sound recognition are usually
adapted from other application domains such as image recognition, which may not
harness the time-frequency representation of a signal. The ConditionaL Neural
Networks (CLNN) and its extension the Masked ConditionaL Neural Networks
(MCLNN) are designed for multidimensional temporal signal recognition. The CLNN
is trained over a window of frames to preserve the inter-frame relation, and
the MCLNN enforces a systematic sparseness over the network's links that mimics
a filterbank-like behavior. The masking operation induces the network to learn
in frequency bands, which decreases the network susceptibility to
frequency-shifts in time-frequency representations. Additionally, the mask
allows an exploration of a range of feature combinations concurrently analogous
to the manual handcrafting of the optimum collection of features for a
recognition task. MCLNN have achieved competitive performance on the Ballroom
music dataset compared to several hand-crafted attempts and outperformed models
based on state-of-the-art Convolutional Neural Networks.



Identification of Seed Cells in Multispectral Images for GrowCut Segmentation

The segmentation of satellite images is a necessary step to perform
object-oriented image classification, which has become relevant due to its
applicability on images with a high spatial resolution. To perform
object-oriented image classification, the studied image must first be segmented
in uniform regions. This segmentation requires manual work by an expert user,
who must exhaustively explore the image to establish thresholds that generate
useful and representative segments without oversegmenting and without
discarding representative segments. We propose a technique that automatically
segments the multispectral image while facing these issues. We identify in the
image homogenous zones according to their spectral signatures through the use
of morphological filters. These homogenous zones are representatives of
different types of land coverings in the image and are used as seeds for the
GrowCut multispectral segmentation algorithm. GrowCut is a cellular automaton
with competitive region growth, its cells are linked to every pixel in the
image through three parameters: the pixel's spectral signature, a label, and a
strength factor that represents the strength with which a cell defends its
label. The seed cells possess maximum strength and maintain their state
throughout the automaton's evolution. Starting from seed cells, each cell in
the image is iteratively attacked by its neighboring cells. When the automaton
stops updating its states, we obtain a segmented image where each pixel has
taken the label of one of its cells. In this paper the algorithm was applied in
an image acquired by Landsat8 on agricultural land of Calabozo, Guarico,
Venezuela where there are different types of land coverings: agriculture, urban
regions, water bodies, and savannas with different degrees of human
intervention. The segmentation obtained is presented as irregular polygons
enclosing geographical objects.



NELS - Never-Ending Learner of Sounds

Sounds are essential to how humans perceive and interact with the world and
are captured in recordings and shared on the Internet on a minute-by-minute
basis. These recordings, which are predominantly videos, constitute the largest
archive of sounds we know. However, most of these recordings have undescribed
content making necessary methods for automatic sound analysis, indexing and
retrieval. These methods have to address multiple challenges, such as the
relation between sounds and language, numerous and diverse sound classes, and
large-scale evaluation. We propose a system that continuously learns from the
web relations between sounds and language, improves sound recognition models
over time and evaluates its learning competency in the large-scale without
references. We introduce the Never-Ending Learner of Sounds (NELS), a project
for continuously learning of sounds and their associated knowledge, available
on line in nels.cs.cmu.edu



A modified fuzzy C means algorithm for shading correction in craniofacial CBCT images

CBCT images suffer from acute shading artifacts primarily due to scatter.
Numerous image-domain correction algorithms have been proposed in the
literature that use patient-specific planning CT images to estimate shading
contributions in CBCT images. However, in the context of radiosurgery
applications such as gamma knife, planning images are often acquired through
MRI which impedes the use of polynomial fitting approaches for shading
correction. We present a new shading correction approach that is independent of
planning CT images. Our algorithm is based on the assumption that true CBCT
images follow a uniform volumetric intensity distribution per material, and
scatter perturbs this uniform texture by contributing cupping and shading
artifacts in the image domain. The framework is a combination of fuzzy C-means
coupled with a neighborhood regularization term and Otsu's method. Experimental
results on artificially simulated craniofacial CBCT images are provided to
demonstrate the effectiveness of our algorithm. Spatial non-uniformity is
reduced from 16% to 7% in soft tissue and from 44% to 8% in bone regions. With
shading-correction, thresholding based segmentation accuracy for bone pixels is
improved from 85% to 91% when compared to thresholding without
shading-correction. The proposed algorithm is thus practical and qualifies as a
plug and play extension into any CBCT reconstruction software for shading
correction.



A Kotel'nikov Representation for Wavelets

This paper presents a wavelet representation using baseband signals, by
exploiting Kotel'nikov results. Details of how to obtain the processes of
envelope and phase at low frequency are shown. The archetypal interpretation of
wavelets as an analysis with a filter bank of constant quality factor is
revisited on these bases. It is shown that if the wavelet spectral support is
limited into the band $[f_m,f_M]$, then an orthogonal analysis is guaranteed
provided that $f_M \leq 3f_m$, a quite simple result, but that invokes some
parallel with the Nyquist rate. Nevertheless, in cases of orthogonal wavelets
whose spectrum does not verify this condition, it is shown how to construct an
"equivalent" filter bank with no spectral overlapping.



An Efficient Data-aided Synchronization in L-DACS1 for Aeronautical Communications

L-band Digital Aeronautical Communication System type-1 (L-DACS1) is an
emerging standard that aims at enhancing air traffic management (ATM) by
transitioning the traditional analog aeronautical communication systems to the
superior and highly efficient digital domain. L-DACS1 employs modern and
efficient orthogonal frequency division multiplexing (OFDM) modulation
technique to achieve more efficient and higher data rate in comparison to the
existing aeronautical communication systems. However, the performance of OFDM
systems is very sensitive to synchronization errors. L-DACS1 transmission is in
the L-band aeronautical channels that suffer from large interference and large
Doppler shifts, which makes the synchronization for L-DACS more challenging.
This paper proposes a novel computationally efficient synchronization method
for L-DACS1 systems that offers robust performance. Through simulation, the
proposed method is shown to provide accurate symbol timing offset (STO)
estimation as well as fractional carrier frequency offset (CFO) estimation in a
range of aeronautical channels. In particular, it can yield excellent
synchronization performance in the face of a large carrier frequency offset.



A New Peak Detection Method for Single or Three-Phase Unbalanced Sinusoidal Signals

In this paper, a fast amplitude detection method for the single or
three-phase unbalanced sinusoidal is reported. The proposed method is a method
of the amplitude detection for a single phase or three phase unbalanced
sinusoidal signal, based on detecting of the pulse width corresponding to the
peak amplitude. The detection period is the half period of the input signal.
This method is independent of the three-phase signal's unbalance and phase
sequence. The proposed method was verified through experiments for the single
or three-phase unbalanced signals.



Prediction of the Optimal Threshold Value in DF Relay Selection Schemes Based on Artificial Neural Networks

In wireless communications, the cooperative communication (CC) technology
promises performance gains compared to traditional Single-Input Single Output
(SISO) techniques. Therefore, the CC technique is one of the nominees for 5G
networks. In the Decode-and-Forward (DF) relaying scheme which is one of the CC
techniques, determination of the threshold value at the relay has a key role
for the system performance and power usage. In this paper, we propose
prediction of the optimal threshold values for the best relay selection scheme
in cooperative communications, based on Artificial Neural Networks (ANNs) for
the first time in literature. The average link qualities and number of relays
have been used as inputs in the prediction of optimal threshold values using
Artificial Neural Networks (ANNs): Multi-Layer Perceptron (MLP) and Radial
Basis Function (RBF) networks. The MLP network has better performance from the
RBF network on the prediction of optimal threshold value when the same number
of neurons is used at the hidden layer for both networks. Besides, the optimal
threshold values obtained using ANNs are verified by the optimal threshold
values obtained numerically using the closed form expression derived for the
system. The results show that the optimal threshold values obtained by ANNs on
the best relay selection scheme provide a minimum Bit-Error-Rate (BER) because
of the reduction of the probability that error propagation may occur. Also, for
the same BER performance goal, prediction of optimal threshold values provides
2dB less power usage, which is great gain in terms of green communicationBER
performance goal, prediction of optimal threshold values provides 2dB less
power usage, which is great gain in terms of green communication.



Image Enhancement and Noise Reduction Using Modified Delay-Multiply-and-Sum Beamformer: Application to Medical Photoacoustic Imaging

Photoacoustic imaging (PAI) is an emerging biomedical imaging modality
capable of providing both high contrast and high resolution of optical and
UltraSound (US) imaging. When a short duration laser pulse illuminates the
tissue as a target of imaging, tissue induces US waves and detected waves can
be used to reconstruct optical absorption distribution. Since receiving part of
PA consists of US waves, a large number of beamforming algorithms in US imaging
can be applied on PA imaging. Delay-and-Sum (DAS) is the most common
beamforming algorithm in US imaging. However, make use of DAS beamformer leads
to low resolution images and large scale of off-axis signals contribution. To
address these problems a new paradigm namely Delay-Multiply-and-Sum (DMAS),
which was used as a reconstruction algorithm in confocal microwave imaging for
breast cancer detection, was introduced for US imaging. Consequently, DMAS was
used in PA imaging systems and it was shown this algorithm results in
resolution enhancement and sidelobe degrading. However, in presence of high
level of noise the reconstructed image still suffers from high contribution of
noise. In this paper, a modified version of DMAS beamforming algorithm is
proposed based on DAS inside DMAS formula expansion. The quantitative and
qualitative results show that proposed method results in more noise reduction
and resolution enhancement in expense of contrast degrading. For the
simulation, two-point target, along with lateral variation in two depths of
imaging are employed and it is evaluated under high level of noise in imaging
medium. Proposed algorithm in compare to DMAS, results in reduction of lateral
valley for about 19 dB followed by more distinguished two-point target.
Moreover, levels of sidelobe are reduced for about 25 dB.



A Digital Predistortion Scheme Exploiting Degrees-of-Freedom for Massive MIMO Systems

The primary source of nonlinear distortion in wireless transmitters is the
power amplifier (PA). Conventional digital predistortion (DPD) schemes use
high-order polynomials to accurately approximate and compensate for the
nonlinearity of the PA. This is not practical for scaling to tens or hundreds
of PAs in massive multiple-input multiple-output (MIMO) systems. There is more
than one candidate precoding matrix in a massive MIMO system because of the
excess degrees-of-freedom (DoFs), and each precoding matrix requires a
different DPD polynomial order to compensate for the PA nonlinearity. This
paper proposes a low-order DPD method achieved by exploiting massive DoFs of
next-generation front ends. We propose a novel indirect learning structure
which adapts the channel and PA distortion iteratively by cascading adaptive
zero forcing precoding and DPD. Our solution uses a 3rd order polynomial to
achieve the same performance as the conventional DPD using an 11th order
polynomial for a 100x10 massive MIMO configuration. Experimental results show a
70% reduction in computational complexity, enabling ultra-low latency
communications.



Double Stage Delay Multiply and Sum Beamforming Algorithm Applied to Ultrasound Medical Imaging

In Ultrasound (US) imaging, Delay and Sum (DAS) is the most common
beamformer, but it leads to low quality images. Delay Multiply and Sum (DMAS)
was introduced to address this problem. However, the reconstructed images using
DMAS still suffer from level of sidelobes and low noise suppression. In this
paper, a novel beamforming algorithm is introduced based on the expansion of
DMAS formula. It is shown that there is a DAS algebra inside the expansion, and
it is proposed to use DMAS instead of the DAS algebra. The introduced method,
namely Double Stage DMAS (DS-DMAS), is evaluated numerically and
experimentally. The quantitative results indicate that DS-DMAS results in about
25% lower level of sidelobes compared to DMAS. Moreover, the introduced method
leads to 23%, 22% and 43% improvement in Signal-to-Noise Ratio,
Full-Width-Half-Maximum and Contrast Ratio, respectively, in comparison with
DMAS beamformer.



A methodology for calculating the latency of GPS-probe data

Crowdsourced GPS probe data has been gaining popularity in recent years as a
source for real-time traffic information. Efforts have been made to evaluate
the quality of such data from different perspectives. A quality indicator of
any traffic data source is latency that describes the punctuality of data,
which is critical for real-time operations, emergency response, and traveler
information systems. This paper offers a methodology for measuring the probe
data latency, with respect to a selected reference source. Although Bluetooth
re-identification data is used as the reference source, the methodology can be
applied to any other ground-truth data source of choice (i.e. Automatic License
Plate Readers, Electronic Toll Tag). The core of the methodology is a maximum
pattern matching algorithm that works with three different fitness objectives.
To test the methodology, sample field reference data were collected on multiple
freeways segments for a two-week period using portable Bluetooth sensors as
ground-truth. Equivalent GPS probe data was obtained from a private vendor, and
its latency was evaluated. Latency at different times of the day, the impact of
road segmentation scheme on latency, and sensitivity of the latency to both
speed slowdown, and recovery from slowdown episodes are also discussed.



Deep Chain HDRI: Reconstructing a High Dynamic Range Image from a Single Low Dynamic Range Image

In this paper, we propose a novel deep neural network model that reconstructs
a high dynamic range (HDR) image from a single low dynamic range (LDR) image.
The proposed model is based on a convolutional neural network composed of
dilated convolutional layers, and infers LDR images with various exposures and
illumination from a single LDR image of the same scene. Then, the final HDR
image can be formed by merging these inference results. It is relatively easy
for the proposed method to find the mapping between the LDR and an HDR with a
different bit depth because of the chaining structure inferring the
relationship between the LDR images with brighter (or darker) exposures from a
given LDR image. The method not only extends the range, but also has the
advantage of restoring the light information of the actual physical world. For
the HDR images obtained by the proposed method, the HDR-VDP2 Q score, which is
the most popular evaluation metric for HDR images, was 56.36 for a display with
a 1920$\times$1200 resolution, which is an improvement of 6 compared with the
scores of conventional algorithms. In addition, when comparing the peak
signal-to-noise ratio values for tone mapped HDR images generated by the
proposed and conventional algorithms, the average value obtained by the
proposed algorithm is 30.86 dB, which is 10 dB higher than those obtained by
the conventional algorithms.



Medical Photoacoustic Beamforming Using Minimum Variance-Based Delay Multiply and Sum

Delay-and-Sum (DAS) beamformer is the most common beamforming algorithm in
Photoacoustic imaging (PAI) due to its simple implementation and real time
imaging. However, it provides poor resolution and high levels of sidelobe. A
new algorithm named Delay-Multiply-and-Sum (DMAS) was introduced. Using DMAS
leads to lower levels of sidelobe compared to DAS, but resolution is not
satisfying yet. In this paper, a novel beamformer is introduced based on the
combination of Minimum Variance (MV) adaptive beamforming and DMAS, so-called
Minimum Variance-Based DMAS (MVB-DMAS). It is shown that expanding the DMAS
equation leads to some terms which contain a DAS equation. It is proposed to
use MV adaptive beamformer instead of existing DAS inside the DMAS algebra
expansion. MVB-DMAS is evaluated numerically compared to DAS, DMAS and MV and
Signal-to-noise ratio (SNR) metric is presented. It is shown that MVB-DMAS
leads to higher image quality and SNR for about 13 dB, 3 dB and 2 dB in
comparison with DAS, DMAS and MV, respectively.



Region of Interest (ROI) Coding for Aerial Surveillance Video using AVC & HEVC

Aerial surveillance from Unmanned Aerial Vehicles (UAVs), i.e. with moving
cameras, is of growing interest for police as well as disaster area monitoring.
For more detailed ground images the camera resolutions are steadily increasing.
Simultaneously the amount of video data to transmit is increasing
significantly, too. To reduce the amount of data, Region of Interest (ROI)
coding systems were introduced which mainly encode some regions in higher
quality at the cost of the remaining image regions. We employ an existing ROI
coding system relying on global motion compensation to retain full image
resolution over the entire image. Different ROI detectors are used to
automatically classify a video image on board of the UAV in ROI and non-ROI. We
propose to replace the modified Advanced Video Coding (AVC) video encoder by a
modified High Efficiency Video Coding (HEVC) encoder. Without any change of the
detection system itself, but by replacing the video coding back-end we are able
to improve the coding efficiency by 32% on average although regular HEVC
provides coding gains of 12-30% only for the same test sequences and similar
PSNR compared to regular AVC coding. Since the employed ROI coding mainly
relies on intra mode coding of new emerging image areas, gains of HEVC-ROI
coding over AVC-ROI coding compared to regular coding of the entire frames
including predictive modes (inter) depend on sequence characteristics. We
present a detailed analysis of bit distribution within the frames to explain
the gains. In total we can provide coding data rates of 0.7-1.0 Mbit/s for full
HDTV video sequences at 30 fps at reasonable quality of more than 37 dB.



DSP-Enhanced OTDR for Detection and Estimation of Events in PONs

To plan a rapid response and minimize operational costs, passive optical
network operators require to automatically detect and identify faults that may
occur in the optical distribution network. In this work, we present
DSP-Enhanced OTDR, a novel methodology for remote fault analysis based on
conventional optical time-domain reflectometry complemented with reference
traces. Together with the mathematical formalism, we derive the detection tests
that result to be uniformly most powerful, which are optimal according to the
Neyman-Pearson criterion. To identify the type of fault and fully characterize
it, the detection stage is followed by the estimation of its characteristic
parameters, such as return loss and insertion loss. We experimentally
demonstrate that this approach allows to detect faults inside the event dead
zone, which overcomes the shortage of conventional event-marking algorithms.



Epoch-Synchronous Overlap-Add (ESOLA) for Time- and Pitch-Scale Modification of Speech Signals

Time- and pitch-scale modifications of speech signals find important
applications in speech synthesis, playback systems, voice conversion,
learning/hearing aids, etc.. There is a requirement for computationally
efficient and real-time implementable algorithms. In this paper, we propose a
high quality and computationally efficient time- and pitch-scaling methodology
based on the glottal closure instants (GCIs) or epochs in speech signals. The
proposed algorithm, termed as epoch-synchronous overlap-add time/pitch-scaling
(ESOLA-TS/PS), segments speech signals into overlapping short-time frames and
then the adjacent frames are aligned with respect to the epochs and the frames
are overlap-added to synthesize time-scale modified speech. Pitch scaling is
achieved by resampling the time-scaled speech by a desired sampling factor. We
also propose a concept of epoch embedding into speech signals, which
facilitates the identification and time-stamping of samples corresponding to
epochs and using them for time/pitch-scaling to multiple scaling factors
whenever desired, thereby contributing to faster and efficient implementation.
The results of perceptual evaluation tests reported in this paper indicate the
superiority of ESOLA over state-of-the-art techniques. ESOLA significantly
outperforms the conventional pitch synchronous overlap-add (PSOLA) techniques
in terms of perceptual quality and intelligibility of the modified speech.
Unlike the waveform similarity overlap-add (WSOLA) or synchronous overlap-add
(SOLA) techniques, the ESOLA technique has the capability to do exact
time-scaling of speech with high quality to any desired modification factor
within a range of 0.5 to 2. Compared to synchronous overlap-add with fixed
synthesis (SOLAFS), the ESOLA is computationally advantageous and at least
three times faster.



Noncoherent compressive channel estimation for mm-wave massive MIMO

Millimeter (mm) wave massive MIMO has the potential for delivering orders of
magnitude increases in mobile data rates, with compact antenna arrays providing
narrow steerable beams for unprecedented levels of spatial reuse. A fundamental
technical bottleneck, however, is rapid spatial channel estimation and beam
adaptation in the face of mobility and blockage. Recently proposed compressive
techniques which exploit the sparsity of mm wave channels are a promising
approach to this problem, with overhead scaling linearly with the number of
dominant paths and logarithmically with the number of array elements. Further,
they can be implemented with RF beamforming with low-precision phase control.
However, these methods make implicit assumptions on long-term phase coherence
that are not satisfied by existing hardware. In this paper, we propose and
evaluate a noncoherent compressive channel estimation technique which can
estimate a sparse spatial channel based on received signal strength (RSS)
alone, and is compatible with off-the-shelf hardware. The approach is based on
cascading phase retrieval (i.e., recovery of complex-valued measurements from
RSS measurements, up to a scalar multiple) with coherent compressive
estimation. While a conventional cascade scheme would multiply two measurement
matrices to obtain an overall matrix whose entries are in a continuum, a key
novelty in our scheme is that we constrain the overall measurement matrix to be
implementable using coarsely quantized pseudorandom phases, employing a virtual
decomposition of the matrix into a product of measurement matrices for phase
retrieval and compressive estimation. Theoretical and simulation results show
that our noncoherent method scales almost as well with array size as its
coherent counterpart, thus inheriting the scalability and low overhead of the
latter.



Gender-dependent emotion recognition based on HMMs and SPHMMs

It is well known that emotion recognition performance is not ideal. The work
of this research is devoted to improving emotion recognition performance by
employing a two-stage recognizer that combines and integrates gender recognizer
and emotion recognizer into one system. Hidden Markov Models (HMMs) and
Suprasegmental Hidden Markov Models (SPHMMs) have been used as classifiers in
the two-stage recognizer. This recognizer has been tested on two distinct and
separate emotional speech databases. The first database is our collected
database and the second one is the Emotional Prosody Speech and Transcripts
database. Six basic emotions including the neutral state have been used in each
database. Our results show that emotion recognition performance based on the
two-stage approach (gender-dependent emotion recognizer) has been significantly
improved compared to that based on emotion recognizer without gender
information and emotion recognizer with correct gender information by an
average of 11% and 5%, respectively. This work shows that the highest emotion
identification performance takes place when the classifiers are completely
biased towards suprasegmental models and no impact of acoustic models. The
results achieved based on the two-stage framework fall within 2.28% of those
obtained in subjective assessment by human judges.



DeepISP: Towards Learning an End-to-End Image Processing Pipeline

We present DeepISP, a full end-to-end deep neural model of the camera image
signal processing (ISP) pipeline. Our model learns a mapping from the raw
low-light mosaiced image to the final visually compelling image and encompasses
low-level tasks such as demosaicing and denoising as well as higher-level tasks
such as color correction and image adjustment. The training and evaluation of
the pipeline were performed on a dedicated dataset containing pairs of
low-light and well-lit images captured by a Samsung S7 smartphone camera in
both raw and processed JPEG formats. The proposed solution achieves
state-of-the-art performance in objective evaluation of PSNR on the subtask of
joint denoising and demosaicing. For the full end-to-end pipeline, it achieves
better visual quality compared to the manufacturer ISP, in both a subjective
human assessment and when rated by a deep model trained for assessing image
quality.



Time series kernel similarities for predicting Paroxysmal Atrial Fibrillation from ECGs

We tackle the problem of classifying Electrocardiography (ECG) signals with
the aim of predicting the onset of Paroxysmal Atrial Fibrillation (PAF). Atrial
fibrillation is the most common type of arrhythmia, but in many cases PAF
episodes are asymptomatic. Therefore, in order to help diagnosing PAF, it is
important to design procedures for detecting and, more importantly, predicting
PAF episodes. We propose a method for predicting PAF events whose first step
consists of a feature extraction procedure that represents each ECG as a
multi-variate time series. Successively, we design a classification framework
based on kernel similarities for multi-variate time series, capable of handling
missing data. We consider different approaches to perform classification in the
original space of the multi-variate time series and in an embedding space,
defined by the kernel similarity measure. We achieve a classification accuracy
comparable with state of the art methods, with the additional advantage of
detecting the PAF onset up to 15 minutes in advance.



Ranking Causal Influence of Financial Markets via Directed Information Graphs

A non-parametric method for ranking stock indices according to their mutual
causal influences is presented. Under the assumption that indices reflect the
underlying economy of a country, such a ranking indicates which countries exert
the most economic influence in an examined subset of the global economy. The
proposed method represents the indices as nodes in a directed graph, where the
edges' weights are estimates of the pair-wise causal influences, quantified
using the directed information functional. This method facilitates using a
relatively small number of samples from each index. The indices are then ranked
according to their net-flow in the estimated graph (sum of the incoming weights
subtracted from the sum of outgoing weights). Daily and minute-by-minute data
from nine indices (three from Asia, three from Europe and three from the US)
were analyzed. The analysis of daily data indicates that the US indices are the
most influential, which is consistent with intuition that the indices
representing larger economies usually exert more influence. Yet, it is also
shown that an index representing a small economy can strongly influence an
index representing a large economy if the smaller economy is indicative of a
larger phenomenon. Finally, it is shown that while inter-region interactions
can be captured using daily data, intra-region interactions require more
frequent samples.



A Novel Contourlet Domain Watermark Detector for Copyright Protection

Digital media can be distributed via Internet easily, so, media owners are
eagerly seeking methods to protect their rights. A typical solution is digital
watermarking for copyright protection. In this paper, we propose a novel
contourlet domain image watermarking scheme for copyright protection. In the
embedding phase, we insert the watermark into the image using an additive
contourlet domain spread spectrum approach. In the detection phase, we design a
detector using likelihood ratio test (LRT). Since the performance of the LRT
detector is completely dependent on the accuracy of the employed statistical
model, we first study the statistical properties of the contourlet
coefficients. This study demonstrates the heteroscedasticity and heavy-tailed
marginal distribution of these coefficients. Therefore, we propose using two
dimensional generalized autoregressive conditional heteroscedasticity
(2D-GARCH) model that is compatible with the contourlet coefficients. Motivated
by the modeling results, we design a new watermark detector based on 2D-GARCH
model. Also, we analyze its performance by computing the receiver operating
characteristics. Experimental results confirm the high efficiency of the
proposed detector. Since a watermark detector for copyright protection should
be robust against attacks, we examine the robustness of the proposed detector
under different kinds of attacks.



Wireless Network Coding in Network MIMO: A New Design for 5G and Beyond

Physical layer network coding (PNC) has been studied to serve wireless
network MIMO systems with much lower backhaul load than approaches such as
Cloud Radio Access Network (Cloud-RAN) and coordinated multipoint (CoMP). In
this paper, we present a design guideline of engineering applicable PNC to
fulfil the request of high user densities in 5G wireless RAN infrastructure.
Unlike compute-and-forward and PNC design criteria for two-way relay channels,
the proposed guideline is designed for uplink of network MIMO (N-MIMO) systems.
We show that the proposed design criteria guarantee that 1) the whole system
operates over binary system; 2) the PNC functions utilised at each access point
overcome all singular fade states; 3) the destination can unambiguously recover
all source messages while the overall backhaul load remains at the lowest
level. We then develop a two-stage search algorithm to identify the optimum PNC
mapping functions which greatly reduces the real-time computational complexity.
The impact of estimated channel information and reduced number of singular fade
states in different QAM modulation schemes is studied in this paper. In
addition, a sub-optimal search method based on lookup table mechanism to
achieve further reduced computational complexity with limited performance loss
is presented. Numerical results show that the proposed schemes achieve low
outage probability with reduced backhaul load.



Optimized Cell Planning for Network Slicing in Heterogeneous Wireless Communication Networks

We propose a cell planning scheme to maximize the resource efficiency of a
wireless communication network while considering quality-of-service
requirements imposed by different mobile services. In dense and heterogeneous
cellular 5G networks, the available time-frequency resources are orthogonally
partitioned among different slices, which are serviced by the cells. The
proposed scheme achieves a joint optimization of the resource distribution
between network slices, the allocation of cells to operate on different slices,
and the allocation of users to cells. Since the original problem formulation is
computationally intractable, we propose a convex inner approximation.
Simulations show that the proposed approach optimizes the resource efficiency
and enables a service-centric network design paradigm.



Distributed Adaptive Learning with Multiple Kernels in Diffusion Networks

We propose an adaptive scheme for distributed learning of nonlinear functions
by a network of nodes. The proposed algorithm consists of a local adaptation
stage utilizing multiple kernels with projections onto hyperslabs and a
diffusion stage to achieve consensus on the estimates over the whole network.
Multiple kernels are incorporated to enhance the approximation of functions
with several high and low frequency components common in practical scenarios.
We provide a thorough convergence analysis of the proposed scheme based on the
metric of the Cartesian product of multiple reproducing kernel Hilbert spaces.
To this end, we introduce a modified consensus matrix considering this specific
metric and prove its equivalence to the ordinary consensus matrix. Besides, the
use of hyperslabs enables a significant reduction of the computational demand
with only a minor loss in the performance. Numerical evaluations with synthetic
and real data are conducted showing the efficacy of the proposed algorithm
compared to the state of the art schemes.



RF Lens-Embedded Antenna Array for mmWave MIMO: Design and Performance

The requirement of high data-rate in the fifth generation wireless systems
(5G) calls for the ultimate utilization of the wide bandwidth in the mmWave
frequency band. Researchers seeking to compensate for mmWave's high path loss
and to achieve both gain and directivity have proposed that mmWave
multiple-input multiple-output (MIMO) systems make use of beamforming systems.
Hybrid beamforming in mmWave demonstrates promising performance in achieving
high gain and directivity by using phase shifters at the analog processing
block. What remains a problem, however, is the actual implementation of mmWave
beamforming systems; to fabricate such a system is costly and complex. With the
aim of reducing such cost and complexity, this article presents actual
prototypes of the lens antenna as an effective device to be used in the future
5G mmWave hybrid beamforming systems. Using a lens as a passive phase shifter
enables beamforming without the heavy network of active phase shifters, while
gain and directivity are achieved by the energy-focusing property of the lens.
Proposed in this article are two types of lens antennas, one for static and the
other for mobile usage. Their performance is evaluated using measurements and
simulation data along with link-level analysis via a software defined radio
(SDR) platform. Results show the promising potential of the lens antenna for
its high gain and directivity, and its improved beam-switching feasibility
compared to when a lens is not used. System-level evaluations reveal the
significant throughput enhancement in both real indoor and outdoor
environments. Moreover, the lens antenna's design issues are also discussed by
evaluating different lens sizes.



Three Dimensional Fluorescence Microscopy Image Synthesis and Segmentation

Advances in fluorescence microscopy enable acquisition of 3D image volumes
with better image quality and deeper penetration into tissue. Segmentation is a
required step to characterize and analyze biological structures in the images
and recent 3D segmentation using deep learning has achieved promising results.
One issue is that deep learning techniques require a large set of groundtruth
data which is impractical to annotate manually for large 3D microscopy volumes.
This paper describes a 3D deep learning nuclei segmentation method using
synthetic 3D volumes for training. A set of synthetic volumes and the
corresponding groundtruth are generated using spatially constrained
cycle-consistent adversarial networks. Segmentation results demonstrate that
our proposed method is capable of segmenting nuclei successfully for various
data sets.



A Review of Micromachined Thermal Accelerometers

Thermal convection based micro-electromechanical accelerometer is a
relatively new kind of acceleration sensor that does not require a solid proof
mass, yielding unique benefits like high shock survival rating, low production
cost, and integrability with CMOS integrated circuit technology. This article
provides a comprehensive survey of the research, development, and current
trends in the field of thermal acceleration sensors, with detailed enumeration
on the theory, operation, modeling, and numerical simulation of such devices.
Different reported varieties and structures of thermal accelerometers have been
reviewed highlighting key design, implementation, and performance aspects.
Materials and technologies used for fabrication of such sensors have also been
discussed. Further, the advantages and challenges for thermal accelerometers
vis-\`a-vis other prominent accelerometer types have been presented, followed
by an overview of associated signal conditioning circuitry and potential
applications.



High-throughput, high-resolution registration-free generated adversarial network microscopy

We combine generative adversarial network (GAN) with light microscopy to
achieve deep learning super-resolution under a large field of view (FOV). By
appropriately adopting prior microscopy data in an adversarial training, the
neural network can recover a high-resolution, accurate image of new specimen
from its single low-resolution measurement. Its capacity has been broadly
demonstrated via imaging various types of samples, such as USAF resolution
target, human pathological slides, fluorescence-labelled fibroblast cells, and
deep tissues in transgenic mouse brain, by both wide-field and light-sheet
microscopes. The gigapixel, multi-color reconstruction of these samples
verifies a successful GAN-based single image super-resolution procedure. We
also propose an image degrading model to generate low resolution images for
training, making our approach free from the complex image registration during
training dataset preparation. After a welltrained network being created, this
deep learning-based imaging approach is capable of recovering a large FOV (~95
mm2), high-resolution (~1.7 {\mu}m) image at high speed (within 1 second),
while not necessarily introducing any changes to the setup of existing
microscopes.



A 3D Non-Stationary Wideband Geometry-Based Channel Model for MIMO Vehicle-to-Vehicle Communication System

In this paper, we present a three-dimensional (3D) non-wide-sense stationary
(non-WSS) wideband geometry-based channel model for vehicle-to-vehicle (V2V)
communication environments. We introduce a two-cylinder model to describe
moving vehicles as well as multiple confocal semi-ellipsoid models to depict
stationary roadside scenarios. The received signal is constructed as a sum of
the line-of-sight (LoS), single-, and double-bounced rays with different
energies. Accordingly, the proposed channel model is sufficient for depicting a
wide variety of V2V environments, such as macro-, micro-, and picocells. The
relative movement between the mobile transmitter (MT) and mobile receiver (MR)
results in time-variant geometric statistics that make our channel model
non-stationary. Using this channel model, the proposed channel statistics,
i.e., the time-variant space correlation functions (CFs), frequency CFs, and
corresponding Doppler power spectral density (PSD), were studied for different
relative moving time instants. The numerical results demonstrate that the
proposed 3D non-WSS wideband channel model is practical for characterizing real
V2V channels.



Code-Frequency Block Group Coding for Anti-Spoofing Pilot Authentication in Multi-Antenna OFDM Systems

A pilot spoofer can paralyze the channel estimation in multi-user orthogonal
frequency-division multiplexing (OFD- M) systems by using the same
publicly-known pilot tones as legitimate nodes. This causes the problem of
pilot authentication (PA). To solve this, we propose, for a two-user
multi-antenna OFDM system, a code-frequency block group (CFBG) coding based PA
mechanism. Here multi-user pilot information, after being randomized
independently to avoid being spoofed, are converted into activation patterns of
subcarrier-block groups on code-frequency domain. Those patterns, though
overlapped and interfered mutually in the wireless transmission environment,
are qualified to be separated and identified as the original pilots with high
accuracy, by exploiting CFBG coding theory and channel characteristic.
Particularly, we develop the CFBG code through two steps, i.e., 1) devising an
ordered signal detection technique to recognize the number of signals
coexisting on each subcarrier block, and encoding each subcarrier block with
the detected number; 2) constructing a zero-false-drop (ZFD) code and block
detection based (BD) code via k-dimensional Latin hypercubes and integrating
those two codes into the CFBG code. This code can bring a desirable pilot
separation error probability (SEP), inversely proportional to the number of
occupied subcarriers and antennas with a power of k. To apply the code to PA, a
scheme of pilot conveying, separation and identification is proposed. Based on
this novel PA, a joint channel estimation and identification mechanism is
proposed to achieve high-precision channel recovery and simultaneously enhance
PA without occupying extra resources. Simulation results verify the
effectiveness of our proposed mechanism.



A Survey of Channel Modeling for UAV Communications

Unmanned aerial vehicles (UAVs) have gained great interest for rapid
deployment in both civil and military applications. UAV communication has its
own distinctive channel characteristics compared with widely used cellular and
satellite systems. Thus, accurate channel characterization is crucial for the
performance optimization and design of efficient UAV communication systems.
However, several challenges exist in UAV channel modeling. For example,
propagation characteristics of UAV channels are still less explored for spatial
and temporal variations in non\textendash stationary channels. Also, airframe
shadowing has not yet been investigated for small size rotary UAVs. This paper
provides an extensive survey on the measurement campaigns launched for UAV
channel modeling using low altitude platforms and discusses various channel
characterization efforts. We also review the contemporary perspective of UAV
channel modeling approaches and outline some future research challenges in this
domain.



Double-Stage Delay Multiply and Sum Beamforming Algorithm: Application to Linear-Array Photoacoustic Imaging

Photoacoustic imaging (PAI) is an emerging medical imaging modality capable
of providing high spatial resolution of Ultrasound (US) imaging and high
contrast of optical imaging. Delay-and-Sum (DAS) is the most common beamforming
algorithm in PAI. However, using DAS beamformer leads to low resolution images
and considerable contribution of off-axis signals. A new paradigm namely
Delay-Multiply-and-Sum (DMAS), which was originally used as a reconstruction
algorithm in confocal microwave imaging, was introduced to overcome the
challenges in DAS. DMAS was used in PAI systems and it was shown that this
algorithm results in resolution improvement and sidelobe degrading. However,
DMAS is still sensitive to high levels of noise, and resolution improvement is
not satisfying. Here, we propose a novel algorithm based on DAS algebra inside
DMAS formula expansion, Double Stage DMAS (DS-DMAS), which improves the image
resolution and levels of sidelobe, and is much less sensitive to high level of
noise compared to DMAS. The performance of DS-DMAS algorithm is evaluated
numerically and experimentally. The resulted images are evaluated qualitatively
and quantitatively using established quality metrics including signal-to-noise
ratio (SNR), full-width-half-maximum (FWHM) and contrast ratio (CR). It is
shown that DS-DMAS outperforms DAS and DMAS at the expense of higher
computational load. DS-DMAS reduces the lateral valley for about 15 dB and
improves the SNR and FWHM better than 13% and 30%, respectively. Moreover, the
levels of sidelobe are reduced for about 10 dB in comparison with those in
DMAS.



Novel digital tissue phenotypic signatures of distant metastasis in colorectal cancer

Distant metastasis is the major cause of death in colorectal cancer (CRC).
Patients at high risk of developing distant metastasis could benefit from
appropriate adjuvant and follow-up treatments if stratified accurately at an
early stage of the disease. Studies have increasingly recognized the role of
diverse cellular components within the tumor microenvironment in the
development and progression of CRC tumors. In this paper, we show that a new
method of automated analysis of digitized images from colorectal cancer tissue
slides can provide important estimates of distant metastasis-free survival
(DMFS, the time before metastasis is first observed) on the basis of details of
the microenvironment. Specifically, we determine what cell types are found in
the vicinity of other cell types, and in what numbers, rather than
concentrating exclusively on the cancerous cells. We then extract novel tissue
phenotypic signatures using statistical measurements about tissue composition.
Such signatures can underpin clinical decisions about the advisability of
various types of adjuvant therapy.



Object-based Multipass InSAR via Robust Low Rank Tensor Decomposition

The most unique advantage of multipass SAR interferometry (InSAR) is the
retrieval of long term geophysical parameters, e.g. linear deformation rates,
over large areas. Recently, an object-based multipass InSAR framework has been
proposed in [1], as an alternative to the typical single-pixel methods, e.g.
Persistent Scatterer Interferometry (PSI), or pixel-cluster-based methods, e.g.
SqueeSAR. This enables the exploitation of inherent properties of InSAR phase
stacks on an object level. As a followon, this paper investigates the inherent
low rank property of such phase tensors, and proposes a Robust Multipass InSAR
technique via Object-based low rank tensor decomposition (RoMIO). We
demonstrate that the filtered InSAR phase stacks can improve the accuracy of
geophysical parameters estimated via conventional multipass InSAR techniques,
e.g. PSI, by a factor of ten to thirty in typical settings. The proposed method
is particularly effective against outliers, such as pixels with unmodeled
phases. These merits in turn can effectively reduce the number of images
required for a reliable estimation. The promising performance of the proposed
method is demonstrated using high-resolution TerraSAR-X image stacks.



The SARptical Dataset for Joint Analysis of SAR and Optical Image in Dense Urban Area

The joint interpretation of very high resolution SAR and optical images in
dense urban area are not trivial due to the distinct imaging geometry of the
two types of images. Especially, the inevitable layover caused by the
side-looking SAR imaging geometry renders this task even more challenging. Only
until recently, the "SARptical" framework [1], [2] proposed a promising
solution to tackle this. SARptical can trace individual SAR scatterers in
corresponding high-resolution optical images, via rigorous 3-D reconstruction
and matching. This paper introduces the SARptical dataset, which is a dataset
of over 10,000 pairs of corresponding SAR, and optical image patches extracted
from TerraSAR-X high-resolution spotlight images and aerial UltraCAM optical
images. This dataset opens new opportunities of multisensory data analysis. One
can analyze the geometry, material, and other properties of the imaged object
in both SAR and optical image domain. More advanced applications such as SAR
and optical image matching via deep learning [3] is now also possible.



Automatic Detection and Positioning of Ground Control Points Using TerraSAR-X Multi-Aspect Acquisitions

Geodetic stereo Synthetic Aperture Radar (SAR) is capable of absolute
three-dimensional localization of natural Persistent Scatterer (PS)s which
allows for Ground Control Point (GCP) generation using only SAR data. The
prerequisite for the method to achieve high precision results is the correct
detection of common scatterers in SAR images acquired from different viewing
geometries. In this contribution, we describe three strategies for automatic
detection of identical targets in SAR images of urban areas taken from
different orbit tracks. Moreover, a complete work-flow for automatic generation
of large number of GCPs using SAR data is presented and its applicability is
shown by exploiting TerraSAR-X (TS-X) high resolution spotlight images over the
city of Oulu, Finland and a test site in Berlin, Germany.



Rate-Interference Tradeoff in OFDM-based Cognitive Radio Networks

In cognitive radio (CR) networks, secondary users (SUs) are allowed to
opportunistically access the primary users (PUs) spectrum to improve the
spectrum utilization; however, this increases the interference levels at the
PUs. In this paper, we consider an orthogonal frequency division multiplexing
OFDM-based CR network and investigate the tradeoff between increasing the SU
transmission rate (hence improving the spectrum utilization) and reducing the
interference levels at the PUs. We formulate a new multiobjective optimization
(MOOP) problem that jointly maximizes the SU transmission rate and minimizes
its transmit power, while imposing interference thresholds to the PUs. Further,
we propose an algorithm to strike a balance between the SU transmission rate
and the interference levels to the PUs. The proposed algorithm considers the
practical scenario of knowing partial channel state information (CSI) of the
links between the SU transmitter and the PUs receivers. Simulation results
illustrate the performance of the proposed algorithm and its superiority when
compared to the work in the literature.



A Novel Algorithm for Rate/Power Allocation in OFDM-based Cognitive Radio Systems with Statistical Interference Constraints

In this paper, we adopt a multiobjective optimization approach to jointly
optimize the rate and power in OFDM-based cognitive radio (CR) systems. We
propose a novel algorithm that jointly maximizes the OFDM-based CR system
throughput and minimizes its transmit power, while guaranteeing a target bit
error rate per subcarrier and a total transmit power threshold for the
secondary user (SU), and restricting both co-channel and adjacent channel
interferences to existing primary users (PUs) in a statistical manner. Since
the interference constraints are met statistically, the SU transmitter does not
require perfect channel-state-information (CSI) feedback from the PUs
receivers. Closed-form expressions are derived for bit and power allocations
per subcarrier. Simulation results illustrate the performance of the proposed
algorithm and compare it to the case of perfect CSI. Further, the results show
that the performance of the proposed algorithm approaches that of an exhaustive
search for the discrete global optimal allocations with significantly reduced
computational complexity.



A Novel Algorithm for Joint Bit and Power Loading for OFDM Systems with Unknown Interference

In this paper, a novel low complexity bit and power loading algorithm is
formulated for orthogonal frequency division multiplexing (OFDM) systems
operating in fading environments and in the presence of unknown interference.
The proposed non-iterative algorithm jointly maximizes the throughput and
minimizes the transmitted power, while guaranteeing a target bit error rate
(BER) per subcarrier. Closed-form expressions are derived for the optimal bit
and power distributions per subcarrier. The performance of the proposed
algorithm is investigated through extensive simulations. A performance
comparison with the algorithm in [1] shows the superiority of the proposed
algorithm with reduced computational effort.



Optimal Bit and Power Loading for OFDM Systems with Average BER and Total Power Constraints

In this paper, a novel joint bit and power loading algorithm is proposed for
orthogonal frequency division multiplexing (OFDM) systems operating in fading
environments. The algorithm jointly maximizes the throughput and minimizes the
transmitted power, while guaranteeing a target average bit error rate (BER) and
meeting a constraint on the total transmit power. Simulation results are
described that illustrate the performance of the proposed scheme and
demonstrate its superiority when compared to the algorithm in [1].



Constrained Joint Bit and Power Allocation for Multicarrier Systems

This paper proposes a novel low complexity joint bit and power suboptimal
allocation algorithm for multicarrier systems operating in fading environments.
The algorithm jointly maximizes the throughput and minimizes the transmitted
power, while guaranteeing a target bit error rate (BER) per subcarrier and
meeting a constraint on the total transmit power. Simulation results are
described that illustrate the performance of the proposed scheme and
demonstrate its superiority when compared to the algorithm in [4] with similar
or reduced computational complexity. Furthermore, the results show that the
performance of the proposed suboptimal algorithm approaches that of an optimal
exhaustive search with significantly lower computational complexity.



Joint Optimization of Bit and Power Loading for Multicarrier Systems

In this letter, a novel low complexity bit and power loading algorithm is
formulated for multicarrier communication systems. The proposed algorithm
jointly maximizes the throughput and minimizes the transmit power through a
weighting coefficient $\alpha$, while meeting constraints on the target bit
error rate (BER) per subcarrier and on the total transmit power. The
optimization problem is solved by the Lagrangian multiplier method if the
initial $\alpha$ causes the transmit power not to violate the power constraint;
otherwise, a bisection search is used to find the appropriate $\alpha$.
Closed-form expressions are derived for the close-to-optimal bit and power
allocations per subcarrier, average throughput, and average transmit power.
Simulation results illustrate the performance of the proposed algorithm and
demonstrate its superiority with respect to existing allocation algorithms.
Furthermore, the results show that the performance of the proposed algorithm
approaches that of the exhaustive search for the discrete optimal allocations.



A Hardware-Efficient Synchronization in L-DACS1 for Aeronautical Communications

L-band digital aeronautical communication system type-1 (L-DACS1) is an
emerging standard that aims at enhancing air traffic management by
transitioning the traditional analog aeronautical communication systems to the
superior and highly efficient digital domain. L-DACS1 employs modern and
efficient orthogonal frequency-division multiplexing (OFDM) modulation
technique to achieve more efficient and higher data rate in comparison to the
existing aeronautical communication systems. However, the performance of OFDM
systems is very sensitive to synchronization errors such as symbol timing
offset (STO) and carrier frequency offset (CFO). STO and CFO estimations are
extremely important for maintaining orthogonality among the subcarriers for the
retrieval of information. This paper proposes a novel efficient hardware
synchronizer for L-DACS1 systems that offers robust performance at low power
and low hardware resource usage. Monte Carlo simulations show that the proposed
synchronization algorithm provides accurate STO estimation as well as
fractional CFO estimation. Implementation of the proposed synchronizer on a
widely used field-programmable gate array (FPGA) (Xilinx xc7z020clg484-1)
results in a very low hardware usage which consumed 6.5%, 3.7%, and 6.4% of the
total number of lookup tables, flip-flops, and digital signal processing
blocks, respectively. The dynamic power of the proposed synchronizer is below 1
mW.



Zero-Delay Gaussian Joint Source-Channel Coding for the Interference Channel

This paper studies zero-delay joint source channel coding (JSCC) for
transmission of correlated Gaussian sources over a Gaussian interference
channel (GIC). We propose to adopt delay-free hybrid digital and analog (HDA)
scheme, which is, transmitting the superposition of scaled source and its
quantized version after applying scalar quantization to the source at each
transmitter. At the corresponding receiver, two kinds of estimators are
presented. It is shown that both the schemes, when optimized, beat the uncoded
transmission if the channel signal-to-noise ratio (CSNR) is higher than a
threshold value for different correlation coefficients and interference values.



Snapshot light-field laryngoscope

The convergence of recent advances in optical fabrication and digital
processing yields a new generation of imaging technology: light-field cameras,
which bridge the realms of applied mathematics, optics, and high-performance
computing. Herein for the first time, we introduce the paradigm of light-field
imaging into laryngoscopy. The resultant probe can image the three-dimensional
(3D) shape of vocal folds within a single camera exposure. Furthermore, to
improve the spatial resolution, we developed an image fusion algorithm,
providing a simple solution to a long-standing problem in light-field imaging.



Waveform Modeling and Generation Using Hierarchical Recurrent Neural Networks for Speech Bandwidth Extension

This paper presents a waveform modeling and generation method using
hierarchical recurrent neural networks (HRNN) for speech bandwidth extension
(BWE). Different from conventional BWE methods which predict spectral
parameters for reconstructing wideband speech waveforms, this BWE method models
and predicts waveform samples directly without using vocoders. Inspired by
SampleRNN which is an unconditional neural audio generator, the HRNN model
represents the distribution of each wideband or high-frequency waveform sample
conditioned on the input narrowband waveform samples using a neural network
composed of long short-term memory (LSTM) layers and feed-forward (FF) layers.
The LSTM layers form a hierarchical structure and each layer operates at a
specific temporal resolution to efficiently capture long-span dependencies
between temporal sequences. Furthermore, additional conditions, such as the
bottleneck (BN) features derived from narrowband speech using a deep neural
network (DNN)-based state classifier, are employed as auxiliary input to
further improve the quality of generated wideband speech. The experimental
results of comparing several waveform modeling methods show that the HRNN-based
method can achieve better speech quality and run-time efficiency than the
dilated convolutional neural network (DCNN)-based method and the plain
sample-level recurrent neural network (SRNN)-based method. Our proposed method
also outperforms the conventional vocoder-based BWE method using LSTM-RNNs in
terms of the subjective quality of the reconstructed wideband speech.



A Distributed Processing Architecture for Modular and Scalable Massive MIMO Base Stations

In this work, a scalable and modular architecture for massive MIMO base
stations with distributed processing is proposed. New antennas can readily be
added by adding a new node as each node handles all the additional involved
processing. The architecture supports conjugate beamforming, zero-forcing, and
MMSE, where for the two latter cases a central matrix inversion is required.
The impact of the time required for this matrix inversion is carefully analyzed
along with a generic frame format. As part of the contribution, careful
computational, memory, and communication analyses are presented. It is shown
that all computations can be mapped to a single computational structure and
that a processing node consisting of a single such processing element can
handle a broad range of bandwidths and number of terminals.



The Temple University Hospital Seizure Detection Corpus

We introduce the TUH EEG Seizure Corpus (TUSZ), which is the largest open
source corpus of its type, and represents an accurate characterization of
clinical conditions. In this paper, we describe the techniques used to develop
TUSZ, evaluate their effectiveness, and present some descriptive statistics on
the resulting corpus.



A Tutorial on Modeling and Inference in Undirected Graphical Models for Hyperspectral Image Analysis

Undirected graphical models have been successfully used to jointly model the
spatial and the spectral dependencies in earth observing hyperspectral images.
They produce less noisy, smooth, and spatially coherent land cover maps and
give top accuracies on many datasets. Moreover, they can easily be combined
with other state-of-the-art approaches, such as deep learning. This has made
them an essential tool for remote sensing researchers and practitioners.
However, graphical models have not been easily accessible to the larger remote
sensing community as they are not discussed in standard remote sensing
textbooks and not included in the popular remote sensing software and
toolboxes. In this tutorial, we provide a theoretical introduction to Markov
random fields and conditional random fields based spatial-spectral
classification for land cover mapping along with a detailed step-by-step
practical guide on applying these methods using freely available software.
Furthermore, the discussed methods are benchmarked on four public hyperspectral
datasets for a fair comparison among themselves and easy comparison with the
vast number of methods in literature which use the same datasets. The source
code necessary to reproduce all the results in the paper is published on-line
to make it easier for the readers to apply these techniques to different remote
sensing problems.



Cyber-Induced Risk Modeling for Microprocessor-Based Relays in Substations

Once critical substations are compromised, attack agents can coordinate among
their peers to plot for maximizing disruption using local control devices. For
defenders, it is critical to enumerate and identify all digital relays to
determine the systemic risks. Any combination of disruptive switching via the
compromised relays can result in misoperation or immediate effect to the
system. The resulting consequence of these attack's initial events would
possibly incur cascading failure to a grid. This paper quantifies the
criticality of substation protective relays with the combination of the outage
level and its corresponding severity risk index. The proposed hypothesized
outages are based on the type of protective relaying, bus configuration of a
substation, and commonly implemented relaying schemes, such as bus
differential, directional overcurrent, and distance relays, are studied. This
preliminary work also provides three approaches of determination in
probabilities for sensitivity analysis. The proposed risk indices are evaluated
using IEEE test systems.



Hardware implementation of auto-mutual information function for condition monitoring

This study is aimed at showing applicability of mutual information, namely
auto-mutual information function for condition monitoring in electrical motors,
through age detection in accelerated motor aging. Vibration data collected in
artificial induction motor experiment is used for verification of both the
original auto-mutual information function algorithm and its hardware
implementation in Verilog, produced from an initial version made with Matlab
HDL (Hardware Description Language) Coder. A conceptual model for industry and
education based on a field programmable logic array development board is
developed and demonstrated on the auto-mutual information function example,
while suggesting other applications as well. It has also been shown that
attractor reconstruction for the vibration data cannot be straightforward.



Multiset-Partition Distribution Matching

Distribution matching is a fixed-length invertible mapping from a uniformly
distributed bit sequence to shaped amplitudes and plays an important role in
the probabilistic amplitude shaping framework. With conventional
constantcomposition distribution matching (CCDM), all output sequences have
identical composition. In this paper, we propose multisetpartition distribution
matching (MPDM) where the composition is constant over all output sequences.
When considering the desired distribution as a multiset, MPDM corresponds to
partitioning this multiset into equal-size subsets. We show that MPDM allows to
address more output sequences and thus has lower rate loss than CCDM in all
nontrivial cases. By imposing some constraints on the partitioning, a
constructive MPDM algorithm is proposed which comprises two parts. A
variable-length prefix of the binary data word determines the composition to be
used, and the remainder of the input word is mapped with a conventional CCDM
algorithm, such as arithmetic coding, according to the chosen composition.
Simulations of 64-ary quadrature amplitude modulation over the additive white
Gaussian noise channel demonstrate that the block-length saving of MPDM over
CCDM for a fixed gap to capacity is approximately a factor of 2.5 to 5 at
medium to high signal-to-noise ratios (SNRs).



Identifying Corresponding Patches in SAR and Optical Images with a Pseudo-Siamese CNN

In this letter, we propose a pseudo-siamese convolutional neural network
(CNN) architecture that enables to solve the task of identifying corresponding
patches in very-high-resolution (VHR) optical and synthetic aperture radar
(SAR) remote sensing imagery. Using eight convolutional layers each in two
parallel network streams, a fully connected layer for the fusion of the
features learned in each stream, and a loss function based on binary
cross-entropy, we achieve a one-hot indication if two patches correspond or
not. The network is trained and tested on an automatically generated dataset
that is based on a deterministic alignment of SAR and optical imagery via
previously reconstructed and subsequently co-registered 3D point clouds. The
satellite images, from which the patches comprising our dataset are extracted,
show a complex urban scene containing many elevated objects (i.e. buildings),
thus providing one of the most difficult experimental environments. The
achieved results show that the network is able to predict corresponding patches
with high accuracy, thus indicating great potential for further development
towards a generalized multi-sensor key-point matching procedure. Index
Terms-synthetic aperture radar (SAR), optical imagery, data fusion, deep
learning, convolutional neural networks (CNN), image matching, deep matching



An axially-variant kernel imaging model applied to ultrasound image reconstruction

Existing ultrasound deconvolution approaches unrealistically assume,
primarily for computational reasons, that the convolution model relies on a
spatially invariant kernel and circulant boundary conditions. We discard both
restrictions and introduce an image formation model applicable to ultrasound
imaging and deconvolution based on an axially varying kernel, that accounts for
arbitrary boundary conditions. Our model has the same computational complexity
as the one employing spatially invariant convolution and has negligible memory
requirements. To accommodate state-of-the-art deconvolution approaches when
applied to a variety of inverse problem formulations, we also provide an
equally efficient adjoint expression of our model. Simulation results confirm
the tractability of our model for the deconvolution of large images. Moreover,
the quality of reconstruction using our model is superior to that obtained
using spatially invariant convolution.



CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition

The popularity of ASR (automatic speech recognition) systems, like Google
Voice, Cortana, brings in security concerns, as demonstrated by recent attacks.
The impacts of such threats, however, are less clear, since they are either
less stealthy (producing noise-like voice commands) or requiring the physical
presence of an attack device (using ultrasound). In this paper, we demonstrate
that not only are more practical and surreptitious attacks feasible but they
can even be automatically constructed. Specifically, we find that the voice
commands can be stealthily embedded into songs, which, when played, can
effectively control the target system through ASR without being noticed. For
this purpose, we developed novel techniques that address a key technical
challenge: integrating the commands into a song in a way that can be
effectively recognized by ASR through the air, in the presence of background
noise, while not being detected by a human listener. Our research shows that
this can be done automatically against real world ASR applications. We also
demonstrate that such CommanderSongs can be spread through Internet (e.g.,
YouTube) and radio, potentially affecting millions of ASR users. We further
present a new mitigation technique that controls this threat.



Impact of the Overall Electrical Filter Shaping in Next-Generation 25G and 50G PON

Next-generation high-speed passive optical network (HS-PON) transceivers
supporting 25, 50 and 100 Gb/s are under the early stage of their
standardization process. One key aspect of this process is the choice of the
best modulation format. To this end, performance comparisons among several
modulation formats against different physical constraints have been presented
in literature and are still being carried out. In our contribution, we
performed an exhaustive analysis on the impact of transceivers electrical
frequency response shape on the performance of 2-levels pulse amplitude
modulation (PAM-2), 4-levels PAM (PAM-4), electrical and optical duobinary
modulation formats with adaptive equalizer at the receiver side. We show by
means of numerical simulations that the specification of the typically used
-3dB bandwidth is not sufficient, since also out-of-band electrical frequency
response specifications (such as the -20dB bandwidth) has a huge impact on the
performance of the analyzed modulation formats. The normalized graphs given at
the end of the paper in terms of -3dB and -20 dB bandwidths can thus be useful
for the design of the next generation of HS-PON transceivers.



Fast binary embeddings, and quantized compressed sensing with structured matrices

This paper deals with two related problems, namely distance-preserving binary
embeddings and quantization for compressed sensing . First, we propose fast
methods to replace points from a subset $\mathcal{X} \subset \mathbb{R}^n$,
associated with the Euclidean metric, with points in the cube $\{\pm 1\}^m$ and
we associate the cube with a pseudo-metric that approximates Euclidean distance
among points in $\mathcal{X}$. Our methods rely on quantizing fast
Johnson-Lindenstrauss embeddings based on bounded orthonormal systems and
partial circulant ensembles, both of which admit fast transforms. Our
quantization methods utilize noise-shaping, and include Sigma-Delta schemes and
distributed noise-shaping schemes. The resulting approximation errors decay
polynomially and exponentially fast in $m$, depending on the embedding method.
This dramatically outperforms the current decay rates associated with binary
embeddings and Hamming distances. Additionally, it is the first such binary
embedding result that applies to fast Johnson-Lindenstrauss maps while
preserving $\ell_2$ norms.
  Second, we again consider noise-shaping schemes, albeit this time to quantize
compressed sensing measurements arising from bounded orthonormal ensembles and
partial circulant matrices. We show that these methods yield a reconstruction
error that again decays with the number of measurements (and bits), when using
convex optimization for reconstruction. Specifically, for Sigma-Delta schemes,
the error decays polynomially in the number of measurements, and it decays
exponentially for distributed noise-shaping schemes based on beta encoding.
These results are near optimal and the first of their kind dealing with bounded
orthonormal systems.



Parametric Modeling of Non-Stationary Signals

Parametric modeling of non-stationary signals is addressed in this article.
We present several models based on the characteristic features of the modeled
signal, together with the methods for accurate estimation of model parameters.
Non-stationary signals, viz. transient system response, speech phonemes, and
electrocardiograph signal are fitted by these feature-based models.



Reconstruction of Compressively Sensed Images using Convex Tikhonov Sparse Dictionary Learning and Adaptive Spectral Filtering

Sparse representation using over-complete dictionaries have shown to produce
good quality results in various image processing tasks. Dictionary learning
algorithms have made it possible to engineer data adaptive dictionaries which
have promising applications in image compression and image enhancement. The
most common sparse dictionary learning algorithms use the techniques of
matching pursuit and K-SVD iteratively for sparse coding and dictionary
learning respectively. While this technique produces good results, it requires
a large number of iterations to converge to an optimal solution. In this
article, we use a closed form stabilized convex optimization technique for both
sparse coding and dictionary learning. The approach results in providing the
best possible dictionary and the sparsest representation resulting in minimum
reconstruction error. Once the image is reconstructed from the compressively
sensed samples, we use adaptive frequency and spatial filtering techniques to
move towards exact image recovery. It is clearly seen from the results that the
proposed algorithm provides much better reconstruction results than
conventional sparse dictionary techniques for a fixed number of iterations.
Depending inversely upon the number of details present in the image, the
proposed algorithm reaches the optimal solution with a significantly lower
number of iterations. Consequently, high PSNR and low MSE is obtained using the
proposed algorithm for our compressive sensing framework.



Detection of Brain Stimuli Using Ramanujan Periodicity Transforms

The ability to efficiently match the frequency of the brain's response to
repetitive visual stimuli in real time is the basis for reliable SSVEP-based
Brain-Computer-Interfacing (BCI). The detection of different stimuli is posed
as a composite hypothesis test, where SSVEPs are assumed to admit a sparse
representation in a Ramanujan Periodicity Transform (RPT) dictionary. For the
binary case, we develop and analyze the performance of an RPT detector based on
a derived generalized likelihood ratio test. Our approach is extended to
multi-hypothesis multi-electrode settings, where we capture the spatial
correlation between the electrodes using pre-stimulus data. We also introduce a
new metric for evaluating SSVEP detection schemes based on their achievable
efficiency and discrimination rate tradeoff for given system resources. We
obtain exact distributions of the test statistic in terms of confluent
hypergeometric functions. Results based on extensive simulations with both
synthesized and real data indicate that the RPT detector substantially
outperforms spectral-based methods. Its performance also surpasses the
state-of-the-art Canonical Correlation Analysis (CCA) methods with respect to
accuracy and sample complexity in short data lengths regimes crucial for
real-time applications. The proposed approach is asymptotically optimal as it
closes the gap to a perfect measurement bound as the data length increases. In
contrast to existing supervised methods which are highly data-dependent, the
RPT detector only uses pre-stimulus data to estimate the per-subject spatial
correlation, thereby dispensing with considerable overhead associated with data
collection for a large number of subjects and stimuli. Our work advances the
theory and practice of emerging real-time BCI and affords a new framework for
comparing SSVEP detection schemes across a wider spectrum of operating regimes.



Optimal Energy Management Strategies in Wireless Data and Energy Cooperative Communications

This paper presents a new cooperative wireless communication network strategy
that incorporates energy cooperation and data cooperation. The model
establishment, design goal formulations, and algorithms for throughput
maximization of the proposed protocol are presented and illustrated using a
three-node network with two energy harvesting (EH) user nodes and a destination
node. Transmission models are established from the performance analysis for a
total of four scenarios. Based on the models, we seek to find optimal energy
management strategies by jointly optimizing time allocation for each user,
power allocations over these time intervals, and data throughputs at user nodes
so as to maximize the sum-throughput or, alternatively, the minimum throughput
of the two users in all scenarios. An accelerated Newton barrier algorithm and
an alternative algorithm based on local quadratic approximation of the
transmission models are developed to solve the aforementioned optimization
problems. Numerical experiments under practical settings provide supportive
observations to our performance analysis.



Mitigating Pilot Contamination in Multi-cell Hybrid Millimeter Wave Systems

In this paper, we investigate the system performance of a multi-cell
multi-user (MU) hybrid millimeter wave (mmWave) multiple-input multiple-output
(MIMO) network adopting the channel estimation algorithm proposed in [1] for
channel estimation. Due to the reuse of orthogonal pilot symbols among
different cells, the channel estimation is expected to be affected by pilot
contamination, which is considered as a fundamental performance bottleneck of
conventional multicell MU massive MIMO networks. To analyze the impact of pilot
contamination on the system performance, we derive the closed-form
approximation expression of the normalized mean squared error (MSE) of the
channel estimation performance. Our analytical and simulation results show that
the channel estimation error incurred by the impact of pilot contamination and
noise vanishes asymptotically with an increasing number of antennas equipped at
each radio frequency (RF) chain deployed at the desired BS. Thus, pilot
contamination is no longer the fundamental problem for multi-cell hybrid mmWave
systems.



KRISM --- Krylov Subspace-based Optical Computing of Hyperspectral Images

We present an adaptive imaging technique that optically computes a low-rank
approximation of a scene's hyperspectral image, conceptualized as a matrix.
Central to the proposed technique is the optical implementation of two
measurement operators: a spectrally-coded imager and a spatially-coded
spectrometer. By iterating between the two operators, we show that the top
singular vectors and singular values of a hyperspectral image can be adaptively
and optically computed with only a few iterations. We present an optical design
that uses pupil plane coding for implementing the two operations and show
several compelling results using a lab prototype to demonstrate the
effectiveness of the proposed hyperspectral imager.



Estimating Distances via Received Signal Strength and Connectivity in Wireless Sensor Networks

Distance estimation is vital for localization and many other applications in
wireless sensor networks (WSNs). Particularly, it is desirable to implement
distance estimation as well as localization without using specific hardware in
low-cost WSNs. As such, both the received signal strength (RSS) based approach
and the connectivity based approach have gained much attention. The RSS based
approach is suitable for estimating short distances, whereas the connectivity
based approach obtains relatively good performance for estimating long
distances. Considering the complementary features of these two approaches, we
propose a fusion method based on the maximum-likelihood estimator (MLE) to
estimate the distance between any pair of neighboring nodes in a WSN through
efficiently fusing the information from the RSS and local connectivity.
Additionally, the method is reported under the practical log-normal shadowing
model, and the associated Cramer-Rao lower bound (CRLB) is also derived for
performance analysis. Both simulations and experiments based on practical
measurements are carried out, and demonstrate that the proposed method
outperforms any single approach and approaches to the CRLB as well.



A model of orbital angular momentum Li-Fi

Twisted light has recently gained enormous interest in communication systems.
Thus far, twisted light has not yet been utilized for visible light
communication to transmit data. Here, by exploiting the color and orbital
angular momentum (OAM) degrees of freedom simultaneously, we construct a much
higher-dimensional space spanned by their hybrid mode basis, which further
increases the information capacity of twisted light. We build a new visible
light communication system using a white light emitting diode, with red, green
and blue (RGB) colors serving as independent channels and with OAM
superposition states encoding the information. We connect our conceptually new
RGB-OAM hybrid coding with the specially designed two-dimensional holographic
gratings based on theta-modulation. After indoor free-space transmission, we
decode the color information with an Xcube prism and subsequently decode the
OAM superposition states with a pattern recognition method based on supervised
machine learning. We succeed in demonstrating the transmission of color images
and a piece of audio with the fidelity over 96%. Our point-to-point scheme with
hybrid RGB-OAM encoding, not only increases significantly the information
capacity of twisted light, but also offers additional security that supplements
the traditional broadcasting visible light communications, e.g., Li-Fi.



Deep Learning Angiography (DLA): Three-dimensional C-arm Cone Beam CT Angiography Using Deep Learning

Background and Purpose: Our purpose was to develop a deep learning
angiography (DLA) method to generate 3D cerebral angiograms from a single
contrast-enhanced acquisition.
  Material and Methods: Under an approved IRB protocol 105 3D-DSA exams were
randomly selected from an internal database. All were acquired using a clinical
system (Axiom Artis zee, Siemens Healthineers) in conjunction with a standard
injection protocol. More than 150 million labeled voxels from 35 subjects were
used for training. A deep convolutional neural network was trained to classify
each image voxel into three tissue types (vasculature, bone and soft tissue).
The trained DLA model was then applied for tissue classification in a
validation cohort of 8 subjects and a final testing cohort consisting of the
remaining 62 subjects. The final vasculature tissue class was used to generate
the 3D-DLA images. To quantify the generalization error of the trained model,
accuracy, sensitivity, precision and F1-scores were calculated for vasculature
classification in relevant anatomy. The 3D-DLA and clinical 3D-DSA images were
subject to a qualitative assessment for the presence of inter-sweep motion
artifacts.
  Results: Vasculature classification accuracy and 95% CI in the testing
dataset was 98.7% ([98.3, 99.1] %). No residual signal from osseous structures
was observed for all 3D-DLA testing cases except for small regions in the otic
capsule and nasal cavity compared to 37% (23/62) of the 3D-DSAs.
  Conclusion: DLA accurately recreated the vascular anatomy of the 3D-DSA
reconstructions without mask. DLA reduced mis-registration artifacts induced by
inter-sweep motion. DLA reduces radiation exposure required to obtain
clinically useful 3D-DSA



Multichannel Sound Event Detection Using 3D Convolutional Neural Networks for Learning Inter-channel Features

In this paper, we propose a stacked convolutional and recurrent neural
network (CRNN) with a 3D convolutional neural network (CNN) in the first layer
for the multichannel sound event detection (SED) task. The 3D CNN enables the
network to simultaneously learn the inter- and intra-channel features from the
input multichannel audio. In order to evaluate the proposed method,
multichannel audio datasets with different number of overlapping sound sources
are synthesized. Each of this dataset has a four-channel first-order Ambisonic,
binaural, and single-channel versions, on which the performance of SED using
the proposed method are compared to study the potential of SED using
multichannel audio. A similar study is also done with the binaural and
single-channel versions of the real-life recording TUT-SED 2017 development
dataset. The proposed method learns to recognize overlapping sound events from
multichannel features faster and performs better SED with a fewer number of
training epochs. The results show that on using multichannel Ambisonic audio in
place of single-channel audio we improve the overall F-score by 7.5%, overall
error rate by 10% and recognize 15.6% more sound events in time frames with
four overlapping sound sources.



Highly-Reverberant Real Environment database: HRRE

Speech recognition in highly-reverberant real environments remains a major
challenge. An evaluation dataset for this task is needed. This report describes
the generation of the Highly-Reverberant Real Environment database (HRRE). This
database contains 13.4 hours of data recorded in real reverberant environments
and consists of 20 different testing conditions which consider a wide range of
reverberation times and speaker-to-microphone distances. These evaluation sets
were generated by re-recording the clean test set of the Aurora-4 database
which corresponds to five loudspeaker-microphone distances in four reverberant
conditions.



Algorithms for the Construction of Incoherent Frames Under Various Design Constraints

Unit norm finite frames are generalizations of orthonormal bases with many
applications in signal processing. An important property of a frame is its
coherence, a measure of how close any two vectors of the frame are to each
other. Low coherence frames are useful in compressed sensing applications. When
used as measurement matrices, they successfully recover highly sparse solutions
to linear inverse problems. This paper describes algorithms for the design of
various low coherence frame types: real, complex, unital (constant magnitude)
complex, sparse real and complex, nonnegative real and complex, and harmonic
(selection of rows from Fourier matrices). The proposed methods are based on
solving a sequence of convex optimization problems that update each vector of
the frame. This update reduces the coherence with the other frame vectors,
while other constraints on its entries are also imposed. Numerical experiments
show the effectiveness of the methods compared to the Welch bound, as well as
other competing algorithms, in compressed sensing applications.



Denoising Signals in Cognitive Radio Systems Using An Evolutionary Algorithm Based Adaptive Filter

Noise originating from several sources in a RF environment degrades the
performance of communication systems. In wideband systems, such as cognitive
radios, noise at the receiver can originate from non-linearity present in the
RF front end, time-varying thermal noise within the receiver radio system, and
noise from adjacent network nodes. Several denoising techniques have been
proposed for cognitive radios, some of which are applied during spectrum
sensing and others to received noisy signal during communication. Examples of
some of these techniques used for noise cancellation in received signals are
least mean square (LMS) and its variants. However, these algorithms have low
performance with non-linear signals and cannot locate a global optimum solution
for noise cancellation. Therefore, application of global search optimization
techniques, such as evolutionary algorithms, is considered for noise
cancellation. In this paper, particle swarm optimization (PSO) and LMS
algorithms are implemented and their performances are evaluated. Extensive
simulations were performed where Gaussian and non-linear random noise were
added to the transmitted signal. The performance comparison was done using two
metrics: bit error rate and mean square error. The results show that PSO
outperforms LMS under both Gaussian and nonlinear random noise.



Noise Cancellation in Cognitive Radio Systems: A Performance Comparison of Evolutionary Algorithms

Noise cancellation is one of the important signal processing functions of any
communication system, as noise affects data integrity. In existing systems,
traditional filters are used to cancel the noise from the received signals.
These filters use fixed hardware which is capable of filtering specific
frequency or a range of frequencies. However, next generation communication
technologies, such as cognitive radio, will require the use of adaptive filters
that can dynamically reconfigure their filtering parameters for any frequency.
To this end, a few noise cancellation techniques have been proposed, including
least mean squares (LMS) and its variants. However, these algorithms are
susceptible to non-linear noise and fail to locate the global optimum solution
for de-noising. In this paper, we investigate the efficiency of two global
search optimization based algorithms, genetic algorithm and particle swarm
optimization in performing noise cancellation in cognitive radio systems. These
algorithms are implemented and their performances are compared to that of LMS
using bit error rate and mean square error as performance evaluation metrics.
Simulations are performed with additive white Gaussian noise and random
nonlinear noise. Results indicate that GA and PSO perform better than LMS for
the case of AWGN corrupted signal but for non-linear random noise PSO
outperforms the other two algorithms.



Compressive Sensing: Performance Comparison Of Sparse Recovery Algorithms

Spectrum sensing is an important process in cognitive radio. A number of
sensing techniques that have been proposed suffer from high processing time,
hardware cost and computational complexity. To address these problems,
compressive sensing has been proposed to decrease the processing time and
expedite the scanning process of the radio spectrum. Selection of a suitable
sparse recovery algorithm is necessary to achieve this goal. A number of sparse
recovery algorithms have been proposed. This paper surveys the sparse recovery
algorithms, classify them into categories, and compares their performances. For
the comparison, we used several metrics such as recovery error, recovery time,
covariance, and phase transition diagram. The results show that techniques
under Greedy category are faster, techniques of Convex and Relaxation category
perform better in term of recovery error, and Bayesian based techniques are
observed to have an advantageous balance of small recovery error and a short
recovery time.



High-throughput intensity diffraction tomography with a computational microscope

We demonstrate a motion-free intensity diffraction tomography technique that
enables direct inversion of 3D phase and absorption from intensity-only
measurements for weakly scattering samples. We derive a novel linear forward
model, featuring slice-wise phase and absorption transfer functions using
angled illumination. This new framework facilitates flexible and efficient data
acquisition, enabling arbitrary sampling of the illumination angles. The
reconstruction algorithm performs 3D synthetic aperture using a robust,
computation and memory efficient slice-wise deconvolution to achieve resolution
up to the incoherent limit. We demonstrate our technique with thick biological
samples having both sparse 3D structures and dense cell clusters. We further
investigate the limitation of our technique when imaging strongly scattering
samples. Imaging performance and the influence of multiple scattering is
evaluated using a 3D sample consisting of stacked phase and absorption
resolution targets. This computational microscopy system is directly built on a
standard commercial microscope with a simple LED array source add-on, and
promises broad applications by leveraging the ubiquitous microscopy platforms
with minimal hardware modifications.



On Psychoacoustically Weighted Cost Functions Towards Resource-Efficient Deep Neural Networks for Speech Denoising

We present a psychoacoustically enhanced cost function to balance network
complexity and perceptual performance of deep neural networks for speech
denoising. While training the network, we utilize perceptual weights added to
the ordinary mean-squared error to emphasize contribution from frequency bins
which are most audible while ignoring error from inaudible bins. To generate
the weights, we employ psychoacoustic models to compute the global masking
threshold from the clean speech spectra. We then evaluate the speech denoising
performance of our perceptually guided neural network by using both objective
and perceptual sound quality metrics, testing on various network structures
ranging from shallow and narrow ones to deep and wide ones. The experimental
results showcase our method as a valid approach for infusing perceptual
significance to deep neural network operations. In particular, the more
perceptually sensible enhancement in performance seen by simple neural network
topologies proves that the proposed method can lead to resource-efficient
speech denoising implementations in small devices without degrading the
perceived signal fidelity.



Characterisation of carbon fibre-reinforced polymer composites through complex Radon-transform analysis of eddy-current data

Maintaining the correct fibre orientations and stacking sequence in
carbon-fibre reinforced polymers (CFRP) during manufacture is essential for
achieving the required mechanical properties of a component. This paper
presents and evaluates a method for the rapid characterisation of the fibre
orientations present in CFRP structures, and the differentiation of different
stacking sequences, through the Radon-transform analysis of complex-valued
eddy-current testing (ECT) inspection data. A high-frequency (20 MHz)
eddy-current inspection system was used to obtain 2D scans of a range of CFRP
samples of differing ply stacking sequences. The complex electrical impedance
scan data was analysed using Radon-transform techniques to quickly and simply
determine the dominant fibre orientations present in the structure. This method
is compared to 2D-fast Fourier transform (2D-FFT) analysis of the same data and
shown to give superior quantitative results with comparatively fewer
computational steps and corrections. Further analysis is presented
demonstrating and examining a method for preserving the complex information
inherent within the eddy-current scan data during Radon-transform analysis.
This investigation shows that the real and imaginary components of the ECT data
encode information about the sacking sequence allowing the distinction between
composites with different stacking structures. This new analysis technique
could be used for in-process analysis of CFRP structures as a more accurate
characterisation method, reducing the chance of costly manufacturing errors.



Analytical modeling and analysis of interleaving on correlated wireless channels

Interleaving is a mechanism universally used in wireless access technologies
to alleviate the effect of channel correlation. In spite of its wide adoption,
to the best of our knowledge, there are no analytical models proposed so far.
In this paper we fill this void proposing three different models of
interleaving. Two of these models are based on numerical algorithms while one
of them allows for closed-form expression for packet error probability.
Although we use block codes with hard decoding to specify the models our
modeling principles are applicable to all forward error correction codes as
long as there exists a functional relationship (possibly, probabilistic)
between the number of incorrectly received bits in a codeword and the codeword
error probability. We evaluate accuracy of our models showing that the worst
case prediction is limited by 50\% across a wide range of input parameters.
Finally, we study the effect of interleaving in detail demonstrating how it
varies with channel correlation, bit error rate and error correction
capability. Numerical results reported in this paper allows to identify the
optimal value of the interleaving depth that need to be used for a channel with
a given degree of correlation. The reference implementations of the models are
available [1].



Binary Compressive Sensing via Smoothed $\ell_0$ Gradient Descent

We present a Compressive Sensing algorithm for reconstructing binary signals
from its linear measurements. The proposed algorithm minimizes a non-convex
cost function expressed as a weighted sum of smoothed $\ell_0$ norms which
takes into account the binariness of signals. We show that for binary signals
the proposed algorithm outperforms other existing algorithms in recovery rate
while requiring a short run time.



On the Secure and Reconfigurable Multi-Layer Network Design for Critical Information Dissemination in the Internet of Battlefield Things (IoBT)

The Internet of things (IoT) is revolutionizing the management and control of
automated systems leading to a paradigm shift in areas such as smart homes,
smart cities, health care, transportation, etc. The IoT technology is also
envisioned to play an important role in improving the effectiveness of military
operations in battlefields. The interconnection of combat equipment and other
battlefield resources for coordinated automated decisions is referred to as the
Internet of battlefield things (IoBT). IoBT networks are significantly
different from traditional IoT networks due to battlefield specific challenges
such as the absence of communication infrastructure, heterogeneity of devices,
and susceptibility to cyber-physical attacks. The combat efficiency and
coordinated decision-making in war scenarios depends highly on real-time data
collection, which in turn relies on the connectivity of the network and
information dissemination in the presence of adversaries. This work aims to
build the theoretical foundations of designing secure and reconfigurable IoBT
networks. Leveraging the theories of stochastic geometry and mathematical
epidemiology, we develop an integrated framework to quantify the information
dissemination among heterogeneous network devices. Consequently, a tractable
optimization problem is formulated that can assist commanders in cost
effectively planning the network and reconfiguring it according to the changing
mission requirements.



Malaria Detection Using Image Processing and Machine Learning

Malaria is mosquito-borne blood disease caused by parasites of the genus
Plasmodium. Conventional diagnostic tool for malaria is the examination of
stained blood cell of patient in microscope. The blood to be tested is placed
in a slide and is observed under a microscope to count the number of infected
RBC. An expert technician is involved in the examination of the slide with
intense visual and mental concentration. This is tiresome and time consuming
process.
  In this paper, we construct a new mage processing system for detection and
quantification of plasmodium parasites in blood smear slide, later we develop
Machine Learning algorithm to learn, detect and determine the types of infected
cells according to its features.



Cardiac Arrhythmia Detection from ECG Combining Convolutional and Long Short-Term Memory Networks

Objectives: Atrial fibrillation (AF) is a common heart rhythm disorder
associated with deadly and debilitating consequences including heart failure,
stroke, poor mental health, reduced quality of life and death. Having an
automatic system that diagnoses various types of cardiac arrhythmias would
assist cardiologists to initiate appropriate preventive measures and to improve
the analysis of cardiac disease. To this end, this paper introduces a new
approach to detect and classify automatically cardiac arrhythmias in
electrocardiograms (ECG) recordings.
  Methods: The proposed approach used a combination of Convolution Neural
Networks (CNNs) and a sequence of Long Short-Term Memory (LSTM) units, with
pooling, dropout and normalization techniques to improve their accuracy. The
network predicted a classification at every 18th input sample and we selected
the final prediction for classification. Results were cross-validated on the
Physionet Challenge 2017 training dataset, which contains 8,528 single lead ECG
recordings lasting from 9s to just over 60s.
  Results: Using the proposed structure and no explicit feature selection,
10-fold stratified cross-validation gave an overall F-measure of 0.83.10-0.015
on the held-out test data (mean-standard deviation over all folds) and 0.80 on
the hidden dataset of the Challenge entry server.



Information Measures for Microphone Arrays

We propose a novel information-theoretic approach for evaluating microphone
arrays that relies on the array physics and geometry rather than the underlying
beamforming algorithm. The analogy between Multiple-Input-Multiple-Output
(MIMO) wireless communication channel and the acoustic channel of microphone
arrays is exploited to define information measures of microphone arrays, which
provide upper bounds of the information rate of the microphone array system.



Non-local tensor completion for multitemporal remotely sensed images inpainting

Remotely sensed images may contain some missing areas because of poor weather
conditions and sensor failure. Information of those areas may play an important
role in the interpretation of multitemporal remotely sensed data. The paper
aims at reconstructing the missing information by a non-local low-rank tensor
completion method (NL-LRTC). First, nonlocal correlations in the spatial domain
are taken into account by searching and grouping similar image patches in a
large search window. Then low-rankness of the identified 4-order tensor groups
is promoted to consider their correlations in spatial, spectral, and temporal
domains, while reconstructing the underlying patterns. Experimental results on
simulated and real data demonstrate that the proposed method is effective both
qualitatively and quantitatively. In addition, the proposed method is
computationally efficient compared to other patch based methods such as the
recent proposed PM-MTGSR method.



Compressed Anomaly Detection with Multiple Mixed Observations

We consider a collection of independent random variables that are identically
distributed, except for a small subset which follows a different, anomalous
distribution. We study the problem of detecting which random variables in the
collection are governed by the anomalous distribution. Recent work proposes to
solve this problem by conducting hypothesis tests based on mixed observations
(e.g. linear combinations) of the random variables. Recognizing the connection
between taking mixed observations and compressed sensing, we view the problem
as recovering the "support" (index set) of the anomalous random variables from
multiple measurement vectors (MMVs). Many algorithms have been developed for
recovering jointly sparse signals and their support from MMVs. We establish the
theoretical and empirical effectiveness of these algorithms at detecting
anomalies. We also extend the LASSO algorithm to an MMV version for our
purpose. Further, we perform experiments on synthetic data, consisting of
samples from the random variables, to explore the trade-off between the number
of mixed observations per sample and the number of samples required to detect
anomalies.



Deep Predictive Models in Interactive Music

Musical performance requires prediction to operate instruments, to perform in
groups and to improvise. In this paper, we investigate how a number of digital
musical instruments (DMIs), including two of our own, have applied predictive
machine learning models that assist users by predicting unknown states of
musical processes. We characterise these predictions as focussed within a
musical instrument, at the level of individual performers, and between members
of an ensemble. These models can connect to existing frameworks for DMI design
and have parallels in the cognitive predictions of human musicians.
  We discuss how recent advances in deep learning highlight the role of
prediction in DMIs, by allowing data-driven predictive models with a long
memory of past states. The systems we review are used to motivate musical
use-cases where prediction is a necessary component, and to highlight a number
of challenges for DMI designers seeking to apply deep predictive models in
interactive music systems of the future.



Performance Comparison of 112 Gb/s DMT, Nyquist PAM4 and Partial-Response PAM4 for Future 5G Ethernet-based Fronthaul Architecture

For a future 5G Ethernet-based fronthaul architecture, 100G trunk lines of a
transmission distance up to 10 km standard single mode fiber (SSMF) in
combination with cheap grey optics to daisy chain cell site network interfaces
are a promising cost- and power-efficient solution. For such a scenario,
different intensity modulation and direct detect (IMDD) Formats at a data rate
of 112 Gb/s, namely Nyquist four-level pulse amplitude modulation (PAM4),
discrete multi-tone Transmission (DMT) and partial-response (PR) PAM4 are
experimentally investigated, using a low-cost electro-absorption modulated
laser (EML), a 25G driver and current state-of-the-art high Speed 84 GS/s CMOS
digital-to-analog converter (DAC) and analog-to-digital converter (ADC) test
chips. Each modulation Format is optimized independently for the desired
scenario and their digital signal processing (DSP) requirements are
investigated. The performance of Nyquist PAM4 and PR PAM4 depend very much on
the efficiency of pre- and post-equalization. We show the necessity for at
least 11 FFE-taps for pre-emphasis and up to 41 FFE coefficients at the
receiver side. In addition, PR PAM4 requires an MLSE with four states to decode
the signal back to a PAM4 signal. On the contrary, bit- and power-loading (BL,
PL) is crucial for DMT and an FFT length of at least 512 is necessary. With
optimized parameters, all Modulation formats result in a very similar
performances, demonstrating a transmission distance of up to 10 km over SSMF
with bit error rates (BERs) below a FEC threshold of 4.4E-3, allowing error
free transmission.



Predicting Wireless Channel Features using Neural Networks

We investigate the viability of using machine-learning techniques for
estimating user-channel features at a large-array base station (BS). In the
scenario we consider, user-pilot broadcasts are observed and processed by the
BS to extract angle-of-arrival (AoA) specific information about
propagation-channel features, such as received signal strength and relative
path delay. The problem of interest involves using this information to predict
the angle-of-departure (AoD) of the dominant propagation paths in the user
channels, i.e., channel features not directly observable at the BS. To
accomplish this task, the data collected in the same propagation environment
are used to train neural networks. Our studies rely on ray-tracing channel data
that have been calibrated against measurements from Shinjuku Square, a famous
hotspot in Tokyo, Japan. We demonstrate that the observed features at the BS
side are correlated with the angular features at the user side. We train neural
networks that exploit different combinations of measured features at the BS to
infer the unknown parameters at the users. The evaluation based on standard
statistical performance metrics suggests that such data-driven methods have the
potential to predict unobserved channel features from observed ones.



Low Complexity Time Domain Semi-Blind MIMO-OFDM Channel Estimation Using Adaptive Bussgang Algorithm

In this paper, a low complexity time domain semi-blind algorithm is proposed
to estimate and track the time varying MIMO OFDM channels. First, the proposed
least mean squares (LMS) based algorithm is developed for the training mode and
then is extended for the blind mode of the operation by combining with the
decision direction (DD) or adaptive Bussgang algorithm (ABA) techniques. In the
blind mode, because of decision errors, a smaller step size is considered for
the LMS algorithm and the channel estimation is run a few times to improve its
precision. In each round of the estimation in the blind mode, the step size is
decreased to form some kind of annealing. Both DD LMS and ABA LMS techniques
are simulated and compared to the full training case and MSE of channel
estimation error is considered as comparison criterion. It is shown for 2x4 DD
LMS and for 4x4 ABA LMS algorithms present near full training case estimation
error. Of course in some scenarios the former proposed technique performs
better and in other scenarios the latter is better and therefore combine of it
can be very interesting in all channel conditions.



Phonetic and Graphemic Systems for Multi-Genre Broadcast Transcription

State-of-the-art English automatic speech recognition systems typically use
phonetic rather than graphemic lexicons. Graphemic systems are known to perform
less well for English as the mapping from the written form to the spoken form
is complicated. However, in recent years the representational power of
deep-learning based acoustic models has improved, raising interest in graphemic
acoustic models for English, due to the simplicity of generating the lexicon.
In this paper, phonetic and graphemic models are compared for an English
Multi-Genre Broadcast transcription task. A range of acoustic models based on
lattice-free MMI training are constructed using phonetic and graphemic
lexicons. For this task, it is found that having a long-span temporal history
reduces the difference in performance between the two forms of models. In
addition, system combination is examined, using parameter smoothing and
hypothesis combination. As the combination approaches become more complicated
the difference between the phonetic and graphemic systems further decreases.
Finally, for all configurations examined the combination of phonetic and
graphemic systems yields consistent gains.



MaD TwinNet: Masker-Denoiser Architecture with Twin Networks for Monaural Sound Source Separation

Monaural singing voice separation task focuses on the prediction of the
singing voice from a single channel music mixture signal. Current state of the
art (SOTA) results in monaural singing voice separation are obtained with deep
learning based methods. In this work we present a novel deep learning based
method that learns long-term temporal patterns and structures of a musical
piece. We build upon the recently proposed Masker-Denoiser (MaD) architecture
and we enhance it with the Twin Networks, a technique to regularize a recurrent
generative network using a backward running copy of the network. We evaluate
our method using the Demixing Secret Dataset and we obtain an increment to
signal-to-distortion ratio (SDR) of 0.37 dB and to signal-to-interference ratio
(SIR) of 0.23 dB, compared to previous SOTA results.



ChronoNet: A Deep Recurrent Neural Network for Abnormal EEG Identification

Brain-related disorders such as epilepsy can be diagnosed by analyzing
electroencephalograms (EEG). However, manual analysis of EEG data requires
highly trained clinicians, and is a procedure that is known to have relatively
low inter-rater agreement (IRA). Moreover, the volume of the data and the rate
at which new data becomes available make manual interpretation a
time-consuming, resource-hungry, and expensive process. In contrast, automated
analysis of EEG data offers the potential to improve the quality of patient
care by shortening the time to diagnosis and reducing manual error. In this
paper, we focus on one of the first steps in interpreting an EEG session -
identifying whether the brain activity is abnormal or normal. To solve this
task, we propose a novel recurrent neural network (RNN) architecture termed
ChronoNet which is inspired by recent developments from the field of image
classification and designed to work efficiently with EEG data. ChronoNet is
formed by stacking multiple 1D convolution layers followed by deep gated
recurrent unit (GRU) layers where each 1D convolution layer uses multiple
filters of exponentially varying lengths and the stacked GRU layers are densely
connected in a feed-forward manner. We used the recently released TUH Abnormal
EEG Corpus dataset for evaluating the performance of ChronoNet. Unlike previous
studies using this dataset, ChronoNet directly takes time-series EEG as input
and learns meaningful representations of brain activity patterns. ChronoNet
outperforms the previously reported best results by 7.79% thereby setting a new
benchmark for this dataset. Furthermore, we demonstrate the domain-independent
nature of ChronoNet by successfully applying it to classify speech commands.



Biomedical Signals Reconstruction Under the Compressive Sensing Approach

The paper analyses the possibility to recover different biomedical signals if
limited number of samples is available. Having in mind that monitoring of
health condition is done by measuring and observing key parameters such as
heart activity through electrocardiogram or anatomy and body processes through
magnetic resonance imaging, it is important to keep the quality of the
reconstructed signal as better as possible. To recover the signal from limited
set of available coefficients, the Compressive Sensing approach and
optimization algorithms are used. The theory is verified by the experimental
results.



Polyp Segmentation in Colonoscopy Images Using Fully Convolutional Network

Colorectal cancer is a one of the highest causes of cancer-related death,
especially in men. Polyps are one of the main causes of colorectal cancer and
early diagnosis of polyps by colonoscopy could result in successful treatment.
Diagnosis of polyps in colonoscopy videos is a challenging task due to
variations in the size and shape of polyps. In this paper we proposed a polyp
segmentation method based on convolutional neural network. Performance of the
method is enhanced by two strategies. First, we perform a novel image patch
selection method in the training phase of the network. Second, in the test
phase, we perform an effective post processing on the probability map that is
produced by the network. Evaluation of the proposed method using the
CVC-ColonDB database shows that our proposed method achieves more accurate
results in comparison with previous colonoscopy video-segmentation methods.



Approximate Message Passing for Underdetermined Audio Source Separation

Approximate message passing (AMP) algorithms have shown great promise in
sparse signal reconstruction due to their low computational requirements and
fast convergence to an exact solution. Moreover, they provide a probabilistic
framework that is often more intuitive than alternatives such as convex
optimisation. In this paper, AMP is used for audio source separation from
underdetermined instantaneous mixtures. In the time-frequency domain, it is
typical to assume a priori that the sources are sparse, so we solve the
corresponding sparse linear inverse problem using AMP. We present a block-based
approach that uses AMP to process multiple time-frequency points
simultaneously. Two algorithms known as AMP and vector AMP (VAMP) are evaluated
in particular. Results show that they are promising in terms of artefact
suppression.



Face Synthesis with Landmark Points from Generative Adversarial Networks and Inverse Latent Space Mapping

Facial landmarks refer to the localization of fundamental facial points on
face images. There have been a tremendous amount of attempts to detect these
points from facial images however, there has never been an attempt to
synthesize a random face and generate its corresponding facial landmarks. This
paper presents a framework for augmenting a dataset in a latent Z-space and
applied to the regression problem of generating a corresponding set of
landmarks from a 2D facial dataset. The BEGAN framework has been used to train
a face generator from CelebA database. The inverse of the generator is
implemented using an Adam optimizer to generate the latent vector corresponding
to each facial image, and a lightweight deep neural network is trained to map
latent Z-space vectors to the landmark space. Initial results are promising and
provide a generic methodology to augment annotated image datasets with
additional intermediate samples.



Coded Status Updates in an Energy Harvesting Erasure Channel

We consider an energy harvesting transmitter sending status updates to a
receiver over an erasure channel, where each status update is of length $k$
symbols. The energy arrivals and the channel erasures are independent and
identically distributed (i.i.d.) and Bernoulli distributed in each slot. In
order to combat the effects of the erasures in the channel and the uncertainty
in the energy arrivals, we use channel coding to encode the status update
symbols. We consider two types of channel coding: maximum distance separable
(MDS) codes and rateless erasure codes. For each of these models, we study two
achievable schemes: best-effort and save-and-transmit. In the best-effort
scheme, the transmitter starts transmission right away, and sends a symbol if
it has energy. In the save-and-transmit scheme, the transmitter remains silent
in the beginning in order to save some energy to minimize energy outages in
future slots. We analyze the average age of information (AoI) under each of
these policies. We show through numerical results that as the average recharge
rate decreases, MDS coding with save-and-transmit outperforms all best-effort
schemes. We show that rateless coding with save-and-transmit outperforms all
the other schemes.



PhaseLin: Linear Phase Retrieval

Phase retrieval deals with the recovery of complex- or real-valued signals
from magnitude measurements. As shown recently, the method PhaseMax enables
phase retrieval via convex optimization and without lifting the problem to a
higher dimension. To succeed, PhaseMax requires an initial guess of the
solution, which can be calculated via spectral initializers. In this paper, we
show that with the availability of an initial guess, phase retrieval can be
carried out with an ever simpler, linear procedure. Our algorithm, called
PhaseLin, is the linear estimator that minimizes the mean squared error (MSE)
when applied to the magnitude measurements. The linear nature of PhaseLin
enables an exact and nonasymptotic MSE analysis for arbitrary measurement
matrices. We furthermore demonstrate that by iteratively using PhaseLin, one
arrives at an efficient phase retrieval algorithm that performs on par with
existing convex and nonconvex methods on synthetic and real-world data.



Scalable Preprocessing of High Volume Bird Acoustic Data

In this work, we examine the problem of efficiently preprocessing high volume
bird acoustic data. We combine several existing preprocessing steps including
noise reduction approaches into a single efficient pipeline by examining each
process individually. We then utilise a distributed computing architecture to
improve execution time. Using a master-slave model with data parallelisation,
we developed a near-linear automated scalable system, capable of preprocessing
bird acoustic recordings 21.76 times faster with 32 cores over 8 virtual
machines, compared to a serial process. This work contributes to the research
area of bioacoustic analysis, which is currently very active because of its
potential to monitor animals quickly at low cost. Overcoming noise interference
is a significant challenge in many bioacoustic studies, and the volume of data
in these studies is increasing. Our work makes large scale bird acoustic
analyses more feasible by parallelising important bird acoustic processing
tasks to significantly reduce execution times.



Monaural Speech Enhancement using Deep Neural Networks by Maximizing a Short-Time Objective Intelligibility Measure

In this paper we propose a Deep Neural Network (DNN) based Speech Enhancement
(SE) system that is designed to maximize an approximation of the Short-Time
Objective Intelligibility (STOI) measure. We formalize an approximate-STOI cost
function and derive analytical expressions for the gradients required for DNN
training and show that these gradients have desirable properties when used
together with gradient based optimization techniques. We show through
simulation experiments that the proposed SE system achieves large improvements
in estimated speech intelligibility, when tested on matched and unmatched
natural noise types, at multiple signal-to-noise ratios. Furthermore, we show
that the SE system, when trained using an approximate-STOI cost function
performs on par with a system trained with a mean square error cost applied to
short-time temporal envelopes. Finally, we show that the proposed SE system
performs on par with a traditional DNN based Short-Time Spectral Amplitude
(STSA) SE system in terms of estimated speech intelligibility. These results
are important because they suggest that traditional DNN based STSA SE systems
might be optimal in terms of estimated speech intelligibility.



Real-Time-Data Analytics in Raw Materials Handling

This paper proposes a system for the ingestion and analysis of real-time
sensor and actor data of bulk materials handling plants and machinery. It
references issues that concern mining sensor data in cyber physical systems
(CPS). The advance of cyber physical systems has created a significant change
in the architecture of sensor and actor data. It affects the complexity of the
observed systems in general, the number of signals being processed, the spatial
distribution of the signal sources on a machine or plant and the global
availability of the data. There are different definitions for what constitutes
cyber physical systems: the most succinct and pertinent to the work shown in
this paper is the definition given by the IEEE: A CPS is a system with a
coupling of the cyber aspects of computing and communications with the physical
aspects of dynamics and engineering that must abide by the laws of physics.
This includes sensor networks, real-time and hybrid systems. Results computed
from sensor and actor data must obey the equations used for modelling the
physics of the observed system - this fundamentally poses an inverse problem.
Such problems are not covered sufficiently by literature addressing mining of
sensor data. Even available standard books on mining sensor data do not discuss
the special nature of sensor data. Typically, present approaches of mining data
rely on correlation as being a sole, reliable measure for significance. It is
not taken into account that the inverse solutions to the model-describing
equations are required to establish a semantic link between a sensor
observation and its precedent cause. Without this link - without causality -
there can be no physics based knowledge discovery.



Satellite Image Scene Classification via ConvNet with Context Aggregation

Scene classification is a fundamental problem to understand the
high-resolution remote sensing imagery. Recently, convolutional neural network
(ConvNet) has achieved remarkable performance in different tasks, and
significant efforts have been made to develop various representations for
satellite image scene classification. In this paper, we present a novel
representation based on a ConvNet with context aggregation. The proposed
two-pathway ResNet (ResNet-TP) architecture adopts the ResNet as backbone, and
the two pathways allow the network to model both local details and regional
context. The ResNet-TP based representation is generated by global average
pooling on the last convolutional layers from both pathways. Experiments on two
scene classification datasets, UCM Land Use and NWPU-RESISC45, show that the
proposed mechanism achieves promising improvements over state-of-the-art
methods.



Convolutional neural network-based regression for depth prediction in digital holography

Digital holography enables us to reconstruct objects in three-dimensional
space from holograms captured by an imaging device. For the reconstruction, we
need to know the depth position of the recoded object in advance. In this
study, we propose depth prediction using convolutional neural network
(CNN)-based regression. In the previous researches, the depth of an object was
estimated through reconstructed images at different depth positions from a
hologram using a certain metric that indicates the most focused depth position;
however, such a depth search is time-consuming. The CNN of the proposed method
can directly predict the depth position with millimeter precision from
holograms.



A Generative Model for Natural Sounds Based on Latent Force Modelling

Recent advances in analysis of subband amplitude envelopes of natural sounds
have resulted in convincing synthesis, showing subband amplitudes to be a
crucial component of perception. Probabilistic latent variable analysis is
particularly revealing, but existing approaches don't incorporate prior
knowledge about the physical behaviour of amplitude envelopes, such as
exponential decay and feedback. We use latent force modelling, a probabilistic
learning paradigm that incorporates physical knowledge into Gaussian process
regression, to model correlation across spectral subband envelopes. We augment
the standard latent force model approach by explicitly modelling correlations
over multiple time steps. Incorporating this prior knowledge strengthens the
interpretation of the latent functions as the source that generated the signal.
We examine this interpretation via an experiment which shows that sounds
generated by sampling from our probabilistic model are perceived to be more
realistic than those generated by similar models based on nonnegative matrix
factorisation, even in cases where our model is outperformed from a
reconstruction error perspective.



The edge cloud: A holistic view of communication, computation and caching

The evolution of communication networks shows a clear shift of focus from
just improving the communications aspects to enabling new important services,
from Industry 4.0 to automated driving, virtual/augmented reality, Internet of
Things (IoT), and so on. This trend is evident in the roadmap planned for the
deployment of the fifth generation (5G) communication networks. This ambitious
goal requires a paradigm shift towards a vision that looks at communication,
computation and caching (3C) resources as three components of a single holistic
system. The further step is to bring these 3C resources closer to the mobile
user, at the edge of the network, to enable very low latency and high
reliability services. The scope of this chapter is to show that signal
processing techniques can play a key role in this new vision. In particular, we
motivate the joint optimization of 3C resources. Then we show how graph-based
representations can play a key role in building effective learning methods and
devising innovative resource allocation techniques.



A Novel Foward-PDE Approach as an Alternative to Empirical Mode Decomposition

In this paper we present a mathematical model of the Empirical Mode
Decomposition (EMD). Although EMD is a powerful tool for signal processing, the
algorithm itself lacks an appropriate theoretical basis. The interpolation and
iteration processes involved in the EMD method have been obstacles for
mathematical modelling. Here, we propose a novel forward heat equation approach
to represent the mean envelope and sifting process. This new model can provide
a better mathematical analysis of classical EMD as well as identifying its
limitations. Our approach achieves a better performance for a "mode-mixing"
signal as compared to the classical EMD approach and is more robust to noise.
Furthermore, we discuss the ability of EMD to separate signals and possible
improvements by adjusting parameters.



Efficient Nonlinear Transforms for Lossy Image Compression

We assess the performance of two techniques in the context of nonlinear
transform coding with artificial neural networks, Sadam and GDN. Both
techniques have been successfully used in state-of-the-art image compression
methods, but their performance has not been individually assessed to this
point. Together, the techniques stabilize the training procedure of nonlinear
image transforms and increase their capacity to approximate the (unknown)
rate-distortion optimal transform functions. Besides comparing their
performance to established alternatives, we detail the implementation of both
methods and provide open-source code along with the paper.



Structure-Aware Bayesian Compressive Sensing for Frequency-Hopping Spectrum Estimation with Missing Observations

In this paper, we address the problem of spectrum estimation of multiple
frequency-hopping (FH) signals in the presence of random missing observations.
The signals are analyzed within the bilinear time-frequency (TF) representation
framework, where a TF kernel is designed by exploiting the inherent FH signal
structures. The designed kernel permits effective suppression of cross-terms
and artifacts due to missing observations while preserving the FH signal
auto-terms. The kernelled results are represented in the instantaneous
autocorrelation function domain, which are then processed using a re-designed
structure-aware Bayesian compressive sensing algorithm to accurately estimate
the FH signal TF spectrum. The proposed method achieves high-resolution FH
signal spectrum estimation even when a large portion of data observations is
missing. Simulation results verify the effectiveness of the proposed method and
its superiority over existing techniques.



Determining JPEG Image Standard Quality Factor from the Quantization Tables

Identifying the quality factor of JPEG images is very useful for applications
in digital image forensics. Though several command-line tools exist and are
used in widely used software such as \emph{GIMP} (GNU Image Manipulation
Program), the well-known image editing software, or the \emph{ImageMagick}
suite, we have found that those may provide inaccurate or even wrong results.
This paper presents a simple method for determining the exact quality factor of
a JPEG image from its quantization tables. The method is presented briefly and
a sample program, written in Unix/Linux Shell bash language is provided.



Blind Joint MIMO Channel Estimation and Decoding

We propose a method for MIMO decoding when channel state information (CSI) is
unknown to both the transmitter and receiver. The proposed method requires some
structure in the transmitted signal for the decoding to be effective, in
particular that the underlying sources are drawn from a hypercubic space. Our
proposed technique fits a minimum volume parallelepiped to the received
samples. This problem can be expressed as a non-convex optimization problem
that can be solved with high probability by gradient descent. Our blind
decoding algorithm can be used when communicating over unknown MIMO wireless
channels using either BPSK or MPAM modulation. We apply our technique to
jointly estimate MIMO channel gain matrices and decode the underlying
transmissions with only knowledge of the transmitted constellation and without
the use of pilot symbols. Our results provide theoretical guarantees that the
proposed algorithm is correct when applied to small MIMO systems. Empirical
results show small sample size requirements, making this algorithm suitable for
block-fading channels with coherence times typically seen in practice. Our
approach has a loss of less than 3dB compared to zero-forcing with perfect CSI,
imposing a similar performance penalty as space-time coding techniques without
the loss of rate incurred by those techniques.



User Pre-Scheduling and Beamforming with Imperfect CSI in 5G Fog Radio Access Networks

We investigate the user-to-cell association (or user-clustering) and
beamforming design for Cloud Radio Access Networks (CRANs) and Fog Radio Access
Networks (FogRANs) for 5G. CRAN enables cloud centralized resource and power
allocation optimization over all the small cells served by multiple Access
Points (APs). However, the fronthaul links connecting each AP to the cloud
introduce delays and cause outdated Channel State Information (CSI). By
contrast, FogRAN enables lower latencies and better CSI qualities, at the cost
of local optimization. To alleviate these issues, we propose a hybrid algorithm
exploiting both the centralized feature of the cloud for globally-optimized
pre-scheduling using outdated CSIs and the distributed nature of FogRAN for
accurate beamforming with high quality CSIs. The centralized phase enables to
consider the interference patterns over the global network, while the
distributed phase allows for latency reduction. Simulation results show that
our hybrid algorithm for FogRAN outperforms the centralized algorithm under
imperfect CSI, both in terms of throughput and delays.



Deep Neural Network-based Cooperative Visual Tracking through Multiple Micro Aerial Vehicles

Multi-camera full-body pose capture of humans and animals in outdoor
environments is a highly challenging problem. Our approach to it involves a
team of cooperating micro aerial vehicles (MAVs) with on-board cameras only.
The key enabling-aspect of our approach is the on-board person detection and
tracking method. Recent state-of-the-art methods based on deep neural networks
(DNN) are highly promising in this context. However, real time DNNs are
severely constrained in input data dimensions, in contrast to available camera
resolutions. Therefore, DNNs often fail at objects with small scale or far away
from the camera, which are typical characteristics of a scenario with aerial
robots. Thus, the core problem addressed in this paper is how to achieve
on-board, real-time, continuous and accurate vision-based detections using DNNs
for visual person tracking through MAVs. Our solution leverages cooperation
among multiple MAVs. First, each MAV fuses its own detections with those
obtained by other MAVs to perform cooperative visual tracking. This allows for
predicting future poses of the tracked person, which are used to selectively
process only the relevant regions of future images, even at high resolutions.
Consequently, using our DNN-based detector we are able to continuously track
even distant humans with high accuracy and speed. We demonstrate the efficiency
of our approach through real robot experiments involving two aerial robots
tracking a person, while maintaining an active perception-driven formation. Our
solution runs fully on-board our MAV's CPU and GPU, with no remote processing.
ROS-based source code is provided for the benefit of the community.



A General Approach for Construction of Deterministic Compressive Sensing Matrices

In this paper, deterministic construction of measurement matrices in
Compressive Sensing (CS) is considered. First, by employing the column
replacement concept, a theorem for construction of large minimum distance
linear codes containing all-one codewords is proposed. Then, by applying an
existing theorem over these linear codes, deterministic sensing matrices are
constructed. To evaluate this procedure, two examples of constructed sensing
matrices are presented. The first example contains a matrix of size
${{p}^{2}}\times {{p}^{3}}$ and coherence ${1}/{p}\;$, and the second one
comprises a matrix with the size $p\left( p-1 \right)\times {{p}^{3}}$ and
coherence ${1}/{\left( p-1 \right)}\;$, where $p$ is a prime integer. Based on
the Welch bound, both examples asymptotically achieve optimal results.
Moreover, by presenting a new theorem, the column replacement is used for
resizing any sensing matrix to a greater-size sensing matrix whose coherence is
calculated. Then, using an example, the outperformance of the proposed method
is compared to a well-known method. Simulation results show the satisfying
performance of the column replacement method either in created or resized
sensing matrices.



Randomness and isometries in echo state networks and compressed sensing

Although largely different concepts, echo state networks and compressed
sensing models both rely on collections of random weights; as the reservoir
dynamics for echo state networks, and the sensing coefficients in compressed
sensing. Several methods for generating the random matrices and metrics to
indicate desirable performance are well-studied in compressed sensing, but less
so for echo state networks. This work explores any overlap in these compressed
sensing methods and metrics for application to echo state networks. Several
methods for generating the random reservoir weights are considered, and a new
metric, inspired by the restricted isometry property for compressed sensing, is
proposed for echo state networks. The methods and metrics are investigated
theoretically and experimentally, with results suggesting that the same types
of random matrices work well for both echo state network and compressed sensing
scenarios, and that echo state network classification accuracy is improved when
the proposed restricted isometry-like constants are close to 1.



Classification of Informative Frames in Colonoscopy Videos Using Convolutional Neural Networks with Binarized Weights

Colorectal cancer is one of the common cancers in the United States. Polyp is
one of the main causes of the colonic cancer and early detection of polyps will
increase chance of cancer treatments. In this paper, we propose a novel
classification of informative frames based on a convolutional neural network
with binarized weights. The proposed CNN is trained with colonoscopy frames
along with the labels of the frames as input data. We also used binarized
weights and kernels to reduce the size of CNN and make it suitable for
implementation in medical hardware. We evaluate our proposed method using Asu
Mayo Test clinic database, which contains colonoscopy videos of different
patients. Our proposed method reaches a dice score of 71.20% and accuracy of
more than 90% using the mentioned dataset.



Comparing approaches for mitigating intergroup variability in personality recognition

Personality have been found to predict many life outcomes, and there have
been huge interests on automatic personality recognition from a speaker's
utterance. Previously, we achieved accuracies between 37%-44% for three-way
classification of high, medium or low for each of the Big Five personality
traits (Openness to Experience, Conscientiousness, Extraversion, Agreeableness,
Neuroticism). We show here that we can improve performance on this task by
accounting for heterogeneity of gender and L1 in our data, which has English
speech from female and male native speakers of Chinese and Standard American
English (SAE). We experiment with personalizing models by L1 and gender and
normalizing features by speaker, L1 group, and/or gender.



Variational image compression with a scale hyperprior

We describe an end-to-end trainable model for image compression based on
variational autoencoders. The model incorporates a hyperprior to effectively
capture spatial dependencies in the latent representation. This hyperprior
relates to side information, a concept universal to virtually all modern image
codecs, but largely unexplored in image compression using artificial neural
networks (ANNs). Unlike existing autoencoder compression methods, our model
trains a complex prior jointly with the underlying autoencoder. We demonstrate
that this model leads to state-of-the-art image compression when measuring
visual quality using the popular MS-SSIM index, and yields rate-distortion
performance surpassing published ANN-based methods when evaluated using a more
traditional metric based on squared error (PSNR). Furthermore, we provide a
qualitative comparison of models trained for different distortion metrics.



Image denoising with generalized Gaussian mixture model patch priors

Patch priors have become an important component of image restoration. A
powerful approach in this category of restoration algorithms is the popular
Expected Patch Log-Likelihood (EPLL) algorithm. EPLL uses a Gaussian mixture
model (GMM) prior learned on clean image patches as a way to regularize
degraded patches. In this paper, we show that a generalized Gaussian mixture
model (GGMM) captures the underlying distribution of patches better than a GMM.
Even though GGMM is a powerful prior to combine with EPLL, the non-Gaussianity
of its components presents major challenges to be applied to a computationally
intensive process of image restoration. Specifically, each patch has to undergo
a patch classification step and a shrinkage step. These two steps can be
efficiently solved with a GMM prior but are computationally impractical when
using a GGMM prior. In this paper, we provide approximations and computational
recipes for fast evaluation of these two steps, so that EPLL can embed a GGMM
prior on an image with more than tens of thousands of patches. Our main
contribution is to analyze the accuracy of our approximations based on thorough
theoretical analysis. Our evaluations indicate that the GGMM prior is
consistently a better fit formodeling image patch distribution and performs
better on average in image denoising task.



Extraction of Uncorrelated Sparse Sources from Signal Mixtures using a Clustering Method

A blind source separation method is described to extract sources from data
mixtures where the underlying sources are assumed to be sparse and
uncorrelated. The approach used is to detect and analyse segments of time where
one source exists on its own. Information from these segments is combined to
counteract the effects of noise and small random correlations between the
sources that would occur in practice. This combined information can then be
used to estimate the sources one at a time using a deflationary method.
Probability density functions are not assumed for any of the sources. A
comparison is made between the proposed method, the Minimum Heading Change
method, Fast-ICA and Clusterwise PCA. It is shown, for the dataset used in this
paper, that the proposed method has the best performance for clean signals if
the input parameters are chosen correctly. However the performance of this
method can be very sensitive to these input parameters and can also be more
sensitive to noise than the Fast-ICA and Clusterwise methods.



Predictive Management of Electric Vehicles in a Community Microgrid

The charging load from Electric vehicles (EVs) is modeled as deferrable load,
meaning that the power consumption can be shifted to different time windows to
achieve various grid objectives. In local community scenarios, EVs are
considered as controllable storage devices in a global optimization problem
together with other microgrid components, such as the building load, renewable
generations, and battery energy storage system, etc. However, the uncertainties
in the driver behaviors have tremendous impact on the cost effectiveness of
microgrid operations, which has not been fully explored in previous literature.
In this paper, we propose a predictive EV management strategy in a community
microgrid, and evaluate it using real-world datasets of system baseload, solar
generation and EV charging behaviors. A two-stage operation model is
established for cost-effective EV management, i.e. wholesale market
participation in the first stage and load profile following in the second
stage. Predictive control strategies, including receding horizon control, are
adapted to solve the energy allocation problem in a decentralized fashion. The
experimental results indicate the proposed approach can considerably reduce the
total energy cost and decrease the ramping index of total system load up to
56.3%.



Background subtraction using the factored 3-way restricted Boltzmann machines

In this paper, we proposed a method for reconstructing the 3D model based on
continuous sensory input. The robot can draw on extremely large data from the
real world using various sensors. However, the sensory inputs are usually too
noisy and high-dimensional data. It is very difficult and time consuming for
robot to process using such raw data when the robot tries to construct 3D
model. Hence, there needs to be a method that can extract useful information
from such sensory inputs. To address this problem our method utilizes the
concept of Object Semantic Hierarchy (OSH). Different from the previous work
that used this hierarchy framework, we extract the motion information using the
Deep Belief Network technique instead of applying classical computer vision
approaches. We have trained on two large sets of random dot images (10,000)
which are translated and rotated, respectively, and have successfully extracted
several bases that explain the translation and rotation motion. Based on this
translation and rotation bases, background subtraction have become possible
using Object Semantic Hierarchy.



Age-Minimal Online Policies for Energy Harvesting Sensors with Random Battery Recharges

We consider an energy harvesting sensor that is sending measurement updates
regarding some physical phenomenon to a destination. The sensor relies on
energy harvested from nature to measure and send its updates, and is equipped
with a battery of finite size to collect its harvested energy. The energy
harvesting process is Poisson with unit rate, and arrives in amounts that fully
recharge the battery. Our setting is online in the sense that the times of
energy arrivals are revealed causally to the sensor after the energy is
harvested; only the statistics of the arrival process is known a priori.
Updates need to be sent in a timely manner to the destination, namely, such
that the long term average age of information is minimized over the course of
communication. The age of information is defined as the time elapsed since the
freshest update has reached the destination. We first show that the optimal
scheduling update policy is a renewal policy, and then show that it has a multi
threshold structure: the sensor sends an update only if the age of information
grows above a certain threshold that depends on the available energy.



Mobile Power Network for Ultimate Mobility without Battery Life Anxiety

Similar to the evolution from the wired Internet to mobile Internet (MI), the
growing demand for power delivery anywhere and anytime appeals for power grid
transformation from wired to mobile domain. We propose here the next generation
of power delivery network -- mobile power network (MPN) for wireless power
transfer within a mobile range from several meters to tens of meters. At first,
we present the MPN's concept evolution and application scenarios. Then, we
introduce the MPN's supporting technology, namely resonant beam charging (RBC).
As a long-range wireless power transfer (WPT) method, RBC can safely deliver
multi-Watt power to multiple devices concurrently. Meanwhile, the recent
progress in RBC research has been summarized. Next, we specify the MPN's
architecture to provide the wide-area WPT coverage. Finally, we discuss the
MPN's features and challenges. MPN can enable the ultimate mobility by cutting
the final cord of mobile devices, realizing the "last-mile" mobile power
delivery.



Real-Time Rejection and Mitigation of Time Synchronization Attacks on the Global Positioning System

This paper introduces the Time Synchronization Attack Rejection and
Mitigation (TSARM) technique for Time Synchronization Attacks (TSAs) over the
Global Positioning System (GPS). The technique estimates the clock bias and
drift of the GPS receiver along with the possible attack contrary to previous
approaches. Having estimated the time instants of the attack, the clock bias
and drift of the receiver are corrected. The proposed technique is
computationally efficient and can be easily implemented in real time, in a
fashion complementary to standard algorithms for position, velocity, and time
estimation in off-the-shelf receivers. The performance of this technique is
evaluated on a set of collected data from a real GPS receiver. Our method
renders excellent time recovery consistent with the application requirements.
The numerical results demonstrate that the TSARM technique outperforms
competing approaches in the literature.



Weakly-supervised Dictionary Learning

We present a probabilistic modeling and inference framework for
discriminative analysis dictionary learning under a weak supervision setting.
Dictionary learning approaches have been widely used for tasks such as
low-level signal denoising and restoration as well as high-level classification
tasks, which can be applied to audio and image analysis. Synthesis dictionary
learning aims at jointly learning a dictionary and corresponding sparse
coefficients to provide accurate data representation. This approach is useful
for denoising and signal restoration, but may lead to sub-optimal
classification performance. By contrast, analysis dictionary learning provides
a transform that maps data to a sparse discriminative representation suitable
for classification. We consider the problem of analysis dictionary learning for
time-series data under a weak supervision setting in which signals are assigned
with a global label instead of an instantaneous label signal. We propose a
discriminative probabilistic model that incorporates both label information and
sparsity constraints on the underlying latent instantaneous label signal using
cardinality control. We present the expectation maximization (EM) procedure for
maximum likelihood estimation (MLE) of the proposed model. To facilitate a
computationally efficient E-step, we propose both a chain and a novel tree
graph reformulation of the graphical model. The performance of the proposed
model is demonstrated on both synthetic and real-world data.



Energy-aware Adaptive Spectrum Access and Power Allocation in LAA Networks via Lyapunov Optimization

To relieve the traffic burden and improve the system capacity,
licensed-assisted access (LAA) has been becoming a promising technology to the
supplementary utilization of the unlicensed spectrum. However, due to the
densification of small base stations (SBSs) and the dynamic variety of the
number of Wi-Fi nodes in the overlapping areas, the licensed channel
interference and the unlicensed channel collision could seriously influence the
Quality of Service (QoS) and the energy consumption. In this paper, jointly
considering time-variant wireless channel conditions, dynamic traffic loads,
and random numbers of Wi-Fi nodes, we address an adaptive spectrum access and
power allocation problem that enables minimizing the system power consumption
under a certain queue stability constraint in the LAA-enabled SBSs and Wi-Fi
networks. The complex stochastic optimization problem is rewritten as the
difference of two convex (D.C.) program in the framework of Lyapunov
optimization, thus developing an online energy-aware optimal algorithm. We also
characterize the performance bounds of the proposed algorithm with a tradeoff
of [O(1=V ); O(V )] between power consumption and delay theoretically. The
numerical results verify the tradeoff and show that our scheme can reduce the
power consumption over the existing scheme by up to 72.1% under the same
traffic delay.



Generating virtual scenarios of multivariate financial data for quantitative trading applications

In this paper, we present a novel approach to the generation of virtual
scenarios of multivariate financial data of arbitrary length and composition of
assets. With this approach, decades of realistic time-synchronized data can be
simulated for a large number of assets, producing diverse scenarios to test and
improve quantitative investment strategies. Our approach is based on the
analysis and synthesis of the time-dependent individual and joint
characteristics of real financial time series, using stochastic sequences of
market trends to draw multivariate returns from time-dependent probability
functions preserving both distributional properties of asset returns and
time-dependent correlation among time series. Moreover, new time-synchronized
assets can be arbitrarily generated through a PCA-based procedure to obtain any
number of assets in the final virtual scenario. For the validation of such
simulated data, they are tested with an extensive set of measurements showing a
significant degree of agreement with the reference performance of real
financial series, better than that obtained with other classical and
state-of-the-art approaches.



Multilateration of the Local Position Measurement

The Local Position Measurement system (LPM) is one of the most precise
systems for 3D position estimation. It is able to operate in- and outdoor and
updates at a rate up to 1000 measurements per second. Previous scientific
publications focused on the time of arrival equation (TOA) provided by the LPM
and filtering after the numerical position estimation. This paper investigates
the advantages of the TOA over the time difference of arrival equation
transformation (TDOA) and the signal smoothing prior to its fitting. The LPM
was designed under the general assumption that the position of the base station
and position of the reference station are known. The information resulting from
this research can prove vital for the systems self-calibration, providing data
aiding in locating the relative position of the base station without prior
knowledge of the transponder and reference station positions.



Development of a Home Automation System Using Wireless Sensor/Actuator Nodes

This work presents the design and implementation of a wireless home
monitoring and automation system consisting of wireless sensor/actuator nodes,
wireless camera, and a home server. The low-cost wireless sensor/actuator node
features temperature, light intensity and motion sensors, and actuator driver
circuits for the control of motors, heaters, and lights. Server and client
programs used to monitor and control the home were also developed. The home
server receives and processes sensor readings, such as temperature and light
intensity readings, and also transmits user commands to wireless nodes. The
system provides ambient condition monitoring, graphing of sensor data,
intrusion detection, automated device control, and video monitoring in order to
achieve improved security and comfort in the home. In addition, users have the
flexibility of determining sensor-actuator interaction at run-time. The
developed system could also put the home in various configurable modes based on
user requests, time or environmental cues.



Multi-frequency phase retrieval from noisy data

The phase retrieval from multi-frequency intensity (power) observations is
considered. The object to be reconstructed is complex-valued. A novel algorithm
is presented that accomplishes both the object phase (absolute phase) retrieval
and denoising for Poissonian and Gaussian measurements. The algorithm is
derived from the maximum likelihood formulation with Block Matching 3D (BM3D)
sparsity priors. These priors result in two filtering: one is in the complex
domain for complex-valued multi-frequency object images and another one in the
real domain for the object phase. The algorithm is iterative with alternating
projections between the object and measurement variables. The simulation
experiments are produced for Fourier transform image formation and random phase
modulations of the object, then the observations are random object diffraction
patterns. The results demonstrate the success of the algorithm for
reconstruction of the complex phase objects with the high-accuracy performance
even for very noisy data.


aaa
Sparsity-certifying Graph Decompositions

We describe a new algorithm, the $(k,\ell)$-pebble game with colors, and use
it obtain a characterization of the family of $(k,\ell)$-sparse graphs and
algorithmic solutions to a family of problems concerning tree decompositions of
graphs. Special instances of sparse graphs appear in rigidity theory and have
received increased attention in recent years. In particular, our colored
pebbles generalize and strengthen the previous results of Lee and Streinu and
give a new proof of the Tutte-Nash-Williams characterization of arboricity. We
also present a new decomposition that certifies sparsity based on the
$(k,\ell)$-pebble game with colors. Our work also exposes connections between
pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and
Westermann and Hendrickson.



A limit relation for entropy and channel capacity per unit cost

In a quantum mechanical model, Diosi, Feldmann and Kosloff arrived at a
conjecture stating that the limit of the entropy of certain mixtures is the
relative entropy as system size goes to infinity. The conjecture is proven in
this paper for density matrices. The first proof is analytic and uses the
quantum law of large numbers. The second one clarifies the relation to channel
capacity per unit cost for classical-quantum channels. Both proofs lead to
generalization of the conjecture.



Intelligent location of simultaneously active acoustic emission sources: Part I

The intelligent acoustic emission locator is described in Part I, while Part
II discusses blind source separation, time delay estimation and location of two
simultaneously active continuous acoustic emission sources.
  The location of acoustic emission on complicated aircraft frame structures is
a difficult problem of non-destructive testing. This article describes an
intelligent acoustic emission source locator. The intelligent locator comprises
a sensor antenna and a general regression neural network, which solves the
location problem based on learning from examples. Locator performance was
tested on different test specimens. Tests have shown that the accuracy of
location depends on sound velocity and attenuation in the specimen, the
dimensions of the tested area, and the properties of stored data. The location
accuracy achieved by the intelligent locator is comparable to that obtained by
the conventional triangulation method, while the applicability of the
intelligent locator is more general since analysis of sonic ray paths is
avoided. This is a promising method for non-destructive testing of aircraft
frame structures by the acoustic emission method.



Intelligent location of simultaneously active acoustic emission sources: Part II

Part I describes an intelligent acoustic emission locator, while Part II
discusses blind source separation, time delay estimation and location of two
continuous acoustic emission sources.
  Acoustic emission (AE) analysis is used for characterization and location of
developing defects in materials. AE sources often generate a mixture of various
statistically independent signals. A difficult problem of AE analysis is
separation and characterization of signal components when the signals from
various sources and the mode of mixing are unknown. Recently, blind source
separation (BSS) by independent component analysis (ICA) has been used to solve
these problems. The purpose of this paper is to demonstrate the applicability
of ICA to locate two independent simultaneously active acoustic emission
sources on an aluminum band specimen. The method is promising for
non-destructive testing of aircraft frame structures by acoustic emission
analysis.



On-line Viterbi Algorithm and Its Relationship to Random Walks

In this paper, we introduce the on-line Viterbi algorithm for decoding hidden
Markov models (HMMs) in much smaller than linear space. Our analysis on
two-state HMMs suggests that the expected maximum memory used to decode
sequence of length $n$ with $m$-state HMM can be as low as $\Theta(m\log n)$,
without a significant slow-down compared to the classical Viterbi algorithm.
Classical Viterbi algorithm requires $O(mn)$ space, which is impractical for
analysis of long DNA sequences (such as complete human genome chromosomes) and
for continuous data streams. We also experimentally demonstrate the performance
of the on-line Viterbi algorithm on a simple HMM for gene finding on both
simulated and real DNA sequences.



Real Options for Project Schedules (ROPS)

Real Options for Project Schedules (ROPS) has three recursive
sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)
optimization shell optimizes parameters of strategic Plans containing multiple
Projects containing ordered Tasks. A middle shell samples probability
distributions of durations of Tasks. An inner shell samples probability
distributions of costs of Tasks. PATHTREE is used to develop options on
schedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to
develop a relative risk analysis among projects.



Sparsely-spread CDMA - a statistical mechanics based analysis

Sparse Code Division Multiple Access (CDMA), a variation on the standard CDMA
method in which the spreading (signature) matrix contains only a relatively
small number of non-zero elements, is presented and analysed using methods of
statistical physics. The analysis provides results on the performance of
maximum likelihood decoding for sparse spreading codes in the large system
limit. We present results for both cases of regular and irregular spreading
matrices for the binary additive white Gaussian noise channel (BIAWGN) with a
comparison to the canonical (dense) random spreading code.



Reducing SAT to 2-SAT

Description of a polynomial time reduction of SAT to 2-SAT of polynomial
size.



Geometric Complexity Theory V: On deciding nonvanishing of a generalized Littlewood-Richardson coefficient

This article has been withdrawn because it has been merged with the earlier
article GCT3 (arXiv: CS/0501076 [cs.CC]) in the series. The merged article is
now available as:
  Geometric Complexity Theory III: on deciding nonvanishing of a
Littlewood-Richardson Coefficient, Journal of Algebraic Combinatorics, vol. 36,
issue 1, 2012, pp. 103-110. (Authors: Ketan Mulmuley, Hari Narayanan and Milind
Sohoni)
  The new article in this GCT5 slot in the series is:
  Geometric Complexity Theory V: Equivalence between blackbox derandomization
of polynomial identity testing and derandomization of Noether's Normalization
Lemma, in the Proceedings of FOCS 2012 (abstract), arXiv:1209.5993 [cs.CC]
(full version) (Author: Ketan Mulmuley)



Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding Matrix

Given a multiple-input multiple-output (MIMO) channel, feedback from the
receiver can be used to specify a transmit precoding matrix, which selectively
activates the strongest channel modes. Here we analyze the performance of
Random Vector Quantization (RVQ), in which the precoding matrix is selected
from a random codebook containing independent, isotropically distributed
entries. We assume that channel elements are i.i.d. and known to the receiver,
which relays the optimal (rate-maximizing) precoder codebook index to the
transmitter using B bits. We first derive the large system capacity of
beamforming (rank-one precoding matrix) as a function of B, where large system
refers to the limit as B and the number of transmit and receive antennas all go
to infinity with fixed ratios. With beamforming RVQ is asymptotically optimal,
i.e., no other quantization scheme can achieve a larger asymptotic rate. The
performance of RVQ is also compared with that of a simpler reduced-rank scalar
quantization scheme in which the beamformer is constrained to lie in a random
subspace. We subsequently consider a precoding matrix with arbitrary rank, and
approximate the asymptotic RVQ performance with optimal and linear receivers
(matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show
that these approximations accurately predict the performance of finite-size
systems of interest. Given a target spectral efficiency, numerical examples
show that the amount of feedback required by the linear MMSE receiver is only
slightly more than that required by the optimal receiver, whereas the matched
filter can require significantly more feedback.



On Almost Periodicity Criteria for Morphic Sequences in Some Particular Cases

In some particular cases we give criteria for morphic sequences to be almost
periodic (=uniformly recurrent). Namely, we deal with fixed points of
non-erasing morphisms and with automatic sequences. In both cases a
polynomial-time algorithm solving the problem is found. A result more or less
supporting the conjecture of decidability of the general problem is given.



Geometric Complexity Theory VI: the flip via saturated and positive integer programming in representation theory and algebraic geometry

This article belongs to a series on geometric complexity theory (GCT), an
approach to the P vs. NP and related problems through algebraic geometry and
representation theory. The basic principle behind this approach is called the
flip. In essence, it reduces the negative hypothesis in complexity theory (the
lower bound problems), such as the P vs. NP problem in characteristic zero, to
the positive hypothesis in complexity theory (the upper bound problems):
specifically, to showing that the problems of deciding nonvanishing of the
fundamental structural constants in representation theory and algebraic
geometry, such as the well known plethysm constants--or rather certain relaxed
forms of these decision probelms--belong to the complexity class P. In this
article, we suggest a plan for implementing the flip, i.e., for showing that
these relaxed decision problems belong to P. This is based on the reduction of
the preceding complexity-theoretic positive hypotheses to mathematical
positivity hypotheses: specifically, to showing that there exist positive
formulae--i.e. formulae with nonnegative coefficients--for the structural
constants under consideration and certain functions associated with them. These
turn out be intimately related to the similar positivity properties of the
Kazhdan-Lusztig polynomials and the multiplicative structural constants of the
canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum
groups. The known proofs of these positivity properties depend on the Riemann
hypothesis over finite fields and the related results. Thus the reduction here,
in conjunction with the flip, in essence, says that the validity of the P vs.
NP conjecture in characteristic zero is intimately linked to the Riemann
hypothesis over finite fields and related problems.



On Punctured Pragmatic Space-Time Codes in Block Fading Channel

This paper considers the use of punctured convolutional codes to obtain
pragmatic space-time trellis codes over block-fading channel. We show that good
performance can be achieved even when puncturation is adopted and that we can
still employ the same Viterbi decoder of the convolutional mother code by using
approximated metrics without increasing the complexity of the decoding
operations.



Differential Recursion and Differentially Algebraic Functions

Moore introduced a class of real-valued "recursive" functions by analogy with
Kleene's formulation of the standard recursive functions. While his concise
definition inspired a new line of research on analog computation, it contains
some technical inaccuracies. Focusing on his "primitive recursive" functions,
we pin down what is problematic and discuss possible attempts to remove the
ambiguity regarding the behavior of the differential recursion operator on
partial functions. It turns out that in any case the purported relation to
differentially algebraic functions, and hence to Shannon's model of analog
computation, fails.



The World as Evolving Information

This paper discusses the benefits of describing the world as information,
especially in the study of the evolution of life and cognition. Traditional
studies encounter problems because it is difficult to describe life and
cognition in terms of matter and energy, since their laws are valid only at the
physical scale. However, if matter and energy, as well as life and cognition,
are described in terms of information, evolution can be described consistently
as information becoming more complex.
  The paper presents eight tentative laws of information, valid at multiple
scales, which are generalizations of Darwinian, cybernetic, thermodynamic,
psychological, philosophical, and complexity principles. These are further used
to discuss the notions of life, cognition and their evolution.



The Complexity of HCP in Digraps with Degree Bound Two

The Hamiltonian cycle problem (HCP) in digraphs D with degree bound two is
solved by two mappings in this paper. The first bijection is between an
incidence matrix C_{nm} of simple digraph and an incidence matrix F of balanced
bipartite undirected graph G; The second mapping is from a perfect matching of
G to a cycle of D. It proves that the complexity of HCP in D is polynomial, and
finding a second non-isomorphism Hamiltonian cycle from a given Hamiltonian
digraph with degree bound two is also polynomial. Lastly it deduces P=NP base
on the results.



Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo Codes

It has been observed that particular rate-1/2 partially systematic parallel
concatenated convolutional codes (PCCCs) can achieve a lower error floor than
that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can
only be identified by means of an exhaustive search, whilst convergence towards
low bit error probabilities can be problematic when the systematic output of a
rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we
present and study a family of rate-1/2 partially systematic PCCCs, which we
call pseudo-randomly punctured codes. We evaluate their bit error rate
performance and we show that they always yield a lower error floor than that of
their rate-1/3 parent codes. Furthermore, we compare analytic results to
simulations and we demonstrate that their performance converges towards the
error floor region, owning to the moderate puncturing of their systematic
output. Consequently, we propose pseudo-random puncturing as a means of
improving the bandwidth efficiency of a PCCC and simultaneously lowering its
error floor.



Inapproximability of Maximum Weighted Edge Biclique and Its Applications

Given a bipartite graph $G = (V_1,V_2,E)$ where edges take on {\it both}
positive and negative weights from set $\mathcal{S}$, the {\it maximum weighted
edge biclique} problem, or $\mathcal{S}$-MWEB for short, asks to find a
bipartite subgraph whose sum of edge weights is maximized. This problem has
various applications in bioinformatics, machine learning and databases and its
(in)approximability remains open. In this paper, we show that for a wide range
of choices of $\mathcal{S}$, specifically when $| \frac{\min\mathcal{S}} {\max
\mathcal{S}} | \in \Omega(\eta^{\delta-1/2}) \cap O(\eta^{1/2-\delta})$ (where
$\eta = \max\{|V_1|, |V_2|\}$, and $\delta \in (0,1/2]$), no polynomial time
algorithm can approximate $\mathcal{S}$-MWEB within a factor of $n^{\epsilon}$
for some $\epsilon > 0$ unless $\mathsf{RP = NP}$. This hardness result gives
justification of the heuristic approaches adopted for various applied problems
in the aforementioned areas, and indicates that good approximation algorithms
are unlikely to exist. Specifically, we give two applications by showing that:
1) finding statistically significant biclusters in the SAMBA model, proposed in
\cite{Tan02} for the analysis of microarray data, is
$n^{\epsilon}$-inapproximable; and 2) no polynomial time algorithm exists for
the Minimum Description Length with Holes problem \cite{Bu05} unless
$\mathsf{RP=NP}$.



Refuting the Pseudo Attack on the REESSE1+ Cryptosystem

We illustrate through example 1 and 2 that the condition at theorem 1 in [8]
dissatisfies necessity, and the converse proposition of fact 1.1 in [8] does
not hold, namely the condition Z/M - L/Ak < 1/(2 Ak^2) is not sufficient for
f(i) + f(j) = f(k). Illuminate through an analysis and ex.3 that there is a
logic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4
to be invalid. Demonstrate through ex.4 and 5 that each or the combination of
qu+1 > qu * D at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) +
f(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4
and alg.2 based on table 1 are disordered and wrong logically. Further,
manifest through a repeated experiment and ex.5 that the data at table 2 is
falsified, and the example in [8] is woven elaborately. We explain why Cx = Ax
* W^f(x) (% M) is changed to Cx = (Ax * W^f(x))^d (% M) in REESSE1+ v2.1. To
the signature fraud, we point out that [8] misunderstands the existence of T^-1
and Q^-1 % (M-1), and forging of Q can be easily avoided through moving H.
Therefore, the conclusion of [8] that REESSE1+ is not secure at all (which
connotes that [8] can extract a related private key from any public key in
REESSE1+) is fully incorrect, and as long as the parameter Omega is fitly
selected, REESSE1+ with Cx = Ax * W^f(x) (% M) is secure.



Optimal Routing for Decode-and-Forward based Cooperation in Wireless Networks

We investigate cooperative wireless relay networks in which the nodes can
help each other in data transmission. We study different coding strategies in
the single-source single-destination network with many relay nodes. Given the
myriad of ways in which nodes can cooperate, there is a natural routing
problem, i.e., determining an ordered set of nodes to relay the data from the
source to the destination. We find that for a given route, the
decode-and-forward strategy, which is an information theoretic cooperative
coding strategy, achieves rates significantly higher than that achievable by
the usual multi-hop coding strategy, which is a point-to-point non-cooperative
coding strategy. We construct an algorithm to find an optimal route (in terms
of rate maximizing) for the decode-and-forward strategy. Since the algorithm
runs in factorial time in the worst case, we propose a heuristic algorithm that
runs in polynomial time. The heuristic algorithm outputs an optimal route when
the nodes transmit independent codewords. We implement these coding strategies
using practical low density parity check codes to compare the performance of
the strategies on different routes.



Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless Networks

This paper investigates the many-to-one throughput capacity (and by symmetry,
one-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has
generally been assumed in prior studies that the many-to-one throughput
capacity is upper-bounded by the link capacity L. Throughput capacity L is not
achievable under 802.11. This paper introduces the notion of "canonical
networks", which is a class of regularly-structured networks whose capacities
can be analyzed more easily than unstructured networks. We show that the
throughput capacity of canonical networks under 802.11 has an analytical upper
bound of 3L/4 when the source nodes are two or more hops away from the sink;
and simulated throughputs of 0.690L (0.740L) when the source nodes are many
hops away. We conjecture that 3L/4 is also the upper bound for general
networks. When all links have equal length, 2L/3 can be shown to be the upper
bound for general networks. Our simulations show that 802.11 networks with
random topologies operated with AODV routing can only achieve throughputs far
below the upper bounds. Fortunately, by properly selecting routes near the
gateway (or by properly positioning the relay nodes leading to the gateway) to
fashion after the structure of canonical networks, the throughput can be
improved significantly by more than 150%. Indeed, in a dense network, it is
worthwhile to deactivate some of the relay nodes near the sink judiciously.



On the Achievable Rate Regions for Interference Channels with Degraded Message Sets

The interference channel with degraded message sets (IC-DMS) refers to a
communication model in which two senders attempt to communicate with their
respective receivers simultaneously through a common medium, and one of the
senders has complete and a priori (non-causal) knowledge about the message
being transmitted by the other. A coding scheme that collectively has
advantages of cooperative coding, collaborative coding, and dirty paper coding,
is developed for such a channel. With resorting to this coding scheme,
achievable rate regions of the IC-DMS in both discrete memoryless and Gaussian
cases are derived, which, in general, include several previously known rate
regions. Numerical examples for the Gaussian case demonstrate that in the
high-interference-gain regime, the derived achievable rate regions offer
considerable improvements over these existing results.



A Low Complexity Algorithm and Architecture for Systematic Encoding of Hermitian Codes

We present an algorithm for systematic encoding of Hermitian codes. For a
Hermitian code defined over GF(q^2), the proposed algorithm achieves a run time
complexity of O(q^2) and is suitable for VLSI implementation. The encoder
architecture uses as main blocks q varying-rate Reed-Solomon encoders and
achieves a space complexity of O(q^2) in terms of finite field multipliers and
memory elements.



Learning from compressed observations

The problem of statistical learning is to construct a predictor of a random
variable $Y$ as a function of a related random variable $X$ on the basis of an
i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable
predictors are drawn from some specified class, and the goal is to approach
asymptotically the performance (expected loss) of the best predictor in the
class. We consider the setting in which one has perfect observation of the
$X$-part of the sample, while the $Y$-part has to be communicated at some
finite bit rate. The encoding of the $Y$-values is allowed to depend on the
$X$-values. Under suitable regularity conditions on the admissible predictors,
the underlying family of probability distributions and the loss function, we
give an information-theoretic characterization of achievable predictor
performance in terms of conditional distortion-rate functions. The ideas are
illustrated on the example of nonparametric regression in Gaussian noise.



Revisiting the Issues On Netflow Sample and Export Performance

The high volume of packets and packet rates of traffic on some router links
makes it exceedingly difficult for routers to examine every packet in order to
keep detailed statistics about the traffic which is traversing the router.
Sampling is commonly applied on routers in order to limit the load incurred by
the collection of information that the router has to undertake when evaluating
flow information for monitoring purposes. The sampling process in nearly all
cases is a deterministic process of choosing 1 in every N packets on a
per-interface basis, and then forming the flow statistics based on the
collected sampled statistics. Even though this sampling may not be significant
for some statistics, such as packet rate, others can be severely distorted.
However, it is important to consider the sampling techniques and their relative
accuracy when applied to different traffic patterns. The main disadvantage of
sampling is the loss of accuracy in the collected trace when compared to the
original traffic stream. To date there has not been a detailed analysis of the
impact of sampling at a router in various traffic profiles and flow criteria.
In this paper, we assess the performance of the sampling process as used in
NetFlow in detail, and we discuss some techniques for the compensation of loss
of monitoring detail.



Optimal Synthesis of Multiple Algorithms

In this paper we give a definition of "algorithm," "finite algorithm,"
"equivalent algorithms," and what it means for a single algorithm to dominate a
set of algorithms. We define a derived algorithm which may have a smaller mean
execution time than any of its component algorithms. We give an explicit
expression for the mean execution time (when it exists) of the derived
algorithm. We give several illustrative examples of derived algorithms with two
component algorithms. We include mean execution time solutions for
two-algorithm processors whose joint density of execution times are of several
general forms. For the case in which the joint density for a two-algorithm
processor is a step function, we give a maximum-likelihood estimation scheme
with which to analyze empirical processing time data.



Hybrid-ARQ in Multihop Networks with Opportunistic Relay Selection

This paper develops a contention-based opportunistic feedback technique
towards relay selection in a dense wireless network. This technique enables the
forwarding of additional parity information from the selected relay to the
destination. For a given network, the effects of varying key parameters such as
the feedback probability are presented and discussed. A primary advantage of
the proposed technique is that relay selection can be performed in a
distributed way. Simulation results find its performance to closely match that
of centralized schemes that utilize GPS information, unlike the proposed
method. The proposed relay selection method is also found to achieve throughput
gains over a point-to-point transmission strategy.



Opportunistic Relay Selection with Limited Feedback

It has been shown that a decentralized relay selection protocol based on
opportunistic feedback from the relays yields good throughput performance in
dense wireless networks. This selection strategy supports a hybrid-ARQ
transmission approach where relays forward parity information to the
destination in the event of a decoding error. Such an approach, however,
suffers a loss compared to centralized strategies that select relays with the
best channel gain to the destination. This paper closes the performance gap by
adding another level of channel feedback to the decentralized relay selection
problem. It is demonstrated that only one additional bit of feedback is
necessary for good throughput performance. The performance impact of varying
key parameters such as the number of relays and the channel feedback threshold
is discussed. An accompanying bit error rate analysis demonstrates the
importance of relay selection.



On packet lengths and overhead for random linear coding over the erasure channel

We assess the practicality of random network coding by illuminating the issue
of overhead and considering it in conjunction with increasingly long packets
sent over the erasure channel. We show that the transmission of increasingly
long packets, consisting of either of an increasing number of symbols per
packet or an increasing symbol alphabet size, results in a data rate
approaching zero over the erasure channel. This result is due to an erasure
probability that increases with packet length. Numerical results for a
particular modulation scheme demonstrate a data rate of approximately zero for
a large, but finite-length packet. Our results suggest a reduction in the
performance gains offered by random network coding.



P-adic arithmetic coding

A new incremental algorithm for data compression is presented. For a sequence
of input symbols algorithm incrementally constructs a p-adic integer number as
an output. Decoding process starts with less significant part of a p-adic
integer and incrementally reconstructs a sequence of input symbols. Algorithm
is based on certain features of p-adic numbers and p-adic norm. p-adic coding
algorithm may be considered as of generalization a popular compression
technique - arithmetic coding algorithms. It is shown that for p = 2 the
algorithm works as integer variant of arithmetic coding; for a special class of
models it gives exactly the same codes as Huffman's algorithm, for another
special model and a specific alphabet it gives Golomb-Rice codes.



Universal Source Coding for Monotonic and Fast Decaying Monotonic Distributions

We study universal compression of sequences generated by monotonic
distributions. We show that for a monotonic distribution over an alphabet of
size $k$, each probability parameter costs essentially $0.5 \log (n/k^3)$ bits,
where $n$ is the coded sequence length, as long as $k = o(n^{1/3})$. Otherwise,
for $k = O(n)$, the total average sequence redundancy is $O(n^{1/3+\epsilon})$
bits overall. We then show that there exists a sub-class of monotonic
distributions over infinite alphabets for which redundancy of
$O(n^{1/3+\epsilon})$ bits overall is still achievable. This class contains
fast decaying distributions, including many distributions over the integers and
geometric distributions. For some slower decays, including other distributions
over the integers, redundancy of $o(n)$ bits overall is achievable, where a
method to compute specific redundancy rates for such distributions is derived.
The results are specifically true for finite entropy monotonic distributions.
Finally, we study individual sequence redundancy behavior assuming a sequence
is governed by a monotonic distribution. We show that for sequences whose
empirical distributions are monotonic, individual redundancy bounds similar to
those in the average case can be obtained. However, even if the monotonicity in
the empirical distribution is violated, diminishing per symbol individual
sequence redundancies with respect to the monotonic maximum likelihood
description length may still be achievable.



Lessons Learned from the deployment of a high-interaction honeypot

This paper presents an experimental study and the lessons learned from the
observation of the attackers when logged on a compromised machine. The results
are based on a six months period during which a controlled experiment has been
run with a high interaction honeypot. We correlate our findings with those
obtained with a worldwide distributed system of lowinteraction honeypots.



Availability assessment of SunOS/Solaris Unix Systems based on Syslogd and wtmpx logfiles : a case study

This paper presents a measurement-based availability assessment study using
field data collected during a 4-year period from 373 SunOS/Solaris Unix
workstations and servers interconnected through a local area network. We focus
on the estimation of machine uptimes, downtimes and availability based on the
identification of failures that caused total service loss. Data corresponds to
syslogd event logs that contain a large amount of information about the normal
activity of the studied systems as well as their behavior in the presence of
failures. It is widely recognized that the information contained in such event
logs might be incomplete or imperfect. The solution investigated in this paper
to address this problem is based on the use of auxiliary sources of data
obtained from wtmpx files maintained by the SunOS/Solaris Unix operating
system. The results obtained suggest that the combined use of wtmpx and syslogd
log files provides more complete information on the state of the target systems
that is useful to provide availability estimations that better reflect reality.



Empirical analysis and statistical modeling of attack processes based on honeypots

Honeypots are more and more used to collect data on malicious activities on
the Internet and to better understand the strategies and techniques used by
attackers to compromise target systems. Analysis and modeling methodologies are
needed to support the characterization of attack processes based on the data
collected from the honeypots. This paper presents some empirical analyses based
on the data collected from the Leurr{\'e}.com honeypot platforms deployed on
the Internet and presents some preliminary modeling studies aimed at fulfilling
such objectives.



An architecture-based dependability modeling framework using AADL

For efficiency reasons, the software system designers' will is to use an
integrated set of methods and tools to describe specifications and designs, and
also to perform analyses such as dependability, schedulability and performance.
AADL (Architecture Analysis and Design Language) has proved to be efficient for
software architecture modeling. In addition, AADL was designed to accommodate
several types of analyses. This paper presents an iterative dependency-driven
approach for dependability modeling using AADL. It is illustrated on a small
example. This approach is part of a complete framework that allows the
generation of dependability analysis and evaluation models from AADL models to
support the analysis of software and system architectures, in critical
application domains.



A Hierarchical Approach for Dependability Analysis of a Commercial Cache-Based RAID Storage Architecture

We present a hierarchical simulation approach for the dependability analysis
and evaluation of a highly available commercial cache-based RAID storage
system. The archi-tecture is complex and includes several layers of
overlap-ping error detection and recovery mechanisms. Three ab-straction levels
have been developed to model the cache architecture, cache operations, and
error detection and recovery mechanism. The impact of faults and errors
oc-curring in the cache and in the disks is analyzed at each level of the
hierarchy. A simulation submodel is associated with each abstraction level. The
models have been devel-oped using DEPEND, a simulation-based environment for
system-level dependability analysis, which provides facili-ties to inject
faults into a functional behavior model, to simulate error detection and
recovery mechanisms, and to evaluate quantitative measures. Several fault
models are defined for each submodel to simulate cache component failures, disk
failures, transmission errors, and data errors in the cache memory and in the
disks. Some of the parame-ters characterizing fault injection in a given
submodel cor-respond to probabilities evaluated from the simulation of the
lower-level submodel. Based on the proposed method-ology, we evaluate and
analyze 1) the system behavior un-der a real workload and high error rate
(focusing on error bursts), 2) the coverage of the error detection mechanisms
implemented in the system and the error latency distribu-tions, and 3) the
accumulation of errors in the cache and in the disks.



Sensor Networks with Random Links: Topology Design for Distributed Consensus

In a sensor network, in practice, the communication among sensors is subject
to:(1) errors or failures at random times; (3) costs; and(2) constraints since
sensors and networks operate under scarce resources, such as power, data rate,
or communication. The signal-to-noise ratio (SNR) is usually a main factor in
determining the probability of error (or of communication failure) in a link.
These probabilities are then a proxy for the SNR under which the links operate.
The paper studies the problem of designing the topology, i.e., assigning the
probabilities of reliable communication among sensors (or of link failures) to
maximize the rate of convergence of average consensus, when the link
communication costs are taken into account, and there is an overall
communication budget constraint. To consider this problem, we address a number
of preliminary issues: (1) model the network as a random topology; (2)
establish necessary and sufficient conditions for mean square sense (mss) and
almost sure (a.s.) convergence of average consensus when network links fail;
and, in particular, (3) show that a necessary and sufficient condition for both
mss and a.s. convergence is for the algebraic connectivity of the mean graph
describing the network topology to be strictly positive. With these results, we
formulate topology design, subject to random link failures and to a
communication cost constraint, as a constrained convex optimization problem to
which we apply semidefinite programming techniques. We show by an extensive
numerical study that the optimal design improves significantly the convergence
speed of the consensus algorithm and can achieve the asymptotic performance of
a non-random network at a fraction of the communication cost.



Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian Vector Broadcast Channels

MIMO technology is one of the most significant advances in the past decade to
increase channel capacity and has a great potential to improve network capacity
for mesh networks. In a MIMO-based mesh network, the links outgoing from each
node sharing the common communication spectrum can be modeled as a Gaussian
vector broadcast channel. Recently, researchers showed that ``dirty paper
coding'' (DPC) is the optimal transmission strategy for Gaussian vector
broadcast channels. So far, there has been little study on how this fundamental
result will impact the cross-layer design for MIMO-based mesh networks. To fill
this gap, we consider the problem of jointly optimizing DPC power allocation in
the link layer at each node and multihop/multipath routing in a MIMO-based mesh
networks. It turns out that this optimization problem is a very challenging
non-convex problem. To address this difficulty, we transform the original
problem to an equivalent problem by exploiting the channel duality. For the
transformed problem, we develop an efficient solution procedure that integrates
Lagrangian dual decomposition method, conjugate gradient projection method
based on matrix differential calculus, cutting-plane method, and subgradient
method. In our numerical example, it is shown that we can achieve a network
performance gain of 34.4% by using DPC.



Architecture for Pseudo Acausal Evolvable Embedded Systems

Advances in semiconductor technology are contributing to the increasing
complexity in the design of embedded systems. Architectures with novel
techniques such as evolvable nature and autonomous behavior have engrossed lot
of attention. This paper demonstrates conceptually evolvable embedded systems
can be characterized basing on acausal nature. It is noted that in acausal
systems, future input needs to be known, here we make a mechanism such that the
system predicts the future inputs and exhibits pseudo acausal nature. An
embedded system that uses theoretical framework of acausality is proposed. Our
method aims at a novel architecture that features the hardware evolability and
autonomous behavior alongside pseudo acausality. Various aspects of this
architecture are discussed in detail along with the limitations.



The on-line shortest path problem under partial monitoring

The on-line shortest path problem is considered under various models of
partial monitoring. Given a weighted directed acyclic graph whose edge weights
can change in an arbitrary (adversarial) way, a decision maker has to choose in
each round of a game a path between two distinguished vertices such that the
loss of the chosen path (defined as the sum of the weights of its composing
edges) be as small as possible. In a setting generalizing the multi-armed
bandit problem, after choosing a path, the decision maker learns only the
weights of those edges that belong to the chosen path. For this problem, an
algorithm is given whose average cumulative loss in n rounds exceeds that of
the best path, matched off-line to the entire sequence of the edge weights, by
a quantity that is proportional to 1/\sqrt{n} and depends only polynomially on
the number of edges of the graph. The algorithm can be implemented with linear
complexity in the number of rounds n and in the number of edges. An extension
to the so-called label efficient setting is also given, in which the decision
maker is informed about the weights of the edges corresponding to the chosen
path at a total of m << n time instances. Another extension is shown where the
decision maker competes against a time-varying path, a generalization of the
problem of tracking the best expert. A version of the multi-armed bandit
setting for shortest path is also discussed where the decision maker learns
only the total weight of the chosen path but not the weights of the individual
edges on the path. Applications to routing in packet switched networks along
with simulation results are also presented.



A neural network approach to ordinal regression

Ordinal regression is an important type of learning, which has properties of
both classification and regression. Here we describe a simple and effective
approach to adapt a traditional neural network to learn ordinal categories. Our
approach is a generalization of the perceptron method for ordinal regression.
On several benchmark datasets, our method (NNRank) outperforms a neural network
classification method. Compared with the ordinal regression methods using
Gaussian processes and support vector machines, NNRank achieves comparable
performance. Moreover, NNRank has the advantages of traditional neural
networks: learning in both online and batch modes, handling very large training
datasets, and making rapid predictions. These features make NNRank a useful and
complementary tool for large-scale data processing tasks such as information
retrieval, web page ranking, collaborative filtering, and protein ranking in
Bioinformatics.



On the Kolmogorov-Chaitin Complexity for short sequences

A drawback of Kolmogorov-Chaitin complexity (K) as a function from s to the
shortest program producing s is its noncomputability which limits its range of
applicability. Moreover, when strings are short, the dependence of K on a
particular universal Turing machine U can be arbitrary. In practice one can
approximate it by computable compression methods. However, such compression
methods do not always provide meaningful approximations--for strings shorter,
for example, than typical compiler lengths. In this paper we suggest an
empirical approach to overcome this difficulty and to obtain a stable
definition of the Kolmogorov-Chaitin complexity for short sequences.
Additionally, a correlation in terms of distribution frequencies was found
across the output of two models of abstract machines, namely unidimensional
cellular automata and deterministic Turing machine.



Fast paths in large-scale dynamic road networks

Efficiently computing fast paths in large scale dynamic road networks (where
dynamic traffic information is known over a part of the network) is a practical
problem faced by several traffic information service providers who wish to
offer a realistic fast path computation to GPS terminal enabled vehicles. The
heuristic solution method we propose is based on a highway hierarchy-based
shortest path algorithm for static large-scale networks; we maintain a static
highway hierarchy and perform each query on the dynamically evaluated network.



Differential Diversity Reception of MDPSK over Independent Rayleigh Channels with Nonidentical Branch Statistics and Asymmetric Fading Spectrum

This paper is concerned with optimum diversity receiver structure and its
performance analysis of differential phase shift keying (DPSK) with
differential detection over nonselective, independent, nonidentically
distributed, Rayleigh fading channels. The fading process in each branch is
assumed to have an arbitrary Doppler spectrum with arbitrary Doppler bandwidth,
but to have distinct, asymmetric fading power spectral density characteristic.
Using 8-DPSK as an example, the average bit error probability (BEP) of the
optimum diversity receiver is obtained by calculating the BEP for each of the
three individual bits. The BEP results derived are given in exact, explicit,
closed-form expressions which show clearly the behavior of the performance as a
function of various system parameters.



Novelty and Collective Attention

The subject of collective attention is central to an information age where
millions of people are inundated with daily messages. It is thus of interest to
understand how attention to novel items propagates and eventually fades among
large populations. We have analyzed the dynamics of collective attention among
one million users of an interactive website -- \texttt{digg.com} -- devoted to
thousands of novel news stories. The observations can be described by a
dynamical model characterized by a single novelty factor. Our measurements
indicate that novelty within groups decays with a stretched-exponential law,
suggesting the existence of a natural time scale over which attention fades.



Novel algorithm to calculate hypervolume indicator of Pareto approximation set

Hypervolume indicator is a commonly accepted quality measure for comparing
Pareto approximation set generated by multi-objective optimizers. The best
known algorithm to calculate it for $n$ points in $d$-dimensional space has a
run time of $O(n^{d/2})$ with special data structures. This paper presents a
recursive, vertex-splitting algorithm for calculating the hypervolume indicator
of a set of $n$ non-comparable points in $d>2$ dimensions. It splits out
multiple child hyper-cuboids which can not be dominated by a splitting
reference point. In special, the splitting reference point is carefully chosen
to minimize the number of points in the child hyper-cuboids. The complexity
analysis shows that the proposed algorithm achieves $O((\frac{d}{2})^n)$ time
and $O(dn^2)$ space complexity in the worst case.



A Doubly Distributed Genetic Algorithm for Network Coding

We present a genetic algorithm which is distributed in two novel ways: along
genotype and temporal axes. Our algorithm first distributes, for every member
of the population, a subset of the genotype to each network node, rather than a
subset of the population to each. This genotype distribution is shown to offer
a significant gain in running time. Then, for efficient use of the
computational resources in the network, our algorithm divides the candidate
solutions into pipelined sets and thus the distribution is in the temporal
domain, rather that in the spatial domain. This temporal distribution may lead
to temporal inconsistency in selection and replacement, however our experiments
yield better efficiency in terms of the time to convergence without incurring
significant penalties.



Text Line Segmentation of Historical Documents: a Survey

There is a huge amount of historical documents in libraries and in various
National Archives that have not been exploited electronically. Although
automatic reading of complete pages remains, in most cases, a long-term
objective, tasks such as word spotting, text/image alignment, authentication
and extraction of specific fields are in use today. For all these tasks, a
major step is document segmentation into text lines. Because of the low quality
and the complexity of these documents (background noise, artifacts due to
aging, interfering lines),automatic text line segmentation remains an open
research field. The objective of this paper is to present a survey of existing
methods, developed during the last decade, and dedicated to documents of
historical interest.



Phase Transitions in the Coloring of Random Graphs

We consider the problem of coloring the vertices of a large sparse random
graph with a given number of colors so that no adjacent vertices have the same
color. Using the cavity method, we present a detailed and systematic analytical
study of the space of proper colorings (solutions).
  We show that for a fixed number of colors and as the average vertex degree
(number of constraints) increases, the set of solutions undergoes several phase
transitions similar to those observed in the mean field theory of glasses.
First, at the clustering transition, the entropically dominant part of the
phase space decomposes into an exponential number of pure states so that beyond
this transition a uniform sampling of solutions becomes hard. Afterward, the
space of solutions condenses over a finite number of the largest states and
consequently the total entropy of solutions becomes smaller than the annealed
one. Another transition takes place when in all the entropically dominant
states a finite fraction of nodes freezes so that each of these nodes is
allowed a single color in all the solutions inside the state. Eventually, above
the coloring threshold, no more solutions are available. We compute all the
critical connectivities for Erdos-Renyi and regular random graphs and determine
their asymptotic values for large number of colors.
  Finally, we discuss the algorithmic consequences of our findings. We argue
that the onset of computational hardness is not associated with the clustering
transition and we suggest instead that the freezing transition might be the
relevant phenomenon. We also discuss the performance of a simple local Walk-COL
algorithm and of the belief propagation algorithm in the light of our results.



Parametric Learning and Monte Carlo Optimization

This paper uncovers and explores the close relationship between Monte Carlo
Optimization of a parametrized integral (MCO), Parametric machine-Learning
(PL), and `blackbox' or `oracle'-based optimization (BO). We make four
contributions. First, we prove that MCO is mathematically identical to a broad
class of PL problems. This identity potentially provides a new application
domain for all broadly applicable PL techniques: MCO. Second, we introduce
immediate sampling, a new version of the Probability Collectives (PC) algorithm
for blackbox optimization. Immediate sampling transforms the original BO
problem into an MCO problem. Accordingly, by combining these first two
contributions, we can apply all PL techniques to BO. In our third contribution
we validate this way of improving BO by demonstrating that cross-validation and
bagging improve immediate sampling. Finally, conventional MC and MCO procedures
ignore the relationship between the sample point locations and the associated
values of the integrand; only the values of the integrand at those locations
are considered. We demonstrate that one can exploit the sample location
information using PL techniques, for example by forming a fit of the sample
locations to the associated values of the integrand. This provides an
additional way to apply PL techniques to improve MCO.



A Disciplined Approach to Adopting Agile Practices: The Agile Adoption Framework

Many organizations aspire to adopt agile processes to take advantage of the
numerous benefits that it offers to an organization. Those benefits include,
but are not limited to, quicker return on investment, better software quality,
and higher customer satisfaction. To date however, there is no structured
process (at least in the public domain) that guides organizations in adopting
agile practices. To address this problem we present the Agile Adoption
Framework. The framework consists of two components: an agile measurement
index, and a 4-Stage process, that together guide and assist the agile adoption
efforts of organizations. More specifically, the agile measurement index is
used to identify the agile potential of projects and organizations. The 4-Stage
process, on the other hand, helps determine (a) whether or not organizations
are ready for agile adoption, and (b) guided by their potential, what set of
agile practices can and should be introduced.



Antenna Combining for the MIMO Downlink Channel

A multiple antenna downlink channel where limited channel feedback is
available to the transmitter is considered. In a vector downlink channel
(single antenna at each receiver), the transmit antenna array can be used to
transmit separate data streams to multiple receivers only if the transmitter
has very accurate channel knowledge, i.e., if there is high-rate channel
feedback from each receiver. In this work it is shown that channel feedback
requirements can be significantly reduced if each receiver has a small number
of antennas and appropriately combines its antenna outputs. A combining method
that minimizes channel quantization error at each receiver, and thereby
minimizes multi-user interference, is proposed and analyzed. This technique is
shown to outperform traditional techniques such as maximum-ratio combining
because minimization of interference power is more critical than maximization
of signal power in the multiple antenna downlink. Analysis is provided to
quantify the feedback savings, and the technique is seen to work well with user
selection and is also robust to receiver estimation error.



Low Density Lattice Codes

Low density lattice codes (LDLC) are novel lattice codes that can be decoded
efficiently and approach the capacity of the additive white Gaussian noise
(AWGN) channel. In LDLC a codeword x is generated directly at the n-dimensional
Euclidean space as a linear transformation of a corresponding integer message
vector b, i.e., x = Gb, where H, the inverse of G, is restricted to be sparse.
The fact that H is sparse is utilized to develop a linear-time iterative
decoding scheme which attains, as demonstrated by simulations, good error
performance within ~0.5dB from capacity at block length of n = 100,000 symbols.
The paper also discusses convergence results and implementation considerations.



Supporting Knowledge and Expertise Finding within Australia's Defence Science and Technology Organisation

This paper reports on work aimed at supporting knowledge and expertise
finding within a large Research and Development (R&D) organisation. The paper
first discusses the nature of knowledge important to R&D organisations and
presents a prototype information system developed to support knowledge and
expertise finding. The paper then discusses a trial of the system within an R&D
organisation, the implications and limitations of the trial, and discusses
future research questions.



Distance preserving mappings from ternary vectors to permutations

Distance-preserving mappings (DPMs) are mappings from the set of all q-ary
vectors of a fixed length to the set of permutations of the same or longer
length such that every two distinct vectors are mapped to permutations with the
same or even larger Hamming distance than that of the vectors. In this paper,
we propose a construction of DPMs from ternary vectors. The constructed DPMs
improve the lower bounds on the maximal size of permutation arrays.



A Language-Based Approach for Improving the Robustness of Network Application Protocol Implementations

The secure and robust functioning of a network relies on the defect-free
implementation of network applications. As network protocols have become
increasingly complex, however, hand-writing network message processing code has
become increasingly error-prone. In this paper, we present a domain-specific
language, Zebu, for describing protocol message formats and related processing
constraints. From a Zebu specification, a compiler automatically generates
stubs to be used by an application to parse network messages. Zebu is easy to
use, as it builds on notations used in RFCs to describe protocol grammars. Zebu
is also efficient, as the memory usage is tailored to application needs and
message fragments can be specified to be processed on demand. Finally,
Zebu-based applications are robust, as the Zebu compiler automatically checks
specification consistency and generates parsing stubs that include validation
of the message structure. Using a mutation analysis in the context of SIP and
RTSP, we show that Zebu significantly improves application robustness.



Calculating Valid Domains for BDD-Based Interactive Configuration

In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework.



Preconditioned Temporal Difference Learning

This paper has been withdrawn by the author. This draft is withdrawn for its
poor quality in english, unfortunately produced by the author when he was just
starting his science route. Look at the ICML version instead:
http://icml2008.cs.helsinki.fi/papers/111.pdf



Trellis-Coded Quantization Based on Maximum-Hamming-Distance Binary Codes

Most design approaches for trellis-coded quantization take advantage of the
duality of trellis-coded quantization with trellis-coded modulation, and use
the same empirically-found convolutional codes to label the trellis branches.
This letter presents an alternative approach that instead takes advantage of
maximum-Hamming-distance convolutional codes. The proposed source codes are
shown to be competitive with the best in the literature for the same
computational complexity.



A Better Good-Turing Estimator for Sequence Probabilities

We consider the problem of estimating the probability of an observed string
drawn i.i.d. from an unknown distribution. The key feature of our study is that
the length of the observed string is assumed to be of the same order as the
size of the underlying alphabet. In this setting, many letters are unseen and
the empirical distribution tends to overestimate the probability of the
observed letters. To overcome this problem, the traditional approach to
probability estimation is to use the classical Good-Turing estimator. We
introduce a natural scaling model and use it to show that the Good-Turing
sequence probability estimator is not consistent. We then introduce a novel
sequence probability estimator that is indeed consistent under the natural
scaling model.



GLRT-Optimal Noncoherent Lattice Decoding

This paper presents new low-complexity lattice-decoding algorithms for
noncoherent block detection of QAM and PAM signals over complex-valued fading
channels. The algorithms are optimal in terms of the generalized likelihood
ratio test (GLRT). The computational complexity is polynomial in the block
length; making GLRT-optimal noncoherent detection feasible for implementation.
We also provide even lower complexity suboptimal algorithms. Simulations show
that the suboptimal algorithms have performance indistinguishable from the
optimal algorithms. Finally, we consider block based transmission, and propose
to use noncoherent detection as an alternative to pilot assisted transmission
(PAT). The new technique is shown to outperform PAT.



On restrictions of balanced 2-interval graphs

The class of 2-interval graphs has been introduced for modelling scheduling
and allocation problems, and more recently for specific bioinformatic problems.
Some of those applications imply restrictions on the 2-interval graphs, and
justify the introduction of a hierarchy of subclasses of 2-interval graphs that
generalize line graphs: balanced 2-interval graphs, unit 2-interval graphs, and
(x,x)-interval graphs. We provide instances that show that all the inclusions
are strict. We extend the NP-completeness proof of recognizing 2-interval
graphs to the recognition of balanced 2-interval graphs. Finally we give hints
on the complexity of unit 2-interval graphs recognition, by studying
relationships with other graph classes: proper circular-arc, quasi-line graphs,
K_{1,5}-free graphs, ...



Exploiting Social Annotation for Automatic Resource Discovery

Information integration applications, such as mediators or mashups, that
require access to information resources currently rely on users manually
discovering and integrating them in the application. Manual resource discovery
is a slow process, requiring the user to sift through results obtained via
keyword-based search. Although search methods have advanced to include evidence
from document contents, its metadata and the contents and link structure of the
referring pages, they still do not adequately cover information sources --
often called ``the hidden Web''-- that dynamically generate documents in
response to a query. The recently popular social bookmarking sites, which allow
users to annotate and share metadata about various information sources, provide
rich evidence for resource discovery. In this paper, we describe a
probabilistic model of the user annotation process in a social bookmarking
system del.icio.us. We then use the model to automatically find resources
relevant to a particular information domain. Our experimental results on data
obtained from \emph{del.icio.us} show this approach as a promising method for
helping automate the resource discovery task.



Personalizing Image Search Results on Flickr

The social media site Flickr allows users to upload their photos, annotate
them with tags, submit them to groups, and also to form social networks by
adding other users as contacts. Flickr offers multiple ways of browsing or
searching it. One option is tag search, which returns all images tagged with a
specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an
insect or a car, tag search results will include many images that are not
relevant to the sense the user had in mind when executing the query. We claim
that users express their photography interests through the metadata they add in
the form of contacts and image annotations. We show how to exploit this
metadata to personalize search results for the user, thereby improving search
performance. First, we show that we can significantly improve search precision
by filtering tag search results by user's contacts or a larger social network
that includes those contact's contacts. Secondly, we describe a probabilistic
model that takes advantage of tag information to discover latent topics
contained in the search results. The users' interests can similarly be
described by the tags they used for annotating their images. The latent topics
found by the model are then used to personalize search results by finding
images on topics that are of interest to the user.



Settling the Complexity of Computing Two-Player Nash Equilibria

We settle a long-standing open question in algorithmic game theory. We prove
that Bimatrix, the problem of finding a Nash equilibrium in a two-player game,
is complete for the complexity class PPAD Polynomial Parity Argument, Directed
version) introduced by Papadimitriou in 1991.
  This is the first of a series of results concerning the complexity of Nash
equilibria. In particular, we prove the following theorems:
  Bimatrix does not have a fully polynomial-time approximation scheme unless
every problem in PPAD is solvable in polynomial time. The smoothed complexity
of the classic Lemke-Howson algorithm and, in fact, of any algorithm for
Bimatrix is not polynomial unless every problem in PPAD is solvable in
randomized polynomial time. Our results demonstrate that, even in the simplest
form of non-cooperative games, equilibrium computation and approximation are
polynomial-time equivalent to fixed point computation. Our results also have
two broad complexity implications in mathematical economics and operations
research: Arrow-Debreu market equilibria are PPAD-hard to compute. The P-Matrix
Linear Complementary Problem is computationally harder than convex programming
unless every problem in PPAD is solvable in polynomial time.



Locally Decodable Codes From Nice Subsets of Finite Fields and Prime Factors of Mersenne Numbers

A k-query Locally Decodable Code (LDC) encodes an n-bit message x as an N-bit
codeword C(x), such that one can probabilistically recover any bit x_i of the
message by querying only k bits of the codeword C(x), even after some constant
fraction of codeword bits has been corrupted. The major goal of LDC related
research is to establish the optimal trade-off between length and query
complexity of such codes.
  Recently [Y] introduced a novel technique for constructing locally decodable
codes and vastly improved the upper bounds for code length. The technique is
based on Mersenne primes. In this paper we extend the work of [Y] and argue
that further progress via these methods is tied to progress on an old number
theory question regarding the size of the largest prime factors of Mersenne
numbers.
  Specifically, we show that every Mersenne number m=2^t-1 that has a prime
factor p>m^\gamma yields a family of k(\gamma)-query locally decodable codes of
length Exp(n^{1/t}). Conversely, if for some fixed k and all \epsilon > 0 one
can use the technique of [Y] to obtain a family of k-query LDCs of length
Exp(n^\epsilon); then infinitely many Mersenne numbers have prime factors arger
than known currently.



A Cut-free Sequent Calculus for Bi-Intuitionistic Logic: Extended Version

Bi-intuitionistic logic is the extension of intuitionistic logic with a
connective dual to implication. Bi-intuitionistic logic was introduced by
Rauszer as a Hilbert calculus with algebraic and Kripke semantics. But her
subsequent ``cut-free'' sequent calculus for BiInt has recently been shown by
Uustalu to fail cut-elimination. We present a new cut-free sequent calculus for
BiInt, and prove it sound and complete with respect to its Kripke semantics.
Ensuring completeness is complicated by the interaction between implication and
its dual, similarly to future and past modalities in tense logic. Our calculus
handles this interaction using extended sequents which pass information from
premises to conclusions using variables instantiated at the leaves of failed
derivation trees. Our simple termination argument allows our calculus to be
used for automated deduction, although this is not its main purpose.



Traitement Des Donnees Manquantes Au Moyen De L'Algorithme De Kohonen

Nous montrons comment il est possible d'utiliser l'algorithme d'auto
organisation de Kohonen pour traiter des donn\'ees avec valeurs manquantes et
estimer ces derni\`eres. Apr\`es un rappel m\'ethodologique, nous illustrons
notre propos \`a partir de trois applications \`a des donn\'ees r\'eelles.
  -----
  We show how it is possible to use the Kohonen self-organizing algorithm to
deal with data which contain missing values and to estimate them. After a
methodological recall, we illustrate our purpose from three real databases
applications.



Self-Organization applied to Dynamic Network Layout

As networks and their structure have become a major field of research, a
strong demand for network visualization has emerged. We address this challenge
by formalizing the well established spring layout in terms of dynamic
equations. We thus open up the design space for new algorithms. Drawing from
the knowledge of systems design, we derive a layout algorithm that remedies
several drawbacks of the original spring layout. This new algorithm relies on
the balancing of two antagonistic forces. We thus call it {\em arf} for
"attractive and repulsive forces". It is, as we claim, particularly suited for
a dynamic layout of smaller networks ($n < 10^3$). We back this claim with
several application examples from on going complex systems research.



Information Theoretic Proofs of Entropy Power Inequalities

While most useful information theoretic inequalities can be deduced from the
basic properties of entropy or mutual information, up to now Shannon's entropy
power inequality (EPI) is an exception: Existing information theoretic proofs
of the EPI hinge on representations of differential entropy using either Fisher
information or minimum mean-square error (MMSE), which are derived from de
Bruijn's identity. In this paper, we first present an unified view of these
proofs, showing that they share two essential ingredients: 1) a data processing
argument applied to a covariance-preserving linear transformation; 2) an
integration over a path of a continuous Gaussian perturbation. Using these
ingredients, we develop a new and brief proof of the EPI through a mutual
information inequality, which replaces Stam and Blachman's Fisher information
inequality (FII) and an inequality for MMSE by Guo, Shamai and Verd\'u used in
earlier proofs. The result has the advantage of being very simple in that it
relies only on the basic properties of mutual information. These ideas are then
generalized to various extended versions of the EPI: Zamir and Feder's
generalized EPI for linear transformations of the random variables, Takano and
Johnson's EPI for dependent variables, Liu and Viswanath's
covariance-constrained EPI, and Costa's concavity inequality for the entropy
power.



The Invar Tensor Package

The Invar package is introduced, a fast manipulator of generic scalar
polynomial expressions formed from the Riemann tensor of a four-dimensional
metric-compatible connection. The package can maximally simplify any polynomial
containing tensor products of up to seven Riemann tensors within seconds. It
has been implemented both in Mathematica and Maple algebraic systems.



Assessment and Propagation of Input Uncertainty in Tree-based Option Pricing Models

This paper aims to provide a practical example on the assessment and
propagation of input uncertainty for option pricing when using tree-based
methods. Input uncertainty is propagated into output uncertainty, reflecting
that option prices are as unknown as the inputs they are based on. Option
pricing formulas are tools whose validity is conditional not only on how close
the model represents reality, but also on the quality of the inputs they use,
and those inputs are usually not observable. We provide three alternative
frameworks to calibrate option pricing tree models, propagating parameter
uncertainty into the resulting option prices. We finally compare our methods
with classical calibration-based results assuming that there is no options
market established. These methods can be applied to pricing of instruments for
which there is not an options market, as well as a methodological tool to
account for parameter and model uncertainty in theoretical option pricing.



Unicast and Multicast Qos Routing with Soft Constraint Logic Programming

We present a formal model to represent and solve the unicast/multicast
routing problem in networks with Quality of Service (QoS) requirements. To
attain this, first we translate the network adapting it to a weighted graph
(unicast) or and-or graph (multicast), where the weight on a connector
corresponds to the multidimensional cost of sending a packet on the related
network link: each component of the weights vector represents a different QoS
metric value (e.g. bandwidth, cost, delay, packet loss). The second step
consists in writing this graph as a program in Soft Constraint Logic
Programming (SCLP): the engine of this framework is then able to find the best
paths/trees by optimizing their costs and solving the constraints imposed on
them (e.g. delay < 40msec), thus finding a solution to QoS routing problems.
Moreover, c-semiring structures are a convenient tool to model QoS metrics. At
last, we provide an implementation of the framework over scale-free networks
and we suggest how the performance can be improved.



Low-density graph codes that are optimal for source/channel coding and binning

We describe and analyze the joint source/channel coding properties of a class
of sparse graphical codes based on compounding a low-density generator matrix
(LDGM) code with a low-density parity check (LDPC) code. Our first pair of
theorems establish that there exist codes from this ensemble, with all degrees
remaining bounded independently of block length, that are simultaneously
optimal as both source and channel codes when encoding and decoding are
performed optimally. More precisely, in the context of lossy compression, we
prove that finite degree constructions can achieve any pair $(R, D)$ on the
rate-distortion curve of the binary symmetric source. In the context of channel
coding, we prove that finite degree codes can achieve any pair $(C, p)$ on the
capacity-noise curve of the binary symmetric channel. Next, we show that our
compound construction has a nested structure that can be exploited to achieve
the Wyner-Ziv bound for source coding with side information (SCSI), as well as
the Gelfand-Pinsker bound for channel coding with side information (CCSI).
Although the current results are based on optimal encoding and decoding, the
proposed graphical codes have sparse structure and high girth that renders them
well-suited to message-passing and other efficient decoding procedures.



Transaction-Oriented Simulation In Ad Hoc Grids

This paper analyses the possibilities of performing parallel
transaction-oriented simulations with a special focus on the space-parallel
approach and discrete event simulation synchronisation algorithms that are
suitable for transaction-oriented simulation and the target environment of Ad
Hoc Grids. To demonstrate the findings a Java-based parallel
transaction-oriented simulator for the simulation language GPSS/H is
implemented on the basis of the promising Shock Resistant Time Warp
synchronisation algorithm and using the Grid framework ProActive. The
validation of this parallel simulator shows that the Shock Resistant Time Warp
algorithm can successfully reduce the number of rolled back Transaction moves
but it also reveals circumstances in which the Shock Resistant Time Warp
algorithm can be outperformed by the normal Time Warp algorithm. The conclusion
of this paper suggests possible improvements to the Shock Resistant Time Warp
algorithm to avoid such problems.



On-line Chain Partitions of Up-growing Semi-orders

On-line chain partition is a two-player game between Spoiler and Algorithm.
Spoiler presents a partially ordered set, point by point. Algorithm assigns
incoming points (immediately and irrevocably) to the chains which constitute a
chain partition of the order. The value of the game for orders of width $w$ is
a minimum number $\fVal(w)$ such that Algorithm has a strategy using at most
$\fVal(w)$ chains on orders of width at most $w$. We analyze the chain
partition game for up-growing semi-orders. Surprisingly, the golden ratio comes
into play and the value of the game is $\lfloor\frac{1+\sqrt{5}}{2}\; w
\rfloor$.



Analysis of the 802.11e Enhanced Distributed Channel Access Function

The IEEE 802.11e standard revises the Medium Access Control (MAC) layer of
the former IEEE 802.11 standard for Quality-of-Service (QoS) provision in the
Wireless Local Area Networks (WLANs). The Enhanced Distributed Channel Access
(EDCA) function of 802.11e defines multiple Access Categories (AC) with
AC-specific Contention Window (CW) sizes, Arbitration Interframe Space (AIFS)
values, and Transmit Opportunity (TXOP) limits to support MAC-level QoS and
prioritization. We propose an analytical model for the EDCA function which
incorporates an accurate CW, AIFS, and TXOP differentiation at any traffic
load. The proposed model is also shown to capture the effect of MAC layer
buffer size on the performance. Analytical and simulation results are compared
to demonstrate the accuracy of the proposed approach for varying traffic loads,
EDCA parameters, and MAC layer buffer space.



Performance Analysis of the IEEE 802.11e Enhanced Distributed Coordination Function using Cycle Time Approach

The recently ratified IEEE 802.11e standard defines the Enhanced Distributed
Channel Access (EDCA) function for Quality-of-Service (QoS) provisioning in the
Wireless Local Area Networks (WLANs). The EDCA uses Carrier Sense Multiple
Access with Collision Avoidance (CSMA/CA) and slotted Binary Exponential
Backoff (BEB) mechanism. We present a simple mathematical analysis framework
for the EDCA function. Our analysis considers the fact that the distributed
random access systems exhibit cyclic behavior where each station successfully
transmits a packet in a cycle. Our analysis shows that an AC-specific cycle
time exists for the EDCA function. Validating the theoretical results via
simulations, we show that the proposed analysis accurately captures EDCA
saturation performance in terms of average throughput, medium access delay, and
packet loss ratio. The cycle time analysis is a simple and insightful
substitute for previously proposed more complex EDCA models.



Fairness Provision in the IEEE 802.11e Infrastructure Basic Service Set

Most of the deployed IEEE 802.11e Wireless Local Area Networks (WLANs) use
infrastructure Basic Service Set (BSS) in which an Access Point (AP) serves as
a gateway between wired and wireless domains. We present the unfairness problem
between the uplink and the downlink flows of any Access Category (AC) in the
802.11e Enhanced Distributed Channel Access (EDCA) when the default settings of
the EDCA parameters are used. We propose a simple analytical model to calculate
the EDCA parameter settings that achieve weighted fair resource allocation for
all uplink and downlink flows. We also propose a simple model-assisted
measurement-based dynamic EDCA parameter adaptation algorithm. Moreover, our
dynamic solution addresses the differences in the transport layer and the
Medium Access Control (MAC) layer interactions of User Datagram Protocol (UDP)
and Transmission Control Protocol (TCP). We show that proposed Contention
Window (CW) and Transmit Opportunity (TXOP) limit adaptation at the AP provides
fair UDP and TCP access between uplink and downlink flows of the same AC while
preserving prioritization among ACs.



An Achievable Rate Region for Interference Channels with Conferencing

In this paper, we propose an achievable rate region for discrete memoryless
interference channels with conferencing at the transmitter side. We employ
superposition block Markov encoding, combined with simultaneous superposition
coding, dirty paper coding, and random binning to obtain the achievable rate
region. We show that, under respective conditions, the proposed achievable
region reduces to Han and Kobayashi achievable region for interference
channels, the capacity region for degraded relay channels, and the capacity
region for the Gaussian vector broadcast channel. Numerical examples for the
Gaussian case are given.



An algebraic generalization of Kripke structures

The Kripke semantics of classical propositional normal modal logic is made
algebraic via an embedding of Kripke structures into the larger class of
pointed stably supported quantales. This algebraic semantics subsumes the
traditional algebraic semantics based on lattices with unary operators, and it
suggests natural interpretations of modal logic, of possible interest in the
applications, in structures that arise in geometry and analysis, such as
foliated manifolds and operator algebras, via topological groupoids and inverse
semigroups. We study completeness properties of the quantale based semantics
for the systems K, T, K4, S4, and S5, in particular obtaining an axiomatization
for S5 which does not use negation or the modal necessity operator. As
additional examples we describe intuitionistic propositional modal logic, the
logic of programs PDL, and the ramified temporal logic CTL.



Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets

In spatially distributed multiuser antenna systems, the received signal
contains multiple carrier-frequency offsets (CFOs) arising from mismatch
between the oscillators of transmitters and receivers. This results in a
time-varying rotation of the data constellation, which needs to be compensated
at the receiver before symbol recovery. In this paper, a new approach for blind
CFO estimation and symbol recovery is proposed. The received base-band signal
is over-sampled, and its polyphase components are used to formulate a virtual
Multiple-Input Multiple-Output (MIMO) problem. By applying blind MIMO system
estimation techniques, the system response can be estimated and decoupled
versions of the user symbols can be recovered, each one of which contains a
distinct CFO. By applying a decision feedback Phase Lock Loop (PLL), the CFO
can be mitigated and the transmitted symbols can be recovered. The estimated
MIMO system response provides information about the CFOs that can be used to
initialize the PLL, speed up its convergence, and avoid ambiguities usually
linked with PLL.



A study of structural properties on profiles HMMs

Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models.



Extensive Games with Possibly Unaware Players

Standard game theory assumes that the structure of the game is common
knowledge among players. We relax this assumption by considering extensive
games where agents may be unaware of the complete structure of the game. In
particular, they may not be aware of moves that they and other agents can make.
We show how such games can be represented; the key idea is to describe the game
from the point of view of every agent at every node of the game tree. We
provide a generalization of Nash equilibrium and show that every game with
awareness has a generalized Nash equilibrium. Finally, we extend these results
to games with awareness of unawareness, where a player i may be aware that a
player j can make moves that i is not aware of, and to subjective games, where
payers may have no common knowledge regarding the actual game and their beliefs
are incompatible with a common prior.



Large System Analysis of Game-Theoretic Power Control in UWB Wireless Networks with Rake Receivers

This paper studies the performance of partial-Rake (PRake) receivers in
impulse-radio ultrawideband wireless networks when an energy-efficient power
control scheme is adopted. Due to the large bandwidth of the system, the
multipath channel is assumed to be frequency-selective. By using noncooperative
game-theoretic models and large system analysis, explicit expressions are
derived in terms of network parameters to measure the effects of self- and
multiple-access interference at a receiving access point. Performance of the
PRake is compared in terms of achieved utilities and loss to that of the
all-Rake receiver.



Introduction to Arabic Speech Recognition Using CMUSphinx System

In this paper Arabic was investigated from the speech recognition problem
point of view. We propose a novel approach to build an Arabic Automated Speech
Recognition System (ASR). This system is based on the open source CMU Sphinx-4,
from the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;
speaker-independent, continuous speech recognition system based on discrete
Hidden Markov Models (HMMs). We build a model using utilities from the
OpenSource CMU Sphinx. We will demonstrate the possible adaptability of this
system to Arabic voice recognition.



A Note on the Inapproximability of Correlation Clustering

We consider inapproximability of the correlation clustering problem defined
as follows: Given a graph $G = (V,E)$ where each edge is labeled either "+"
(similar) or "-" (dissimilar), correlation clustering seeks to partition the
vertices into clusters so that the number of pairs correctly (resp.
incorrectly) classified with respect to the labels is maximized (resp.
minimized). The two complementary problems are called MaxAgree and MinDisagree,
respectively, and have been studied on complete graphs, where every edge is
labeled, and general graphs, where some edge might not have been labeled.
Natural edge-weighted versions of both problems have been studied as well. Let
S-MaxAgree denote the weighted problem where all weights are taken from set S,
we show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\delta})$
essentially belongs to the same hardness class in the following sense: if there
is a polynomial time algorithm that approximates S-MaxAgree within a factor of
$\lambda = O(\log{|V|})$ with high probability, then for any choice of S',
S'-MaxAgree can be approximated in polynomial time within a factor of $(\lambda
+ \epsilon)$, where $\epsilon > 0$ can be arbitrarily small, with high
probability. A similar statement also holds for $S-MinDisagree. This result
implies it is hard (assuming $NP \neq RP$) to approximate unweighted MaxAgree
within a factor of $80/79-\epsilon$, improving upon a previous known factor of
$116/115-\epsilon$ by Charikar et. al. \cite{Chari05}.



Arabic Speech Recognition System using CMU-Sphinx4

In this paper we present the creation of an Arabic version of Automated
Speech Recognition System (ASR). This system is based on the open source
Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition
system based on discrete hidden Markov models (HMMs). We investigate the
changes that must be made to the model to adapt Arabic voice recognition.
  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,
CMUSphinx-4, Artificial intelligence.



On the Hardness of Approximating Stopping and Trapping Sets in LDPC Codes

We prove that approximating the size of stopping and trapping sets in Tanner
graphs of linear block codes, and more restrictively, the class of low-density
parity-check (LDPC) codes, is NP-hard. The ramifications of our findings are
that methods used for estimating the height of the error-floor of moderate- and
long-length LDPC codes based on stopping and trapping set enumeration cannot
provide accurate worst-case performance predictions.



The Wiretap Channel with Feedback: Encryption over the Channel

In this work, the critical role of noisy feedback in enhancing the secrecy
capacity of the wiretap channel is established. Unlike previous works, where a
noiseless public discussion channel is used for feedback, the feed-forward and
feedback signals share the same noisy channel in the present model. Quite
interestingly, this noisy feedback model is shown to be more advantageous in
the current setting. More specifically, the discrete memoryless modulo-additive
channel with a full-duplex destination node is considered first, and it is
shown that the judicious use of feedback increases the perfect secrecy capacity
to the capacity of the source-destination channel in the absence of the
wiretapper. In the achievability scheme, the feedback signal corresponds to a
private key, known only to the destination. In the half-duplex scheme, a novel
feedback technique that always achieves a positive perfect secrecy rate (even
when the source-wiretapper channel is less noisy than the source-destination
channel) is proposed. These results hinge on the modulo-additive property of
the channel, which is exploited by the destination to perform encryption over
the channel without revealing its key to the source. Finally, this scheme is
extended to the continuous real valued modulo-$\Lambda$ channel where it is
shown that the perfect secrecy capacity with feedback is also equal to the
capacity in the absence of the wiretapper.



Kekul\'e Cells for Molecular Computation

The configurations of single and double bonds in polycyclic hydrocarbons are
abstracted as Kekul\'e states of graphs. Sending a so-called soliton over an
open channel between ports (external nodes) of the graph changes the Kekul\'e
state and therewith the set of open channels in the graph. This switching
behaviour is proposed as a basis for molecular computation. The proposal is
highly speculative but may have tremendous impact.
  Kekul\'e states with the same boundary behaviour (port assignment) can be
regarded as equivalent. This gives rise to the abstraction of Kekul\'e cells.
The basic theory of Kekul\'e states and Kekul\'e cells is developed here, up to
the classification of Kekul\'e cells with $\leq 4$ ports. To put the theory in
context, we generalize Kekul\'e states to semi-Kekul\'e states, which form the
solutions of a linear system of equations over the field of the bits 0 and 1.
We briefly study so-called omniconjugated graphs, in which every port
assignment of the right signature has a Kekul\'e state. Omniconjugated graphs
may be useful as connectors between computational elements. We finally
investigate some examples with potentially useful switching behaviour.



Using Image Attributes for Human Identification Protocols

A secure human identification protocol aims at authenticating human users to
a remote server when even the users' inputs are not hidden from an adversary.
Recently, the authors proposed a human identification protocol in the RSA
Conference 2007, which is loosely based on the ability of humans to efficiently
process an image. The advantage being that an automated adversary is not
effective in attacking the protocol without human assistance. This paper
extends that work by trying to solve some of the open problems. First, we
analyze the complexity of defeating the proposed protocols by quantifying the
workload of a human adversary. Secondly, we propose a new construction based on
textual CAPTCHAs (Reverse Turing Tests) in order to make the generation of
automated challenges easier. We also present a brief experiment involving real
human users to find out the number of possible attributes in a given image and
give some guidelines for the selection of challenge questions based on the
results. Finally, we analyze the previously proposed protocol in detail for the
relationship between the secrets. Our results show that we can construct human
identification protocols based on image evaluation with reasonably
``quantified'' security guarantees based on our model.



Parallel computing for the finite element method

A finite element method is presented to compute time harmonic microwave
fields in three dimensional configurations. Nodal-based finite elements have
been coupled with an absorbing boundary condition to solve open boundary
problems. This paper describes how the modeling of large devices has been made
possible using parallel computation, New algorithms are then proposed to
implement this formulation on a cluster of workstations (10 DEC ALPHA 300X) and
on a CRAY C98. Analysis of the computation efficiency is performed using simple
problems. The electromagnetic scattering of a plane wave by a perfect electric
conducting airplane is finally given as example.



Parallel computation of the rank of large sparse matrices from algebraic K-theory

This paper deals with the computation of the rank and of some integer Smith
forms of a series of sparse matrices arising in algebraic K-theory. The number
of non zero entries in the considered matrices ranges from 8 to 37 millions.
The largest rank computation took more than 35 days on 50 processors. We report
on the actual algorithms we used to build the matrices, their link to the
motivic cohomology and the linear algebra and parallelizations required to
perform such huge computations. In particular, these results are part of the
first computation of the cohomology of the linear group GL_7(Z).



Scaling Laws of Cognitive Networks

We consider a cognitive network consisting of n random pairs of cognitive
transmitters and receivers communicating simultaneously in the presence of
multiple primary users. Of interest is how the maximum throughput achieved by
the cognitive users scales with n. Furthermore, how far these users must be
from a primary user to guarantee a given primary outage. Two scenarios are
considered for the network scaling law: (i) when each cognitive transmitter
uses constant power to communicate with a cognitive receiver at a bounded
distance away, and (ii) when each cognitive transmitter scales its power
according to the distance to a considered primary user, allowing the cognitive
transmitter-receiver distances to grow. Using single-hop transmission, suitable
for cognitive devices of opportunistic nature, we show that, in both scenarios,
with path loss larger than 2, the cognitive network throughput scales linearly
with the number of cognitive users. We then explore the radius of a primary
exclusive region void of cognitive transmitters. We obtain bounds on this
radius for a given primary outage constraint. These bounds can help in the
design of a primary network with exclusive regions, outside of which cognitive
users may transmit freely. Our results show that opportunistic secondary
spectrum access using single-hop transmission is promising.



A Nice Labelling for Tree-Like Event Structures of Degree 3

We address the problem of &#64257;nding nice labellings for event structures
of degree 3. We develop a minimum theory by which we prove that the labelling
number of an event structure of degree 3 is bounded by a linear function of the
height. The main theorem we present in this paper states that event structures
of degree 3 whose causality order is a tree have a nice labelling with 3
colors. Finally, we exemplify how to use this theorem to construct upper bounds
for the labelling number of other event structures of degree 3.



Power control algorithms for CDMA networks based on large system analysis

Power control is a fundamental task accomplished in any wireless cellular
network; its aim is to set the transmit power of any mobile terminal, so that
each user is able to achieve its own target SINR. While conventional power
control algorithms require knowledge of a number of parameters of the signal of
interest and of the multiaccess interference, in this paper it is shown that in
a large CDMA system much of this information can be dispensed with, and
effective distributed power control algorithms may be implemented with very
little information on the user of interest. An uplink CDMA system subject to
flat fading is considered with a focus on the cases in which a linear MMSE
receiver and a non-linear MMSE serial interference cancellation receiver are
adopted; for the latter case new formulas are also given for the system SINR in
the large system asymptote. Experimental results show an excellent agreement
between the performance and the power profile of the proposed distributed
algorithms and that of conventional ones that require much greater prior
knowledge.



Power control and receiver design for energy efficiency in multipath CDMA channels with bandlimited waveforms

This paper is focused on the cross-layer design problem of joint multiuser
detection and power control for energy-efficiency optimization in a wireless
data network through a game-theoretic approach. Building on work of Meshkati,
et al., wherein the tools of game-theory are used in order to achieve
energy-efficiency in a simple synchronous code division multiple access system,
system asynchronism, the use of bandlimited chip-pulses, and the multipath
distortion induced by the wireless channel are explicitly incorporated into the
analysis. Several non-cooperative games are proposed wherein users may vary
their transmit power and their uplink receiver in order to maximize their
utility, which is defined here as the ratio of data throughput to transmit
power. In particular, the case in which a linear multiuser detector is adopted
at the receiver is considered first, and then, the more challenging case in
which non-linear decision feedback multiuser detectors are employed is
considered. The proposed games are shown to admit a unique Nash equilibrium
point, while simulation results show the effectiveness of the proposed
solutions, as well as that the use of a decision-feedback multiuser receiver
brings remarkable performance improvements.



Bounded Pushdown dimension vs Lempel Ziv information density

In this paper we introduce a variant of pushdown dimension called bounded
pushdown (BPD) dimension, that measures the density of information contained in
a sequence, relative to a BPD automata, i.e. a finite state machine equipped
with an extra infinite memory stack, with the additional requirement that every
input symbol only allows a bounded number of stack movements. BPD automata are
a natural real-time restriction of pushdown automata. We show that BPD
dimension is a robust notion by giving an equivalent characterization of BPD
dimension in terms of BPD compressors. We then study the relationships between
BPD compression, and the standard Lempel-Ziv (LZ) compression algorithm, and
show that in contrast to the finite-state compressor case, LZ is not universal
for bounded pushdown compressors in a strong sense: we construct a sequence
that LZ fails to compress signicantly, but that is compressed by at least a
factor 2 by a BPD compressor. As a corollary we obtain a strong separation
between finite-state and BPD dimension.



Light Logics and Optimal Reduction: Completeness and Complexity

Typing of lambda-terms in Elementary and Light Affine Logic (EAL, LAL, resp.)
has been studied for two different reasons: on the one hand the evaluation of
typed terms using LAL (EAL, resp.) proof-nets admits a guaranteed polynomial
(elementary, resp.) bound; on the other hand these terms can also be evaluated
by optimal reduction using the abstract version of Lamping's algorithm. The
first reduction is global while the second one is local and asynchronous. We
prove that for LAL (EAL, resp.) typed terms, Lamping's abstract algorithm also
admits a polynomial (elementary, resp.) bound. We also show its soundness and
completeness (for EAL and LAL with type fixpoints), by using a simple geometry
of interaction model (context semantics).



Optimum Linear LLR Calculation for Iterative Decoding on Fading Channels

On a fading channel with no channel state information at the receiver,
calculating true log-likelihood ratios (LLR) is complicated. Existing work
assume that the power of the additive noise is known and use the expected value
of the fading gain in a linear function of the channel output to find
approximate LLRs. In this work, we first assume that the power of the additive
noise is known and we find the optimum linear approximation of LLRs in the
sense of maximum achievable transmission rate on the channel. The maximum
achievable rate under this linear LLR calculation is almost equal to the
maximum achievable rate under true LLR calculation. We also observe that this
method appears to be the optimum in the sense of bit error rate performance
too. These results are then extended to the case that the noise power is
unknown at the receiver and a performance almost identical to the case that the
noise power is perfectly known is obtained.



Physical Layer Network Coding

A main distinguishing feature of a wireless network compared with a wired
network is its broadcast nature, in which the signal transmitted by a node may
reach several other nodes, and a node may receive signals from several other
nodes simultaneously. Rather than a blessing, this feature is treated more as
an interference-inducing nuisance in most wireless networks today (e.g., IEEE
802.11). This paper shows that the concept of network coding can be applied at
the physical layer to turn the broadcast property into a capacity-boosting
advantage in wireless ad hoc networks. Specifically, we propose a
physical-layer network coding (PNC) scheme to coordinate transmissions among
nodes. In contrast to straightforward network coding which performs coding
arithmetic on digital bit streams after they have been received, PNC makes use
of the additive nature of simultaneously arriving electromagnetic (EM) waves
for equivalent coding operation. And in doing so, PNC can potentially achieve
100% and 50% throughput increases compared with traditional transmission and
straightforward network coding, respectively, in multi-hop networks. More
specifically, the information-theoretic capacity of PNC is almost double that
of traditional transmission in the SNR region of practical interest (higher
than 0dB). We believe this is a first paper that ventures into EM-wave-based
network coding at the physical layer and demonstrates its potential for
boosting network capacity.



Algebraic Distributed Space-Time Codes with Low ML Decoding Complexity

"Extended Clifford algebras" are introduced as a means to obtain low ML
decoding complexity space-time block codes. Using left regular matrix
representations of two specific classes of extended Clifford algebras, two
systematic algebraic constructions of full diversity Distributed Space-Time
Codes (DSTCs) are provided for any power of two number of relays. The left
regular matrix representation has been shown to naturally result in space-time
codes meeting the additional constraints required for DSTCs. The DSTCs so
constructed have the salient feature of reduced Maximum Likelihood (ML)
decoding complexity. In particular, the ML decoding of these codes can be
performed by applying the lattice decoder algorithm on a lattice of four times
lesser dimension than what is required in general. Moreover these codes have a
uniform distribution of power among the relays and in time, thus leading to a
low Peak to Average Power Ratio at the relays.



STBCs from Representation of Extended Clifford Algebras

A set of sufficient conditions to construct $\lambda$-real symbol Maximum
Likelihood (ML) decodable STBCs have recently been provided by Karmakar et al.
STBCs satisfying these sufficient conditions were named as Clifford Unitary
Weight (CUW) codes. In this paper, the maximal rate (as measured in complex
symbols per channel use) of CUW codes for $\lambda=2^a,a\in\mathbb{N}$ is
obtained using tools from representation theory. Two algebraic constructions of
codes achieving this maximal rate are also provided. One of the constructions
is obtained using linear representation of finite groups whereas the other
construction is based on the concept of right module algebra over
non-commutative rings. To the knowledge of the authors, this is the first paper
in which matrices over non-commutative rings is used to construct STBCs. An
algebraic explanation is provided for the 'ABBA' construction first proposed by
Tirkkonen et al and the tensor product construction proposed by Karmakar et al.
Furthermore, it is established that the 4 transmit antenna STBC originally
proposed by Tirkkonen et al based on the ABBA construction is actually a single
complex symbol ML decodable code if the design variables are permuted and
signal sets of appropriate dimensions are chosen.



Signal Set Design for Full-Diversity Low-Decoding-Complexity Differential Scaled-Unitary STBCs

The problem of designing high rate, full diversity noncoherent space-time
block codes (STBCs) with low encoding and decoding complexity is addressed.
First, the notion of $g$-group encodable and $g$-group decodable linear STBCs
is introduced. Then for a known class of rate-1 linear designs, an explicit
construction of fully-diverse signal sets that lead to four-group encodable and
four-group decodable differential scaled unitary STBCs for any power of two
number of antennas is provided. Previous works on differential STBCs either
sacrifice decoding complexity for higher rate or sacrifice rate for lower
decoding complexity.



Noncoherent Low-Decoding-Complexity Space-Time Codes for Wireless Relay Networks

The differential encoding/decoding setup introduced by Kiran et al, Oggier et
al and Jing et al for wireless relay networks that use codebooks consisting of
unitary matrices is extended to allow codebooks consisting of scaled unitary
matrices. For such codebooks to be used in the Jing-Hassibi protocol for
cooperative diversity, the conditions that need to be satisfied by the relay
matrices and the codebook are identified. A class of previously known rate one,
full diversity, four-group encodable and four-group decodable Differential
Space-Time Codes (DSTCs) is proposed for use as Distributed DSTCs (DDSTCs) in
the proposed set up. To the best of our knowledge, this is the first known low
decoding complexity DDSTC scheme for cooperative wireless networks.



Narratives within immersive technologies

The main goal of this project is to research technical advances in order to
enhance the possibility to develop narratives within immersive mediated
environments. An important part of the research is concerned with the question
of how a script can be written, annotated and realized for an immersive
context. A first description of the main theoretical framework and the ongoing
work and a first script example is provided. This project is part of the
program for presence research, and it will exploit physiological feedback and
Computational Intelligence within virtual reality.



Existence Proofs of Some EXIT Like Functions

The Extended BP (EBP) Generalized EXIT (GEXIT) function introduced in
\cite{MMRU05} plays a fundamental role in the asymptotic analysis of sparse
graph codes. For transmission over the binary erasure channel (BEC) the
analytic properties of the EBP GEXIT function are relatively simple and well
understood. The general case is much harder and even the existence of the curve
is not known in general. We introduce some tools from non-linear analysis which
can be useful to prove the existence of EXIT like curves in some cases. The
main tool is the Krasnoselskii-Rabinowitz (KR) bifurcation theorem.



Computing Extensions of Linear Codes

This paper deals with the problem of increasing the minimum distance of a
linear code by adding one or more columns to the generator matrix. Several
methods to compute extensions of linear codes are presented. Many codes
improving the previously known lower bounds on the minimum distance have been
found.



A-infinity structure on simplicial complexes

A discrete (finite-difference) analogue of differential forms is considered,
defined on simplicial complexes, including triangulations of continuous
manifolds. Various operations are explicitly defined on these forms, including
exterior derivative and exterior product. The latter one is non-associative.
Instead, as anticipated, it is a part of non-trivial A-infinity structure,
involving a chain of poly-linear operations, constrained by nilpotency
relation: (d + \wedge + m + ...)^n = 0 with n=2.



Joint universal lossy coding and identification of stationary mixing sources

The problem of joint universal source coding and modeling, treated in the
context of lossless codes by Rissanen, was recently generalized to fixed-rate
lossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We
extend these results to variable-rate lossy block coding of stationary ergodic
sources and show that, for bounded metric distortion measures, any finitely
parametrized family of stationary sources satisfying suitable mixing,
smoothness and Vapnik-Chervonenkis learnability conditions admits universal
schemes for joint lossy source coding and identification. We also give several
explicit examples of parametric sources satisfying the regularity conditions.



Opportunistic Communications in an Orthogonal Multiaccess Relay Channel

The problem of resource allocation is studied for a two-user fading
orthogonal multiaccess relay channel (MARC) where both users (sources)
communicate with a destination in the presence of a relay. A half-duplex relay
is considered that transmits on a channel orthogonal to that used by the
sources. The instantaneous fading state between every transmit-receive pair in
this network is assumed to be known at both the transmitter and receiver. Under
an average power constraint at each source and the relay, the sum-rate for the
achievable strategy of decode-and-forward (DF) is maximized over all power
allocations (policies) at the sources and relay. It is shown that the sum-rate
maximizing policy exploits the multiuser fading diversity to reveal the
optimality of opportunistic channel use by each user. A geometric
interpretation of the optimal power policy is also presented.



Minimum Expected Distortion in Gaussian Layered Broadcast Coding with Successive Refinement

A transmitter without channel state information (CSI) wishes to send a
delay-limited Gaussian source over a slowly fading channel. The source is coded
in superimposed layers, with each layer successively refining the description
in the previous one. The receiver decodes the layers that are supported by the
channel realization and reconstructs the source up to a distortion. In the
limit of a continuum of infinite layers, the optimal power distribution that
minimizes the expected distortion is given by the solution to a set of linear
differential equations in terms of the density of the fading distribution. In
the optimal power distribution, as SNR increases, the allocation over the
higher layers remains unchanged; rather the extra power is allocated towards
the lower layers. On the other hand, as the bandwidth ratio b (channel uses per
source symbol) tends to zero, the power distribution that minimizes expected
distortion converges to the power distribution that maximizes expected
capacity. While expected distortion can be improved by acquiring CSI at the
transmitter (CSIT) or by increasing diversity from the realization of
independent fading paths, at high SNR the performance benefit from diversity
exceeds that from CSIT, especially when b is large.



Supervised Feature Selection via Dependence Estimation

We introduce a framework for filtering features that employs the
Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence
between the features and the labels. The key idea is that good features should
maximise such dependence. Feature selection for various supervised learning
problems (including classification and regression) is unified under this
framework, and the solutions can be approximated using a backward-elimination
algorithm. We demonstrate the usefulness of our method on both artificial and
real world datasets.



A Channel that Heats Up

Motivated by on-chip communication, a channel model is proposed where the
variance of the additive noise depends on the weighted sum of the past channel
input powers. For this channel, an expression for the capacity per unit cost is
derived, and it is shown that the expression holds also in the presence of
feedback.



Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A Case Study with the UCI Thyroid Disease Database

The random initialization of weights of a multilayer perceptron makes it
possible to model its training process as a Las Vegas algorithm, i.e. a
randomized algorithm which stops when some required training error is obtained,
and whose execution time is a random variable. This modeling is used to perform
a case study on a well-known pattern recognition benchmark: the UCI Thyroid
Disease Database. Empirical evidence is presented of the training time
probability distribution exhibiting a heavy tail behavior, meaning a big
probability mass of long executions. This fact is exploited to reduce the
training time cost by applying two simple restart strategies. The first assumes
full knowledge of the distribution yielding a 40% cut down in expected time
with respect to the training without restarts. The second, assumes null
knowledge, yielding a reduction ranging from 9% to 23%.



Random Access Broadcast: Stability and Throughput Analysis

A wireless network in which packets are broadcast to a group of receivers
through use of a random access protocol is considered in this work. The
relation to previous work on networks of interacting queues is discussed and
subsequently, the stability and throughput regions of the system are analyzed
and presented. A simple network of two source nodes and two destination nodes
is considered first. The broadcast service process is analyzed assuming a
channel that allows for packet capture and multipacket reception. In this small
network, the stability and throughput regions are observed to coincide. The
same problem for a network with N sources and M destinations is considered
next. The channel model is simplified in that multipacket reception is no
longer permitted. Bounds on the stability region are developed using the
concept of stability rank and the throughput region of the system is compared
to the bounds. Our results show that as the number of destination nodes
increases, the stability and throughput regions diminish. Additionally, a
previous conjecture that the stability and throughput regions coincide for a
network of arbitrarily many sources is supported for a broadcast scenario by
the results presented in this work.



The Complexity of Simple Stochastic Games

In this paper we survey the computational time complexity of assorted simple
stochastic game problems, and we give an overview of the best known algorithms
associated with each problem.



Writing on Dirty Paper with Resizing and its Application to Quasi-Static Fading Broadcast Channels

This paper studies a variant of the classical problem of ``writing on dirty
paper'' in which the sum of the input and the interference, or dirt, is
multiplied by a random variable that models resizing, known to the decoder but
not to the encoder. The achievable rate of Costa's dirty paper coding (DPC)
scheme is calculated and compared to the case of the decoder's also knowing the
dirt. In the ergodic case, the corresponding rate loss vanishes asymptotically
in the limits of both high and low signal-to-noise ratio (SNR), and is small at
all finite SNR for typical distributions like Rayleigh, Rician, and Nakagami.
In the quasi-static case, the DPC scheme is lossless at all SNR in terms of
outage probability. Quasi-static fading broadcast channels (BC) without
transmit channel state information (CSI) are investigated as an application of
the robustness properties. It is shown that the DPC scheme leads to an outage
achievable rate region that strictly dominates that of time division.



Minimum cost distributed source coding over a network

This work considers the problem of transmitting multiple compressible sources
over a network at minimum cost. The aim is to find the optimal rates at which
the sources should be compressed and the network flows using which they should
be transmitted so that the cost of the transmission is minimal. We consider
networks with capacity constraints and linear cost functions. The problem is
complicated by the fact that the description of the feasible rate region of
distributed source coding problems typically has a number of constraints that
is exponential in the number of sources. This renders general purpose solvers
inefficient. We present a framework in which these problems can be solved
efficiently by exploiting the structure of the feasible rate regions coupled
with dual decomposition and optimization techniques such as the subgradient
method and the proximal bundle method.



On Algebraic Decoding of $q$-ary Reed-Muller and Product-Reed-Solomon Codes

We consider a list decoding algorithm recently proposed by Pellikaan-Wu
\cite{PW2005} for $q$-ary Reed-Muller codes $\mathcal{RM}_q(\ell, m, n)$ of
length $n \leq q^m$ when $\ell \leq q$. A simple and easily accessible
correctness proof is given which shows that this algorithm achieves a relative
error-correction radius of $\tau \leq (1 - \sqrt{{\ell q^{m-1}}/{n}})$. This is
an improvement over the proof using one-point Algebraic-Geometric codes given
in \cite{PW2005}. The described algorithm can be adapted to decode
Product-Reed-Solomon codes.
  We then propose a new low complexity recursive algebraic decoding algorithm
for Reed-Muller and Product-Reed-Solomon codes. Our algorithm achieves a
relative error correction radius of $\tau \leq \prod_{i=1}^m (1 -
\sqrt{k_i/q})$. This technique is then proved to outperform the Pellikaan-Wu
method in both complexity and error correction radius over a wide range of code
rates.



A High-Throughput Cross-Layer Scheme for Distributed Wireless Ad Hoc Networks

In wireless ad hoc networks, distributed nodes can collaboratively form an
antenna array for long-distance communications to achieve high energy
efficiency. In recent work, Ochiai, et al., have shown that such collaborative
beamforming can achieve a statistically nice beampattern with a narrow main
lobe and low sidelobes. However, the process of collaboration introduces
significant delay, since all collaborating nodes need access to the same
information. In this paper, a technique that significantly reduces the
collaboration overhead is proposed. It consists of two phases. In the first
phase, nodes transmit locally in a random access fashion. Collisions, when they
occur, are viewed as linear mixtures of the collided packets. In the second
phase, a set of cooperating nodes acts as a distributed antenna system and
beamform the received analog waveform to one or more faraway destinations. This
step requires multiplication of the received analog waveform by a complex
number, which is independently computed by each cooperating node, and which
enables separation of the collided packets based on their final destination.
The scheme requires that each node has global knowledge of the network
coordinates. The proposed scheme can achieve high throughput, which in certain
cases exceeds one.



Nature-Inspired Interconnects for Self-Assembled Large-Scale Network-on-Chip Designs

Future nano-scale electronics built up from an Avogadro number of components
needs efficient, highly scalable, and robust means of communication in order to
be competitive with traditional silicon approaches. In recent years, the
Networks-on-Chip (NoC) paradigm emerged as a promising solution to interconnect
challenges in silicon-based electronics. Current NoC architectures are either
highly regular or fully customized, both of which represent implausible
assumptions for emerging bottom-up self-assembled molecular electronics that
are generally assumed to have a high degree of irregularity and imperfection.
Here, we pragmatically and experimentally investigate important design
trade-offs and properties of an irregular, abstract, yet physically plausible
3D small-world interconnect fabric that is inspired by modern network-on-chip
paradigms. We vary the framework's key parameters, such as the connectivity,
the number of switch nodes, the distribution of long- versus short-range
connections, and measure the network's relevant communication characteristics.
We further explore the robustness against link failures and the ability and
efficiency to solve a simple toy problem, the synchronization task. The results
confirm that (1) computation in irregular assemblies is a promising and
disruptive computing paradigm for self-assembled nano-scale electronics and (2)
that 3D small-world interconnect fabrics with a power-law decaying distribution
of shortcut lengths are physically plausible and have major advantages over
local 2D and 3D regular topologies.



Modern Coding Theory: The Statistical Mechanics and Computer Science Point of View

These are the notes for a set of lectures delivered by the two authors at the
Les Houches Summer School on `Complex Systems' in July 2006. They provide an
introduction to the basic concepts in modern (probabilistic) coding theory,
highlighting connections with statistical mechanics. We also stress common
concepts with other disciplines dealing with similar problems that can be
generically referred to as `large graphical models'.
  While most of the lectures are devoted to the classical channel coding
problem over simple memoryless channels, we present a discussion of more
complex channel models. We conclude with an overview of the main open
challenges in the field.



Higher-order theories

We extend our approach to abstract syntax (with binding constructions)
through modules and linearity. First we give a new general definition of arity,
yielding the companion notion of signature. Then we obtain a modularity result
as requested by Ghani and Uustalu (2003): in our setting, merging two
extensions of syntax corresponds to building an amalgamated sum. Finally we
define a natural notion of equation concerning a signature and prove the
existence of an initial semantics for a so-called representable signature
equipped with a set of equations.



Recommending Related Papers Based on Digital Library Access Records

An important goal for digital libraries is to enable researchers to more
easily explore related work. While citation data is often used as an indicator
of relatedness, in this paper we demonstrate that digital access records (e.g.
http-server logs) can be used as indicators as well. In particular, we show
that measures based on co-access provide better coverage than co-citation, that
they are available much sooner, and that they are more accurate for recent
papers.



On Verifying and Engineering the Well-gradedness of a Union-closed Family

Current techniques for generating a knowledge space, such as QUERY,
guarantees that the resulting structure is closed under union, but not that it
satisfies wellgradedness, which is one of the defining conditions for a
learning space. We give necessary and sufficient conditions on the base of a
union-closed set family that ensures that the family is well-graded. We
consider two cases, depending on whether or not the family contains the empty
set. We also provide algorithms for efficiently testing these conditions, and
for augmenting a set family in a minimal way to one that satisfies these
conditions.



Optimal Routing for the Gaussian Multiple-Relay Channel with Decode-and-Forward

In this paper, we study a routing problem on the Gaussian multiple relay
channel, in which nodes employ a decode-and-forward coding strategy. We are
interested in routes for the information flow through the relays that achieve
the highest DF rate. We first construct an algorithm that provably finds
optimal DF routes. As the algorithm runs in factorial time in the worst case,
we propose a polynomial time heuristic algorithm that finds an optimal route
with high probability. We demonstrate that that the optimal (and near optimal)
DF routes are good in practice by simulating a distributed DF coding scheme
using low density parity check codes with puncturing and incremental
redundancy.



Using Access Data for Paper Recommendations on ArXiv.org

This thesis investigates in the use of access log data as a source of
information for identifying related scientific papers. This is done for
arXiv.org, the authority for publication of e-prints in several fields of
physics.
  Compared to citation information, access logs have the advantage of being
immediately available, without manual or automatic extraction of the citation
graph. Because of that, a main focus is on the question, how far user behavior
can serve as a replacement for explicit meta-data, which potentially might be
expensive or completely unavailable. Therefore, we compare access, content, and
citation-based measures of relatedness on different recommendation tasks. As a
final result, an online recommendation system has been built that can help
scientists to find further relevant literature, without having to search for
them actively.



Arbitrary Rate Permutation Modulation for the Gaussian Channel

In this paper non-group permutation modulated sequences for the Gaussian
channel are considered. Without the restriction to group codes rather than
subsets of group codes, arbitrary rates are achievable. The code construction
utilizes the known optimal group constellations to ensure at least the same
performance but exploit the Gray code ordering structure of multiset
permutations as a selection criterion at the decoder. The decoder achieves near
maximum likelihood performance at low computational cost and low additional
memory requirements at the receiver.



Achievable Rates for Two-Way Wire-Tap Channels

We consider two-way wire-tap channels, where two users are communicating with
each other in the presence of an eavesdropper, who has access to the
communications through a multiple-access channel. We find achievable rates for
two different scenarios, the Gaussian two-way wire-tap channel, (GTW-WT), and
the binary additive two-way wire-tap channel, (BATW-WT). It is shown that the
two-way channels inherently provide a unique advantage for wire-tapped
scenarios, as the users know their own transmitted signals and in effect help
encrypt the other user's messages, similar to a one-time pad. We compare the
achievable rates to that of the Gaussian multiple-access wire-tap channel
(GMAC-WT) to illustrate this advantage.



Detection of two-sided alternatives in a Brownian motion model

This work examines the problem of sequential detection of a change in the
drift of a Brownian motion in the case of two-sided alternatives. Applications
to real life situations in which two-sided changes can occur are discussed.
Traditionally, 2-CUSUM stopping rules have been used for this problem due to
their asymptotically optimal character as the mean time between false alarms
tends to $\infty$. In particular, attention has focused on 2-CUSUM harmonic
mean rules due to the simplicity in calculating their first moments. In this
paper, we derive closed-form expressions for the first moment of a general
2-CUSUM stopping rule. We use these expressions to obtain explicit upper and
lower bounds for it. Moreover, we derive an expression for the rate of change
of this first moment as one of the threshold parameters changes. Based on these
expressions we obtain explicit upper and lower bounds to this rate of change.
Using these expressions we are able to find the best 2-CUSUM stopping rule with
respect to the extended Lorden criterion. In fact, we demonstrate not only the
existence but also the uniqueness of the best 2-CUSUM stopping both in the case
of a symmetric change and in the case of a non-symmetric case. Furthermore, we
discuss the existence of a modification of the 2-CUSUM stopping rule that has a
strictly better performance than its classical 2-CUSUM counterpart for small
values of the mean time between false alarms. We conclude with a discussion on
the open problem of strict optimality in the case of two-sided alternatives.



Space Time Codes from Permutation Codes

A new class of space time codes with high performance is presented. The code
design utilizes tailor-made permutation codes, which are known to have large
minimal distances as spherical codes. A geometric connection between spherical
and space time codes has been used to translate them into the final space time
codes. Simulations demonstrate that the performance increases with the block
lengths, a result that has been conjectured already in previous work. Further,
the connection to permutation codes allows for moderate complex en-/decoding
algorithms.



Algorithm for Evaluation of the Interval Power Function of Unconstrained Arguments

We describe an algorithm for evaluation of the interval extension of the
power function of variables x and y given by the expression x^y. Our algorithm
reduces the general case to the case of non-negative bases.



Experimenting with recursive queries in database and logic programming systems

This paper considers the problem of reasoning on massive amounts of (possibly
distributed) data. Presently, existing proposals show some limitations: {\em
(i)} the quantity of data that can be handled contemporarily is limited, due to
the fact that reasoning is generally carried out in main-memory; {\em (ii)} the
interaction with external (and independent) DBMSs is not trivial and, in
several cases, not allowed at all; {\em (iii)} the efficiency of present
implementations is still not sufficient for their utilization in complex
reasoning tasks involving massive amounts of data. This paper provides a
contribution in this setting; it presents a new system, called DLV$^{DB}$,
which aims to solve these problems. Moreover, the paper reports the results of
a thorough experimental analysis we have carried out for comparing our system
with several state-of-the-art systems (both logic and databases) on some
classical deductive problems; the other tested systems are: LDL++, XSB, Smodels
and three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even
the commercial Database Systems on recursive queries. To appear in Theory and
Practice of Logic Programming (TPLP)



Computing modular polynomials in quasi-linear time

We analyse and compare the complexity of several algorithms for computing
modular polynomials. We show that an algorithm relying on floating point
evaluation of modular functions and on interpolation, which has received little
attention in the literature, has a complexity that is essentially (up to
logarithmic factors) linear in the size of the computed polynomials. In
particular, it obtains the classical modular polynomials $\Phi_\ell$ of prime
level $\ell$ in time O (\ell^3 \log^4 \ell \log \log \ell). Besides treating
modular polynomials for $\Gamma^0 (\ell)$, which are an important ingredient in
many algorithms dealing with isogenies of elliptic curves, the algorithm is
easily adapted to more general situations. Composite levels are handled just as
easily as prime levels, as well as polynomials between a modular function and
its transform of prime level, such as the Schl\"afli polynomials and their
generalisations. Our distributed implementation of the algorithm confirms the
theoretical analysis by computing modular equations of record level around
10000 in less than two weeks on ten processors.



Euclidean Shortest Paths in Simple Cube Curves at a Glance

This paper reports about the development of two provably correct approximate
algorithms which calculate the Euclidean shortest path (ESP) within a given
cube-curve with arbitrary accuracy, defined by $\epsilon >0$, and in time
complexity $\kappa(\epsilon) \cdot {\cal O}(n)$, where $\kappa(\epsilon)$ is
the length difference between the path used for initialization and the
minimum-length path, divided by $\epsilon$. A run-time diagram also illustrates
this linear-time behavior of the implemented ESP algorithm.



Generalized Stability Condition for Generalized and Doubly-Generalized LDPC Codes

In this paper, the stability condition for low-density parity-check (LDPC)
codes on the binary erasure channel (BEC) is extended to generalized LDPC
(GLDPC) codes and doublygeneralized LDPC (D-GLDPC) codes. It is proved that, in
both cases, the stability condition only involves the component codes with
minimum distance 2. The stability condition for GLDPC codes is always expressed
as an upper bound to the decoding threshold. This is not possible for D-GLDPC
codes, unless all the generalized variable nodes have minimum distance at least
3. Furthermore, a condition called derivative matching is defined in the paper.
This condition is sufficient for a GLDPC or DGLDPC code to achieve the
stability condition with equality. If this condition is satisfied, the
threshold of D-GLDPC codes (whose generalized variable nodes have all minimum
distance at least 3) and GLDPC codes can be expressed in closed form.



Characterization of P2P IPTV Traffic: Scaling Analysis

P2P IPTV applications arise on the Internet and will be massively used in the
future. It is expected that P2P IPTV will contribute to increase the overall
Internet traffic. In this context, it is important to measure the impact of P2P
IPTV on the networks and to characterize this traffic. Dur- ing the 2006 FIFA
World Cup, we performed an extensive measurement campaign. We measured network
traffic generated by broadcasting soc- cer games by the most popular P2P IPTV
applications, namely PPLive, PPStream, SOPCast and TVAnts. From the collected
data, we charac- terized the P2P IPTV traffic structure at different time
scales by using wavelet based transform method. To the best of our knowledge,
this is the first work, which presents a complete multiscale analysis of the
P2P IPTV traffic. Our results show that the scaling properties of the TCP
traffic present periodic behavior whereas the UDP traffic is stationary and
lead to long- range depedency characteristics. For all the applications, the
download traffic has different characteristics than the upload traffic. The
signaling traffic has a significant impact on the download traffic but it has
negligible impact on the upload. Both sides of the traffic and its granularity
has to be taken into account to design accurate P2P IPTV traffic models.



Alternative axiomatics and complexity of deliberative STIT theories

We propose two alternatives to Xu's axiomatization of the Chellas STIT. The
first one also provides an alternative axiomatization of the deliberative STIT.
The second one starts from the idea that the historic necessity operator can be
defined as an abbreviation of operators of agency, and can thus be eliminated
from the logic of the Chellas STIT. The second axiomatization also allows us to
establish that the problem of deciding the satisfiability of a STIT formula
without temporal operators is NP-complete in the single-agent case, and is
NEXPTIME-complete in the multiagent case, both for the deliberative and the
Chellas' STIT.



Neighbor Discovery in Wireless Networks:A Multiuser-Detection Approach

We examine the problem of determining which nodes are neighbors of a given
one in a wireless network. We consider an unsupervised network operating on a
frequency-flat Gaussian channel, where $K+1$ nodes associate their identities
to nonorthogonal signatures, transmitted at random times, synchronously, and
independently. A number of neighbor-discovery algorithms, based on different
optimization criteria, are introduced and analyzed. Numerical results show how
reduced-complexity algorithms can achieve a satisfactory performance.



2D Path Solutions from a Single Layer Excitable CNN Model

An easily implementable path solution algorithm for 2D spatial problems,
based on excitable/programmable characteristics of a specific cellular
nonlinear network (CNN) model is presented and numerically investigated. The
network is a single layer bioinspired model which was also implemented in CMOS
technology. It exhibits excitable characteristics with regionally bistable
cells. The related response realizes propagations of trigger autowaves, where
the excitable mode can be globally preset and reset. It is shown that, obstacle
distributions in 2D space can also be directly mapped onto the coupled cell
array in the network. Combining these two features, the network model can serve
as the main block in a 2D path computing processor. The related algorithm and
configurations are numerically experimented with circuit level parameters and
performance estimations are also presented. The simplicity of the model also
allows alternative technology and device level implementation, which may become
critical in autonomous processor design of related micro or nanoscale robotic
applications.



Sample size cognizant detection of signals in white noise

The detection and estimation of signals in noisy, limited data is a problem
of interest to many scientific and engineering communities. We present a
computationally simple, sample eigenvalue based procedure for estimating the
number of high-dimensional signals in white noise when there are relatively few
samples. We highlight a fundamental asymptotic limit of sample eigenvalue based
detection of weak high-dimensional signals from a limited sample size and
discuss its implication for the detection of two closely spaced signals.
  This motivates our heuristic definition of the 'effective number of
identifiable signals.' Numerical simulations are used to demonstrate the
consistency of the algorithm with respect to the effective number of signals
and the superior performance of the algorithm with respect to Wax and Kailath's
"asymptotically consistent" MDL based estimator.



Coalition Games with Cooperative Transmission: A Cure for the Curse of Boundary Nodes in Selfish Packet-Forwarding Wireless Networks

In wireless packet-forwarding networks with selfish nodes, applications of a
repeated game can induce the nodes to forward each others' packets, so that the
network performance can be improved. However, the nodes on the boundary of such
networks cannot benefit from this strategy, as the other nodes do not depend on
them. This problem is sometimes known as the curse of the boundary nodes. To
overcome this problem, an approach based on coalition games is proposed, in
which the boundary nodes can use cooperative transmission to help the backbone
nodes in the middle of the network. In return, the backbone nodes are willing
to forward the boundary nodes' packets. The stability of the coalitions is
studied using the concept of a core. Then two types of fairness, namely, the
min-max fairness using nucleolus and the average fairness using the Shapley
function are investigated. Finally, a protocol is designed using both repeated
games and coalition games. Simulation results show how boundary nodes and
backbone nodes form coalitions together according to different fairness
criteria. The proposed protocol can improve the network connectivity by about
50%, compared with pure repeated game schemes.



Straggler Identification in Round-Trip Data Streams via Newton's Identities and Invertible Bloom Filters

We introduce the straggler identification problem, in which an algorithm must
determine the identities of the remaining members of a set after it has had a
large number of insertion and deletion operations performed on it, and now has
relatively few remaining members. The goal is to do this in o(n) space, where n
is the total number of identities. The straggler identification problem has
applications, for example, in determining the set of unacknowledged packets in
a high-bandwidth multicast data stream. We provide a deterministic solution to
the straggler identification problem that uses only O(d log n) bits and is
based on a novel application of Newton's identities for symmetric polynomials.
This solution can identify any subset of d stragglers from a set of n O(log
n)-bit identifiers, assuming that there are no false deletions of identities
not already in the set. Indeed, we give a lower bound argument that shows that
any small-space deterministic solution to the straggler identification problem
cannot be guaranteed to handle false deletions. Nevertheless, we show that
there is a simple randomized solution using O(d log n log(1/epsilon)) bits that
can maintain a multiset and solve the straggler identification problem,
tolerating false deletions, where epsilon>0 is a user-defined parameter
bounding the probability of an incorrect response. This randomized solution is
based on a new type of Bloom filter, which we call the invertible Bloom filter.



Vocabulary growth in collaborative tagging systems

We analyze a large-scale snapshot of del.icio.us and investigate how the
number of different tags in the system grows as a function of a suitably
defined notion of time. We study the temporal evolution of the global
vocabulary size, i.e. the number of distinct tags in the entire system, as well
as the evolution of local vocabularies, that is the growth of the number of
distinct tags used in the context of a given resource or user. In both cases,
we find power-law behaviors with exponents smaller than one. Surprisingly, the
observed growth behaviors are remarkably regular throughout the entire history
of the system and across very different resources being bookmarked. Similar
sub-linear laws of growth have been observed in written text, and this
qualitative universality calls for an explanation and points in the direction
of non-trivial cognitive processes in the complex interaction patterns
characterizing collaborative tagging.



Direct Optimization of Ranking Measures

Web page ranking and collaborative filtering require the optimization of
sophisticated performance measures. Current Support Vector approaches are
unable to optimize them directly and focus on pairwise comparisons instead. We
present a new approach which allows direct optimization of the relevant loss
functions. This is achieved via structured estimation in Hilbert spaces. It is
most related to Max-Margin-Markov networks optimization of multivariate
performance measures. Key to our approach is that during training the ranking
problem can be viewed as a linear assignment problem, which can be solved by
the Hungarian Marriage algorithm. At test time, a sort operation is sufficient,
as our algorithm assigns a relevance score to every (document, query) pair.
Experiments show that the our algorithm is fast and that it works very well.



Lifetime Improvement in Wireless Sensor Networks via Collaborative Beamforming and Cooperative Transmission

Collaborative beamforming (CB) and cooperative transmission (CT) have
recently emerged as communication techniques that can make effective use of
collaborative/cooperative nodes to create a virtual
multiple-input/multiple-output (MIMO) system. Extending the lifetime of
networks composed of battery-operated nodes is a key issue in the design and
operation of wireless sensor networks. This paper considers the effects on
network lifetime of allowing closely located nodes to use CB/CT to reduce the
load or even to avoid packet-forwarding requests to nodes that have critical
battery life. First, the effectiveness of CB/CT in improving the signal
strength at a faraway destination using energy in nearby nodes is studied.
Then, the performance improvement obtained by this technique is analyzed for a
special 2D disk case. Further, for general networks in which
information-generation rates are fixed, a new routing problem is formulated as
a linear programming problem, while for other general networks, the cost for
routing is dynamically adjusted according to the amount of energy remaining and
the effectiveness of CB/CT. From the analysis and the simulation results, it is
seen that the proposed method can reduce the payloads of energy-depleting nodes
by about 90% in the special case network considered and improve the lifetimes
of general networks by about 10%, compared with existing techniques.



General-Purpose Computing on a Semantic Network Substrate

This article presents a model of general-purpose computing on a semantic
network substrate. The concepts presented are applicable to any semantic
network representation. However, due to the standards and technological
infrastructure devoted to the Semantic Web effort, this article is presented
from this point of view. In the proposed model of computing, the application
programming interface, the run-time program, and the state of the computing
virtual machine are all represented in the Resource Description Framework
(RDF). The implementation of the concepts presented provides a practical
computing paradigm that leverages the highly-distributed and standardized
representational-layer of the Semantic Web.



Lifetime Improvement of Wireless Sensor Networks by Collaborative Beamforming and Cooperative Transmission

Extending network lifetime of battery-operated devices is a key design issue
that allows uninterrupted information exchange among distributive nodes in
wireless sensor networks. Collaborative beamforming (CB) and cooperative
transmission (CT) have recently emerged as new communication techniques that
enable and leverage effective resource sharing among collaborative/cooperative
nodes. In this paper, we seek to maximize the lifetime of sensor networks by
using the new idea that closely located nodes can use CB/CT to reduce the load
or even avoid packet forwarding requests to nodes that have critical battery
life. First, we study the effectiveness of CB/CT to improve the signal strength
at a faraway destination using energy in nearby nodes. Then, a 2D disk case is
analyzed to assess the resulting performance improvement. For general networks,
if information-generation rates are fixed, the new routing problem is
formulated as a linear programming problem; otherwise, the cost for routing is
dynamically adjusted according to the amount of energy remaining and the
effectiveness of CB/CT. From the analysis and simulation results, it is seen
that the proposed schemes can improve the lifetime by about 90% in the 2D disk
network and by about 10% in the general networks, compared to existing schemes.



Cooperative Transmission Protocols with High Spectral Efficiency and High Diversity Order Using Multiuser Detection and Network Coding

Cooperative transmission is an emerging communication technique that takes
advantages of the broadcast nature of wireless channels. However, due to low
spectral efficiency and the requirement of orthogonal channels, its potential
for use in future wireless networks is limited. In this paper, by making use of
multiuser detection (MUD) and network coding, cooperative transmission
protocols with high spectral efficiency, diversity order, and coding gain are
developed. Compared with the traditional cooperative transmission protocols
with single-user detection, in which the diversity gain is only for one source
user, the proposed MUD cooperative transmission protocols have the merits that
the improvement of one user's link can also benefit the other users. In
addition, using MUD at the relay provides an environment in which network
coding can be employed. The coding gain and high diversity order can be
obtained by fully utilizing the link between the relay and the destination.
  From the analysis and simulation results, it is seen that the proposed
protocols achieve higher diversity gain, better asymptotic efficiency, and
lower bit error rate, compared to traditional MUD and to existing cooperative
transmission protocols.



Diversity-Multiplexing Tradeoff in Selective-Fading MIMO Channels

We establish the optimal diversity-multiplexing (DM) tradeoff of coherent
time, frequency and time-frequency selective-fading MIMO channels and provide a
code design criterion for DM-tradeoff optimality. Our results are based on the
analysis of the "Jensen channel" associated to a given selective-fading MIMO
channel. While the original problem seems analytically intractable due to the
mutual information being a sum of correlated random variables, the Jensen
channel is equivalent to the original channel in the sense of the DM-tradeoff
and lends itself nicely to analytical treatment. Finally, as a consequence of
our results, we find that the classical rank criterion for space-time code
design (in selective-fading MIMO channels) ensures optimality in the sense of
the DM-tradeoff.



Estimation Diversity and Energy Efficiency in Distributed Sensing

Distributed estimation based on measurements from multiple wireless sensors
is investigated. It is assumed that a group of sensors observe the same
quantity in independent additive observation noises with possibly different
variances. The observations are transmitted using amplify-and-forward (analog)
transmissions over non-ideal fading wireless channels from the sensors to a
fusion center, where they are combined to generate an estimate of the observed
quantity. Assuming that the Best Linear Unbiased Estimator (BLUE) is used by
the fusion center, the equal-power transmission strategy is first discussed,
where the system performance is analyzed by introducing the concept of
estimation outage and estimation diversity, and it is shown that there is an
achievable diversity gain on the order of the number of sensors. The optimal
power allocation strategies are then considered for two cases: minimum
distortion under power constraints; and minimum power under distortion
constraints. In the first case, it is shown that by turning off bad sensors,
i.e., sensors with bad channels and bad observation quality, adaptive power
gain can be achieved without sacrificing diversity gain. Here, the adaptive
power gain is similar to the array gain achieved in Multiple-Input
Single-Output (MISO) multi-antenna systems when channel conditions are known to
the transmitter. In the second case, the sum power is minimized under
zero-outage estimation distortion constraint, and some related energy
efficiency issues in sensor networks are discussed.



The Trade-off between Processing Gains of an Impulse Radio UWB System in the Presence of Timing Jitter

In time hopping impulse radio, $N_f$ pulses of duration $T_c$ are transmitted
for each information symbol. This gives rise to two types of processing gain:
(i) pulse combining gain, which is a factor $N_f$, and (ii) pulse spreading
gain, which is $N_c=T_f/T_c$, where $T_f$ is the mean interval between two
subsequent pulses. This paper investigates the trade-off between these two
types of processing gain in the presence of timing jitter. First, an additive
white Gaussian noise (AWGN) channel is considered and approximate closed form
expressions for bit error probability are derived for impulse radio systems
with and without pulse-based polarity randomization. Both symbol-synchronous
and chip-synchronous scenarios are considered. The effects of multiple-access
interference and timing jitter on the selection of optimal system parameters
are explained through theoretical analysis. Finally, a multipath scenario is
considered and the trade-off between processing gains of a synchronous impulse
radio system with pulse-based polarity randomization is analyzed. The effects
of the timing jitter, multiple-access interference and inter-frame interference
are investigated. Simulation studies support the theoretical results.



Bayesian approach to rough set

This paper proposes an approach to training rough set models using Bayesian
framework trained using Markov Chain Monte Carlo (MCMC) method. The prior
probabilities are constructed from the prior knowledge that good rough set
models have fewer rules. Markov Chain Monte Carlo sampling is conducted through
sampling in the rough set granule space and Metropolis algorithm is used as an
acceptance criteria. The proposed method is tested to estimate the risk of HIV
given demographic data. The results obtained shows that the proposed approach
is able to achieve an average accuracy of 58% with the accuracy varying up to
66%. In addition the Bayesian rough set give the probabilities of the estimated
HIV status as well as the linguistic rules describing how the demographic
parameters drive the risk of HIV.



On sensing capacity of sensor networks for the class of linear observation, fixed SNR models

In this paper we address the problem of finding the sensing capacity of
sensor networks for a class of linear observation models and a fixed SNR
regime. Sensing capacity is defined as the maximum number of signal dimensions
reliably identified per sensor observation. In this context sparsity of the
phenomena is a key feature that determines sensing capacity. Precluding the SNR
of the environment the effect of sparsity on the number of measurements
required for accurate reconstruction of a sparse phenomena has been widely
dealt with under compressed sensing. Nevertheless the development there was
motivated from an algorithmic perspective. In this paper our aim is to derive
these bounds in an information theoretic set-up and thus provide algorithm
independent conditions for reliable reconstruction of sparse signals. In this
direction we first generalize the Fano's inequality and provide lower bounds to
the probability of error in reconstruction subject to an arbitrary distortion
criteria. Using these lower bounds to the probability of error, we derive upper
bounds to sensing capacity and show that for fixed SNR regime sensing capacity
goes down to zero as sparsity goes down to zero. This means that
disproportionately more sensors are required to monitor very sparse events. Our
next main contribution is that we show the effect of sensing diversity on
sensing capacity, an effect that has not been considered before. Sensing
diversity is related to the effective \emph{coverage} of a sensor with respect
to the field. In this direction we show the following results (a) Sensing
capacity goes down as sensing diversity per sensor goes down; (b) Random
sampling (coverage) of the field by sensors is better than contiguous location
sampling (coverage).



An Adaptive Strategy for the Classification of G-Protein Coupled Receptors

One of the major problems in computational biology is the inability of
existing classification models to incorporate expanding and new domain
knowledge. This problem of static classification models is addressed in this
paper by the introduction of incremental learning for problems in
bioinformatics. Many machine learning tools have been applied to this problem
using static machine learning structures such as neural networks or support
vector machines that are unable to accommodate new information into their
existing models. We utilize the fuzzy ARTMAP as an alternate machine learning
system that has the ability of incrementally learning new data as it becomes
available. The fuzzy ARTMAP is found to be comparable to many of the widespread
machine learning systems. The use of an evolutionary strategy in the selection
and combination of individual classifiers into an ensemble system, coupled with
the incremental learning ability of the fuzzy ARTMAP is proven to be suitable
as a pattern classifier. The algorithm presented is tested using data from the
G-Coupled Protein Receptors Database and shows good accuracy of 83%. The system
presented is also generally applicable, and can be used in problems in genomics
and proteomics.



Polynomial algorithms for protein similarity search for restricted mRNA structures

In this paper we consider the problem of computing an mRNA sequence of
maximal similarity for a given mRNA of secondary structure constraints,
introduced by Backofen et al. in [BNS02] denoted as the MRSO problem. The
problem is known to be NP-complete for planar associated implied structure
graphs of vertex degree at most 3. In [BFHV05] a first polynomial dynamic
programming algorithms for MRSO on implied structure graphs with maximum vertex
degree 3 of bounded cut-width is shown. We give a simple but more general
polynomial dynamic programming solution for the MRSO problem for associated
implied structure graphs of bounded clique-width. Our result implies that MRSO
is polynomial for graphs of bounded tree-width, co-graphs, $P_4$-sparse graphs,
and distance hereditary graphs. Further we conclude that the problem of
comparing two solutions for MRSO is hard for the class of problems which can be
solved in polynomial time with a number of parallel queries to an oracle in NP.



Une plate-forme dynamique pour l'\'evaluation des performances des bases de donn\'ees \`a objets

In object-oriented or object-relational databases such as multimedia
databases or most XML databases, access patterns are not static, i.e.,
applications do not always access the same objects in the same order
repeatedly. However, this has been the way these databases and associated
optimisation techniques such as clustering have been evaluated up to now. This
paper opens up research regarding this issue by proposing a dynamic object
evaluation framework (DOEF). DOEF accomplishes access pattern change by
defining configurable styles of change. It is a preliminary prototype that has
been designed to be open and fully extensible. Though originally designed for
the object-oriented model, it can also be used within the object-relational
model with few adaptations. Furthermore, new access pattern change models can
be added too. To illustrate the capabilities of DOEF, we conducted two
different sets of experiments. In the first set of experiments, we used DOEF to
compare the performances of four state of the art dynamic clustering
algorithms. The results show that DOEF is effective at determining the
adaptability of each dynamic clustering algorithm to changes in access pattern.
They also led us to conclude that dynamic clustering algorithms can cope with
moderate levels of access pattern change, but that performance rapidly degrades
to be worse than no clustering when vigorous styles of access pattern change
are applied. In the second set of experiments, we used DOEF to compare the
performance of two different object stores: Platypus and SHORE. The use of DOEF
exposed the poor swapping performance of Platypus.



Conception d'un banc d'essais d\'ecisionnel

We present in this paper a new benchmark for evaluating the performances of
data warehouses. Benchmarking is useful either to system users for comparing
the performances of different systems, or to system engineers for testing the
effect of various design choices. While the TPC (Transaction Processing
Performance Council) standard benchmarks address the first point, they are not
tuneable enough to address the second one. Our Data Warehouse Engineering
Benchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses
and workloads. DWEB is fully parameterized. However, two levels of
parameterization keep it easy to tune. Since DWEB mainly meets engineering
benchmarking needs, it is complimentary to the TPC standard benchmarks, and not
a competitor. Finally, DWEB is implemented as a Java free software that can be
interfaced with most existing relational database management systems.



Smooth R\'enyi Entropy of Ergodic Quantum Information Sources

We prove that the average smooth Renyi entropy rate will approach the entropy
rate of a stationary, ergodic information source, which is equal to the Shannon
entropy rate for a classical information source and the von Neumann entropy
rate for a quantum information source.



Comparing Robustness of Pairwise and Multiclass Neural-Network Systems for Face Recognition

Noise, corruptions and variations in face images can seriously hurt the
performance of face recognition systems. To make such systems robust,
multiclass neuralnetwork classifiers capable of learning from noisy data have
been suggested. However on large face data sets such systems cannot provide the
robustness at a high level. In this paper we explore a pairwise neural-network
system as an alternative approach to improving the robustness of face
recognition. In our experiments this approach is shown to outperform the
multiclass neural-network system in terms of the predictive accuracy on the
face images corrupted by noise.



Vers l'auto-administration des entrep\^ots de donn\'ees

With the wide development of databases in general and data warehouses in
particular, it is important to reduce the tasks that a database administrator
must perform manually. The idea of using data mining techniques to extract
useful knowledge for administration from the data themselves has existed for
some years. However, little research has been achieved. The aim of this study
is to search for a way of extracting useful knowledge from stored data to
automatically apply performance optimization techniques, and more particularly
indexing techniques. We have designed a tool that extracts frequent itemsets
from a given workload to compute an index configuration that helps optimizing
data access time. The experiments we performed showed that the index
configurations generated by our tool allowed performance gains of 15% to 25% on
a test database and a test data warehouse.



$\delta$-sequences and Evaluation Codes defined by Plane Valuations at Infinity

We introduce the concept of $\delta$-sequence. A $\delta$-sequence $\Delta$
generates a well-ordered semigroup $S$ in $\mathbb{Z}^2$ or $\mathbb{R}$. We
show how to construct (and compute parameters) for the dual code of any
evaluation code associated with a weight function defined by $\Delta$ from the
polynomial ring in two indeterminates to a semigroup $S$ as above. We prove
that this is a simple procedure which can be understood by considering a
particular class of valuations of function fields of surfaces, called plane
valuations at infinity. We also give algorithms to construct an unlimited
number of $\delta$-sequences of the different existing types, and so this paper
provides the tools to know and use a new large set of codes.



Simulating spin systems on IANUS, an FPGA-based computer

We describe the hardwired implementation of algorithms for Monte Carlo
simulations of a large class of spin models. We have implemented these
algorithms as VHDL codes and we have mapped them onto a dedicated processor
based on a large FPGA device. The measured performance on one such processor is
comparable to O(100) carefully programmed high-end PCs: it turns out to be even
better for some selected spin models. We describe here codes that we are
currently executing on the IANUS massively parallel FPGA-based system.



On Energy Efficient Hierarchical Cross-Layer Design: Joint Power Control and Routing for Ad Hoc Networks

In this paper, a hierarchical cross-layer design approach is proposed to
increase energy efficiency in ad hoc networks through joint adaptation of
nodes' transmitting powers and route selection. The design maintains the
advantages of the classic OSI model, while accounting for the cross-coupling
between layers, through information sharing. The proposed joint power control
and routing algorithm is shown to increase significantly the overall energy
efficiency of the network, at the expense of a moderate increase in complexity.
Performance enhancement of the joint design using multiuser detection is also
investigated, and it is shown that the use of multiuser detection can increase
the capacity of the ad hoc network significantly for a given level of energy
consumption.



Capacity of a Class of Modulo-Sum Relay Channels

This paper characterizes the capacity of a class of modulo additive noise
relay channels, in which the relay observes a corrupted version of the noise
and has a separate channel to the destination. The capacity is shown to be
strictly below the cut-set bound in general and achievable using a
quantize-and-forward strategy at the relay. This result confirms a conjecture
by Ahlswede and Han about the capacity of channels with rate limited state
information at the destination for this particular class of channels.



Rough Sets Computations to Impute Missing Data

Many techniques for handling missing data have been proposed in the
literature. Most of these techniques are overly complex. This paper explores an
imputation technique based on rough set computations. In this paper,
characteristic relations are introduced to describe incompletely specified
decision tables.It is shown that the basic rough set idea of lower and upper
approximations for incompletely specified decision tables may be defined in a
variety of different ways. Empirical results obtained using real data are given
and they provide a valuable and promising insight to the problem of missing
data. Missing data were predicted with an accuracy of up to 99%.



Sabbath Day Home Automation: "It's Like Mixing Technology and Religion"

We present a qualitative study of 20 American Orthodox Jewish families' use
of home automation for religious purposes. These lead users offer insight into
real-life, long-term experience with home automation technologies. We discuss
how automation was seen by participants to contribute to spiritual experience
and how participants oriented to the use of automation as a religious custom.
We also discuss the relationship of home automation to family life. We draw
design implications for the broader population, including surrender of control
as a design resource, home technologies that support long-term goals and
lifestyle choices, and respite from technology.



Capacity Gain from Two-Transmitter and Two-Receiver Cooperation

Capacity improvement from transmitter and receiver cooperation is
investigated in a two-transmitter, two-receiver network with phase fading and
full channel state information available at all terminals. The transmitters
cooperate by first exchanging messages over an orthogonal transmitter
cooperation channel, then encoding jointly with dirty paper coding. The
receivers cooperate by using Wyner-Ziv compress-and-forward over an analogous
orthogonal receiver cooperation channel. To account for the cost of
cooperation, the allocation of network power and bandwidth among the data and
cooperation channels is studied. It is shown that transmitter cooperation
outperforms receiver cooperation and improves capacity over non-cooperative
transmission under most operating conditions when the cooperation channel is
strong. However, a weak cooperation channel limits the transmitter cooperation
rate; in this case receiver cooperation is more advantageous.
Transmitter-and-receiver cooperation offers sizable additional capacity gain
over transmitter-only cooperation at low SNR, whereas at high SNR transmitter
cooperation alone captures most of the cooperative capacity improvement.



Lower Bounds on Implementing Robust and Resilient Mediators

We consider games that have (k,t)-robust equilibria when played with a
mediator, where an equilibrium is (k,t)-robust if it tolerates deviations by
coalitions of size up to k and deviations by up to $t$ players with unknown
utilities. We prove lower bounds that match upper bounds on the ability to
implement such mediators using cheap talk (that is, just allowing communication
among the players). The bounds depend on (a) the relationship between k, t, and
n, the total number of players in the system; (b) whether players know the
exact utilities of other players; (c) whether there are broadcast channels or
just point-to-point channels; (d) whether cryptography is available; and (e)
whether the game has a $k+t)-punishment strategy; that is, a strategy that, if
used by all but at most $k+t$ players, guarantees that every player gets a
worse outcome than they do with the equilibrium strategy.



Evaluating Personal Archiving Strategies for Internet-based Information

Internet-based personal digital belongings present different vulnerabilities
than locally stored materials. We use responses to a survey of people who have
recovered lost websites, in combination with supplementary interviews, to paint
a fuller picture of current curatorial strategies and practices. We examine the
types of personal, topical, and commercial websites that respondents have lost
and the reasons they have lost this potentially valuable material. We further
explore what they have tried to recover and how the loss influences their
subsequent practices. We found that curation of personal digital materials in
online stores bears some striking similarities to the curation of similar
materials stored locally in that study participants continue to archive
personal assets by relying on a combination of benign neglect, sporadic
backups, and unsystematic file replication. However, we have also identified
issues specific to Internet-based material: how risk is spread by distributing
the files among multiple servers and services; the circular reasoning
participants use when they discuss the safety of their digital assets; and the
types of online material that are particularly vulnerable to loss. The study
reveals ways in which expectations of permanence and notification are violated
and situations in which benign neglect has far greater consequences for the
long-term fate of important digital assets.



The Long Term Fate of Our Digital Belongings: Toward a Service Model for Personal Archives

We conducted a preliminary field study to understand the current state of
personal digital archiving in practice. Our aim is to design a service for the
long-term storage, preservation, and access of digital belongings by examining
how personal archiving needs intersect with existing and emerging archiving
technologies, best practices, and policies. Our findings not only confirmed
that experienced home computer users are creating, receiving, and finding an
increasing number of digital belongings, but also that they have already lost
irreplaceable digital artifacts such as photos, creative efforts, and records.
Although participants reported strategies such as backup and file replication
for digital safekeeping, they were seldom able to implement them consistently.
Four central archiving themes emerged from the data: (1) people find it
difficult to evaluate the worth of accumulated materials; (2) personal storage
is highly distributed both on- and offline; (3) people are experiencing
magnified curatorial problems associated with managing files in the aggregate,
creating appropriate metadata, and migrating materials to maintainable formats;
and (4) facilities for long-term access are not supported by the current
desktop metaphor. Four environmental factors further complicate archiving in
consumer settings: the pervasive influence of malware; consumer reliance on ad
hoc IT providers; an accretion of minor system and registry inconsistencies;
and strong consumer beliefs about the incorruptibility of digital forms, the
reliability of digital technologies, and the social vulnerability of networked
storage.



An Automated Evaluation Metric for Chinese Text Entry

In this paper, we propose an automated evaluation metric for text entry. We
also consider possible improvements to existing text entry evaluation metrics,
such as the minimum string distance error rate, keystrokes per character, cost
per correction, and a unified approach proposed by MacKenzie, so they can
accommodate the special characteristics of Chinese text. Current methods lack
an integrated concern about both typing speed and accuracy for Chinese text
entry evaluation. Our goal is to remove the bias that arises due to human
factors. First, we propose a new metric, called the correction penalty (P),
based on Fitts' law and Hick's law. Next, we transform it into the approximate
amortized cost (AAC) of information theory. An analysis of the AAC of Chinese
text input methods with different context lengths is also presented.



On the Development of Text Input Method - Lessons Learned

Intelligent Input Methods (IM) are essential for making text entries in many
East Asian scripts, but their application to other languages has not been fully
explored. This paper discusses how such tools can contribute to the development
of computer processing of other oriental languages. We propose a design
philosophy that regards IM as a text service platform, and treats the study of
IM as a cross disciplinary subject from the perspectives of software
engineering, human-computer interaction (HCI), and natural language processing
(NLP). We discuss these three perspectives and indicate a number of possible
future research directions.



Periodicity of certain piecewise affine planar maps

We determine periodic and aperiodic points of certain piecewise affine maps
in the Euclidean plane. Using these maps, we prove for
$\lambda\in\{\frac{\pm1\pm\sqrt5}2,\pm\sqrt2,\pm\sqrt3\}$ that all integer
sequences $(a_k)_{k\in\mathbb Z}$ satisfying $0\le a_{k-1}+\lambda
a_k+a_{k+1}<1$ are periodic.



The Complexity of Weighted Boolean #CSP

This paper gives a dichotomy theorem for the complexity of computing the
partition function of an instance of a weighted Boolean constraint satisfaction
problem. The problem is parameterised by a finite set F of non-negative
functions that may be used to assign weights to the configurations (feasible
solutions) of a problem instance. Classical constraint satisfaction problems
correspond to the special case of 0,1-valued functions. We show that the
partition function, i.e. the sum of the weights of all configurations, can be
computed in polynomial time if either (1) every function in F is of ``product
type'', or (2) every function in F is ``pure affine''. For every other fixed
set F, computing the partition function is FP^{#P}-complete.



Network statistics on early English Syntax: Structural criteria

This paper includes a reflection on the role of networks in the study of
English language acquisition, as well as a collection of practical criteria to
annotate free-speech corpora from children utterances. At the theoretical
level, the main claim of this paper is that syntactic networks should be
interpreted as the outcome of the use of the syntactic machinery. Thus, the
intrinsic features of such machinery are not accessible directly from (known)
network properties. Rather, what one can see are the global patterns of its use
and, thus, a global view of the power and organization of the underlying
grammar. Taking a look into more practical issues, the paper examines how to
build a net from the projection of syntactic relations. Recall that, as opposed
to adult grammars, early-child language has not a well-defined concept of
structure. To overcome such difficulty, we develop a set of systematic criteria
assuming constituency hierarchy and a grammar based on lexico-thematic
relations. At the end, what we obtain is a well defined corpora annotation that
enables us i) to perform statistics on the size of structures and ii) to build
a network from syntactic relations over which we can perform the standard
measures of complexity. We also provide a detailed example.



Distributed Algorithms for Spectrum Allocation, Power Control, Routing, and Congestion Control in Wireless Networks

We develop distributed algorithms to allocate resources in multi-hop wireless
networks with the aim of minimizing total cost. In order to observe the
fundamental duplexing constraint that co-located transmitters and receivers
cannot operate simultaneously on the same frequency band, we first devise a
spectrum allocation scheme that divides the whole spectrum into multiple
sub-bands and activates conflict-free links on each sub-band. We show that the
minimum number of required sub-bands grows asymptotically at a logarithmic rate
with the chromatic number of network connectivity graph. A simple distributed
and asynchronous algorithm is developed to feasibly activate links on the
available sub-bands. Given a feasible spectrum allocation, we then design
node-based distributed algorithms for optimally controlling the transmission
powers on active links for each sub-band, jointly with traffic routes and user
input rates in response to channel states and traffic demands. We show that
under specified conditions, the algorithms asymptotically converge to the
optimal operating point.



Avoiding Rotated Bitboards with Direct Lookup

This paper describes an approach for obtaining direct access to the attacked
squares of sliding pieces without resorting to rotated bitboards. The technique
involves creating four hash tables using the built in hash arrays from an
interpreted, high level language. The rank, file, and diagonal occupancy are
first isolated by masking the desired portion of the board. The attacked
squares are then directly retrieved from the hash tables. Maintaining
incrementally updated rotated bitboards becomes unnecessary as does all the
updating, mapping and shifting required to access the attacked squares.
Finally, rotated bitboard move generation speed is compared with that of the
direct hash table lookup method.



Stochastic Optimization Algorithms

When looking for a solution, deterministic methods have the enormous
advantage that they do find global optima. Unfortunately, they are very
CPU-intensive, and are useless on untractable NP-hard problems that would
require thousands of years for cutting-edge computers to explore. In order to
get a result, one needs to revert to stochastic algorithms, that sample the
search space without exploring it thoroughly. Such algorithms can find very
good results, without any guarantee that the global optimum has been reached;
but there is often no other choice than using them. This chapter is a short
introduction to the main methods used in stochastic optimization.



Minimizing Unsatisfaction in Colourful Neighbourhoods

Colouring sparse graphs under various restrictions is a theoretical problem
of significant practical relevance. Here we consider the problem of maximizing
the number of different colours available at the nodes and their
neighbourhoods, given a predetermined number of colours. In the analytical
framework of a tree approximation, carried out at both zero and finite
temperatures, solutions obtained by population dynamics give rise to estimates
of the threshold connectivity for the incomplete to complete transition, which
are consistent with those of existing algorithms. The nature of the transition
as well as the validity of the tree approximation are investigated.



A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA Networks with Delay Constraints

A game-theoretic framework is used to study the effect of constellation size
on the energy efficiency of wireless networks for M-QAM modulation. A
non-cooperative game is proposed in which each user seeks to choose its
transmit power (and possibly transmit symbol rate) as well as the constellation
size in order to maximize its own utility while satisfying its delay
quality-of-service (QoS) constraint. The utility function used here measures
the number of reliable bits transmitted per joule of energy consumed, and is
particularly suitable for energy-constrained networks. The best-response
strategies and Nash equilibrium solution for the proposed game are derived. It
is shown that in order to maximize its utility (in bits per joule), a user must
choose the lowest constellation size that can accommodate the user's delay
constraint. Using this framework, the tradeoffs among energy efficiency, delay,
throughput and constellation size are also studied and quantified. The effect
of trellis-coded modulation on energy efficiency is also discussed.



Energy-Efficient Resource Allocation in Wireless Networks with Quality-of-Service Constraints

A game-theoretic model is proposed to study the cross-layer problem of joint
power and rate control with quality of service (QoS) constraints in
multiple-access networks. In the proposed game, each user seeks to choose its
transmit power and rate in a distributed manner in order to maximize its own
utility while satisfying its QoS requirements. The user's QoS constraints are
specified in terms of the average source rate and an upper bound on the average
delay where the delay includes both transmission and queuing delays. The
utility function considered here measures energy efficiency and is particularly
suitable for wireless networks with energy constraints. The Nash equilibrium
solution for the proposed non-cooperative game is derived and a closed-form
expression for the utility achieved at equilibrium is obtained. It is shown
that the QoS requirements of a user translate into a "size" for the user which
is an indication of the amount of network resources consumed by the user. Using
this competitive multiuser framework, the tradeoffs among throughput, delay,
network capacity and energy efficiency are studied. In addition, analytical
expressions are given for users' delay profiles and the delay performance of
the users at Nash equilibrium is quantified.



A Unified Approach to Energy-Efficient Power Control in Large CDMA Systems

A unified approach to energy-efficient power control is proposed for
code-division multiple access (CDMA) networks. The approach is applicable to a
large family of multiuser receivers including the matched filter, the
decorrelator, the linear minimum mean-square error (MMSE) receiver, and the
(nonlinear) optimal detectors. It exploits the linear relationship that has
been shown to exist between the transmit power and the output
signal-to-interference-plus-noise ratio (SIR) in the large-system limit. It is
shown that, for this family of receivers, when users seek to selfishly maximize
their own energy efficiency, the Nash equilibrium is SIR-balanced. In addition,
a unified power control (UPC) algorithm for reaching the Nash equilibrium is
proposed. The algorithm adjusts the user's transmit powers by iteratively
computing the large-system multiuser efficiency, which is independent of
instantaneous spreading sequences. The convergence of the algorithm is proved
for the matched filter, the decorrelator, and the MMSE receiver, and is
demonstrated by means of simulation for an optimal detector. Moreover, the
performance of the algorithm in finite-size systems is studied and compared
with that of a conventional power control scheme, in which user powers depend
on the instantaneous spreading sequences.



A Note on Ontology and Ordinary Language

We argue for a compositional semantics grounded in a strongly typed ontology
that reflects our commonsense view of the world and the way we talk about it.
Assuming such a structure we show that the semantics of various natural
language phenomena may become nearly trivial.



An algorithm for clock synchronization with the gradient property in sensor networks

We introduce a distributed algorithm for clock synchronization in sensor
networks. Our algorithm assumes that nodes in the network only know their
immediate neighborhoods and an upper bound on the network's diameter.
Clock-synchronization messages are only sent as part of the communication,
assumed reasonably frequent, that already takes place among nodes. The
algorithm has the gradient property of [2], achieving an O(1) worst-case skew
between the logical clocks of neighbors. As in the case of [3,8], the
algorithm's actions are such that no constant lower bound exists on the rate at
which logical clocks progress in time, and for this reason the lower bound of
[2,5] that forbids constant skew between neighbors does not apply.



Acyclic Preference Systems in P2P Networks

In this work we study preference systems natural for the Peer-to-Peer
paradigm. Most of them fall in three categories: global, symmetric and
complementary. All these systems share an acyclicity property. As a
consequence, they admit a stable (or Pareto efficient) configuration, where no
participant can collaborate with better partners than their current ones. We
analyze the representation of the such preference systems and show that any
acyclic system can be represented with a symmetric mark matrix. This gives a
method to merge acyclic preference systems and retain the acyclicity. We also
consider such properties of the corresponding collaboration graph, as
clustering coefficient and diameter. In particular, studying the example of
preferences based on real latency measurements, we observe that its stable
configuration is a small-world graph.



Ensemble Learning for Free with Evolutionary Algorithms ?

Evolutionary Learning proceeds by evolving a population of classifiers, from
which it generally returns (with some notable exceptions) the single
best-of-run classifier as final result. In the meanwhile, Ensemble Learning,
one of the most efficient approaches in supervised Machine Learning for the
last decade, proceeds by building a population of diverse classifiers. Ensemble
Learning with Evolutionary Computation thus receives increasing attention. The
Evolutionary Ensemble Learning (EEL) approach presented in this paper features
two contributions. First, a new fitness function, inspired by co-evolution and
enforcing the classifier diversity, is presented. Further, a new selection
criterion based on the classification margin is proposed. This criterion is
used to extract the classifier ensemble from the final population only
(Off-line) or incrementally along evolution (On-line). Experiments on a set of
benchmark problems show that Off-line outperforms single-hypothesis
evolutionary learning and state-of-art Boosting and generates smaller
classifier ensembles.



The Complexity of Model Checking Higher-Order Fixpoint Logic

Higher-Order Fixpoint Logic (HFL) is a hybrid of the simply typed
\lambda-calculus and the modal \lambda-calculus. This makes it a highly
expressive temporal logic that is capable of expressing various interesting
correctness properties of programs that are not expressible in the modal
\lambda-calculus.
  This paper provides complexity results for its model checking problem. In
particular we consider those fragments of HFL built by using only types of
bounded order k and arity m. We establish k-fold exponential time completeness
for model checking each such fragment. For the upper bound we use fixpoint
elimination to obtain reachability games that are singly-exponential in the
size of the formula and k-fold exponential in the size of the underlying
transition system. These games can be solved in deterministic linear time. As a
simple consequence, we obtain an exponential time upper bound on the expression
complexity of each such fragment.
  The lower bound is established by a reduction from the word problem for
alternating (k-1)-fold exponential space bounded Turing Machines. Since there
are fixed machines of that type whose word problems are already hard with
respect to k-fold exponential time, we obtain, as a corollary, k-fold
exponential time completeness for the data complexity of our fragments of HFL,
provided m exceeds 3. This also yields a hierarchy result in expressive power.



Diversity of MIMO Multihop Relay Channels - Part I: Amplify-and-Forward

In this two-part paper, we consider the multiantenna multihop relay channels
in which the source signal arrives at the destination through N independent
relaying hops in series. The main concern of this work is to design relaying
strategies that utilize efficiently the relays in such a way that the diversity
is maximized. In part I, we focus on the amplify-and-forward (AF) strategy with
which the relays simply scale the received signal and retransmit it. More
specifically, we characterize the diversity-multiplexing tradeoff (DMT) of the
AF scheme in a general multihop channel with arbitrary number of antennas and
arbitrary number of hops. The DMT is in closed-form expression as a function of
the number of antennas at each node. First, we provide some basic results on
the DMT of the general Rayleigh product channels. It turns out that these
results have very simple and intuitive interpretation. Then, the results are
applied to the AF multihop channels which is shown to be equivalent to the
Rayleigh product channel, in the DMT sense. Finally, the project-and-forward
(PF) scheme, a variant of the AF scheme, is proposed. We show that the PF
scheme has the same DMT as the AF scheme, while the PF can have significant
power gain over the AF scheme in some cases. In part II, we will derive the
upper bound on the diversity of the multihop channels and show that it can be
achieved by partitioning the multihop channel into AF subchannels.



Critical phenomena in complex networks

The combination of the compactness of networks, featuring small diameters,
and their complex architectures results in a variety of critical effects
dramatically different from those in cooperative systems on lattices. In the
last few years, researchers have made important steps toward understanding the
qualitatively new critical phenomena in complex networks. We review the
results, concepts, and methods of this rapidly developing field. Here we mostly
consider two closely related classes of these critical phenomena, namely
structural phase transitions in the network architectures and transitions in
cooperative models on networks as substrates. We also discuss systems where a
network and interacting agents on it influence each other. We overview a wide
range of critical phenomena in equilibrium and growing networks including the
birth of the giant connected component, percolation, k-core percolation,
phenomena near epidemic thresholds, condensation transitions, critical
phenomena in spin models placed on networks, synchronization, and
self-organized criticality effects in interacting systems on networks. We also
discuss strong finite size effects in these systems and highlight open problems
and perspectives.



Checking Equivalence of Quantum Circuits and States

Quantum computing promises exponential speed-ups for important simulation and
optimization problems. It also poses new CAD problems that are similar to, but
more challenging, than the related problems in classical (non-quantum) CAD,
such as determining if two states or circuits are functionally equivalent.
While differences in classical states are easy to detect, quantum states, which
are represented by complex-valued vectors, exhibit subtle differences leading
to several notions of equivalence. This provides flexibility in optimizing
quantum circuits, but leads to difficult new equivalence-checking issues for
simulation and synthesis. We identify several different equivalence-checking
problems and present algorithms for practical benchmarks, including quantum
communication and search circuits, which are shown to be very fast and robust
for hundreds of qubits.



Can the Internet cope with stress?

When will the Internet become aware of itself? In this note the problem is
approached by asking an alternative question: Can the Internet cope with
stress? By extrapolating the psychological difference between coping and
defense mechanisms a distributed software experiment is outlined which could
reject the hypothesis that the Internet is not a conscious entity.



Joint Detection and Identification of an Unobservable Change in the Distribution of a Random Sequence

This paper examines the joint problem of detection and identification of a
sudden and unobservable change in the probability distribution function (pdf)
of a sequence of independent and identically distributed (i.i.d.) random
variables to one of finitely many alternative pdf's. The objective is quick
detection of the change and accurate inference of the ensuing pdf. Following a
Bayesian approach, a new sequential decision strategy for this problem is
revealed and is proven optimal. Geometrical properties of this strategy are
demonstrated via numerical examples.



Reliable Memories Built from Unreliable Components Based on Expander Graphs

In this paper, memories built from components subject to transient faults are
considered. A fault-tolerant memory architecture based on low-density
parity-check codes is proposed and the existence of reliable memories for the
adversarial failure model is proved. The proof relies on the expansion property
of the underlying Tanner graph of the code. An equivalence between the
Taylor-Kuznetsov (TK) scheme and Gallager B algorithm is established and the
results are extended to the independent failure model. It is also shown that
the proposed memory architecture has lower redundancy compared to the TK
scheme. The results are illustrated with specific numerical examples.



Constructions of q-Ary Constant-Weight Codes

This paper introduces a new combinatorial construction for q-ary
constant-weight codes which yields several families of optimal codes and
asymptotically optimal codes. The construction reveals intimate connection
between q-ary constant-weight codes and sets of pairwise disjoint combinatorial
designs of various types.



An efficient centralized binary multicast network coding algorithm for any cyclic network

We give an algorithm for finding network encoding and decoding equations for
error-free multicasting networks with multiple sources and sinks. The algorithm
given is efficient (polynomial complexity) and works on any kind of network
(acyclic, link cyclic, flow cyclic, or even in the presence of knots). The key
idea will be the appropriate use of the delay (both natural and additional)
during the encoding. The resulting code will always work with finite delay with
binary encoding coefficients.



About the domino problem in the hyperbolic plane, a new solution: complement

In this paper, we complete the construction of paper arXiv:cs.CG/0701096v2.
Together with the proof contained in arXiv:cs.CG/0701096v2, this paper
definitely proves that the general problem of tiling the hyperbolic plane with
{\it \`a la} Wang tiles is undecidable.



An Energy Efficiency Perspective on Training for Fading Channels

In this paper, the bit energy requirements of training-based transmission
over block Rayleigh fading channels are studied. Pilot signals are employed to
obtain the minimum mean-square-error (MMSE) estimate of the channel fading
coefficients. Energy efficiency is analyzed in the worst case scenario where
the channel estimate is assumed to be perfect and the error in the estimate is
considered as another source of additive Gaussian noise. It is shown that bit
energy requirement grows without bound as the snr goes to zero, and the minimum
bit energy is achieved at a nonzero snr value below which one should not
operate. The effect of the block length on both the minimum bit energy and the
snr value at which the minimum is achieved is investigated. Flash training
schemes are analyzed and shown to improve the energy efficiency in the low-snr
regime. Energy efficiency analysis is also carried out when peak power
constraints are imposed on pilot signals.



On the Low-SNR Capacity of Phase-Shift Keying with Hard-Decision Detection

The low-snr capacity of M-ary PSK transmission over both the additive white
Gaussian noise (AWGN) and fading channels is analyzed when hard-decision
detection is employed at the receiver. Closed-form expressions for the first
and second derivatives of the capacity at zero snr are obtained. The
spectral-efficiency/bit-energy tradeoff in the low-snr regime is analyzed by
finding the wideband slope and the bit energy required at zero spectral
efficiency. Practical design guidelines are drawn from the
information-theoretic analysis. The fading channel analysis is conducted for
both coherent and noncoherent cases, and the performance penalty in the
low-power regime for not knowing the channel is identified.



Training Optimization for Gauss-Markov Rayleigh Fading Channels

In this paper, pilot-assisted transmission over Gauss-Markov Rayleigh fading
channels is considered. A simple scenario, where a single pilot signal is
transmitted every T symbols and T-1 data symbols are transmitted in between the
pilots, is studied. First, it is assumed that binary phase-shift keying (BPSK)
modulation is employed at the transmitter. With this assumption, the training
period, and data and training power allocation are jointly optimized by
maximizing an achievable rate expression. Achievable rates and energy-per-bit
requirements are computed using the optimal training parameters. Secondly, a
capacity lower bound is obtained by considering the error in the estimate as
another source of additive Gaussian noise, and the training parameters are
optimized by maximizing this lower bound.



Performance Analysis for Multichannel Reception of OOFSK Signaling

In this paper, the error performance of on-off frequency shift keying (OOFSK)
modulation over fading channels is analyzed when the receiver is equipped with
multiple antennas. The analysis is conducted in two cases: the coherent
scenario where the fading is perfectly known at the receiver, and the
noncoherent scenario where neither the receiver nor the transmitter knows the
fading coefficients. For both cases, the maximum a posteriori probability (MAP)
detection rule is derived and analytical probability of error expressions are
obtained. The effect of fading correlation among the receiver antennas is also
studied. Simulation results indicate that for sufficiently low duty cycle
values, lower probability of error values with respect to FSK signaling are
achieved. Equivalently, when compared to FSK modulation, OOFSK with low duty
cycle requires less energy to achieve the same probability of error, which
renders this modulation a more energy efficient transmission technique.



Error Probability Analysis of Peaky Signaling over Fading Channels

In this paper, the performance of signaling strategies with high
peak-to-average power ratio is analyzed in both coherent and noncoherent fading
channels. Two recently proposed modulation schemes, namely on-off binary
phase-shift keying and on-off quaternary phase-shift keying, are considered.
For these modulation formats, the optimal decision rules used at the detector
are identified and analytical expressions for the error probabilities are
obtained. Numerical techniques are employed to compute the error probabilities.
It is concluded that increasing the peakedness of the signals results in
reduced error rates for a given power level and hence improve the energy
efficiency.



Comparison of Discrete and Continuous Wavelet Transforms

In this paper we outline several points of view on the interplay between
discrete and continuous wavelet transforms; stressing both pure and applied
aspects of both. We outline some new links between the two transform
technologies based on the theory of representations of generators and
relations. By this we mean a finite system of generators which are represented
by operators in Hilbert space. We further outline how these representations
yield sub-band filter banks for signal and image processing algorithms.



Oblivious Transfer based on Key Exchange

Key-exchange protocols have been overlooked as a possible means for
implementing oblivious transfer (OT). In this paper we present a protocol for
mutual exchange of secrets, 1-out-of-2 OT and coin flipping similar to
Diffie-Hellman protocol using the idea of obliviously exchanging encryption
keys. Since, Diffie-Hellman scheme is widely used, our protocol may provide a
useful alternative to the conventional methods for implementation of oblivious
transfer and a useful primitive in building larger cryptographic schemes.



Fault Classification in Cylinders Using Multilayer Perceptrons, Support Vector Machines and Guassian Mixture Models

Gaussian mixture models (GMM) and support vector machines (SVM) are
introduced to classify faults in a population of cylindrical shells. The
proposed procedures are tested on a population of 20 cylindrical shells and
their performance is compared to the procedure, which uses multi-layer
perceptrons (MLP). The modal properties extracted from vibration data are used
to train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM
produces 94% classification accuracy while the MLP produces 88% classification
rates.



The Parameter-Less Self-Organizing Map algorithm

The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network
algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a
learning rate and annealing schemes for learning rate and neighbourhood size.
We discuss the relative performance of the PLSOM and the SOM and demonstrate
some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally
we discuss some example applications of the PLSOM and present a proof of
ordering under certain limited conditions.



Using Images to create a Hierarchical Grid Spatial Index

This paper presents a hybrid approach to spatial indexing of two dimensional
data. It sheds new light on the age old problem by thinking of the traditional
algorithms as working with images. Inspiration is drawn from an analogous
situation that is found in machine and human vision. Image processing
techniques are used to assist in the spatial indexing of the data. A fixed grid
approach is used and bins with too many records are sub-divided hierarchically.
Search queries are pre-computed for bins that do not contain any data records.
This has the effect of dividing the search space up into non rectangular
regions which are based on the spatial properties of the data. The bucketing
quad tree can be considered as an image with a resolution of two by two for
each layer. The results show that this method performs better than the quad
tree if there are more divisions per layer. This confirms our suspicions that
the algorithm works better if it gets to look at the data with higher
resolution images. An elegant class structure is developed where the
implementation of concrete spatial indexes for a particular data type merely
relies on rendering the data onto an image.



Riemannian level-set methods for tensor-valued data

We present a novel approach for the derivation of PDE modeling
curvature-driven flows for matrix-valued data. This approach is based on the
Riemannian geometry of the manifold of Symmetric Positive Definite Matrices
Pos(n).



Power Allocation for Discrete-Input Non-Ergodic Block-Fading Channels

We consider power allocation algorithms for fixed-rate transmission over
Nakagami-m non-ergodic block-fading channels with perfect transmitter and
receiver channel state information and discrete input signal constellations
under both short- and long-term power constraints. Optimal power allocation
schemes are shown to be direct applications of previous results in the
literature. We show that the SNR exponent of the optimal short-term scheme is
given by the Singleton bound. We also illustrate the significant gains
available by employing long-term power constraints. Due to the nature of the
expressions involved, the complexity of optimal schemes may be prohibitive for
system implementation. We propose simple sub-optimal power allocation schemes
whose outage probability performance is very close to the minimum outage
probability obtained by optimal schemes.



More Efficient Algorithms and Analyses for Unequal Letter Cost Prefix-Free Coding

There is a large literature devoted to the problem of finding an optimal
(min-cost) prefix-free code with an unequal letter-cost encoding alphabet of
size. While there is no known polynomial time algorithm for solving it
optimally there are many good heuristics that all provide additive errors to
optimal. The additive error in these algorithms usually depends linearly upon
the largest encoding letter size.
  This paper was motivated by the problem of finding optimal codes when the
encoding alphabet is infinite. Because the largest letter cost is infinite, the
previous analyses could give infinite error bounds. We provide a new algorithm
that works with infinite encoding alphabets. When restricted to the finite
alphabet case, our algorithm often provides better error bounds than the best
previous ones known.



The Complexity of Games on Higher Order Pushdown Automata

We prove an n-EXPTIME lower bound for the problem of deciding the winner in a
reachability game on Higher Order Pushdown Automata (HPDA) of level n. This
bound matches the known upper bound for parity games on HPDA. As a consequence
the mu-calculus model checking over graphs given by n-HPDA is n-EXPTIME
complete.



Dynamic Clustering in Object-Oriented Databases: An Advocacy for Simplicity

We present in this paper three dynamic clustering techniques for
Object-Oriented Databases (OODBs). The first two, Dynamic, Statistical &
Tunable Clustering (DSTC) and StatClust, exploit both comprehensive usage
statistics and the inter-object reference graph. They are quite elaborate.
However, they are also complex to implement and induce a high overhead. The
third clustering technique, called Detection & Reclustering of Objects (DRO),
is based on the same principles, but is much simpler to implement. These three
clustering algorithm have been implemented in the Texas persistent object store
and compared in terms of clustering efficiency (i.e., overall performance
increase) and overhead using the Object Clustering Benchmark (OCB). The results
obtained showed that DRO induced a lighter overhead while still achieving
better overall performance.



Inverse-free Berlekamp-Massey-Sakata Algorithm and Small Decoders for Algebraic-Geometric Codes

This paper proposes a novel algorithm for finding error-locators of
algebraic-geometric codes that can eliminate the division-calculations of
finite fields from the Berlekamp-Massey-Sakata algorithm. This inverse-free
algorithm provides full performance in correcting a certain class of errors,
generic errors, which includes most errors, and can decode codes on algebraic
curves without the determination of unknown syndromes. Moreover, we propose
three different kinds of architectures that our algorithm can be applied to,
and we represent the control operation of shift-registers and switches at each
clock-timing with numerical simulations. We estimate the performance in
comparison of the total running time and the numbers of multipliers and
shift-registers in three architectures with those of the conventional ones for
codes on algebraic curves.



WDM and Directed Star Arboricity

A digraph is $m$-labelled if every arc is labelled by an integer in $\{1,
\dots,m\}$. Motivated by wavelength assignment for multicasts in optical
networks, we introduce and study $n$-fibre colourings of labelled digraphs.
These are colourings of the arcs of $D$ such that at each vertex $v$, and for
each colour $\alpha$, $in(v,\alpha)+out(v,\alpha)\leq n$ with $in(v,\alpha)$
the number of arcs coloured $\alpha$ entering $v$ and $out(v,\alpha)$ the
number of labels $l$ such that there is at least one arc of label $l$ leaving
$v$ and coloured with $\alpha$. The problem is to find the minimum number of
colours $\lambda_n(D)$ such that the $m$-labelled digraph $D$ has an $n$-fibre
colouring. In the particular case when $D$ is $1$-labelled, $\lambda_1(D)$ is
called the directed star arboricity of $D$, and is denoted by $dst(D)$. We
first show that $dst(D)\leq 2\Delta^-(D)+1$, and conjecture that if
$\Delta^-(D)\geq 2$, then $dst(D)\leq 2\Delta^-(D)$. We also prove that for a
subcubic digraph $D$, then $dst(D)\leq 3$, and that if $\Delta^+(D),
\Delta^-(D)\leq 2$, then $dst(D)\leq 4$. Finally, we study
$\lambda_n(m,k)=\max\{\lambda_n(D) \tq D \mbox{is $m$-labelled} \et
\Delta^-(D)\leq k\}$. We show that if $m\geq n$, then $\ds
\left\lceil\frac{m}{n}\left\lceil \frac{k}{n}\right\rceil + \frac{k}{n}
\right\rceil\leq \lambda_n(m,k) \leq\left\lceil\frac{m}{n}\left\lceil
\frac{k}{n}\right\rceil + \frac{k}{n} \right\rceil + C \frac{m^2\log k}{n}$ for
some constant $C$. We conjecture that the lower bound should be the right value
of $\lambda_n(m,k)$.



Optimal Delay-Throughput Trade-offs in Mobile Ad-Hoc Networks: Hybrid Random Walk and One-Dimensional Mobility Models

Optimal delay-throughput trade-offs for two-dimensional i.i.d mobility models
have been established in [23], where we showed that the optimal trade-offs can
be achieved using rate-less codes when the required delay guarantees are
sufficient large. In this paper, we extend the results to other mobility models
including two-dimensional hybrid random walk model, one-dimensional i.i.d.
mobility model and one-dimensional hybrid random walk model. We consider both
fast mobiles and slow mobiles, and establish the optimal delay-throughput
trade-offs under some conditions. Joint coding-scheduling algorithms are also
proposed to achieve the optimal trade-offs.



Algorithms for laying points optimally on a plane and a circle

Two averaging algorithms are considered which are intended for choosing an
optimal plane and an optimal circle approximating a group of points in
three-dimensional Euclidean space.



Edges and Switches, Tunnels and Bridges

Edge casing is a well-known method to improve the readability of drawings of
non-planar graphs. A cased drawing orders the edges of each edge crossing and
interrupts the lower edge in an appropriate neighborhood of the crossing.
Certain orders will lead to a more readable drawing than others. We formulate
several optimization criteria that try to capture the concept of a "good" cased
drawing. Further, we address the algorithmic question of how to turn a given
drawing into an optimal cased drawing. For many of the resulting optimization
problems, we either find polynomial time algorithms or NP-hardness results.



Undirected Graphs of Entanglement Two

Entanglement is a complexity measure of directed graphs that origins in fixed
point theory. This measure has shown its use in designing efficient algorithms
to verify logical properties of transition systems. We are interested in the
problem of deciding whether a graph has entanglement at most k. As this measure
is defined by means of games, game theoretic ideas naturally lead to design
polynomial algorithms that, for fixed k, decide the problem. Known
characterizations of directed graphs of entanglement at most 1 lead, for k = 1,
to design even faster algorithms. In this paper we present an explicit
characterization of undirected graphs of entanglement at most 2. With such a
characterization at hand, we devise a linear time algorithm to decide whether
an undirected graph has this property.



Frugal Colouring of Graphs

A $k$-frugal colouring of a graph $G$ is a proper colouring of the vertices
of $G$ such that no colour appears more than $k$ times in the neighbourhood of
a vertex. This type of colouring was introduced by Hind, Molloy and Reed in
1997. In this paper, we study the frugal chromatic number of planar graphs,
planar graphs with large girth, and outerplanar graphs, and relate this
parameter with several well-studied colourings, such as colouring of the
square, cyclic colouring, and $L(p,q)$-labelling. We also study frugal
edge-colourings of multigraphs.



Encoding for the Blackwell Channel with Reinforced Belief Propagation

A key idea in coding for the broadcast channel (BC) is binning, in which the
transmitter encode information by selecting a codeword from an appropriate bin
(the messages are thus the bin indexes). This selection is normally done by
solving an appropriate (possibly difficult) combinatorial problem. Recently it
has been shown that binning for the Blackwell channel --a particular BC-- can
be done by iterative schemes based on Survey Propagation (SP). This method uses
decimation for SP and suffers a complexity of O(n^2). In this paper we propose
a new variation of the Belief Propagation (BP) algorithm, named Reinforced BP
algorithm, that turns BP into a solver. Our simulations show that this new
algorithm has complexity O(n log n). Using this new algorithm together with a
non-linear coding scheme, we can efficiently achieve rates close to the border
of the capacity region of the Blackwell channel.



Batch Processor Sharing with Hyper-Exponential Service Time

We study Batch Processor-Sharing (BPS) queuing model with hyper-exponential
service time distribution and Poisson batch arrival process. One of the main
goals to study BPS is the possibility of its application in size-based
scheduling, which is used in differentiation between Short and Long flows in
the Internet. In the case of hyper-exponential service time distribution we
find an analytical expression of the expected conditional response time for the
BPS queue. We show, that the expected conditional response time is a concave
function of the service time. We apply the received results to the Two Level
Processor-Sharing (TLPS) model with hyper-exponential service time distribution
and find the expression of the expected response time for the TLPS model. TLPS
scheduling discipline can be applied to size-based differentiation in TCP/IP
networks and Web server request handling.



Multiresolution Approximation of Polygonal Curves in Linear Complexity

We propose a new algorithm to the problem of polygonal curve approximation
based on a multiresolution approach. This algorithm is suboptimal but still
maintains some optimality between successive levels of resolution using dynamic
programming. We show theoretically and experimentally that this algorithm has a
linear complexity in time and space. We experimentally compare the outcomes of
our algorithm to the optimal "full search" dynamic programming solution and
finally to classical merge and split approaches. The experimental evaluations
confirm the theoretical derivations and show that the proposed approach
evaluated on 2D coastal maps either show a lower time complexity or provide
polygonal approximations closer to the input discrete curves.



VOODB: A Generic Discrete-Event Random Simulation Model to Evaluate the Performances of OODBs

Performance of object-oriented database systems (OODBs) is still an issue to
both designers and users nowadays. The aim of this paper is to propose a
generic discrete-event random simulation model, called VOODB, in order to
evaluate the performances of OODBs in general, and the performances of
optimization methods like clustering in particular. Such optimization methods
undoubtedly improve the performances of OODBs. Yet, they also always induce
some kind of overhead for the system. Therefore, it is important to evaluate
their exact impact on the overall performances. VOODB has been designed as a
generic discrete-event random simulation model by putting to use a modelling
approach, and has been validated by simulating the behavior of the O2 OODB and
the Texas persistent object store. Since our final objective is to compare
object clustering algorithms, some experiments have also been conducted on the
DSTC clustering technique, which is implemented in Texas. To validate VOODB,
performance results obtained by simulation for a given experiment have been
compared to the results obtained by benchmarking the real systems in the same
conditions. Benchmarking and simulation performance evaluations have been
observed to be consistent, so it appears that simulation can be a reliable
approach to evaluate the performances of OODBs.



OCB: A Generic Benchmark to Evaluate the Performances of Object-Oriented Database Systems

We present in this paper a generic object-oriented benchmark (the Object
Clustering Benchmark) that has been designed to evaluate the performances of
clustering policies in object-oriented databases. OCB is generic because its
sample database may be customized to fit the databases introduced by the main
existing benchmarks (e.g., OO1). OCB's current form is clustering-oriented
because of its clustering-oriented workload, but it can be easily adapted to
other purposes. Lastly, OCB's code is compact and easily portable. OCB has been
implemented in a real system (Texas, running on a Sun workstation), in order to
test a specific clustering policy called DSTC. A few results concerning this
test are presented.



Performance Evaluation for Clustering Algorithms in Object-Oriented Database Systems

It is widely acknowledged that good object clustering is critical to the
performance of object-oriented databases. However, object clustering always
involves some kind of overhead for the system. The aim of this paper is to
propose a modelling methodology in order to evaluate the performances of
different clustering policies. This methodology has been used to compare the
performances of three clustering algorithms found in the literature (Cactis, CK
and ORION) that we considered representative of the current research in the
field of object clustering. The actual performance evaluation was performed
using simulation. Simulation experiments we performed showed that the Cactis
algorithm is better than the ORION algorithm and that the CK algorithm totally
outperforms both other algorithms in terms of response time and clustering
overhead.



The Design of Efficiently-Encodable Rate-Compatible LDPC Codes

We present a new class of irregular low-density parity-check (LDPC) codes for
moderate block lengths (up to a few thousand bits) that are well-suited for
rate-compatible puncturing. The proposed codes show good performance under
puncturing over a wide range of rates and are suitable for usage in incremental
redundancy hybrid-automatic repeat request (ARQ) systems. In addition, these
codes are linear-time encodable with simple shift-register circuits. For a
block length of 1200 bits the codes outperform optimized irregular LDPC codes
and extended irregular repeat-accumulate (eIRA) codes for all puncturing rates
0.6~0.9 (base code performance is almost the same) and are particularly good at
high puncturing rates where good puncturing performance has been previously
difficult to achieve.



Succinct Indexable Dictionaries with Applications to Encoding $k$-ary Trees, Prefix Sums and Multisets

We consider the {\it indexable dictionary} problem, which consists of storing
a set $S \subseteq \{0,...,m-1\}$ for some integer $m$, while supporting the
operations of $\Rank(x)$, which returns the number of elements in $S$ that are
less than $x$ if $x \in S$, and -1 otherwise; and $\Select(i)$ which returns
the $i$-th smallest element in $S$. We give a data structure that supports both
operations in O(1) time on the RAM model and requires ${\cal B}(n,m) + o(n) +
O(\lg \lg m)$ bits to store a set of size $n$, where ${\cal B}(n,m) = \ceil{\lg
{m \choose n}}$ is the minimum number of bits required to store any $n$-element
subset from a universe of size $m$. Previous dictionaries taking this space
only supported (yes/no) membership queries in O(1) time. In the cell probe
model we can remove the $O(\lg \lg m)$ additive term in the space bound,
answering a question raised by Fich and Miltersen, and Pagh.
  We present extensions and applications of our indexable dictionary data
structure, including:
  An information-theoretically optimal representation of a $k$-ary cardinal
tree that supports standard operations in constant time,
  A representation of a multiset of size $n$ from $\{0,...,m-1\}$ in ${\cal
B}(n,m+n) + o(n)$ bits that supports (appropriate generalizations of) $\Rank$
and $\Select$ operations in constant time, and
  A representation of a sequence of $n$ non-negative integers summing up to $m$
in ${\cal B}(n,m+n) + o(n)$ bits that supports prefix sum queries in constant
time.



Iterative Rounding for the Closest String Problem

The closest string problem is an NP-hard problem, whose task is to find a
string that minimizes maximum Hamming distance to a given set of strings. This
can be reduced to an integer program (IP). However, to date, there exists no
known polynomial-time algorithm for IP. In 2004, Meneses et al. introduced a
branch-and-bound (B & B) method for solving the IP problem. Their algorithm is
not always efficient and has the exponential time complexity. In the paper, we
attempt to solve efficiently the IP problem by a greedy iterative rounding
technique. The proposed algorithm is polynomial time and much faster than the
existing B & B IP for the CSP. If the number of strings is limited to 3, the
algorithm is provably at most 1 away from the optimum. The empirical results
show that in many cases we can find an exact solution. Even though we fail to
find an exact solution, the solution found is very close to exact solution.



Rate Bounds for MIMO Relay Channels

This paper considers the multi-input multi-output (MIMO) relay channel where
multiple antennas are employed by each terminal. Compared to single-input
single-output (SISO) relay channels, MIMO relay channels introduce additional
degrees of freedom, making the design and analysis of optimal cooperative
strategies more complex. In this paper, a partial cooperation strategy that
combines transmit-side message splitting and block-Markov encoding is
presented. Lower bounds on capacity that improve on a previously proposed
non-cooperative lower bound are derived for Gaussian MIMO relay channels.



Clustering Co-occurrence of Maximal Frequent Patterns in Streams

One way of getting a better view of data is using frequent patterns. In this
paper frequent patterns are subsets that occur a minimal number of times in a
stream of itemsets. However, the discovery of frequent patterns in streams has
always been problematic. Because streams are potentially endless it is in
principle impossible to say if a pattern is often occurring or not. Furthermore
the number of patterns can be huge and a good overview of the structure of the
stream is lost quickly. The proposed approach will use clustering to facilitate
the analysis of the structure of the stream.
  A clustering on the co-occurrence of patterns will give the user an improved
view on the structure of the stream. Some patterns might occur so much together
that they should form a combined pattern. In this way the patterns in the
clustering will be the largest frequent patterns: maximal frequent patterns.
  Our approach to decide if patterns occur often together will be based on a
method of clustering when only the distance between pairs is known. The number
of maximal frequent patterns is much smaller and combined with clustering
methods these patterns provide a good view on the structure of the stream.



Clustering with Lattices in the Analysis of Graph Patterns

Mining frequent subgraphs is an area of research where we have a given set of
graphs (each graph can be seen as a transaction), and we search for (connected)
subgraphs contained in many of these graphs. In this work we will discuss
techniques used in our framework Lattice2SAR for mining and analysing frequent
subgraph data and their corresponding lattice information. Lattice information
is provided by the graph mining algorithm gSpan; it contains all
supergraph-subgraph relations of the frequent subgraph patterns -- and their
supports.
  Lattice2SAR is in particular used in the analysis of frequent graph patterns
where the graphs are molecules and the frequent subgraphs are fragments. In the
analysis of fragments one is interested in the molecules where patterns occur.
This data can be very extensive and in this paper we focus on a technique of
making it better available by using the lattice information in our clustering.
Now we can reduce the number of times the highly compressed occurrence data
needs to be accessed by the user. The user does not have to browse all the
occurrence data in search of patterns occurring in the same molecules. Instead
one can directly see which frequent subgraphs are of interest.



NodeTrix: Hybrid Representation for Analyzing Social Networks

The need to visualize large social networks is growing as hardware
capabilities make analyzing large networks feasible and many new data sets
become available. Unfortunately, the visualizations in existing systems do not
satisfactorily answer the basic dilemma of being readable both for the global
structure of the network and also for detailed analysis of local communities.
To address this problem, we present NodeTrix, a hybrid representation for
networks that combines the advantages of two traditional representations:
node-link diagrams are used to show the global structure of a network, while
arbitrary portions of the network can be shown as adjacency matrices to better
support the analysis of communities. A key contribution is a set of interaction
techniques. These allow analysts to create a NodeTrix visualization by dragging
selections from either a node-link or a matrix, flexibly manipulate the
NodeTrix representation to explore the dataset, and create meaningful summary
visualizations of their findings. Finally, we present a case study applying
NodeTrix to the analysis of the InfoVis 2004 coauthorship dataset to illustrate
the capabilities of NodeTrix as both an exploration tool and an effective means
of communicating results.



Risk Assessment Algorithms Based On Recursive Neural Networks

The assessment of highly-risky situations at road intersections have been
recently revealed as an important research topic within the context of the
automotive industry. In this paper we shall introduce a novel approach to
compute risk functions by using a combination of a highly non-linear processing
model in conjunction with a powerful information encoding procedure.
Specifically, the elements of information either static or dynamic that appear
in a road intersection scene are encoded by using directed positional acyclic
labeled graphs. The risk assessment problem is then reformulated in terms of an
inductive learning task carried out by a recursive neural network. Recursive
neural networks are connectionist models capable of solving supervised and
non-supervised learning problems represented by directed ordered acyclic
graphs. The potential of this novel approach is demonstrated through well
predefined scenarios. The major difference of our approach compared to others
is expressed by the fact of learning the structure of the risk. Furthermore,
the combination of a rich information encoding procedure with a generalized
model of dynamical recurrent networks permit us, as we shall demonstrate, a
sophisticated processing of information that we believe as being a first step
for building future advanced intersection safety systems



Privacy - an Issue for eLearning? A Trend Analysis Reflecting the Attitude of European eLearning Users

Availing services provided via the Internet became a widely accepted means in
organising one's life. Beside others, eLearning goes with this trend as well.
But, while employing Internet service makes life more convenient, at the same
time, it raises risks with respect to the protection of the users' privacy.
This paper analyses the attitudes of eLearning users towards their privacy by,
initially, pointing out terminology and legal issues connected with privacy.
Further, the concept and implementation as well as a result analysis of a
conducted study is presented, which explores the problem area from different
perspectives. The paper will show that eLearning users indeed care for the
protection of their personal information when using eLearning services.
However, their attitudes and behaviour slightly differ. In conclusion, we
provide first approaches of assisting possibilities for users how to resolve
the difference of requirements and their actual activities with respect to
privacy protection.



Moving Walkways, Escalators, and Elevators

We study a simple geometric model of transportation facility that consists of
two points between which the travel speed is high. This elementary definition
can model shuttle services, tunnels, bridges, teleportation devices, escalators
or moving walkways. The travel time between a pair of points is defined as a
time distance, in such a way that a customer uses the transportation facility
only if it is helpful.
  We give algorithms for finding the optimal location of such a transportation
facility, where optimality is defined with respect to the maximum travel time
between two points in a given set.



Learning to Bluff

The act of bluffing confounds game designers to this day. The very nature of
bluffing is even open for debate, adding further complication to the process of
creating intelligent virtual players that can bluff, and hence play,
realistically. Through the use of intelligent, learning agents, and carefully
designed agent outlooks, an agent can in fact learn to predict its opponents
reactions based not only on its own cards, but on the actions of those around
it. With this wider scope of understanding, an agent can in learn to bluff its
opponents, with the action representing not an illogical action, as bluffing is
often viewed, but rather as an act of maximising returns through an effective
statistical optimisation. By using a tee dee lambda learning algorithm to
continuously adapt neural network agent intelligence, agents have been shown to
be able to learn to bluff without outside prompting, and even to learn to call
each others bluffs in free, competitive play.



Soft constraint abstraction based on semiring homomorphism

The semiring-based constraint satisfaction problems (semiring CSPs), proposed
by Bistarelli, Montanari and Rossi \cite{BMR97}, is a very general framework of
soft constraints. In this paper we propose an abstraction scheme for soft
constraints that uses semiring homomorphism. To find optimal solutions of the
concrete problem, the idea is, first working in the abstract problem and
finding its optimal solutions, then using them to solve the concrete problem.
  In particular, we show that a mapping preserves optimal solutions if and only
if it is an order-reflecting semiring homomorphism. Moreover, for a semiring
homomorphism $\alpha$ and a problem $P$ over $S$, if $t$ is optimal in
$\alpha(P)$, then there is an optimal solution $\bar{t}$ of $P$ such that
$\bar{t}$ has the same value as $t$ in $\alpha(P)$.



The Optimization of a Novel Prismatic Drive

The design of a mechanical transmission taking into account the transmitted
forces is reported in this paper. This transmission is based on Slide-o-Cam, a
cam mechanism with multiple rollers mounted on a common translating follower.
The design of Slide-o-Cam, a transmission intended to produce a sliding motion
from a turning drive, or vice versa, was reported elsewhere. This transmission
provides pure-rolling motion, thereby reducing the friction of rack-and-pinions
and linear drives. The pressure angle is a relevant performance index for this
transmission because it determines the amount of force transmitted to the load
vs. that transmitted to the machine frame. To assess the transmission
capability of the mechanism, the Hertz formula is introduced to calculate the
stresses on the rollers and on the cams. The final transmission is intended to
replace the current ball-screws in the Orthoglide, a three-DOF parallel robot
for the production of translational motions, currently under development for
machining applications at Ecole Centrale de Nantes.



MIMO detection employing Markov Chain Monte Carlo

We propose a soft-output detection scheme for Multiple-Input-Multiple-Output
(MIMO) systems. The detector employs Markov Chain Monte Carlo method to compute
bit reliabilities from the signals received and is thus suited for coded MIMO
systems. It offers a good trade-off between achievable performance and
algorithmic complexity.



Approximate textual retrieval

An approximate textual retrieval algorithm for searching sources with high
levels of defects is presented. It considers splitting the words in a query
into two overlapping segments and subsequently building composite regular
expressions from interlacing subsets of the segments. This procedure reduces
the probability of missed occurrences due to source defects, yet diminishes the
retrieval of irrelevant, non-contextual occurrences.



Equivalence of LP Relaxation and Max-Product for Weighted Matching in General Graphs

Max-product belief propagation is a local, iterative algorithm to find the
mode/MAP estimate of a probability distribution. While it has been successfully
employed in a wide variety of applications, there are relatively few
theoretical guarantees of convergence and correctness for general loopy graphs
that may have many short cycles. Of these, even fewer provide exact ``necessary
and sufficient'' characterizations.
  In this paper we investigate the problem of using max-product to find the
maximum weight matching in an arbitrary graph with edge weights. This is done
by first constructing a probability distribution whose mode corresponds to the
optimal matching, and then running max-product. Weighted matching can also be
posed as an integer program, for which there is an LP relaxation. This
relaxation is not always tight. In this paper we show that \begin{enumerate}
\item If the LP relaxation is tight, then max-product always converges, and
that too to the correct answer. \item If the LP relaxation is loose, then
max-product does not converge. \end{enumerate} This provides an exact,
data-dependent characterization of max-product performance, and a precise
connection to LP relaxation, which is a well-studied optimization technique.
Also, since LP relaxation is known to be tight for bipartite graphs, our
results generalize other recent results on using max-product to find weighted
matchings in bipartite graphs.



Bayesian Approach to Neuro-Rough Models

This paper proposes a neuro-rough model based on multi-layered perceptron and
rough set. The neuro-rough model is then tested on modelling the risk of HIV
from demographic data. The model is formulated using Bayesian framework and
trained using Monte Carlo method and Metropolis criterion. When the model was
tested to estimate the risk of HIV infection given the demographic data it was
found to give the accuracy of 62%. The proposed model is able to combine the
accuracy of the Bayesian MLP model and the transparency of Bayesian rough set
model.



Medical Image Segmentation and Localization using Deformable Templates

This paper presents deformable templates as a tool for segmentation and
localization of biological structures in medical images. Structures are
represented by a prototype template, combined with a parametric warp mapping
used to deform the original shape. The localization procedure is achieved using
a multi-stage, multi-resolution algorithm de-signed to reduce computational
complexity and time. The algorithm initially identifies regions in the image
most likely to contain the desired objects and then examines these regions at
progressively increasing resolutions. The final stage of the algorithm involves
warping the prototype template to match the localized objects. The algorithm is
presented along with the results of four example applications using MRI, x-ray
and ultrasound images.



Non-cooperative games for spreading code optimization, power control and receiver design in wireless data networks

This paper focuses on the issue of energy efficiency in wireless data
networks through a game theoretic approach. The case considered is that in
which each user is allowed to vary its transmit power, spreading code, and
uplink receiver in order to maximize its own utility, which is here defined as
the ratio of data throughput to transmit power. In particular, the case in
which linear multiuser detectors are employed at the receiver is treated first,
and, then, the more challenging case in which non-linear decision feedback
multiuser receivers are adopted is addressed. It is shown that, for both
receivers, the problem at hand of utility maximization can be regarded as a
non-cooperative game, and it is proved that a unique Nash equilibrium point
exists. Simulation results show that significant performance gains can be
obtained through both non-linear processing and spreading code optimization; in
particular, for systems with a number of users not larger than the processing
gain, remarkable gains come from spreading code optimization, while, for
overloaded systems, the largest gainscome from the use of non-linear
processing. In every case, however, the non-cooperative games proposed here are
shown to outperform competing alternatives.



Overview of the Netsukuku network

Netsukuku is a P2P network system designed to handle a large number of nodes
with minimal CPU and memory resources. It can be easily used to build a
worldwide distributed, anonymous and not controlled network, separated from the
Internet, without the support of any servers, ISPs or authority controls. In
this document, we give a generic and non technical description of the Netsukuku
network, emphasizing its main ideas and features.



Quantum Shortest Path Netsukuku

This document describes the QSPN, the routing discovery algorithm used by
Netsukuku. Through a deductive analysis the main proprieties of the QSPN are
shown. Moreover, a second version of the algorithm, is presented.



The Netsukuku network topology

In this document, we describe the fractal structure of the Netsukuku
topology. Moreover, we show how it is possible to use the QSPN v2 on the high
levels of the fractal.



ANDNA: the distributed hostname management system of Netsukuku

We present the Abnormal Netsukuku Domain Name Anarchy system. ANDNA is the
distributed, non hierarchical and decentralised system of hostname management
used in the Netsukuku network.



Enhancement of Noisy Planar Nuclear Medicine Images using Mean Field Annealing

Nuclear medicine (NM) images inherently suffer from large amounts of noise
and blur. The purpose of this research is to reduce the noise and blur while
maintaining image integrity for improved diagnosis. The proposed solution is to
increase image quality after the standard pre- and post-processing undertaken
by a gamma camera system. Mean Field Annealing (MFA) is the image processing
technique used in this research. It is a computational iterative technique that
makes use of the Point Spread Function (PSF) and the noise associated with the
NM image. MFA is applied to NM images with the objective of reducing noise
while not compromising edge integrity. Using a sharpening filter as a
post-processing technique (after MFA) yields image enhancement of planar NM
images.



The Multiobjective Optimization of a Prismatic Drive

The multiobjective optimization of Slide-o-Cam is reported in this paper.
Slide-o-Cam is a cam mechanism with multiple rollers mounted on a common
translating follower. This transmission provides pure-rolling motion, thereby
reducing the friction of rack-and-pinions and linear drives. A Pareto frontier
is obtained by means of multiobjective optimization. This optimization is based
on three objective functions: (i) the pressure angle, which is a suitable
performance index for the transmission because it determines the amount of
force transmitted to the load vs. that transmitted to the machine frame; (ii)
the Hertz pressure used to evaluate the stresses produced on the contact
surface between cam and roller; and (iii) the size of the mechanism,
characterized by the number of cams and their width.



Game-Theoretic Power Control in Impulse Radio UWB Wireless Networks

In this paper, a game-theoretic model for studying power control for wireless
data networks in frequency-selective multipath environments is analyzed. The
uplink of an impulse-radio ultrawideband system is considered. The effects of
self-interference and multiple-access interference on the performance of Rake
receivers are investigated for synchronous systems. Focusing on energy
efficiency, a noncooperative game is proposed in which users in the network are
allowed to choose their transmit powers to maximize their own utilities, and
the Nash equilibrium for the proposed game is derived. It is shown that, due to
the frequency selective multipath, the noncooperative solution is achieved at
different signal-to-interference-plus-noise ratios, respectively of the channel
realization. A large-system analysis is performed to derive explicit
expressions for the achieved utilities. The Pareto-optimal (cooperative)
solution is also discussed and compared with the noncooperative approach.



Satisfiability Parsimoniously Reduces to the Tantrix(TM) Rotation Puzzle Problem

Holzer and Holzer (Discrete Applied Mathematics 144(3):345--358, 2004) proved
that the Tantrix(TM) rotation puzzle problem is NP-complete. They also showed
that for infinite rotation puzzles, this problem becomes undecidable. We study
the counting version and the unique version of this problem. We prove that the
satisfiability problem parsimoniously reduces to the Tantrix(TM) rotation
puzzle problem. In particular, this reduction preserves the uniqueness of the
solution, which implies that the unique Tantrix(TM) rotation puzzle problem is
as hard as the unique satisfiability problem, and so is DP-complete under
polynomial-time randomized reductions, where DP is the second level of the
boolean hierarchy over NP.



Variable-Rate Distributed Source Coding in the Presence of Byzantine Sensors

The distributed source coding problem is considered when the sensors, or
encoders, are under Byzantine attack; that is, an unknown number of sensors
have been reprogrammed by a malicious intruder to undermine the reconstruction
at the fusion center. Three different forms of the problem are considered. The
first is a variable-rate setup, in which the decoder adaptively chooses the
rates at which the sensors transmit. An explicit characterization of the
variable-rate minimum achievable sum rate is stated, given by the maximum
entropy over the set of distributions indistinguishable from the true source
distribution by the decoder. In addition, two forms of the fixed-rate problem
are considered, one with deterministic coding and one with randomized coding.
The achievable rate regions are given for both these problems, with a larger
region achievable using randomized coding, though both are suboptimal compared
to variable-rate coding.



Computing Minimal Polynomials of Matrices

We present and analyse a Monte-Carlo algorithm to compute the minimal
polynomial of an $n\times n$ matrix over a finite field that requires $O(n^3)$
field operations and O(n) random vectors, and is well suited for successful
practical implementation. The algorithm, and its complexity analysis, use
standard algorithms for polynomial and matrix operations. We compare features
of the algorithm with several other algorithms in the literature. In addition
we present a deterministic verification procedure which is similarly efficient
in most cases but has a worst-case complexity of $O(n^4)$. Finally, we report
the results of practical experiments with an implementation of our algorithms
in comparison with the current algorithms in the {\sf GAP} library.



Performance Comparison of Energy-Efficient Power Control for CDMA and Multiuser UWB Networks

This paper studies the performance of a wireless data network using
energy-efficient power control techniques when different multiple access
schemes, namely direct-sequence code division multiple access (DS-CDMA) and
impulse-radio ultrawideband (IR-UWB), are considered. Due to the large
bandwidth of the system, the multipath channel is assumed to be
frequency-selective. By making use of noncooperative game-theoretic models and
large-system analysis tools, explicit expressions for the achieved utilities at
the Nash equilibrium are derived in terms of the network parameters. A measure
of the loss of DS-CDMA with respect to IR-UWB is proposed, which proves
substantial equivalence between the two schemes. Simulation results are
provided to validate the analysis.



An Independent Evaluation of Subspace Face Recognition Algorithms

This paper explores a comparative study of both the linear and kernel
implementations of three of the most popular Appearance-based Face Recognition
projection classes, these being the methodologies of Principal Component
Analysis, Linear Discriminant Analysis and Independent Component Analysis. The
experimental procedure provides a platform of equal working conditions and
examines the ten algorithms in the categories of expression, illumination,
occlusion and temporal delay. The results are then evaluated based on a
sequential combination of assessment tools that facilitate both intuitive and
statistical decisiveness among the intra and interclass comparisons. The best
categorical algorithms are then incorporated into a hybrid methodology, where
the advantageous effects of fusion strategies are considered.



On Isotropic Sets of Points in the Plane. Application to the Design of Robot Archirectures

Various performance indices are used for the design of serial manipulators.
One method of optimization relies on the condition number of the Jacobian
matrix. The minimization of the condition number leads, under certain
conditions, to isotropic configurations, for which the roundoff-error
amplification is lowest. In this paper, the isotropy conditions, introduced
elsewhere, are the motivation behind the introduction of isotropic sets of
points. By connecting together these points, we define families of isotropic
manipulators. This paper is devoted to planar manipulators, the concepts being
currently extended to their spatial counterparts. Furthermore, only
manipulators with revolute joints are considered here.



The Kinematic Analysis of a Symmetrical Three-Degree-of-Freedom Planar Parallel Manipulator

Presented in this paper is the kinematic analysis of a symmetrical
three-degree-of-freedom planar parallel manipulator. In opposite to serial
manipulators, parallel manipulators can admit not only multiple inverse
kinematic solutions, but also multiple direct kinematic solutions. This
property produces more complicated kinematic models but allows more flexibility
in trajectory planning. To take into account this property, the notion of
aspects, i.e. the maximal singularity-free domains, was introduced, based on
the notion of working modes, which makes it possible to separate the inverse
kinematic solutions. The aim of this paper is to show that a non-singular
assembly-mode changing trajectory exist for a symmetrical planar parallel
manipulator, with equilateral base and platform triangle.



Uniqueness Domains in the Workspace of Parallel Manipulators

This work investigates new kinematic features of parallel manipulators. It is
well known that parallel manipulators admit generally several direct kinematic
solutions for a given set of input joint values. The aim of this paper is to
characterize the uniqueness domains in the workspace of parallel manipulators,
as well as their image in the joint space. The study focuses on the most usual
case of parallel manipulators with only one inverse kinematic solution. The
notion of aspect introduced for serial manipulators in [Borrel 86] is redefined
for such parallel manipulators. Then, it is shown that it is possible to link
several solutions to the forward kinematic problem without meeting a
singularity, thus meaning that the aspects are not uniqueness domains. An
additional set of surfaces, namely the characteristic surfaces, are
characterized which divide the workspace into basic regions and yield new
uniqueness domains. This study is illustrated all along the paper with a 3-RPR
planar parallel manipulator. An octree model of spaces is used to compute the
joint space, the workspace and all other newly defined sets.



The Kinematic design of a 3-dof Hybrid Manipulator

This paper focuses on the kinematic properties of a new
three-degree-of-freedom hybrid manipulator. This manipulator is obtained by
adding in series to a five-bar planar mechanism (similar to the one studied by
Bajpai and Roth) a third revolute passing through the line of centers of the
two actuated revolute joints of the above linkage. The resulting architecture
is hybrid in that it has both serial and parallel links. Fully-parallel
manipulators are known for the existence of particularly undesirable
singularities (referred to as parallel singularities) where control is lost [4]
and [6]. On the other hand, due to their cantilever type of kinematic
arrangement, fully serial manipulators suffer from a lack of stiffness and from
relatively large positioning errors. The hybrid manipulator studied is
intrinsically stiffer and more accurate. Furthermore, since all actuators are
located on the first axis, the inertial effects are considerably reduced. In
addition, it is shown that the special kinematic structure of our manipulator
has the potential of avoiding parallel singularities by a suitable choice of
the "working mode", thus leading to larger workspaces. The influence of the
different structural dimensions (e.g. the link lengths) on the kinematic and
mechanical properties are analysed in view of the optimal design of such hybrid
manipulators.



Definition sets for the Direct Kinematics of Parallel Manipulators

The aim of this paper is to characterize the uniqueness domains in the
workspace of parallel manipulators, as well as their image in the joint space.
The notion of aspect introduced for serial manipulators in [Borrel 86] is
redefined for such parallel manipulators. Then, it is shown that it is possible
to link several solutions to the direct kinematic problem without meeting a
singularity, thus meaning that the aspects are not uniqueness domains.
Additional surfaces are characterized in the workspace which yield new
uniqueness domains. An octree model of spaces is used to compute the joint
space, the workspace and all other newly defined sets. This study is
illustrated all along the paper with a 3-RPR planar parallel manipulator.



Improved Analysis of Kannan's Shortest Lattice Vector Algorithm

The security of lattice-based cryptosystems such as NTRU, GGH and Ajtai-Dwork
essentially relies upon the intractability of computing a shortest non-zero
lattice vector and a closest lattice vector to a given target vector in high
dimensions. The best algorithms for these tasks are due to Kannan, and, though
remarkably simple, their complexity estimates have not been improved since more
than twenty years. Kannan's algorithm for solving the shortest vector problem
is in particular crucial in Schnorr's celebrated block reduction algorithm, on
which are based the best known attacks against the lattice-based encryption
schemes mentioned above. Understanding precisely Kannan's algorithm is of prime
importance for providing meaningful key-sizes. In this paper we improve the
complexity analyses of Kannan's algorithms and discuss the possibility of
improving the underlying enumeration strategy.



Artificial Neural Networks and Support Vector Machines for Water Demand Time Series Forecasting

Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two.



A New Three-DOF Parallel Mechanism: Milling Machine Applications

This paper describes a new parallel kinematic architecture for machining
applications, namely, the orthoglide. This machine features three fixed
parallel linear joints which are mounted orthogonally and a mobile platform
which moves in the Cartesian x-y-z space with fixed orientation. The main
interest of the orthoglide is that it takes benefit from the advantages of the
popular PPP serial machines (regular Cartesian workspace shape and uniform
performances) as well as from the parallel kinematic arrangement of the links
(less inertia and better dynamic performances), which makes the orthoglide well
suited to high-speed machining applications. Possible extension of the
orthoglide to 5-axis machining is also investigated.



Cellular Systems with Full-Duplex Amplify-and-Forward Relaying and Cooperative Base-Stations

In this paper the benefits provided by multi-cell processing of signals
transmitted by mobile terminals which are received via dedicated relay
terminals (RTs) are assessed. Unlike previous works, each RT is assumed here to
be capable of full-duplex operation and receives the transmission of adjacent
relay terminals. Focusing on intra-cell TDMA and non-fading channels, a
simplified uplink cellular model introduced by Wyner is considered. This
framework facilitates analytical derivation of the per-cell sum-rate of
multi-cell and conventional single-cell receivers. In particular, the analysis
is based on the observation that the signal received at the base stations can
be interpreted as the outcome of a two-dimensional linear time invariant
system. Numerical results are provided as well in order to provide further
insight into the performance benefits of multi-cell processing with relaying.



Tracking User Attention in Collaborative Tagging Communities

Collaborative tagging has recently attracted the attention of both industry
and academia due to the popularity of content-sharing systems such as
CiteULike, del.icio.us, and Flickr. These systems give users the opportunity to
add data items and to attach their own metadata (or tags) to stored data. The
result is an effective content management tool for individual users. Recent
studies, however, suggest that, as tagging communities grow, the added content
and the metadata become harder to manage due to an ease in content diversity.
Thus, mechanisms that cope with increase of diversity are fundamental to
improve the scalability and usability of collaborative tagging systems. This
paper analyzes whether usage patterns can be harnessed to improve navigability
in a growing knowledge space. To this end, it presents a characterization of
two collaborative tagging communities that target scientific literature:
CiteULike and Bibsonomy. We explore three main directions: First, we analyze
the tagging activity distribution across the user population. Second, we define
new metrics for similarity in user interest and use these metrics to uncover
the structure of the tagging communities we study. The structure we uncover
suggests a clear segmentation of interests into a large number of individuals
with unique preferences and a core set of users with interspersed interests.
Finally, we offer preliminary results that demonstrate that the interest-based
structure of the tagging community can be used to facilitate content usage as
communities scale.



Recognizing Partial Cubes in Quadratic Time

We show how to test whether a graph with n vertices and m edges is a partial
cube, and if so how to find a distance-preserving embedding of the graph into a
hypercube, in the near-optimal time bound O(n^2), improving previous O(nm)-time
solutions.



Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs with Missing Values

An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.



Optimal Cache-Oblivious Mesh Layouts

A mesh is a graph that divides physical space into regularly-shaped regions.
Meshes computations form the basis of many applications, e.g. finite-element
methods, image rendering, and collision detection. In one important mesh
primitive, called a mesh update, each mesh vertex stores a value and repeatedly
updates this value based on the values stored in all neighboring vertices. The
performance of a mesh update depends on the layout of the mesh in memory.
  This paper shows how to find a memory layout that guarantees that the mesh
update has asymptotically optimal memory performance for any set of memory
parameters. Such a memory layout is called cache-oblivious. Formally, for a
$d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where
$M=\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers.
The paper also shows how the mesh-update performance degrades for smaller
caches, where $M=o(B^d)$.
  The paper then gives two algorithms for finding cache-oblivious mesh layouts.
The first layout algorithm runs in time $O(|G|\log^2|G|)$ both in expectation
and with high probability on a RAM. It uses $O(1+|G|\log^2(|G|/M)/B)$ memory
transfers in expectation and $O(1+(|G|/B)(\log^2(|G|/M) + \log|G|))$ memory
transfers with high probability in the cache-oblivious and disk-access machine
(DAM) models. The layout is obtained by finding a fully balanced decomposition
tree of $G$ and then performing an in-order traversal of the leaves of the
tree. The second algorithm runs faster by almost a $\log|G|/\log\log|G|$ factor
in all three memory models, both in expectation and with high probability. The
layout obtained by finding a relax-balanced decomposition tree of $G$ and then
performing an in-order traversal of the leaves of the tree.



Strategies for the Design of a Slide-o-Cam Transmission

The optimization of the pressure angle in a cam-follower transmission is
reported in this paper. This transmission is based on Slide-o-Cam, a cam
mechanism with multiple rollers mounted on a common translating follower. The
design of Slide-o-Cam, a transmission intended to produce a sliding motion from
a turning drive, or vice versa, was reported elsewhere. This transmission
provides pure-rolling motion, thereby reducing the friction of rack-and-pinions
and linear drives. The pressure angle is a suitable performance index for this
transmission because it determines the amount of force transmitted to the load
vs. that transmitted to the machine frame. Two alternative design strategies
are studied, namely, (i) increase the number of lobes on each cam or (ii)
increase the number of cams. This device is intended to replace the current
ball-screws in Orthoglide, a three-DOF parallel robot for the production of
translational motions, currently under development at Ecole Centrale de Nantes
for machining applications.



Regions of Feasible Point-to-Point Trajectories in the Cartesian Workspace of Fully-Parallel Manipulators

The goal of this paper is to define the n-connected regions in the Cartesian
workspace of fully-parallel manipulators, i.e. the maximal regions where it is
possible to execute point-to-point motions. The manipulators considered in this
study may have multiple direct and inverse kinematic solutions. The N-connected
regions are characterized by projection, onto the Cartesian workspace, of the
connected components of the reachable configuration space defined in the
Cartesian product of the Cartesian space by the joint space. Generalized octree
models are used for the construction of all spaces. This study is illustrated
with a simple planar fully-parallel manipulator.



The Design of Parallel Kinematic Machine Tools Using Kinetostatic Performance Criteria

Most industrial machine tools have a serial kinematic architecture, which
means that each axis has to carry the following one, including its actuators
and joints. High Speed Machining highlights some drawbacks of such
architectures: heavy moving parts require from the machine structure high
stiffness to limit bending problems that lower the machine accuracy, and limit
the dynamic performances of the feed axes. That is why PKMs attract more and
more researchers and companies, because they are claimed to offer several
advantages over their serial counterparts, like high structural rigidity and
high dynamic capacities. Indeed, the parallel kinematic arrangement of the
links provides higher stiffness and lower moving masses that reduce inertia
effects. Thus, PKMs have better dynamic performances. However, the design of a
parallel kinematic machine tool (PKMT) is a hard task that requires further
research studies before wide industrial use can be expected. Many criteria need
to be taken into account in the design of a PKMT. We pay special attention to
the description of kinetostatic criteria that rely on the conditioning of the
Jacobian matrix of the mechanism. The organisation of this paper is as follows:
next section introduces general remarks about PKMs, then is explained why PKMs
can be interesting alternative machine tool designs. Then are presented
existing PKMTs. An application to the design of a small-scale machine tool
prototype developed at IRCCyN is presented at the end of this paper.



Mining Patterns with a Balanced Interval

In many applications it will be useful to know those patterns that occur with
a balanced interval, e.g., a certain combination of phone numbers are called
almost every Friday or a group of products are sold a lot on Tuesday and
Thursday.
  In previous work we proposed a new measure of support (the number of
occurrences of a pattern in a dataset), where we count the number of times a
pattern occurs (nearly) in the middle between two other occurrences. If the
number of non-occurrences between two occurrences of a pattern stays almost the
same then we call the pattern balanced.
  It was noticed that some very frequent patterns obviously also occur with a
balanced interval, meaning in every transaction. However more interesting
patterns might occur, e.g., every three transactions. Here we discuss a
solution using standard deviation and average. Furthermore we propose a simpler
approach for pruning patterns with a balanced interval, making estimating the
pruning threshold more intuitive.



S\'eparation des Solutions aux Mod\`eles G\'eom\'etriques Direct et Inverse pour les Manipulateurs Pleinement Parall\`eles

This article provides a formalism making it possible to manage the solutions
of the direct and inverse kinematic models of the fully parallel manipulators.
We introduce the concept of working modes to separate the solutions from the
opposite geometrical model. Then, we define, for each working mode, the aspects
of these manipulators. To separate the solutions from the direct kinematics
model, we introduce the concept of characteristic surfaces. Then, we define the
uniqueness domains, as being the greatest domains of the workspace in which
there is unicity of solutions. The principal applications of this work are the
design, the trajectory planning.



On the Kinetostatic Optimization of Revolute-Coupled Planar Manipulators

Proposed in this paper is a kinetostatic performance index for the optimum
dimensioning of planar manipulators of the serial type. The index is based on
the concept of distance of the underlying Jacobian matrix to a given isotropic
matrix that is used as a reference model for purposes of performance
evaluation. Applications of the index fall in the realm of design, but control
applications are outlined. The paper focuses on planar manipulators, the basic
concepts being currently extended to their three-dimensional counterparts.



Achievable Rates and Optimal Resource Allocation for Imperfectly-Known Fading Relay Channels

In this paper, achievable rates of imperfectly-known fading relay channels
are studied. It is assumed that communication starts with the network training
phase in which the receivers estimate the fading coefficients of their
respective channels. In the data transmission phase, amplify-and-forward and
decode-and-forward relaying schemes are considered, and the corresponding
achievable rate expressions are obtained. The achievable rate expressions are
then employed to identify the optimal resource allocation strategies.



Ordering Finite-State Markov Channels by Mutual Information

In previous work, an ordering result was given for the symbolwise probability
of error using general Markov channels, under iterative decoding of LDPC codes.
In this paper, the ordering result is extended to mutual information, under the
assumption of an iid input distribution. For certain channels, in which the
capacity-achieving input distribution is iid, this allows ordering of the
channels by capacity. The complexity of analyzing general Markov channels is
mitigated by this ordering, since it is possible to immediately determine that
a wide class of channels, with different numbers of states, has a smaller
mutual information than a given channel.



IDF revisited: A simple new derivation within the Robertson-Sp\"arck Jones probabilistic model

There have been a number of prior attempts to theoretically justify the
effectiveness of the inverse document frequency (IDF). Those that take as their
starting point Robertson and Sparck Jones's probabilistic model are based on
strong or complex assumptions. We show that a more intuitively plausible
assumption suffices. Moreover, the new assumption, while conceptually very
simple, provides a solution to an estimation problem that had been deemed
intractable by Robertson and Walker (1997).



Multiple Antenna Secure Broadcast over Wireless Networks

In wireless data networks, communication is particularly susceptible to
eavesdropping due to its broadcast nature. Security and privacy systems have
become critical for wireless providers and enterprise networks. This paper
considers the problem of secret communication over the Gaussian broadcast
channel, where a multi-antenna transmitter sends independent confidential
messages to two users with perfect secrecy. That is, each user would like to
obtain its own message reliably and confidentially. First, a computable
Sato-type outer bound on the secrecy capacity region is provided for a
multi-antenna broadcast channel with confidential messages. Next, a dirty-paper
secure coding scheme and its simplified version are described. For each case,
the corresponding achievable rate region is derived under the perfect secrecy
requirement. Finally, two numerical examples demonstrate that the Sato-type
outer bound is consistent with the boundary of the simplified dirty-paper
coding secrecy rate region.



Symbol Error Rates of Maximum-Likelihood Detector: Convex/Concave Behavior and Applications

Convexity/concavity properties of symbol error rates (SER) of the maximum
likelihood detector operating in the AWGN channel (non-fading and fading) are
studied. Generic conditions are identified under which the SER is a
convex/concave function of the SNR. Universal bounds for the SER 1st and 2nd
derivatives are obtained, which hold for arbitrary constellations and are tight
for some of them. Applications of the results are discussed, which include
optimum power allocation in spatial multiplexing systems, optimum power/time
sharing to decrease or increase (jamming problem) error rate, and implication
for fading channels.



Artificial Intelligence for Conflict Management

Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs.



Control of Complex Systems Using Bayesian Networks and Genetic Algorithm

A method based on Bayesian neural networks and genetic algorithm is proposed
to control the fermentation process. The relationship between input and output
variables is modelled using Bayesian neural network that is trained using
hybrid Monte Carlo method. A feedback loop based on genetic algorithm is used
to change input variables so that the output variables are as close to the
desired target as possible without the loss of confidence level on the
prediction that the neural network gives. The proposed procedure is found to
reduce the distance between the desired target and measured outputs
significantly.



Kinematic Calibration of the Orthoglide-Type Mechanisms

The paper proposes a novel calibration approach for the Orthoglide-type
mechanisms based on observations of the manipulator leg parallelism during
motions between the prespecified test postures. It employs a low-cost measuring
system composed of standard comparator indicators attached to the universal
magnetic stands. They are sequentially used for measuring the deviation of the
relevant leg location while the manipulator moves the TCP along the Cartesian
axes. Using the measured differences, the developed algorithm estimates the
joint offsets that are treated as the most essential parameters to be adjusted.
The sensitivity of the measurement methods and the calibration accuracy are
also studied. Experimental results are presented that demonstrate validity of
the proposed calibration technique.



The Design of a Novel Prismatic Drive for a Three-DOF Parallel-Kinematics Machine

The design of a novel prismatic drive is reported in this paper. This
transmission is based on Slide-O-Cam, a cam mechanism with multiple rollers
mounted on a common translating follower. The design of Slide-O-Cam was
reported elsewhere. This drive thus provides pure-rolling motion, thereby
reducing the friction of rack-and-pinions and linear drives. Such properties
can be used to design new transmissions for parallel-kinematics machines. In
this paper, this transmission is optimized to replace ball-screws in
Orthoglide, a three-DOF parallel robot optimized for machining applications.



Calibration of quasi-isotropic parallel kinematic Machines: Orthoglide

The paper proposes a novel approach for the geometrical model calibration of
quasi-isotropic parallel kinematic mechanisms of the Orthoglide family. It is
based on the observations of the manipulator leg parallelism during motions
between the specific test postures and employs a low-cost measuring system
composed of standard comparator indicators attached to the universal magnetic
stands. They are sequentially used for measuring the deviation of the relevant
leg location while the manipulator moves the TCP along the Cartesian axes.
Using the measured differences, the developed algorithm estimates the joint
offsets and the leg lengths that are treated as the most essential parameters.
Validity of the proposed calibration technique is confirmed by the experimental
results.



Rate Adaptation for Cognitive Radio under Interference from Primary Spectrum User

A cognitive radio can operate as a secondary system in a given spectrum. This
operation should use limited power in order not to disturb the communication by
primary spectrum user. Under such conditions, in this paper we investigate how
to maximize the spectral efficiency in the secondary system. A secondary
receiver observes a multiple access channel of two users, the secondary and the
primary transmitter, respectively. We show that, for spectrally-efficient
operation, the secondary system should apply Opportunistic Interference
Cancellation (OIC). With OIC, the secondary system decodes the primary signal
when such an opportunity is created by the primary rate and the power received
from the primary system. For such an operation, we derive the achievable data
rate in the secondary system. When the primary signal is decodable, we devise a
method, based on superposition coding, by which the secondary system can
achieve the maximal possible rate. Finally, we investigate the power allocation
in the secondary system when multiple channels are used. We show that the
optimal power allocation with OIC can be achieved through intercepted
water-filling instead of the conventional water-filling. The results show a
significant gain for the rate achieved through an opportunistic interference
cancellation.



Evolving Symbolic Controllers

The idea of symbolic controllers tries to bridge the gap between the top-down
manual design of the controller architecture, as advocated in Brooks'
subsumption architecture, and the bottom-up designer-free approach that is now
standard within the Evolutionary Robotics community. The designer provides a
set of elementary behavior, and evolution is given the goal of assembling them
to solve complex tasks. Two experiments are presented, demonstrating the
efficiency and showing the recursiveness of this approach. In particular, the
sensitivity with respect to the proposed elementary behaviors, and the
robustness w.r.t. generalization of the resulting controllers are studied in
detail.



Design of a 3 Axis Parallel Machine Tool for High Speed Machining: The Orthoglide

The Orthoglide project aims at designing a new 3-axis machine tool for High
Speed Machining. Basis kinematics is a 3 degree-of-freedom translational
parallel mechanism. This basis was submitted to isotropic and manipulability
constraints that allowed the optmization of its kinematic architecture and legs
architecture. Thus, several leg morphologies are convenient for the chosen
mechanism. We explain the process that led us to the choice we made for the
Orthoglide. A static study is presented to show how singular configurations of
the legs can cause stiffness problems.



The Isoconditioning Loci of Planar Three-DOF Parallel Manipulators

The subject of this paper is a special class of parallel manipulators. First,
we analyze a family of three-degree-of-freedom manipulators. Two Jacobian
matrices appear in the kinematic relations between the joint-rate and the
Cartesian-velocity vectors, which are called the "inverse kinematics" and the
"direct kinematics" matrices. The singular configurations of these matrices are
studied. The isotropic configurations are then studied based on the
characteristic length of this manipulator. The isoconditioning loci of all
Jacobian matrices are computed to define a global performance index to compare
the different working modes.



A Novel method for the design of 2-DOF Parallel mechanisms for machining applications

Parallel Kinematic Mechanisms (PKM) are interesting alternative designs for
machine tools. A design method based on velocity amplification factors analysis
is presented in this paper. The comparative study of two simple
two-degree-of-freedom PKM dedicated to machining applications is led through
this method: the common desired properties are the largest square Cartesian
workspace for given kinetostatic performances. The orientation and position of
the Cartesian workspace are chosen to avoid singularities and to produce the
best ratio between Cartesian workspace size and mechanism size. The machine
size of each resulting design is used as a comparative criterion.



Design of a Three-Axis Isotropic Parallel Manipulator for Machining Applications: The Orthoglide

The orthoglide is a 3-DOF parallel mechanism designed at IRCCyN for machining
applications. It features three fixed parallel linear joints which are mounted
orthogonally and a mobile platform which moves in the Cartesian x-y-z space
with fixed orientation. The orthoglide has been designed as function of a
prescribed Cartesian workspace with prescribed kinetostatic performances. The
interesting features of the orthoglide are a regular Cartesian workspace shape,
uniform performances in all directions and good compactness. A small-scale
prototype of the orthoglide under development is presented at the end of this
paper.



Workspace Analysis of the Orthoglide using Interval Analysis

This paper addresses the workspace analysis of the orthoglide, a 3-DOF
parallel mechanism designed for machining applications. This machine features
three fixed parallel linear joints which are mounted orthogonally and a mobile
platform which moves in the Cartesian x-y-z space with fixed orientation. The
workspace analysis is conducted on the bases of prescribed kinetostatic
performances. The interesting features of the orthoglide are a regular
Cartesian workspace shape, uniform performances in all directions and good
compactness. Interval analysis based methods for computing the dextrous
workspace and the largest cube enclosed in this workspace are presented.



P\'eriph\'eriques haptiques et simulation d'objets, de robots et de mannequins dans un environnement de CAO-Robotique : eM-Virtual Desktop

This paper presents the development of a new software in order to manage
objects, robots and mannequins in using the possibilities given by the haptic
feedback of the Phantom desktop devices. The haptic device provides 6
positional degree of freedom sensing but three degrees force feedback. This
software called eM-Virtual Desktop is integrated in the Tecnomatix's solution
called eM-Workplace. The eM-Workplace provides powerful solutions for planning
and designing of complex assembly facilities, lines and workplaces. In the
digital mockup context, the haptic interfaces can be used to reduce the
development cycle of products. Three different loops are used to manage the
graphic, the collision detection and the haptic feedback according to theirs
own frequencies. The developed software is currently tested in industrial
context by a European automotive constructor.



Predicting the Presence of Internet Worms using Novelty Detection

Internet worms cause billions of dollars in damage yearly, affecting millions
of users worldwide. For countermeasures to be deployed timeously, it is
necessary to use an automated system to detect the spread of a worm. This paper
discusses a method of determining the presence of a worm, based on routing
information currently available from Internet routers. An autoencoder, which is
a specialized type of neural network, was used to detect anomalies in normal
routing behavior. The autoencoder was trained using information from a single
router, and was able to detect both global instability caused by worms as well
as localized routing instability.



Robust Multi-Cellular Developmental Design

This paper introduces a continuous model for Multi-cellular Developmental
Design. The cells are fixed on a 2D grid and exchange "chemicals" with their
neighbors during the growth process. The quantity of chemicals that a cell
produces, as well as the differentiation value of the cell in the phenotype,
are controlled by a Neural Network (the genotype) that takes as inputs the
chemicals produced by the neighboring cells at the previous time step. In the
proposed model, the number of iterations of the growth process is not
pre-determined, but emerges during evolution: only organisms for which the
growth process stabilizes give a phenotype (the stable state), others are
declared nonviable. The optimization of the controller is done using the NEAT
algorithm, that optimizes both the topology and the weights of the Neural
Networks. Though each cell only receives local information from its neighbors,
the experimental results of the proposed approach on the 'flags' problems (the
phenotype must match a given 2D pattern) are almost as good as those of a
direct regression approach using the same model with global information.
Moreover, the resulting multi-cellular organisms exhibit almost perfect
self-healing characteristics.



Diversity-Multiplexing Tradeoff via Asymptotic Analysis of Large MIMO Systems

Diversity-multiplexing tradeoff (DMT) presents a compact framework to compare
various MIMO systems and channels in terms of the two main advantages they
provide (i.e. high data rate and/or low error rate). This tradeoff was
characterized asymptotically (SNR-> infinity) for i.i.d. Rayleigh fading
channel by Zheng and Tse [1]. The asymptotic DMT overestimates the finite-SNR
one [2]. In this paper, using the recent results on the asymptotic (in the
number of antennas) outage capacity distribution, we derive and analyze the
finite-SNR DMT for a broad class of channels (not necessarily Rayleigh fading).
Based on this, we give the convergence conditions for the asymptotic DMT to be
approached by the finite-SNR one. The multiplexing gain definition is shown to
affect critically the convergence point: when the multiplexing gain is defined
via the mean (ergodic) capacity, the convergence takes place at realistic SNR
values. Furthermore, in this case the diversity gain can also be used to
estimate the outage probability with reasonable accuracy. The multiplexing gain
definition via the high-SNR asymptote of the mean capacity (as in [1]) results
in very slow convergence for moderate to large systems (as 1/ln(SNR)^2) and,
hence, the asymptotic DMT cannot be used at realistic SNR values. For this
definition, the high-SNR threshold increases exponentially in the number of
antennas and in the multiplexing gain. For correlated keyhole channel, the
diversity gain is shown to decrease with correlation and power imbalance of the
channel. While the SNR-asymptotic DMT of Zheng and Tse does not capture this
effect, the size-asymptotic DMT does.



On Optimum Power Allocation for the V-BLAST

A unified analytical framework for optimum power allocation in the unordered
V-BLAST algorithm and its comparative performance analysis are presented.
Compact closed-form approximations for the optimum power allocation are
derived, based on average total and block error rates. The choice of the
criterion has little impact on the power allocation and, overall, the optimum
strategy is to allocate more power to lower step transmitters and less to
higher ones. High-SNR approximations for optimized average block and total
error rates are given. The SNR gain of optimization is rigorously defined and
studied using analytical tools, including lower and upper bounds, high and low
SNR approximations. The gain is upper bounded by the number of transmitters,
for any modulation format and type of fading channel. While the average
optimization is less complex than the instantaneous one, its performance is
almost as good at high SNR. A measure of robustness of the optimized algorithm
is introduced and evaluated. The optimized algorithm is shown to be robust to
perturbations in individual and total transmit powers. Based on the algorithm
robustness, a pre-set power allocation is suggested as a low-complexity
alternative to the other optimization strategies, which exhibits only a minor
loss in performance over the practical SNR range.



The Optimal Design of Three Degree-of-Freedom Parallel Mechanisms for Machining Applications

The subject of this paper is the optimal design of a parallel mechanism
intended for three-axis machining applications. Parallel mechanisms are
interesting alternative designs in this context but most of them are designed
for three- or six-axis machining applications. In the last case, the position
and the orientation of the tool are coupled and the shape of the workspace is
complex. The aim of this paper is to use a simple parallel mechanism with
two-degree-of-freedom (dof) for translational motions and to add one leg to
have one-dof rotational motion. The kinematics and singular configurations are
studied as well as an optimization method. The three-degree-of-freedom
mechanisms analyzed in this paper can be extended to four-axis machines by
adding a fourth axis in series with the first two.



Classification of one family of 3R positioning Manipulators

The aim of this paper is to classify one family of 3R serial positioning
manipulators. This categorization is based on the number of cusp points of
quaternary, binary, generic and non-generic manipulators. It was found three
subsets of manipulators with 0, 2 or 4 cusp points and one homotopy class for
generic quaternary manipulators. This classification allows us to define the
design parameters for which the manipulator is cuspidal or not, i.e., for which
the manipulator can or cannot change posture without meeting a singularity,
respectively.



Degree Optimization and Stability Condition for the Min-Sum Decoder

The min-sum (MS) algorithm is arguably the second most fundamental algorithm
in the realm of message passing due to its optimality (for a tree code) with
respect to the {\em block error} probability \cite{Wiberg}. There also seems to
be a fundamental relationship of MS decoding with the linear programming
decoder \cite{Koetter}. Despite its importance, its fundamental properties have
not nearly been studied as well as those of the sum-product (also known as BP)
algorithm.
  We address two questions related to the MS rule. First, we characterize the
stability condition under MS decoding. It turns out to be essentially the same
condition as under BP decoding. Second, we perform a degree distribution
optimization. Contrary to the case of BP decoding, under MS decoding the
thresholds of the best degree distributions for standard irregular LDPC
ensembles are significantly bounded away from the Shannon threshold. More
precisely, on the AWGN channel, for the best codes that we find, the gap to
capacity is 1dB for a rate 0.3 code and it is 0.4dB when the rate is 0.9 (the
gap decreases monotonically as we increase the rate).
  We also used the optimization procedure to design codes for modified MS
algorithm where the output of the check node is scaled by a constant
$1/\alpha$. For $\alpha = 1.25$, we observed that the gap to capacity was
lesser for the modified MS algorithm when compared with the MS algorithm.
However, it was still quite large, varying from 0.75 dB to 0.2 dB for rates
between 0.3 and 0.9.
  We conclude by posing what we consider to be the most important open
questions related to the MS algorithm.



An Approximation Algorithm for Shortest Descending Paths

A path from s to t on a polyhedral terrain is descending if the height of a
point p never increases while we move p along the path from s to t. No
efficient algorithm is known to find a shortest descending path (SDP) from s to
t in a polyhedral terrain. We give a simple approximation algorithm that solves
the SDP problem on general terrains. Our algorithm discretizes the terrain with
O(n^2 X / e) Steiner points so that after an O(n^2 X / e * log(n X /e))-time
preprocessing phase for a given vertex s, we can determine a (1+e)-approximate
SDP from s to any point v in O(n) time if v is either a vertex of the terrain
or a Steiner point, and in O(n X /e) time otherwise. Here n is the size of the
terrain, and X is a parameter of the geometry of the terrain.



Logic Column 18: Alternative Logics: A Book Review

This article discusses two books on the topic of alternative logics in
science: "Deviant Logic", by Susan Haack, and "Alternative Logics: Do Sciences
Need Them?", edited by Paul Weingartner.



Matroid Pathwidth and Code Trellis Complexity

We relate the notion of matroid pathwidth to the minimum trellis
state-complexity (which we term trellis-width) of a linear code, and to the
pathwidth of a graph. By reducing from the problem of computing the pathwidth
of a graph, we show that the problem of determining the pathwidth of a
representable matroid is NP-hard. Consequently, the problem of computing the
trellis-width of a linear code is also NP-hard. For a finite field $\F$, we
also consider the class of $\F$-representable matroids of pathwidth at most
$w$, and correspondingly, the family of linear codes over $\F$ with
trellis-width at most $w$. These are easily seen to be minor-closed. Since
these matroids (and codes) have branchwidth at most $w$, a result of Geelen and
Whittle shows that such matroids (and the corresponding codes) are
characterized by finitely many excluded minors. We provide the complete list of
excluded minors for $w=1$, and give a partial list for $w=2$.



Machine and Component Residual Life Estimation through the Application of Neural Networks

This paper concerns the use of neural networks for predicting the residual
life of machines and components. In addition, the advantage of using
condition-monitoring data to enhance the predictive capability of these neural
networks was also investigated. A number of neural network variations were
trained and tested with the data of two different reliability-related datasets.
The first dataset represents the renewal case where the failed unit is repaired
and restored to a good-as-new condition. Data was collected in the laboratory
by subjecting a series of similar test pieces to fatigue loading with a
hydraulic actuator. The average prediction error of the various neural networks
being compared varied from 431 to 841 seconds on this dataset, where test
pieces had a characteristic life of 8,971 seconds. The second dataset was
collected from a group of pumps used to circulate a water and magnetite
solution within a plant. The data therefore originated from a repaired system
affected by reliability degradation. When optimized, the multi-layer perceptron
neural networks trained with the Levenberg-Marquardt algorithm and the general
regression neural network produced a sum-of-squares error within 11.1% of each
other. The potential for using neural networks for residual life prediction and
the advantage of incorporating condition-based data into the model were proven
for both examples.



The Orthoglide: Kinematics and Workspace Analysis

The paper addresses kinematic and geometrical aspects of the Orthoglide, a
three-DOF parallel mechanism. This machine consists of three fixed linear
joints, which are mounted orthogonally, three identical legs and a mobile
platform, which moves in the Cartesian x-y-z space with fixed orientation. New
solutions to solve inverse/direct kinematics are proposed and a detailed
workspace analysis is performed taking into account specific joint limit
constraints.



Subjective Evaluation of Forms in an Immersive Environment

User's perception of product, by essence subjective, is a major topic in
marketing and industrial design. Many methods, based on users' tests, are used
so as to characterise this perception. We are interested in three main methods:
multidimensional scaling, semantic differential method, and preference mapping.
These methods are used to built a perceptual space, in order to position the
new product, to specify requirements by the study of user's preferences, to
evaluate some product attributes, related in particular to style (aesthetic).
These early stages of the design are primordial for a good orientation of the
project. In parallel, virtual reality tools and interfaces are more and more
efficient for suggesting to the user complex feelings, and creating in this way
various levels of perceptions. In this article, we present on an example the
use of multidimensional scaling, semantic differential method and preference
mapping for the subjective assessment of virtual products. These products,
which geometrical form is variable, are defined with a CAD model and are
proposed to the user with a spacemouse and stereoscopic glasses. Advantages and
limitations of such evaluation is next discussed..



Realistic Rendering of Kinetostatic Indices of Mechanisms

The work presented in this paper is related to the use of a haptic device in
an environment of robotic simulation. Such device introduces a new approach to
feel and to understand the boundaries of the workspace of mechanisms as well as
its kinetostatic properties. Indeed, these concepts are abstract and thus often
difficult to understand for the end-users. To catch his attention, we propose
to amplify the problems of the mechanisms in order to help him to take the good
decisions.



A New Concept of Modular Parallel Mechanism for Machining Applications

The subject of this paper is the design of a new concept of modular parallel
mechanisms for three, four or five-axis machining applications. Most parallel
mechanisms are designed for three- or six-axis machining applications. In the
last case, the position and the orientation of the tool are coupled and the
shape of the workspace is complex. The aim of this paper is to use a simple
parallel mechanism with two-degree-of-freedom (dof) for translation motions and
to add one or two legs to add one or two-dofs for rotation motions. The
kinematics and singular configurations are studied for each mechanism.



A Workspace based Classification of 3R Orthogonal Manipulators

A classification of a family of 3-revolute (3R) positioning manipulators is
established. This classification is based on the topology of their workspace.
The workspace is characterized in a half-cross section by the singular curves
of the manipulator. The workspace topology is defined by the number of cusps
and nodes that appear on these singular curves. The design parameters space is
shown to be partitioned into nine subspaces of distinct workspace topologies.
Each separating surface is given as an explicit expression in the
DH-parameters.



Singularity Surfaces and Maximal Singularity-Free Boxes in the Joint Space of Planar 3-RPR Parallel Manipulators

In this paper, a method to compute joint space singularity surfaces of 3-RPR
planar parallel manipulators is first presented. Then, a procedure to determine
maximal joint space singularity-free boxes is introduced. Numerical examples
are given in order to illustrate graphically the results. This study is of high
interest for planning trajectories in the joint space of 3-RPR parallel
manipulators and for manipulators design as it may constitute a tool for
choosing appropriate joint limits and thus for sizing the link lengths of the
manipulator.



Kinematics analysis of the parallel module of the VERNE machine

The paper derives the inverse and forward kinematic equations of a spatial
three-degree-of-freedom parallel mechanism, which is the parallel module of a
hybrid serial-parallel 5-axis machine tool. This parallel mechanism consists of
a moving platform that is connected to a fixed base by three non-identical
legs. Each leg is made up of one prismatic and two pair spherical joint, which
are connected in a way that the combined effects of the three legs lead to an
over-constrained mechanism with complex motion. This motion is defined as a
simultaneous combination of rotation and translation.



Does P=NP?

This paper has been withdrawn Abstract: This paper has been withdrawn by the
author due to the publication.



An Algorithm for Computing Cusp Points in the Joint Space of 3-RPR Parallel Manipulators

This paper presents an algorithm for detecting and computing the cusp points
in the joint space of 3-RPR planar parallel manipulators. In manipulator
kinematics, cusp points are special points, which appear on the singular curves
of the manipulators. The nonsingular change of assembly mode of 3-RPR parallel
manipulators was shown to be associated with the existence of cusp points. At
each of these points, three direct kinematic solutions coincide. In the
literature, a condition for the existence of three coincident direct kinematic
solutions was established, but has never been exploited, because the algebra
involved was too complicated to be solved. The algorithm presented in this
paper solves this equation and detects all the cusp points in the joint space
of these manipulators.



Typer la d\'e-s\'erialisation sans s\'erialiser les types

In this paper, we propose a way of assigning static type information to
unmarshalling functions and we describe a verification technique for
unmarshalled data that preserves the execution safety provided by static type
checking. This technique, whose correctness is proven, relies on singleton
types whose values are transmitted to unmarshalling routines at runtime, and on
an efficient checking algorithm able to deal with sharing and cycles.



DWEB: A Data Warehouse Engineering Benchmark

Data warehouse architectural choices and optimization techniques are critical
to decision support query performance. To facilitate these choices, the
performance of the designed data warehouse must be assessed. This is usually
done with the help of benchmarks, which can either help system users comparing
the performances of different systems, or help system engineers testing the
effect of various design choices. While the TPC standard decision support
benchmarks address the first point, they are not tuneable enough to address the
second one and fail to model different data warehouse schemas. By contrast, our
Data Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc
synthetic data warehouses and workloads. DWEB is fully parameterized to fulfill
data warehouse design needs. However, two levels of parameterization keep it
relatively easy to tune. Finally, DWEB is implemented as a Java free software
that can be interfaced with most existing relational database management
systems. A sample usage of DWEB is also provided in this paper.



DOEF: A Dynamic Object Evaluation Framework

In object-oriented or object-relational databases such as multimedia
databases or most XML databases, access patterns are not static, i.e.,
applications do not always access the same objects in the same order
repeatedly. However, this has been the way these databases and associated
optimisation techniques like clustering have been evaluated up to now. This
paper opens up research regarding this issue by proposing a dynamic object
evaluation framework (DOEF) that accomplishes access pattern change by defining
configurable styles of change. This preliminary prototype has been designed to
be open and fully extensible. To illustrate the capabilities of DOEF, we used
it to compare the performances of four state of the art dynamic clustering
algorithms. The results show that DOEF is indeed effective at determining the
adaptability of each dynamic clustering algorithm to changes in access pattern.



Decision tree modeling with relational views

Data mining is a useful decision support technique that can be used to
discover production rules in warehouses or corporate data. Data mining research
has made much effort to apply various mining algorithms efficiently on large
databases. However, a serious problem in their practical application is the
long processing time of such algorithms. Nowadays, one of the key challenges is
to integrate data mining methods within the framework of traditional database
systems. Indeed, such implementations can take advantage of the efficiency
provided by SQL engines. In this paper, we propose an integrating approach for
decision trees within a classical database system. In other words, we try to
discover knowledge from relational databases, in the form of production rules,
via a procedure embedding SQL queries. The obtained decision tree is defined by
successive, related relational views. Each view corresponds to a given
population in the underlying decision tree. We selected the classical Induction
Decision Tree (ID3) algorithm to build the decision tree. To prove that our
implementation of ID3 works properly, we successfully compared the output of
our procedure with the output of an existing and validated data mining
software, SIPINA. Furthermore, since our approach is tuneable, it can be
generalized to any other similar decision tree-based method.



Warehousing Web Data

In a data warehousing process, mastering the data preparation phase allows
substantial gains in terms of time and performance when performing
multidimensional analysis or using data mining algorithms. Furthermore, a data
warehouse can require external data. The web is a prevalent data source in this
context. In this paper, we propose a modeling process for integrating diverse
and heterogeneous (so-called multiform) data into a unified format.
Furthermore, the very schema definition provides first-rate metadata in our
data warehousing context. At the conceptual level, a complex object is
represented in UML. Our logical model is an XML schema that can be described
with a DTD or the XML-Schema language. Eventually, we have designed a Java
prototype that transforms our multiform input data into XML documents
representing our physical model. Then, the XML documents we obtain are mapped
into a relational database we view as an ODS (Operational Data Storage), whose
content will have to be re-modeled in a multidimensional way to allow its
storage in a star schema-based warehouse and, later, its analysis.



Web data modeling for integration in data warehouses

In a data warehousing process, the data preparation phase is crucial.
Mastering this phase allows substantial gains in terms of time and performance
when performing a multidimensional analysis or using data mining algorithms.
Furthermore, a data warehouse can require external data. The web is a prevalent
data source in this context, but the data broadcasted on this medium are very
heterogeneous. We propose in this paper a UML conceptual model for a complex
object representing a superclass of any useful data source (databases, plain
texts, HTML and XML documents, images, sounds, video clips...). The translation
into a logical model is achieved with XML, which helps integrating all these
diverse, heterogeneous data into a unified format, and whose schema definition
provides first-rate metadata in our data warehousing context. Moreover, we
benefit from XML's flexibility, extensibility and from the richness of the
semi-structured data model, but we are still able to later map XML documents
into a database if more structuring is needed.



Mixing the Objective Caml and C# Programming Models in the .Net Framework

We present a new code generator, called O'Jacare.net, to inter-operate
between C# and Objective Caml through their object models. O'Jacare.net defines
a basic IDL (Interface Definition Language) that describes classes and
interfaces in order to communicate between Objective Caml and C#. O'Jacare.net
generates all needed wrapper classes and takes advantage of static type
checking in both worlds. Although the IDL intersects these two object models,
O'Jacare.net allows to combine features from both.



Actin - Technical Report

The Boolean satisfiability problem (SAT) can be solved efficiently with
variants of the DPLL algorithm. For industrial SAT problems, DPLL with conflict
analysis dependent dynamic decision heuristics has proved to be particularly
efficient, e.g. in Chaff. In this work, algorithms that initialize the variable
activity values in the solver MiniSAT v1.14 by analyzing the CNF are evolved
using genetic programming (GP), with the goal to reduce the total number of
conflicts of the search and the solving time. The effect of using initial
activities other than zero is examined by initializing with random numbers. The
possibility of countering the detrimental effects of reordering the CNF with
improved initialization is investigated. The best result found (with validation
testing on further problems) was used in the solver Actin, which was submitted
to SAT-Race 2006.



A note on module-composed graphs

In this paper we consider module-composed graphs, i.e. graphs which can be
defined by a sequence of one-vertex insertions v_1,...,v_n, such that the
neighbourhood of vertex v_i, 2<= i<= n, forms a module (a homogeneous set) of
the graph defined by vertices v_1,..., v_{i-1}.
  We show that module-composed graphs are HHDS-free and thus homogeneously
orderable, weakly chordal, and perfect. Every bipartite distance hereditary
graph, every (co-2C_4,P_4)-free graph and thus every trivially perfect graph is
module-composed. We give an O(|V_G|(|V_G|+|E_G|)) time algorithm to decide
whether a given graph G is module-composed and construct a corresponding
module-sequence.
  For the case of bipartite graphs, module-composed graphs are exactly distance
hereditary graphs, which implies simple linear time algorithms for their
recognition and construction of a corresponding module-sequence.



Unfolding Manhattan Towers

We provide an algorithm for unfolding the surface of any orthogonal
polyhedron that falls into a particular shape class we call Manhattan Towers,
to a nonoverlapping planar orthogonal polygon. The algorithm cuts along edges
of a 4x5x1 refinement of the vertex grid.



Wireless Networking to Support Data and Voice Communication Using Spread Spectrum Technology in The Physical Layer

Wireless networking is rapidly growing and becomes an inexpensive technology
which allows multiple users to simultaneously access the network and the
internet while roaming about the campus. In the present work, the software
development of a wireless LAN(WLAN) is highlighted. This WLAN utilizes direct
sequence spread spectrum (DSSS) technology at 902MHz RF carrier frequency in
its physical layer. Cost effective installation and antijaming property of
spread spectrum technology are the major advantages of this work.



HMM Speaker Identification Using Linear and Non-linear Merging Techniques

Speaker identification is a powerful, non-invasive and in-expensive biometric
technique. The recognition accuracy, however, deteriorates when noise levels
affect a specific band of frequency. In this paper, we present a sub-band based
speaker identification that intends to improve the live testing performance.
Each frequency sub-band is processed and classified independently. We also
compare the linear and non-linear merging techniques for the sub-bands
recognizer. Support vector machines and Gaussian Mixture models are the
non-linear merging techniques that are investigated. Results showed that the
sub-band based method used with linear merging techniques enormously improved
the performance of the speaker identification over the performance of wide-band
recognizers when tested live. A live testing improvement of 9.78% was achieved



A Class of LDPC Erasure Distributions with Closed-Form Threshold Expression

In this paper, a family of low-density parity-check (LDPC) degree
distributions, whose decoding threshold on the binary erasure channel (BEC)
admits a simple closed form, is presented. These degree distributions are a
subset of the check regular distributions (i.e. all the check nodes have the
same degree), and are referred to as $p$-positive distributions. It is given
proof that the threshold for a $p$-positive distribution is simply expressed by
$[\lambda'(0)\rho'(1)]^{-1}$. Besides this closed form threshold expression,
the $p$-positive distributions exhibit three additional properties. First, for
given code rate, check degree and maximum variable degree, they are in some
cases characterized by a threshold which is extremely close to that of the best
known check regular distributions, under the same set of constraints. Second,
the threshold optimization problem within the $p$-positive class can be solved
in some cases with analytic methods, without using any numerical optimization
tool. Third, these distributions can achieve the BEC capacity. The last
property is shown by proving that the well-known binomial degree distributions
belong to the $p$-positive family.



Non-Computability of Consciousness

With the great success in simulating many intelligent behaviors using
computing devices, there has been an ongoing debate whether all conscious
activities are computational processes. In this paper, the answer to this
question is shown to be no. A certain phenomenon of consciousness is
demonstrated to be fully represented as a computational process using a quantum
computer. Based on the computability criterion discussed with Turing machines,
the model constructed is shown to necessarily involve a non-computable element.
The concept that this is solely a quantum effect and does not work for a
classical case is also discussed.



Principal Component Analysis and Automatic Relevance Determination in Damage Identification

This paper compares two neural network input selection schemes, the Principal
Component Analysis (PCA) and the Automatic Relevance Determination (ARD) based
on Mac-Kay's evidence framework. The PCA takes all the input data and projects
it onto a lower dimension space, thereby reduc-ing the dimension of the input
space. This input reduction method often results with parameters that have
significant influence on the dynamics of the data being diluted by those that
do not influence the dynamics of the data. The ARD selects the most relevant
input parameters and discards those that do not contribute significantly to the
dynamics of the data being modelled. The ARD sometimes results with important
input parameters being discarded thereby compromising the dynamics of the data.
The PCA and ARD methods are implemented together with a Multi-Layer-Perceptron
(MLP) network for fault identification in structures and the performance of the
two methods is as-sessed. It is observed that ARD and PCA give similar
accu-racy levels when used as input-selection schemes. There-fore, the choice
of input-selection scheme is dependent on the nature of the data being
processed.



Using artificial intelligence for data reduction in mechanical engineering

In this paper artificial neural networks and support vector machines are used
to reduce the amount of vibration data that is required to estimate the Time
Domain Average of a gear vibration signal. Two models for estimating the time
domain average of a gear vibration signal are proposed. The models are tested
on data from an accelerated gear life test rig. Experimental results indicate
that the required data for calculating the Time Domain Average of a gear
vibration signal can be reduced by up to 75% when the proposed models are
implemented.



Evolutionary Optimisation Methods for Template Based Image Registration

This paper investigates the use of evolutionary optimisation techniques to
register a template with a scene image. An error function is created to measure
the correspondence of the template to the image. The problem presented here is
to optimise the horizontal, vertical and scaling parameters that register the
template with the scene. The Genetic Algorithm, Simulated Annealing and
Particle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation
with starting points chosen in a pre-processing stage. The paper investigates
the precision and accuracy of each method and shows that all four methods
perform favourably for image registration. SA is the most precise, GA is the
most accurate. PSO is a good mix of both and the Simplex method returns local
minima the most. A pre-processing stage should be investigated for the
evolutionary methods in order to improve performance. Discrete versions of the
optimisation methods should be investigated to further improve computational
performance.



Option Pricing Using Bayesian Neural Networks

Options have provided a field of much study because of the complexity
involved in pricing them. The Black-Scholes equations were developed to price
options but they are only valid for European styled options. There is added
complexity when trying to price American styled options and this is why the use
of neural networks has been proposed. Neural Networks are able to predict
outcomes based on past data. The inputs to the networks here are stock
volatility, strike price and time to maturity with the output of the network
being the call option price. There are two techniques for Bayesian neural
networks used. One is Automatic Relevance Determination (for Gaussian
Approximation) and one is a Hybrid Monte Carlo method, both used with
Multi-Layer Perceptrons.



Capacity of Underspread Noncoherent WSSUS Fading Channels under Peak Signal Constraints

We characterize the capacity of the general class of noncoherent underspread
wide-sense stationary uncorrelated scattering (WSSUS) time-frequency-selective
Rayleigh fading channels, under peak constraints in time and frequency and in
time only. Capacity upper and lower bounds are found which are explicit in the
channel's scattering function and allow to identify the capacity-maximizing
bandwidth for a given scattering function and a given peak-to-average power
ratio.



A Tighter Analysis of Setcover Greedy Algorithm for Test Set

Setcover greedy algorithm is a natural approximation algorithm for test set
problem. This paper gives a precise and tighter analysis of performance
guarantee of this algorithm. The author improves the performance guarantee
$2\ln n$ which derives from set cover problem to $1.1354\ln n$ by applying the
potential function technique. In addition, the author gives a nontrivial lower
bound $1.0004609\ln n$ of performance guarantee of this algorithm. This lower
bound, together with the matching bound of information content heuristic,
confirms the fact information content heuristic is slightly better than
setcover greedy algorithm in worst case.



Scalability and Optimisation of a Committee of Agents Using Genetic Algorithm

A population of committees of agents that learn by using neural networks is
implemented to simulate the stock market. Each committee of agents, which is
regarded as a player in a game, is optimised by continually adapting the
architecture of the agents using genetic algorithms. The committees of agents
buy and sell stocks by following this procedure: (1) obtain the current price
of stocks; (2) predict the future price of stocks; (3) and for a given price
trade until all the players are mutually satisfied. The trading of stocks is
conducted by following these rules: (1) if a player expects an increase in
price then it tries to buy the stock; (2) else if it expects a drop in the
price, it sells the stock; (3)and the order in which a player participates in
the game is random. The proposed procedure is implemented to simulate trading
of three stocks, namely, the Dow Jones, the Nasdaq and the S&P 500. A linear
relationship between the number of players and agents versus the computational
time to run the complete simulation is observed. It is also found that no
player has a monopolistic advantage.



Finite Element Model Updating Using Response Surface Method

This paper proposes the response surface method for finite element model
updating. The response surface method is implemented by approximating the
finite element model surface response equation by a multi-layer perceptron. The
updated parameters of the finite element model were calculated using genetic
algorithm by optimizing the surface response equation. The proposed method was
compared to the existing methods that use simulated annealing or genetic
algorithm together with a full finite element model for finite element model
updating. The proposed method was tested on an unsymmetri-cal H-shaped
structure. It was observed that the proposed method gave the updated natural
frequen-cies and mode shapes that were of the same order of accuracy as those
given by simulated annealing and genetic algorithm. Furthermore, it was
observed that the response surface method achieved these results at a
computational speed that was more than 2.5 times as fast as the genetic
algorithm and a full finite element model and 24 times faster than the
simulated annealing.



Dynamic Model Updating Using Particle Swarm Optimization Method

This paper proposes the use of particle swarm optimization method (PSO) for
finite element (FE) model updating. The PSO method is compared to the existing
methods that use simulated annealing (SA) or genetic algorithms (GA) for FE
model for model updating. The proposed method is tested on an unsymmetrical
H-shaped structure. It is observed that the proposed method gives updated
natural frequencies the most accurate and followed by those given by an updated
model that was obtained using the GA and a full FE model. It is also observed
that the proposed method gives updated mode shapes that are best correlated to
the measured ones, followed by those given by an updated model that was
obtained using the SA and a full FE model. Furthermore, it is observed that the
PSO achieves this accuracy at a computational speed that is faster than that by
the GA and a full FE model which is faster than the SA and a full FE model.



Modeling and Controlling Interstate Conflict

Bayesian neural networks were used to model the relationship between input
parameters, Democracy, Allies, Contingency, Distance, Capability, Dependency
and Major Power, and the output parameter which is either peace or conflict.
The automatic relevance determination was used to rank the importance of input
variables. Control theory approach was used to identify input variables that
would give a peaceful outcome. It was found that using all four controllable
variables Democracy, Allies, Capability and Dependency; or using only
Dependency or only Capabilities avoids all the predicted conflicts.



Energy-Efficient Resource Allocation in Wireless Networks: An Overview of Game-Theoretic Approaches

An overview of game-theoretic approaches to energy-efficient resource
allocation in wireless networks is presented. Focusing on multiple-access
networks, it is demonstrated that game theory can be used as an effective tool
to study resource allocation in wireless networks with quality-of-service (QoS)
constraints. A family of non-cooperative (distributed) games is presented in
which each user seeks to choose a strategy that maximizes its own utility while
satisfying its QoS requirements. The utility function considered here measures
the number of reliable bits that are transmitted per joule of energy consumed
and, hence, is particulary suitable for energy-constrained networks. The
actions available to each user in trying to maximize its own utility are at
least the choice of the transmit power and, depending on the situation, the
user may also be able to choose its transmission rate, modulation, packet size,
multiuser receiver, multi-antenna processing algorithm, or carrier allocation
strategy. The best-response strategy and Nash equilibrium for each game is
presented. Using this game-theoretic framework, the effects of power control,
rate control, modulation, temporal and spatial signal processing, carrier
allocation strategy and delay QoS constraints on energy efficiency and network
capacity are quantified.



A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA Networks with Delay QoS Constraints

A game-theoretic framework is used to study the effect of constellation size
on the energy efficiency of wireless networks for M-QAM modulation. A
non-cooperative game is proposed in which each user seeks to choose its
transmit power (and possibly transmit symbol rate) as well as the constellation
size in order to maximize its own utility while satisfying its delay
quality-of-service (QoS) constraint. The utility function used here measures
the number of reliable bits transmitted per joule of energy consumed, and is
particularly suitable for energy-constrained networks. The best-response
strategies and Nash equilibrium solution for the proposed game are derived. It
is shown that in order to maximize its utility (in bits per joule), a user must
choose the lowest constellation size that can accommodate the user's delay
constraint. This strategy is different from one that would maximize spectral
efficiency. Using this framework, the tradeoffs among energy efficiency, delay,
throughput and constellation size are also studied and quantified. In addition,
the effect of trellis-coded modulation on energy efficiency is discussed.



Random Linear Network Coding: A free cipher?

We consider the level of information security provided by random linear
network coding in network scenarios in which all nodes comply with the
communication protocols yet are assumed to be potential eavesdroppers (i.e.
"nice but curious"). For this setup, which differs from wiretapping scenarios
considered previously, we develop a natural algebraic security criterion, and
prove several of its key properties. A preliminary analysis of the impact of
network topology on the overall network coding security, in particular for
complete directed acyclic graphs, is also included.



Scheduling Dags under Uncertainty

This paper introduces a parallel scheduling problem where a directed acyclic
graph modeling $t$ tasks and their dependencies needs to be executed on $n$
unreliable workers. Worker $i$ executes task $j$ correctly with probability
$p_{i,j}$. The goal is to find a regimen $\Sigma$, that dictates how workers
get assigned to tasks (possibly in parallel and redundantly) throughout
execution, so as to minimize the expected completion time. This fundamental
parallel scheduling problem arises in grid computing and project management
fields, and has several applications.
  We show a polynomial time algorithm for the problem restricted to the case
when dag width is at most a constant and the number of workers is also at most
a constant. These two restrictions may appear to be too severe. However, they
are fundamentally required. Specifically, we demonstrate that the problem is
NP-hard with constant number of workers when dag width can grow, and is also
NP-hard with constant dag width when the number of workers can grow. When both
dag width and the number of workers are unconstrained, then the problem is
inapproximable within factor less than 5/4, unless P=NP.



Ontology-Supported and Ontology-Driven Conceptual Navigation on the World Wide Web

This paper presents the principles of ontology-supported and ontology-driven
conceptual navigation. Conceptual navigation realizes the independence between
resources and links to facilitate interoperability and reusability. An engine
builds dynamic links, assembles resources under an argumentative scheme and
allows optimization with a possible constraint, such as the user's available
time. Among several strategies, two are discussed in detail with examples of
applications. On the one hand, conceptual specifications for linking and
assembling are embedded in the resource meta-description with the support of
the ontology of the domain to facilitate meta-communication. Resources are like
agents looking for conceptual acquaintances with intention. On the other hand,
the domain ontology and an argumentative ontology drive the linking and
assembling strategies.



A Technical Report On Grid Benchmarking using ATLAS V.O

Grids include heterogeneous resources, which are based on different hardware
and software architectures or components. In correspondence with this diversity
of the infrastructure, the execution time of any single job, as well as the
total grid performance can both be affected substantially, which can be
demonstrated by measurements. Running a simple benchmarking suite can show this
heterogeneity and give us results about the differences over the grid sites.



Optimal Watermark Embedding and Detection Strategies Under Limited Detection Resources

An information-theoretic approach is proposed to watermark embedding and
detection under limited detector resources. First, we consider the attack-free
scenario under which asymptotically optimal decision regions in the
Neyman-Pearson sense are proposed, along with the optimal embedding rule.
Later, we explore the case of zero-mean i.i.d. Gaussian covertext distribution
with unknown variance under the attack-free scenario. For this case, we propose
a lower bound on the exponential decay rate of the false-negative probability
and prove that the optimal embedding and detecting strategy is superior to the
customary linear, additive embedding strategy in the exponential sense.
Finally, these results are extended to the case of memoryless attacks and
general worst case attacks. Optimal decision regions and embedding rules are
offered, and the worst attack channel is identified.



Crystallization in large wireless networks

We analyze fading interference relay networks where M single-antenna
source-destination terminal pairs communicate concurrently and in the same
frequency band through a set of K single-antenna relays using half-duplex
two-hop relaying. Assuming that the relays have channel state information
(CSI), it is shown that in the large-M limit, provided K grows fast enough as a
function of M, the network "decouples" in the sense that the individual
source-destination terminal pair capacities are strictly positive. The
corresponding required rate of growth of K as a function of M is found to be
sufficient to also make the individual source-destination fading links converge
to nonfading links. We say that the network "crystallizes" as it breaks up into
a set of effectively isolated "wires in the air". A large-deviations analysis
is performed to characterize the "crystallization" rate, i.e., the rate (as a
function of M,K) at which the decoupled links converge to nonfading links. In
the course of this analysis, we develop a new technique for characterizing the
large-deviations behavior of certain sums of dependent random variables. For
the case of no CSI at the relay level, assuming amplify-and-forward relaying,
we compute the per source-destination terminal pair capacity for M,K converging
to infinity, with K/M staying fixed, using tools from large random matrix
theory.



Double Sided Watermark Embedding and Detection with Perceptual Analysis

In our previous work, we introduced a double-sided technique that utilizes
but not reject the host interference. Due to its nice property of utilizing but
not rejecting the host interference, it has a big advantage over the host
interference schemes in that the perceptual analysis can be easily implemented
for our scheme to achieve the locally bounded maximum embedding strength. Thus,
in this work, we detail how to implement the perceptual analysis in our
double-sided schemes since the perceptual analysis is very important for
improving the fidelity of watermarked contents. Through the extensive
performance comparisons, we can further validate the performance advantage of
our double-sided schemes.



Towards Informative Statistical Flow Inversion

A problem which has recently attracted research attention is that of
estimating the distribution of flow sizes in internet traffic. On high traffic
links it is sometimes impossible to record every packet. Researchers have
approached the problem of estimating flow lengths from sampled packet data in
two separate ways. Firstly, different sampling methodologies can be tried to
more accurately measure the desired system parameters. One such method is the
sample-and-hold method where, if a packet is sampled, all subsequent packets in
that flow are sampled. Secondly, statistical methods can be used to ``invert''
the sampled data and produce an estimate of flow lengths from a sample.
  In this paper we propose, implement and test two variants on the
sample-and-hold method. In addition we show how the sample-and-hold method can
be inverted to get an estimation of the genuine distribution of flow sizes.
Experiments are carried out on real network traces to compare standard packet
sampling with three variants of sample-and-hold. The methods are compared for
their ability to reconstruct the genuine distribution of flow sizes in the
traffic.



A Branch and Cut Algorithm for the Halfspace Depth Problem

The concept of data depth in non-parametric multivariate descriptive
statistics is the generalization of the univariate rank method to multivariate
data. Halfspace depth is a measure of data depth. Given a set S of points and a
point p, the halfspace depth (or rank) k of p is defined as the minimum number
of points of S contained in any closed halfspace with p on its boundary.
Computing halfspace depth is NP-hard, and it is equivalent to the Maximum
Feasible Subsystem problem. In this thesis a mixed integer program is
formulated with the big-M method for the halfspace depth problem. We suggest a
branch and cut algorithm. In this algorithm, Chinneck's heuristic algorithm is
used to find an upper bound and a related technique based on sensitivity
analysis is used for branching. Irreducible Infeasible Subsystem (IIS) hitting
set cuts are applied. We also suggest a binary search algorithm which may be
more stable numerically. The algorithms are implemented with the BCP framework
from the COIN-OR project.



A Closed-Form Method for LRU Replacement under Generalized Power-Law Demand

We consider the well known \emph{Least Recently Used} (LRU) replacement
algorithm and analyze it under the independent reference model and generalized
power-law demand. For this extensive family of demand distributions we derive a
closed-form expression for the per object steady-state hit ratio. To the best
of our knowledge, this is the first analytic derivation of the per object hit
ratio of LRU that can be obtained in constant time without requiring laborious
numeric computations or simulation. Since most applications of replacement
algorithms include (at least) some scenarios under i.i.d. requests, our method
has substantial practical value, especially when having to analyze multiple
caches, where existing numeric methods and simulation become too time
consuming.



On the Hopcroft's minimization algorithm

We show that the absolute worst case time complexity for Hopcroft's
minimization algorithm applied to unary languages is reached only for de Bruijn
words. A previous paper by Berstel and Carton gave the example of de Bruijn
words as a language that requires O(n log n) steps by carefully choosing the
splitting sets and processing these sets in a FIFO mode. We refine the previous
result by showing that the Berstel/Carton example is actually the absolute
worst case time complexity in the case of unary languages. We also show that a
LIFO implementation will not achieve the same worst time complexity for the
case of unary languages. Lastly, we show that the same result is valid also for
the cover automata and a modification of the Hopcroft's algorithm, modification
used in minimization of cover automata.



A first-order Temporal Logic for Actions

We present a multi-modal action logic with first-order modalities, which
contain terms which can be unified with the terms inside the subsequent
formulas and which can be quantified. This makes it possible to handle
simultaneously time and states. We discuss applications of this language to
action theory where it is possible to express many temporal aspects of actions,
as for example, beginning, end, time points, delayed preconditions and results,
duration and many others. We present tableaux rules for a decidable fragment of
this logic.



Bit-Interleaved Coded Multiple Beamforming with Imperfect CSIT

This paper addresses the performance of bit-interleaved coded multiple
beamforming (BICMB) [1], [2] with imperfect knowledge of beamforming vectors.
Most studies for limited-rate channel state information at the transmitter
(CSIT) assume that the precoding matrix has an invariance property under an
arbitrary unitary transform. In BICMB, this property does not hold. On the
other hand, the optimum precoder and detector for BICMB are invariant under a
diagonal unitary transform. In order to design a limited-rate CSIT system for
BICMB, we propose a new distortion measure optimum under this invariance. Based
on this new distortion measure, we introduce a new set of centroids and employ
the generalized Lloyd algorithm for codebook design. We provide simulation
results demonstrating the performance improvement achieved with the proposed
distortion measure and the codebook design for various receivers with linear
detectors. We show that although these receivers have the same performance for
perfect CSIT, their performance varies under imperfect CSIT.



Multi-Dimensional Recurrent Neural Networks

Recurrent neural networks (RNNs) have proved effective at one dimensional
sequence learning tasks, such as speech and online handwriting recognition.
Some of the properties that make RNNs suitable for such tasks, for example
robustness to input warping, and the ability to access contextual information,
are also desirable in multidimensional domains. However, there has so far been
no direct way of applying RNNs to data with more than one spatio-temporal
dimension. This paper introduces multi-dimensional recurrent neural networks
(MDRNNs), thereby extending the potential applicability of RNNs to vision,
video processing, medical imaging and many other areas, while avoiding the
scaling problems that have plagued other multi-dimensional models. Experimental
results are provided for two image segmentation tasks.



Mean Field Models of Message Throughput in Dynamic Peer-to-Peer Systems

The churn rate of a peer-to-peer system places direct limitations on the rate
at which messages can be effectively communicated to a group of peers. These
limitations are independent of the topology and message transmission latency.
In this paper we consider a peer-to-peer network, based on the Engset model,
where peers arrive and depart independently at random. We show how the arrival
and departure rates directly limit the capacity for message streams to be
broadcast to all other peers, by deriving mean field models that accurately
describe the system behavior. Our models cover the unit and more general k
buffer cases, i.e. where a peer can buffer at most k messages at any one time,
and we give results for both single and multi-source message streams. We define
coverage rate as peer-messages per unit time, i.e. the rate at which a number
of peers receive messages, and show that the coverage rate is limited by the
churn rate and buffer size. Our theory introduces an Instantaneous Message
Exchange (IME) model and provides a template for further analysis of more
complicated systems. Using the IME model, and assuming random processes, we
have obtained very accurate equations of the system dynamics in a variety of
interesting cases, that allow us to tune a peer-to-peer system. It remains to
be seen if we can maintain this accuracy for general processes and when
applying a non-instantaneous model.



CDMA Technology for Intelligent Transportation Systems

Scientists and Technologists involved in the development of radar and remote
sensing systems all over the world are now trying to involve themselves in
saving of manpower in the form of developing a new application of their ideas
in Intelligent Transport system(ITS). The world statistics shows that by
incorporating such wireless radar system in the car would decrease the world
road accident by 8-10% yearly. The wireless technology has to be chosen
properly which is capable of tackling the severe interferences present in the
open road. A combined digital technology like Spread spectrum along with
diversity reception will help a lot in this regard. Accordingly, the choice is
for FHSS based space diversity system which will utilize carrier frequency
around 5.8 GHz ISM band with available bandwidth of 80 MHz and no license. For
efficient design, the radio channel is characterized on which the design is
based. Out of two available modes e.g. Communication and Radar modes, the radar
mode is providing the conditional measurement of the range of the nearest car
after authentication of the received code, thus ensuring the reliability and
accuracy of measurement. To make the system operational in simultaneous mode,
we have started the Software Defined Radio approach for best speed and
flexibility.



RADAR Imaging in the Open field At 300 MHz-3000 MHz Radio Band

With the technological growth of broadband wireless technology like CDMA and
UWB, a lots of development efforts towards wireless communication system and
Imaging radar system are well justified. Efforts are also being imparted
towards a Convergence Technology.. the convergence between a communication and
radar technology which will result in ITS (Intelligent Transport System) and
other applications. This encourages present authors for this development. They
are trying to utilize or converge the communication technologies towards radar
and to achieve the Interference free and clutter free quality remote images of
targets using DS-UWB wireless technology.



Scientific citations in Wikipedia

The Internet-based encyclopaedia Wikipedia has grown to become one of the
most visited web-sites on the Internet. However, critics have questioned the
quality of entries, and an empirical study has shown Wikipedia to contain
errors in a 2005 sample of science entries. Biased coverage and lack of sources
are among the "Wikipedia risks". The present work describes a simple assessment
of these aspects by examining the outbound links from Wikipedia articles to
articles in scientific journals with a comparison against journal statistics
from Journal Citation Reports such as impact factors. The results show an
increasing use of structured citation markup and good agreement with the
citation pattern seen in the scientific literature though with a slight
tendency to cite articles in high-impact journals such as Nature and Science.
These results increase confidence in Wikipedia as an good information organizer
for science in general.



Parallelized approximation algorithms for minimum routing cost spanning trees

We parallelize several previously proposed algorithms for the minimum routing
cost spanning tree problem and some related problems.



Improvements to the Psi-SSA representation

Modern compiler implementations use the Static Single Assignment
representation as a way to efficiently implement optimizing algorithms. However
this representation is not well adapted to architectures with a predicated
instruction set. The Psi-SSA representation extends the SSA representation such
that standard SSA algorithms can be easily adapted to an architecture with a
fully predicated instruction set. A new pseudo operation, the Psi operation, is
introduced to merge several conditional definitions into a unique definition.



Best insertion algorithm for resource-constrained project scheduling problem

This paper considers heuristics for well known resource-constrained project
scheduling problem (RCPSP). First a feasible schedule is constructed using
randomized best insertion algorithm. The construction is followed by a local
search where a new solution is generated as follows: first we randomly delete m
activities from the list, which are then reinserted in the list in consecutive
order. At the end of run, the schedule with the minimum makespan is selected.
Experimental work shows very good results on standard test instances found in
PSPLIB



Elementary transformation analysis for Array-OL

Array-OL is a high-level specification language dedicated to the definition
of intensive signal processing applications. Several tools exist for
implementing an Array-OL specification as a data parallel program. While
Array-OL can be used directly, it is often convenient to be able to deduce part
of the specification from a sequential version of the application. This paper
proposes such an analysis and examines its feasibility and its limits.



On the freezing of variables in random constraint satisfaction problems

The set of solutions of random constraint satisfaction problems (zero energy
groundstates of mean-field diluted spin glasses) undergoes several structural
phase transitions as the amount of constraints is increased. This set first
breaks down into a large number of well separated clusters. At the freezing
transition, which is in general distinct from the clustering one, some
variables (spins) take the same value in all solutions of a given cluster. In
this paper we study the critical behavior around the freezing transition, which
appears in the unfrozen phase as the divergence of the sizes of the
rearrangements induced in response to the modification of a variable. The
formalism is developed on generic constraint satisfaction problems and applied
in particular to the random satisfiability of boolean formulas and to the
coloring of random graphs. The computation is first performed in random tree
ensembles, for which we underline a connection with percolation models and with
the reconstruction problem of information theory. The validity of these results
for the original random ensembles is then discussed in the framework of the
cavity method.



Sequential mechanism design

In the customary VCG (Vickrey-Clarke-Groves) mechanism truth-telling is a
dominant strategy. In this paper we study the sequential VCG mechanism and show
that other dominant strategies may then exist. We illustrate how this fact can
be used to minimize taxes using examples concerned with Clarke tax and public
projects.



From Nondeterministic B\"uchi and Streett Automata to Deterministic Parity Automata

In this paper we revisit Safra's determinization constructions for automata
on infinite words. We show how to construct deterministic automata with fewer
states and, most importantly, parity acceptance conditions. Determinization is
used in numerous applications, such as reasoning about tree automata,
satisfiability of CTL*, and realizability and synthesis of logical
specifications. The upper bounds for all these applications are reduced by
using the smaller deterministic automata produced by our construction. In
addition, the parity acceptance conditions allows to use more efficient
algorithms (when compared to handling Rabin or Streett acceptance conditions).



On tractability and congruence distributivity

Constraint languages that arise from finite algebras have recently been the
object of study, especially in connection with the Dichotomy Conjecture of
Feder and Vardi. An important class of algebras are those that generate
congruence distributive varieties and included among this class are lattices,
and more generally, those algebras that have near-unanimity term operations. An
algebra will generate a congruence distributive variety if and only if it has a
sequence of ternary term operations, called Jonsson terms, that satisfy certain
equations.
  We prove that constraint languages consisting of relations that are invariant
under a short sequence of Jonsson terms are tractable by showing that such
languages have bounded relational width.



Response Prediction of Structural System Subject to Earthquake Motions using Artificial Neural Network

This paper uses Artificial Neural Network (ANN) models to compute response of
structural system subject to Indian earthquakes at Chamoli and Uttarkashi
ground motion data. The system is first trained for a single real earthquake
data. The trained ANN architecture is then used to simulate earthquakes with
various intensities and it was found that the predicted responses given by ANN
model are accurate for practical purposes. When the ANN is trained by a part of
the ground motion data, it can also identify the responses of the structural
system well. In this way the safeness of the structural systems may be
predicted in case of future earthquakes without waiting for the earthquake to
occur for the lessons. Time period and the corresponding maximum response of
the building for an earthquake has been evaluated, which is again trained to
predict the maximum response of the building at different time periods. The
trained time period versus maximum response ANN model is also tested for real
earthquake data of other place, which was not used in the training and was
found to be in good agreement.



Fault Classification using Pseudomodal Energies and Neuro-fuzzy modelling

This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.



Multi-Access MIMO Systems with Finite Rate Channel State Feedback

This paper characterizes the effect of finite rate channel state feedback on
the sum rate of a multi-access multiple-input multiple-output (MIMO) system. We
propose to control the users jointly, specifically, we first choose the users
jointly and then select the corresponding beamforming vectors jointly. To
quantify the sum rate, this paper introduces the composite Grassmann manifold
and the composite Grassmann matrix. By characterizing the distortion rate
function on the composite Grassmann manifold and calculating the logdet
function of a random composite Grassmann matrix, a good sum rate approximation
is derived. According to the distortion rate function on the composite
Grassmann manifold, the loss due to finite beamforming decreases exponentially
as the feedback bits on beamforming increases.



Quantization Bounds on Grassmann Manifolds of Arbitrary Dimensions and MIMO Communications with Feedback

This paper considers the quantization problem on the Grassmann manifold with
dimension n and p. The unique contribution is the derivation of a closed-form
formula for the volume of a metric ball in the Grassmann manifold when the
radius is sufficiently small. This volume formula holds for Grassmann manifolds
with arbitrary dimension n and p, while previous results are only valid for
either p=1 or a fixed p with asymptotically large n. Based on the volume
formula, the Gilbert-Varshamov and Hamming bounds for sphere packings are
obtained. Assuming a uniformly distributed source and a distortion metric based
on the squared chordal distance, tight lower and upper bounds are established
for the distortion rate tradeoff. Simulation results match the derived results.
As an application of the derived quantization bounds, the information rate of a
Multiple-Input Multiple-Output (MIMO) system with finite-rate channel-state
feedback is accurately quantified for arbitrary finite number of antennas,
while previous results are only valid for either Multiple-Input Single-Output
(MISO) systems or those with asymptotically large number of transmit antennas
but fixed number of receive antennas.



On the Information Rate of MIMO Systems with Finite Rate Channel State Feedback and Power On/Off Strategy

This paper quantifies the information rate of multiple-input multiple-output
(MIMO) systems with finite rate channel state feedback and power on/off
strategy. In power on/off strategy, a beamforming vector (beam) is either
turned on (denoted by on-beam) with a constant power or turned off. We prove
that the ratio of the optimal number of on-beams and the number of antennas
converges to a constant for a given signal-to-noise ratio (SNR) when the number
of transmit and receive antennas approaches infinity simultaneously and when
beamforming is perfect. Based on this result, a near optimal strategy, i.e.,
power on/off strategy with a constant number of on-beams, is discussed. For
such a strategy, we propose the power efficiency factor to quantify the effect
of imperfect beamforming. A formula is proposed to compute the maximum power
efficiency factor achievable given a feedback rate. The information rate of the
overall MIMO system can be approximated by combining the asymptotic results and
the formula for power efficiency factor. Simulations show that this
approximation is accurate for all SNR regimes.



How Many Users should be Turned On in a Multi-Antenna Broadcast Channel?

This paper considers broadcast channels with L antennas at the base station
and m single-antenna users, where each user has perfect channel knowledge and
the base station obtains channel information through a finite rate feedback.
The key observation of this paper is that the optimal number of on-users (users
turned on), say s, is a function of signal-to-noise ratio (SNR) and other
system parameters. Towards this observation, we use asymptotic analysis to
guide the design of feedback and transmission strategies. As L, m and the
feedback rates approach infinity linearly, we derive the asymptotic optimal
feedback strategy and a realistic criterion to decide which users should be
turned on. Define the corresponding asymptotic throughput per antenna as the
spatial efficiency. It is a function of the number of on-users s, and
therefore, s should be appropriately chosen. Based on the above asymptotic
results, we also develop a scheme for a system with finite many antennas and
users. Compared with other works where s is presumed constant, our scheme
achieves a significant gain by choosing the appropriate s. Furthermore, our
analysis and scheme is valid for heterogeneous systems where different users
may have different path loss coefficients and feedback rates.



Unequal dimensional small balls and quantization on Grassmann Manifolds

The Grassmann manifold G_{n,p}(L) is the set of all p-dimensional planes
(through the origin) in the n-dimensional Euclidean space L^{n}, where L is
either R or C. This paper considers an unequal dimensional quantization in
which a source in G_{n,p}(L) is quantized through a code in G_{n,q}(L), where p
and q are not necessarily the same. It is different from most works in
literature where p\equiv q. The analysis for unequal dimensional quantization
is based on the volume of a metric ball in G_{n,p}(L) whose center is in
G_{n,q}(L). Our chief result is a closed-form formula for the volume of a
metric ball when the radius is sufficiently small. This volume formula holds
for Grassmann manifolds with arbitrary n, p, q and L, while previous results
pertained only to some special cases. Based on this volume formula, several
bounds are derived for the rate distortion tradeoff assuming the quantization
rate is sufficiently high. The lower and upper bounds on the distortion rate
function are asymptotically identical, and so precisely quantify the asymptotic
rate distortion tradeoff. We also show that random codes are asymptotically
optimal in the sense that they achieve the minimum achievable distortion with
probability one as n and the code rate approach infinity linearly. Finally, we
discuss some applications of the derived results to communication theory. A
geometric interpretation in the Grassmann manifold is developed for capacity
calculation of additive white Gaussian noise channel. Further, the derived
distortion rate function is beneficial to characterizing the effect of
beamforming matrix selection in multi-antenna communications.



Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings

The work proposes the application of fuzzy set theory (FST) to diagnose the
condition of high voltage bushings. The diagnosis uses dissolved gas analysis
(DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil
impregnated paper (OIP) bushings. FST and neural networks are compared in terms
of accuracy and computational efficiency. Both FST and NN simulations were able
to diagnose the bushings condition with 10% error. By using fuzzy theory, the
maintenance department can classify bushings and know the extent of degradation
in the component.



A Study in a Hybrid Centralised-Swarm Agent Community

This paper describes a systems architecture for a hybrid Centralised/Swarm
based multi-agent system. The issue of local goal assignment for agents is
investigated through the use of a global agent which teaches the agents
responses to given situations. We implement a test problem in the form of a
Pursuit game, where the Multi-Agent system is a set of captor agents. The
agents learn solutions to certain board positions from the global agent if they
are unable to find a solution. The captor agents learn through the use of
multi-layer perceptron neural networks. The global agent is able to solve board
positions through the use of a Genetic Algorithm. The cooperation between
agents and the results of the simulation are discussed here. .



On-Line Condition Monitoring using Computational Intelligence

This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.



TrustMIX: Trustworthy MIX for Energy Saving in Sensor Networks

MIX has recently been proposed as a new sensor scheme with better energy
management for data-gathering in Wireless Sensor Networks. However, it is not
known how it performs when some of the sensors carry out sinkhole attacks. In
this paper, we propose a variant of MIX with adjunct computational trust
management to limit the impact of such sinkhole attacks. We evaluate how MIX
resists sinkhole attacks with and without computational trust management. The
main result of this paper is to find that MIX is very vulnerable to sinkhole
attacks but that the adjunct trust management efficiently reduces the impact of
such attacks while preserving the main feature of MIX: increased lifetime of
the network.



Statistical Mechanics of Nonlinear On-line Learning for Ensemble Teachers

We analyze the generalization performance of a student in a model composed of
nonlinear perceptrons: a true teacher, ensemble teachers, and the student. We
calculate the generalization error of the student analytically or numerically
using statistical mechanics in the framework of on-line learning. We treat two
well-known learning rules: Hebbian learning and perceptron learning. As a
result, it is proven that the nonlinear model shows qualitatively different
behaviors from the linear model. Moreover, it is clarified that Hebbian
learning and perceptron learning show qualitatively different behaviors from
each other. In Hebbian learning, we can analytically obtain the solutions. In
this case, the generalization error monotonically decreases. The steady value
of the generalization error is independent of the learning rate. The larger the
number of teachers is and the more variety the ensemble teachers have, the
smaller the generalization error is. In perceptron learning, we have to
numerically obtain the solutions. In this case, the dynamical behaviors of the
generalization error are non-monotonic. The smaller the learning rate is, the
larger the number of teachers is; and the more variety the ensemble teachers
have, the smaller the minimum value of the generalization error is.



The Use of ITIL for Process Optimisation in the IT Service Centre of Harz University, exemplified in the Release Management Process

This paper details the use of the IT Infrastructure Library Framework (ITIL)
for optimising process workflows in the IT Service Centre of Harz University in
Wernigerode, Germany, exemplified by the Release Management Process. It is
described, how, during the course of a special ITIL project, the As-Is-Status
of the various original processes was documented as part of the process life
cycle and then transformed in the To-Be-Status, according to the ITIL Best
Practice Framework. It is also shown, how the ITIL framework fits into the
four-layered-process model, that could be derived from interviews with the
universities IT support staff, and how the various modified processes
interconnect with each other to form a value chain. The paper highlights the
final results of the project and gives an outlook on the future use of ITIL as
a business modelling tool in the IT Service Centre of Harz University. It is
currently being considered, whether the process model developed during the
project could be used as a reference model for other university IT centres.



Reduced Complexity Sphere Decoding for Square QAM via a New Lattice Representation

Sphere decoding (SD) is a low complexity maximum likelihood (ML) detection
algorithm, which has been adapted for different linear channels in digital
communications. The complexity of the SD has been shown to be exponential in
some cases, and polynomial in others and under certain assumptions. The sphere
radius and the number of nodes visited throughout the tree traversal search are
the decisive factors for the complexity of the algorithm. The radius problem
has been addressed and treated widely in the literature. In this paper, we
propose a new structure for SD, which drastically reduces the overall
complexity. The complexity is measured in terms of the floating point
operations per second (FLOPS) and the number of nodes visited throughout the
algorithm tree search. This reduction in the complexity is due to the ability
of decoding the real and imaginary parts of each jointly detected symbol
independently of each other, making use of the new lattice representation. We
further show by simulations that the new approach achieves 80% reduction in the
overall complexity compared to the conventional SD for a 2x2 system, and almost
50% reduction for the 4x4 and 6x6 cases, thus relaxing the requirements for
hardware implementation.



Using Genetic Algorithms to Optimise Rough Set Partition Sizes for HIV Data Analysis

In this paper, we present a method to optimise rough set partition sizes, to
which rule extraction is performed on HIV data. The genetic algorithm
optimisation technique is used to determine the partition sizes of a rough set
in order to maximise the rough sets prediction accuracy. The proposed method is
tested on a set of demographic properties of individuals obtained from the
South African antenatal survey. Six demographic variables were used in the
analysis, these variables are; race, age of mother, education, gravidity,
parity, and age of father, with the outcome or decision being either HIV
positive or negative. Rough set theory is chosen based on the fact that it is
easy to interpret the extracted rules. The prediction accuracy of equal width
bin partitioning is 57.7% while the accuracy achieved after optimising the
partitions is 72.8%. Several other methods have been used to analyse the HIV
data and their results are stated and compared to that of rough set theory
(RST).



Improved Approximability Result for Test Set with Small Redundancy

Test set with redundancy is one of the focuses in recent bioinformatics
research. Set cover greedy algorithm (SGA for short) is a commonly used
algorithm for test set with redundancy. This paper proves that the
approximation ratio of SGA can be $(2-\frac{1}{2r})\ln n+{3/2}\ln r+O(\ln\ln
n)$ by using the potential function technique. This result is better than the
approximation ratio $2\ln n$ which directly derives from set multicover, when
$r=o(\frac{\ln n}{\ln\ln n})$, and is an extension of the approximability
results for plain test set.



Condition Monitoring of HV Bushings in the Presence of Missing Data Using Evolutionary Computing

The work proposes the application of neural networks with particle swarm
optimisation (PSO) and genetic algorithms (GA) to compensate for missing data
in classifying high voltage bushings. The classification is done using DGA data
from 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates
methods for oil impregnated paper (OIP) bushings. PSO and GA were compared in
terms of accuracy and computational efficiency. Both GA and PSO simulations
were able to estimate missing data values to an average 95% accuracy when only
one variable was missing. However PSO rapidly deteriorated to 66% accuracy with
two variables missing simultaneously, compared to 84% for GA. The data
estimated using GA was found to classify the conditions of bushings than the
PSO.



Informatics Carnot Machine

Based on Planck's blackbody equation it is argued that a single mode light
pulse, with a large number of photons, carries one entropy unit. Similarly, an
empty radiation mode carries no entropy. In this case, the calculated entropy
that a coded sequence of light pulses is carrying is simply the Gibbs mixing
entropy, which is identical to the logical Shannon information. This approach
is supported by a demonstration that information transmission and
amplification, by a sequence of light pulses in an optical fiber, is a classic
Carnot machine comprising of two isothermals and two adiabatic. Therefore it is
concluded that entropy under certain conditions is information.



Computational Intelligence for Condition Monitoring

Condition monitoring techniques are described in this chapter. Two aspects of
condition monitoring process are considered: (1) feature extraction; and (2)
condition classification. Feature extraction methods described and implemented
are fractals, Kurtosis and Mel-frequency Cepstral Coefficients. Classification
methods described and implemented are support vector machines (SVM), hidden
Markov models (HMM), Gaussian mixture models (GMM) and extension neural
networks (ENN). The effectiveness of these features were tested using SVM, HMM,
GMM and ENN on condition monitoring of bearings and are found to give good
results.



Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in hypre and PETSc

We describe our software package Block Locally Optimal Preconditioned
Eigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as
a stand-alone serial library, as an external package to PETSc (``Portable,
Extensible Toolkit for Scientific Computation'', a general purpose suite of
tools for the scalable solution of partial differential equations and related
problems developed by Argonne National Laboratory), and is also built into {\it
hypre} (``High Performance Preconditioners'', scalable linear solvers package
developed by Lawrence Livermore National Laboratory). The present BLOPEX
release includes only one solver--the Locally Optimal Block Preconditioned
Conjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\it
hypre} provides users with advanced high-quality parallel preconditioners for
linear systems, in particular, with domain decomposition and multigrid
preconditioners. With BLOPEX, the same preconditioners can now be efficiently
used for symmetric eigenvalue problems. PETSc facilitates the integration of
independently developed application modules with strict attention to component
interoperability, and makes BLOPEX extremely easy to compile and use with
preconditioners that are available via PETSc. We present the LOBPCG algorithm
in BLOPEX for {\it hypre} and PETSc. We demonstrate numerically the scalability
of BLOPEX by testing it on a number of distributed and shared memory parallel
systems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron
workstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition
and {\it hypre} multigrid preconditioning. We test BLOPEX on a model problem,
the standard 7-point finite-difference approximation of the 3-D Laplacian, with
the problem size in the range $10^5-10^8$.



On the monotonization of the training set

We consider the problem of minimal correction of the training set to make it
consistent with monotonic constraints. This problem arises during analysis of
data sets via techniques that require monotone data. We show that this problem
is NP-hard in general and is equivalent to finding a maximal independent set in
special orgraphs. Practically important cases of that problem considered in
detail. These are the cases when a partial order given on the replies set is a
total order or has a dimension 2. We show that the second case can be reduced
to maximization of a quadratic convex function on a convex set. For this case
we construct an approximate polynomial algorithm based on convex optimization.



Virtualization: A double-edged sword

Virtualization became recently a hot topic once again, after being dormant
for more than twenty years. In the meantime, it has been almost forgotten, that
virtual machines are not so perfect isolating environments as it seems, when
looking at the principles. These lessons were already learnt earlier when the
first virtualized systems have been exposed to real life usage.
  Contemporary virtualization software enables instant creation and destruction
of virtual machines on a host, live migration from one host to another,
execution history manipulation, etc. These features are very useful in
practice, but also causing headaches among security specialists, especially in
current hostile network environments.
  In the present contribution we discuss the principles, potential benefits and
risks of virtualization in a deja vu perspective, related to previous
experiences with virtualization in the mainframe era.



Worst-Case Background Knowledge for Privacy-Preserving Data Publishing

Recent work has shown the necessity of considering an attacker's background
knowledge when reasoning about privacy in data publishing. However, in
practice, the data publisher does not know what background knowledge the
attacker possesses. Thus, it is important to consider the worst-case. In this
paper, we initiate a formal study of worst-case background knowledge. We
propose a language that can express any background knowledge about the data. We
provide a polynomial time algorithm to measure the amount of disclosure of
sensitive information in the worst case, given that the attacker has at most a
specified number of pieces of information in this language. We also provide a
method to efficiently sanitize the data so that the amount of disclosure in the
worst case is less than a specified threshold.



The poset metrics that allow binary codes of codimension m to be m-, (m-1)-, or (m-2)-perfect

A binary poset code of codimension M (of cardinality 2^{N-M}, where N is the
code length) can correct maximum M errors. All possible poset metrics that
allow codes of codimension M to be M-, (M-1)- or (M-2)-perfect are described.
Some general conditions on a poset which guarantee the nonexistence of perfect
poset codes are derived; as examples, we prove the nonexistence of R-perfect
poset codes for some R in the case of the crown poset and in the case of the
union of disjoin chains. Index terms: perfect codes, poset codes



An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF

Admission control as a mechanism for providing QoS requires an accurate
description of the requested flow as well as already admitted flows. Since
802.11 WLAN capacity is shared between flows belonging to all stations,
admission control requires knowledge of all flows in the WLAN. Further,
estimation of the load-dependent WLAN capacity through analytical model
requires inputs about channel data rate, payload size and the number of
stations. These factors combined point to a centralized admission control
whereas for 802.11 DCF it is ideally performed in a distributed manner. The use
of measurements from the channel avoids explicit inputs about the state of the
channel described above. BUFFET, a model based measurement-assisted distributed
admission control scheme for DCF proposed in this paper relies on measurements
to derive model inputs and predict WLAN saturation, thereby maintaining average
delay within acceptable limits. Being measurement based, it adapts to a
combination of data rates and payload sizes, making it completely autonomous
and distributed. Performance analysis using OPNET simulations suggests that
BUFFET is able to ensure average delay under 7ms at a near-optimal throughput.



Voronoi Diagram of Polygonal Chains under the Discrete Fr\'echet Distance

Polygonal chains are fundamental objects in many applications like pattern
recognition and protein structure alignment. A well-known measure to
characterize the similarity of two polygonal chains is the famous Fr\`{e}chet
distance. In this paper, for the first time, we consider the Voronoi diagram of
polygonal chains in $d$-dimension ($d=2,3$) under the discrete Fr\`{e}chet
distance. Given $n$ polygonal chains ${\cal C}$ in $d$-dimension ($d=2,3$),
each with at most $k$ vertices, we prove fundamental properties of such a
Voronoi diagram {\em VD}$_F({\cal C})$ by presenting the first known upper and
lower bounds for {\em VD}$_F({\cal C})$.



Capacity of Sparse Multipath Channels in the Ultra-Wideband Regime

This paper studies the ergodic capacity of time- and frequency-selective
multipath fading channels in the ultrawideband (UWB) regime when training
signals are used for channel estimation at the receiver. Motivated by recent
measurement results on UWB channels, we propose a model for sparse multipath
channels. A key implication of sparsity is that the independent degrees of
freedom (DoF) in the channel scale sub-linearly with the signal space dimension
(product of signaling duration and bandwidth). Sparsity is captured by the
number of resolvable paths in delay and Doppler. Our analysis is based on a
training and communication scheme that employs signaling over orthogonal
short-time Fourier (STF) basis functions. STF signaling naturally relates
sparsity in delay-Doppler to coherence in time-frequency. We study the impact
of multipath sparsity on two fundamental metrics of spectral efficiency in the
wideband/low-SNR limit introduced by Verdu: first- and second-order optimality
conditions. Recent results by Zheng et. al. have underscored the large gap in
spectral efficiency between coherent and non-coherent extremes and the
importance of channel learning in bridging the gap. Building on these results,
our results lead to the following implications of multipath sparsity: 1) The
coherence requirements are shared in both time and frequency, thereby
significantly relaxing the required scaling in coherence time with SNR; 2)
Sparse multipath channels are asymptotically coherent -- for a given but large
bandwidth, the channel can be learned perfectly and the coherence requirements
for first- and second-order optimality met through sufficiently large signaling
duration; and 3) The requirement of peaky signals in attaining capacity is
eliminated or relaxed in sparse environments.



Non-Coherent Capacity and Reliability of Sparse Multipath Channels in the Wideband Regime

In contrast to the prevalent assumption of rich multipath in information
theoretic analysis of wireless channels, physical channels exhibit sparse
multipath, especially at large bandwidths. We propose a model for sparse
multipath fading channels and present results on the impact of sparsity on
non-coherent capacity and reliability in the wideband regime. A key implication
of sparsity is that the statistically independent degrees of freedom in the
channel, that represent the delay-Doppler diversity afforded by multipath,
scale at a sub-linear rate with the signal space dimension (time-bandwidth
product). Our analysis is based on a training-based communication scheme that
uses short-time Fourier (STF) signaling waveforms. Sparsity in delay-Doppler
manifests itself as time-frequency coherence in the STF domain. From a capacity
perspective, sparse channels are asymptotically coherent: the gap between
coherent and non-coherent extremes vanishes in the limit of large signal space
dimension without the need for peaky signaling. From a reliability viewpoint,
there is a fundamental tradeoff between channel diversity and learnability that
can be optimized to maximize the error exponent at any rate by appropriately
choosing the signaling duration as a function of bandwidth.



Scanning and Sequential Decision Making for Multi-Dimensional Data - Part II: the Noisy Case

We consider the problem of sequential decision making on random fields
corrupted by noise. In this scenario, the decision maker observes a noisy
version of the data, yet judged with respect to the clean data. In particular,
we first consider the problem of sequentially scanning and filtering noisy
random fields. In this case, the sequential filter is given the freedom to
choose the path over which it traverses the random field (e.g., noisy image or
video sequence), thus it is natural to ask what is the best achievable
performance and how sensitive this performance is to the choice of the scan. We
formally define the problem of scanning and filtering, derive a bound on the
best achievable performance and quantify the excess loss occurring when
non-optimal scanners are used, compared to optimal scanning and filtering.
  We then discuss the problem of sequential scanning and prediction of noisy
random fields. This setting is a natural model for applications such as
restoration and coding of noisy images. We formally define the problem of
scanning and prediction of a noisy multidimensional array and relate the
optimal performance to the clean scandictability defined by Merhav and
Weissman. Moreover, bounds on the excess loss due to sub-optimal scans are
derived, and a universal prediction algorithm is suggested.
  This paper is the second part of a two-part paper. The first paper dealt with
sequential decision making on noiseless data arrays, namely, when the decision
maker is judged with respect to the same data array it observes.



Cryptanalysis of group-based key agreement protocols using subgroup distance functions

We introduce a new approach for cryptanalysis of key agreement protocols
based on noncommutative groups. This approach uses functions that estimate the
distance of a group element to a given subgroup. We test it against the
Shpilrain-Ushakov protocol, which is based on Thompson's group F.



An online algorithm for generating fractal hash chains applied to digital chains of custody

This paper gives an online algorithm for generating Jakobsson's fractal hash
chains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm
for preimage traversal since his algorithm assumes the entire hash chain is
precomputed and a particular list of Ceiling(log n) hash elements or pebbles
are saved. Our online algorithm for hash chain traversal incrementally
generates a hash chain of n hash elements without knowledge of n before it
starts. For any n, our algorithm stores only the Ceiling(log n) pebbles which
are precisely the inputs for Jakobsson's amortized hash chain preimage
traversal algorithm. This compact representation is useful to generate,
traverse, and store a number of large digital hash chains on a small and
constrained device. We also give an application using both Jakobsson's and our
new algorithm applied to digital chains of custody for validating dynamically
changing forensics data.



A stochastic non-cooperative game for energy efficiency in wireless data networks

In this paper the issue of energy efficiency in CDMA wireless data networks
is addressed through a game theoretic approach. Building on a recent paper by
the first two authors, wherein a non-cooperative game for spreading-code
optimization, power control, and receiver design has been proposed to maximize
the ratio of data throughput to transmit power for each active user, a
stochastic algorithm is here described to perform adaptive implementation of
the said non-cooperative game. The proposed solution is based on a combination
of RLS-type and LMS-type adaptations, and makes use of readily available
measurements. Simulation results show that its performance approaches with
satisfactory accuracy that of the non-adaptive game, which requires a much
larger amount of prior information.



An Extensible Timing Infrastructure for Adaptive Large-scale Applications

Real-time access to accurate and reliable timing information is necessary to
profile scientific applications, and crucial as simulations become increasingly
complex, adaptive, and large-scale. The Cactus Framework provides flexible and
extensible capabilities for timing information through a well designed
infrastructure and timing API. Applications built with Cactus automatically
gain access to built-in timers, such as gettimeofday and getrusage,
system-specific hardware clocks, and high-level interfaces such as PAPI. We
describe the Cactus timer interface, its motivation, and its implementation. We
then demonstrate how this timing information can be used by an example
scientific application to profile itself, and to dynamically adapt itself to a
changing environment at run time.



Spectral Efficiency of Spectrum Pooling Systems

In this contribution, we investigate the idea of using cognitive radio to
reuse locally unused spectrum to increase the total system capacity. We
consider a multiband/wideband system in which the primary and cognitive users
wish to communicate to different receivers, subject to mutual interference and
assume that each user knows only his channel and the unused spectrum through
adequate sensing. The basic idea under the proposed scheme is based on the
notion of spectrum pooling. The idea is quite simple: a cognitive radio will
listen to the channel and, if sensed idle, will transmit during the voids. It
turns out that, although its simplicity, the proposed scheme showed very
interesting features with respect to the spectral efficiency and the maximum
number of possible pairwise cognitive communications. We impose the constraint
that users successively transmit over available bands through selfish water
filling. For the first time, our study has quantified the asymptotic (with
respect to the band) achievable gain of using spectrum pooling in terms of
spectral efficiency compared to classical radio systems. We then derive the
total spectral efficiency as well as the maximum number of possible pairwise
communications of such a spectrum pooling system.



A competitive multi-agent model of interbank payment systems

We develop a dynamic multi-agent model of an interbank payment system where
banks choose their level of available funds on the basis of private payoff
maximisation. The model consists of the repetition of a simultaneous move stage
game with incomplete information, incomplete monitoring, and stochastic
payoffs. Adaptation takes place with bayesian updating, with banks maximizing
immediate payoffs. We carry out numerical simulations to solve the model and
investigate two special scenarios: an operational incident and exogenous
throughput guidelines for payment submission. We find that the demand for
intraday credit is an S-shaped function of the cost ratio between intraday
credit costs and the costs associated with delaying payments. We also find that
the demand for liquidity is increased both under operational incidents and in
the presence of effective throughput guidelines.



On the Shannon capacity and queueing stability of random access multicast

We study and compare the Shannon capacity region and the stable throughput
region for a random access system in which source nodes multicast their
messages to multiple destination nodes. Under an erasure channel model which
accounts for interference and allows for multipacket reception, we first
characterize the Shannon capacity region. We then consider a queueing-theoretic
formulation and characterize the stable throughput region for two different
transmission policies: a retransmission policy and random linear coding. Our
results indicate that for large blocklengths, the random linear coding policy
provides a higher stable throughput than the retransmission policy.
Furthermore, our results provide an example of a transmission policy for which
the Shannon capacity region strictly outer bounds the stable throughput region,
which contradicts an unproven conjecture that the Shannon capacity and stable
throughput coincide for random access systems.



Measuring and Localing Homology Classes

We develop a method for measuring and localizing homology classes. This
involves two problems. First, we define relevant notions of size for both a
homology class and a homology group basis, using ideas from relative homology.
Second, we propose an algorithm to compute the optimal homology basis, using
techniques from persistent homology and finite field algebra. Classes of the
computed optimal basis are localized with cycles conveying their sizes. The
algorithm runs in $O(\beta^4 n^3 \log^2 n)$ time, where $n$ is the size of the
simplicial complex and $\beta$ is the Betti number of the homology group.



Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement

A transmitter without channel state information (CSI) wishes to send a
delay-limited Gaussian source over a slowly fading channel. The source is coded
in superimposed layers, with each layer successively refining the description
in the previous one. The receiver decodes the layers that are supported by the
channel realization and reconstructs the source up to a distortion. The
expected distortion is minimized by optimally allocating the transmit power
among the source layers. For two source layers, the allocation is optimal when
power is first assigned to the higher layer up to a power ceiling that depends
only on the channel fading distribution; all remaining power, if any, is
allocated to the lower layer. For convex distortion cost functions with convex
constraints, the minimization is formulated as a convex optimization problem.
In the limit of a continuum of infinite layers, the minimum expected distortion
is given by the solution to a set of linear differential equations in terms of
the density of the fading distribution. As the bandwidth ratio b (channel uses
per source symbol) tends to zero, the power distribution that minimizes
expected distortion converges to the one that maximizes expected capacity.
While expected distortion can be improved by acquiring CSI at the transmitter
(CSIT) or by increasing diversity from the realization of independent fading
paths, at high SNR the performance benefit from diversity exceeds that from
CSIT, especially when b is large.



Computability of simple games: A characterization and application to the core

The class of algorithmically computable simple games (i) includes the class
of games that have finite carriers and (ii) is included in the class of games
that have finite winning coalitions. This paper characterizes computable games,
strengthens the earlier result that computable games violate anonymity, and
gives examples showing that the above inclusions are strict. It also extends
Nakamura's theorem about the nonemptyness of the core and shows that computable
games have a finite Nakamura number, implying that the number of alternatives
that the players can deal with rationally is restricted.



Recovering Multiplexing Loss Through Successive Relaying Using Repetition Coding

In this paper, a transmission protocol is studied for a two relay wireless
network in which simple repetition coding is applied at the relays.
Information-theoretic achievable rates for this transmission scheme are given,
and a space-time V-BLAST signalling and detection method that can approach them
is developed. It is shown through the diversity multiplexing tradeoff analysis
that this transmission scheme can recover the multiplexing loss of the
half-duplex relay network, while retaining some diversity gain. This scheme is
also compared with conventional transmission protocols that exploit only the
diversity of the network at the cost of a multiplexing loss. It is shown that
the new transmission protocol offers significant performance advantages over
conventional protocols, especially when the interference between the two relays
is sufficiently strong.



Acyclicity of Preferences, Nash Equilibria, and Subgame Perfect Equilibria: a Formal and Constructive Equivalence

In 1953, Kuhn showed that every sequential game has a Nash equilibrium by
showing that a procedure, named ``backward induction'' in game theory, yields a
Nash equilibrium. It actually yields Nash equilibria that define a proper
subclass of Nash equilibria. In 1965, Selten named this proper subclass subgame
perfect equilibria. In game theory, payoffs are rewards usually granted at the
end of a game. Although traditional game theory mainly focuses on real-valued
payoffs that are implicitly ordered by the usual total order over the reals,
works of Simon or Blackwell already involved partially ordered payoffs. This
paper generalises the notion of sequential game by replacing real-valued payoff
functions with abstract atomic objects, called outcomes, and by replacing the
usual total order over the reals with arbitrary binary relations over outcomes,
called preferences. This introduces a general abstract formalism where Nash
equilibrium, subgame perfect equilibrium, and ``backward induction'' can still
be defined. This paper proves that the following three propositions are
equivalent: 1) Preferences over the outcomes are acyclic. 2) Every sequential
game has a Nash equilibrium. 3) Every sequential game has a subgame perfect
equilibrium. The result is fully computer-certified using Coq. Beside the
additional guarantee of correctness, the activity of formalisation using Coq
also helps clearly identify the useful definitions and the main articulations
of the proof.



Optimal Separable Algorithms to Compute the Reverse Euclidean Distance Transformation and Discrete Medial Axis in Arbitrary Dimension

In binary images, the distance transformation (DT) and the geometrical
skeleton extraction are classic tools for shape analysis. In this paper, we
present time optimal algorithms to solve the reverse Euclidean distance
transformation and the reversible medial axis extraction problems for
$d$-dimensional images. We also present a $d$-dimensional medial axis filtering
process that allows us to control the quality of the reconstructed shape.



Multiuser detection in a dynamic environment Part I: User identification and data detection

In random-access communication systems, the number of active users varies
with time, and has considerable bearing on receiver's performance. Thus,
techniques aimed at identifying not only the information transmitted, but also
that number, play a central role in those systems. An example of application of
these techniques can be found in multiuser detection (MUD). In typical MUD
analyses, receivers are based on the assumption that the number of active users
is constant and known at the receiver, and coincides with the maximum number of
users entitled to access the system. This assumption is often overly
pessimistic, since many users might be inactive at any given time, and
detection under the assumption of a number of users larger than the real one
may impair performance.
  The main goal of this paper is to introduce a general approach to the problem
of identifying active users and estimating their parameters and data in a
random-access system where users are continuously entering and leaving the
system. The tool whose use we advocate is Random-Set Theory: applying this, we
derive optimum receivers in an environment where the set of transmitters
comprises an unknown number of elements. In addition, we can derive
Bayesian-filter equations which describe the evolution with time of the a
posteriori probability density of the unknown user parameters, and use this
density to derive optimum detectors. In this paper we restrict ourselves to
interferer identification and data detection, while in a companion paper we
shall examine the more complex problem of estimating users' parameters.



The Road to Quantum Artificial Intelligence

This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available.



Open Access Publishing in Particle Physics: A Brief Introduction for the non-Expert

Open Access to particle physics literature does not sound particularly new or
exciting, since particle physicists have been reading preprints for decades,
and arXiv.org for 15 years. However new movements in Europe are attempting to
make the peer-reviewed literature of the field fully Open Access. This is not a
new movement, nor is it restricted to this field. However, given the field's
history of preprints and eprints, it is well suited to a change to a fully Open
Access publishing model. Data shows that 90% of HEP published literature is
freely available online, meaning that HEP libraries have little need for
expensive journal subscriptions. As libraries begin to cancel journal
subscriptions, the peer review process will lose its primary source of funding.
Open Access publishing models can potentially address this issue. European
physicists and funding agencies are proposing a consortium, SCOAP3, that might
solve many of the objections to traditional Open Access publishing models in
Particle Physics. These proposed changes should be viewed as a starting point
for a serious look at the field's publication model, and are at least worthy of
attention, if not adoption.



Linear Tabling Strategies and Optimizations

Recently, the iterative approach named linear tabling has received
considerable attention because of its simplicity, ease of implementation, and
good space efficiency. Linear tabling is a framework from which different
methods can be derived based on the strategies used in handling looping
subgoals. One decision concerns when answers are consumed and returned. This
paper describes two strategies, namely, {\it lazy} and {\it eager} strategies,
and compares them both qualitatively and quantitatively. The results indicate
that, while the lazy strategy has good locality and is well suited for finding
all solutions, the eager strategy is comparable in speed with the lazy strategy
and is well suited for programs with cuts. Linear tabling relies on depth-first
iterative deepening rather than suspension to compute fixpoints. Each cluster
of inter-dependent subgoals as represented by a top-most looping subgoal is
iteratively evaluated until no subgoal in it can produce any new answers. Naive
re-evaluation of all looping subgoals, albeit simple, may be computationally
unacceptable. In this paper, we also introduce semi-naive optimization, an
effective technique employed in bottom-up evaluation of logic programs to avoid
redundant joins of answers, into linear tabling. We give the conditions for the
technique to be safe (i.e. sound and complete) and propose an optimization
technique called {\it early answer promotion} to enhance its effectiveness.
Benchmarking in B-Prolog demonstrates that with this optimization linear
tabling compares favorably well in speed with the state-of-the-art
implementation of SLG.



Linearly bounded infinite graphs

Linearly bounded Turing machines have been mainly studied as acceptors for
context-sensitive languages. We define a natural class of infinite automata
representing their observable computational behavior, called linearly bounded
graphs. These automata naturally accept the same languages as the linearly
bounded machines defining them. We present some of their structural properties
as well as alternative characterizations in terms of rewriting systems and
context-sensitive transductions. Finally, we compare these graphs to rational
graphs, which are another class of automata accepting the context-sensitive
languages, and prove that in the bounded-degree case, rational graphs are a
strict sub-class of linearly bounded graphs.



Making Random Choices Invisible to the Scheduler

When dealing with process calculi and automata which express both
nondeterministic and probabilistic behavior, it is customary to introduce the
notion of scheduler to solve the nondeterminism. It has been observed that for
certain applications, notably those in security, the scheduler needs to be
restricted so not to reveal the outcome of the protocol's random choices, or
otherwise the model of adversary would be too strong even for ``obviously
correct'' protocols. We propose a process-algebraic framework in which the
control on the scheduler can be specified in syntactic terms, and we show how
to apply it to solve the problem mentioned above. We also consider the
definition of (probabilistic) may and must preorders, and we show that they are
precongruences with respect to the restricted schedulers. Furthermore, we show
that all the operators of the language, except replication, distribute over
probabilistic summation, which is a useful property for verification.



Multidimensional Coded Modulation in Block-Fading Channnels

We study the problem of constructing coded modulation schemes over
multidimensional signal sets in Nakagami-$m$ block-fading channels. In
particular, we consider the optimal diversity reliability exponent of the error
probability when the multidimensional constellation is obtained as the rotation
of classical complex-plane signal constellations. We show that multidimensional
rotations of full dimension achieve the optimal diversity reliability exponent,
also achieved by Gaussian constellations. Multidimensional rotations of full
dimension induce a large decoding complexity, and in some cases it might be
beneficial to use multiple rotations of smaller dimension. We also study the
diversity reliability exponent in this case, which yields the optimal
rate-diversity-complexity tradeoff in block-fading channels with discrete
inputs.



Generalizing Consistency and other Constraint Properties to Quantified Constraints

Quantified constraints and Quantified Boolean Formulae are typically much
more difficult to reason with than classical constraints, because quantifier
alternation makes the usual notion of solution inappropriate. As a consequence,
basic properties of Constraint Satisfaction Problems (CSP), such as consistency
or substitutability, are not completely understood in the quantified case.
These properties are important because they are the basis of most of the
reasoning methods used to solve classical (existentially quantified)
constraints, and one would like to benefit from similar reasoning methods in
the resolution of quantified constraints. In this paper, we show that most of
the properties that are used by solvers for CSP can be generalized to
quantified CSP. This requires a re-thinking of a number of basic concepts; in
particular, we propose a notion of outcome that generalizes the classical
notion of solution and on which all definitions are based. We propose a
systematic study of the relations which hold between these properties, as well
as complexity results regarding the decision of these properties. Finally, and
since these problems are typically intractable, we generalize the approach used
in CSP and propose weaker, easier to check notions based on locality, which
allow to detect these properties incompletely but in polynomial time.



MI image registration using prior knowledge

Subtraction of aligned images is a means to assess changes in a wide variety
of clinical applications. In this paper we explore the information theoretical
origin of Mutual Information (MI), which is based on Shannon's entropy.However,
the interpretation of standard MI registration as a communication channel
suggests that MI is too restrictive a criterion. In this paper the concept of
Mutual Information (MI) is extended to (Normalized) Focussed Mutual Information
(FMI) to incorporate prior knowledge to overcome some shortcomings of MI. We
use this to develop new methodologies to successfully address specific
registration problems, the follow-up of dental restorations, cephalometry, and
the monitoring of implants.



A Logic of Reachable Patterns in Linked Data-Structures

We define a new decidable logic for expressing and checking invariants of
programs that manipulate dynamically-allocated objects via pointers and
destructive pointer updates. The main feature of this logic is the ability to
limit the neighborhood of a node that is reachable via a regular expression
from a designated node. The logic is closed under boolean operations
(entailment, negation) and has a finite model property. The key technical
result is the proof of decidability. We show how to express precondition,
postconditions, and loop invariants for some interesting programs. It is also
possible to express properties such as disjointness of data-structures, and
low-level heap mutations. Moreover, our logic can express properties of
arbitrary data-structures and of an arbitrary number of pointer fields. The
latter provides a way to naturally specify postconditions that relate the
fields on entry to a procedure to the fields on exit. Therefore, it is possible
to use the logic to automatically prove partial correctness of programs
performing low-level heap mutations.



On How Developers Test Open Source Software Systems

Engineering software systems is a multidisciplinary activity, whereby a
number of artifacts must be created - and maintained - synchronously. In this
paper we investigate whether production code and the accompanying tests
co-evolve by exploring a project's versioning system, code coverage reports and
size-metrics. Three open source case studies teach us that testing activities
usually start later on during the lifetime and are more "phased", although we
did not observe increasing testing activity before releases. Furthermore, we
note large differences in the levels of test coverage given the proportion of
test code.



Triple-loop networks with arbitrarily many minimum distance diagrams

Minimum distance diagrams are a way to encode the diameter and routing
information of multi-loop networks. For the widely studied case of double-loop
networks, it is known that each network has at most two such diagrams and that
they have a very definite form "L-shape''.
  In contrast, in this paper we show that there are triple-loop networks with
an arbitrarily big number of associated minimum distance diagrams. For doing
this, we build-up on the relations between minimum distance diagrams and
monomial ideals.



Subjective Information Measure and Rate Fidelity Theory

Using fish-covering model, this paper intuitively explains how to extend
Hartley's information formula to the generalized information formula step by
step for measuring subjective information: metrical information (such as
conveyed by thermometers), sensory information (such as conveyed by color
vision), and semantic information (such as conveyed by weather forecasts). The
pivotal step is to differentiate condition probability and logical condition
probability of a message. The paper illustrates the rationality of the formula,
discusses the coherence of the generalized information formula and Popper's
knowledge evolution theory. For optimizing data compression, the paper
discusses rate-of-limiting-errors and its similarity to complexity-distortion
based on Kolmogorov's complexity theory, and improves the rate-distortion
theory into the rate-fidelity theory by replacing Shannon's distortion with
subjective mutual information. It is proved that both the rate-distortion
function and the rate-fidelity function are equivalent to a
rate-of-limiting-errors function with a group of fuzzy sets as limiting
condition, and can be expressed by a formula of generalized mutual information
for lossy coding, or by a formula of generalized entropy for lossless coding.
By analyzing the rate-fidelity function related to visual discrimination and
digitized bits of pixels of images, the paper concludes that subjective
information is less than or equal to objective (Shannon's) information; there
is an optimal matching point at which two kinds of information are equal; the
matching information increases with visual discrimination (defined by confusing
probability) rising; for given visual discrimination, too high resolution of
images or too much objective information is wasteful.



Structural Health Monitoring Using Neural Network Based Vibrational System Identification

Composite fabrication technologies now provide the means for producing
high-strength, low-weight panels, plates, spars and other structural components
which use embedded fiber optic sensors and piezoelectric transducers. These
materials, often referred to as smart structures, make it possible to sense
internal characteristics, such as delaminations or structural degradation. In
this effort we use neural network based techniques for modeling and analyzing
dynamic structural information for recognizing structural defects. This yields
an adaptable system which gives a measure of structural integrity for composite
structures.



Distributed Transmit Diversity in Relay Networks

We analyze fading relay networks, where a single-antenna source-destination
terminal pair communicates through a set of half-duplex single-antenna relays
using a two-hop protocol with linear processing at the relay level. A family of
relaying schemes is presented which achieves the entire optimal
diversity-multiplexing (DM) tradeoff curve. As a byproduct of our analysis, it
follows that delay diversity and phase-rolling at the relay level are optimal
with respect to the entire DM-tradeoff curve, provided the delays and the
modulation frequencies, respectively, are chosen appropriately.



Power-Efficient Direct-Voting Assurance for Data Fusion in Wireless Sensor Networks

Wireless sensor networks place sensors into an area to collect data and send
them back to a base station. Data fusion, which fuses the collected data before
they are sent to the base station, is usually implemented over the network.
Since the sensor is typically placed in locations accessible to malicious
attackers, information assurance of the data fusion process is very important.
A witness-based approach has been proposed to validate the fusion data. In this
approach, the base station receives the fusion data and "votes" on the data
from a randomly chosen sensor node. The vote comes from other sensor nodes,
called "witnesses," to verify the correctness of the fusion data. Because the
base station obtains the vote through the chosen node, the chosen node could
forge the vote if it is compromised. Thus, the witness node must encrypt the
vote to prevent this forgery. Compared with the vote, the encryption requires
more bits, increasing transmission burden from the chosen node to the base
station. The chosen node consumes more power. This work improves the
witness-based approach using direct voting mechanism such that the proposed
scheme has better performance in terms of assurance, overhead, and delay. The
witness node transmits the vote directly to the base station. Forgery is not a
problem in this scheme. Moreover, fewer bits are necessary to represent the
vote, significantly reducing the power consumption. Performance analysis and
simulation results indicate that the proposed approach can achieve a 40 times
better overhead than the witness-based approach.



Morphing Ensemble Kalman Filters

A new type of ensemble filter is proposed, which combines an ensemble Kalman
filter (EnKF) with the ideas of morphing and registration from image
processing. This results in filters suitable for nonlinear problems whose
solutions exhibit moving coherent features, such as thin interfaces in wildfire
modeling. The ensemble members are represented as the composition of one common
state with a spatial transformation, called registration mapping, plus a
residual. A fully automatic registration method is used that requires only
gridded data, so the features in the model state do not need to be identified
by the user. The morphing EnKF operates on a transformed state consisting of
the registration mapping and the residual. Essentially, the morphing EnKF uses
intermediate states obtained by morphing instead of linear combinations of the
states.



Optimal Iris Fuzzy Sketches

Fuzzy sketches, introduced as a link between biometry and cryptography, are a
way of handling biometric data matching as an error correction issue. We focus
here on iris biometrics and look for the best error-correcting code in that
respect. We show that two-dimensional iterative min-sum decoding leads to
results near the theoretical limits. In particular, we experiment our
techniques on the Iris Challenge Evaluation (ICE) database and validate our
findings.



On the Obfuscation Complexity of Planar Graphs

Being motivated by John Tantalo's Planarity Game, we consider straight line
plane drawings of a planar graph $G$ with edge crossings and wonder how
obfuscated such drawings can be. We define $obf(G)$, the obfuscation complexity
of $G$, to be the maximum number of edge crossings in a drawing of $G$.
Relating $obf(G)$ to the distribution of vertex degrees in $G$, we show an
efficient way of constructing a drawing of $G$ with at least $obf(G)/3$ edge
crossings. We prove bounds $(\delta(G)^2/24-o(1))n^2 < \obf G <3 n^2$ for an
$n$-vertex planar graph $G$ with minimum vertex degree $\delta(G)\ge 2$.
  The shift complexity of $G$, denoted by $shift(G)$, is the minimum number of
vertex shifts sufficient to eliminate all edge crossings in an arbitrarily
obfuscated drawing of $G$ (after shifting a vertex, all incident edges are
supposed to be redrawn correspondingly). If $\delta(G)\ge 3$, then $shift(G)$
is linear in the number of vertices due to the known fact that the matching
number of $G$ is linear. However, in the case $\delta(G)\ge2$ we notice that
$shift(G)$ can be linear even if the matching number is bounded. As for
computational complexity, we show that, given a drawing $D$ of a planar graph,
it is NP-hard to find an optimum sequence of shifts making $D$ crossing-free.



On the expressive power of planar perfect matching and permanents of bounded treewidth matrices

Valiant introduced some 25 years ago an algebraic model of computation along
with the complexity classes VP and VNP, which can be viewed as analogues of the
classical classes P and NP. They are defined using non-uniform sequences of
arithmetic circuits and provides a framework to study the complexity for
sequences of polynomials. Prominent examples of difficult (that is,
VNP-complete) problems in this model includes the permanent and hamiltonian
polynomials. While the permanent and hamiltonian polynomials in general are
difficult to evaluate, there have been research on which special cases of these
polynomials admits efficient evaluation. For instance, Barvinok has shown that
if the underlying matrix has bounded rank, both the permanent and the
hamiltonian polynomials can be evaluated in polynomial time, and thus are in
VP. Courcelle, Makowsky and Rotics have shown that for matrices of bounded
treewidth several difficult problems (including evaluating the permanent and
hamiltonian polynomials) can be solved efficiently. An earlier result of this
flavour is Kasteleyn's theorem which states that the sum of weights of perfect
matchings of a planar graph can be computed in polynomial time, and thus is in
VP also. For general graphs this problem is VNP-complete. In this paper we
investigate the expressive power of the above results. We show that the
permanent and hamiltonian polynomials for matrices of bounded treewidth both
are equivalent to arithmetic formulas. Also, arithmetic weakly skew circuits
are shown to be equivalent to the sum of weights of perfect matchings of planar
graphs.



On complexity of optimized crossover for binary representations

We consider the computational complexity of producing the best possible
offspring in a crossover, given two solutions of the parents. The crossover
operators are studied on the class of Boolean linear programming problems,
where the Boolean vector of variables is used as the solution representation.
By means of efficient reductions of the optimized gene transmitting crossover
problems (OGTC) we show the polynomial solvability of the OGTC for the maximum
weight set packing problem, the minimum weight set partition problem and for
one of the versions of the simple plant location problem. We study a connection
between the OGTC for linear Boolean programming problem and the maximum weight
independent set problem on 2-colorable hypergraph and prove the NP-hardness of
several special cases of the OGTC problem in Boolean linear programming.



Maximizing Maximal Angles for Plane Straight-Line Graphs

Let $G=(S, E)$ be a plane straight-line graph on a finite point set
$S\subset\R^2$ in general position. The incident angles of a vertex $p \in S$
of $G$ are the angles between any two edges of $G$ that appear consecutively in
the circular order of the edges incident to $p$.
  A plane straight-line graph is called $\phi$-open if each vertex has an
incident angle of size at least $\phi$. In this paper we study the following
type of question: What is the maximum angle $\phi$ such that for any finite set
$S\subset\R^2$ of points in general position we can find a graph from a certain
class of graphs on $S$ that is $\phi$-open? In particular, we consider the
classes of triangulations, spanning trees, and paths on $S$ and give tight
bounds in most cases.



Symbolic Reachability Analysis of Higher-Order Context-Free Processes

We consider the problem of symbolic reachability analysis of higher-order
context-free processes. These models are generalizations of the context-free
processes (also called BPA processes) where each process manipulates a data
structure which can be seen as a nested stack of stacks. Our main result is
that, for any higher-order context-free process, the set of all predecessors of
a given regular set of configurations is regular and effectively constructible.
This result generalizes the analogous result which is known for level 1
context-free processes. We show that this result holds also in the case of
backward reachability analysis under a regular constraint on configurations. As
a corollary, we obtain a symbolic model checking algorithm for the temporal
logic E(U,X) with regular atomic predicates, i.e., the fragment of CTL
restricted to the EU and EX modalities.



Towards Understanding the Origin of Genetic Languages

Molecular biology is a nanotechnology that works--it has worked for billions
of years and in an amazing variety of circumstances. At its core is a system
for acquiring, processing and communicating information that is universal, from
viruses and bacteria to human beings. Advances in genetics and experience in
designing computers have taken us to a stage where we can understand the
optimisation principles at the root of this system, from the availability of
basic building blocks to the execution of tasks. The languages of DNA and
proteins are argued to be the optimal solutions to the information processing
tasks they carry out. The analysis also suggests simpler predecessors to these
languages, and provides fascinating clues about their origin. Obviously, a
comprehensive unraveling of the puzzle of life would have a lot to say about
what we may design or convert ourselves into.



Translating a first-order modal language to relational algebra

This paper is about Kripke structures that are inside a relational database
and queried with a modal language. At first the modal language that is used is
introduced, followed by a definition of the database and relational algebra.
Based on these definitions two things are presented: a mapping from components
of the modal structure to a relational database schema and instance, and a
translation from queries in the modal language to relational algebra queries.



Interior Point Decoding for Linear Vector Channels

In this paper, a novel decoding algorithm for low-density parity-check (LDPC)
codes based on convex optimization is presented. The decoding algorithm, called
interior point decoding, is designed for linear vector channels. The linear
vector channels include many practically important channels such as inter
symbol interference channels and partial response channels. It is shown that
the maximum likelihood decoding (MLD) rule for a linear vector channel can be
relaxed to a convex optimization problem, which is called a relaxed MLD
problem. The proposed decoding algorithm is based on a numerical optimization
technique so called interior point method with barrier function. Approximate
variations of the gradient descent and the Newton methods are used to solve the
convex optimization problem. In a decoding process of the proposed algorithm, a
search point always lies in the fundamental polytope defined based on a
low-density parity-check matrix. Compared with a convectional joint message
passing decoder, the proposed decoding algorithm achieves better BER
performance with less complexity in the case of partial response channels in
many cases.



Average Stopping Set Weight Distribution of Redundant Random Matrix Ensembles

In this paper, redundant random matrix ensembles (abbreviated as redundant
random ensembles) are defined and their stopping set (SS) weight distributions
are analyzed. A redundant random ensemble consists of a set of binary matrices
with linearly dependent rows. These linearly dependent rows (redundant rows)
significantly reduce the number of stopping sets of small size. An upper and
lower bound on the average SS weight distribution of the redundant random
ensembles are shown. From these bounds, the trade-off between the number of
redundant rows (corresponding to decoding complexity of BP on BEC) and the
critical exponent of the asymptotic growth rate of SS weight distribution
(corresponding to decoding performance) can be derived. It is shown that, in
some cases, a dense matrix with linearly dependent rows yields asymptotically
(i.e., in the regime of small erasure probability) better performance than
regular LDPC matrices with comparable parameters.



On Undetected Error Probability of Binary Matrix Ensembles

In this paper, an analysis of the undetected error probability of ensembles
of binary matrices is presented. The ensemble called the Bernoulli ensemble
whose members are considered as matrices generated from i.i.d. Bernoulli source
is mainly considered here. The main contributions of this work are (i)
derivation of the error exponent of the average undetected error probability
and (ii) closed form expressions for the variance of the undetected error
probability. It is shown that the behavior of the exponent for a sparse
ensemble is somewhat different from that for a dense ensemble. Furthermore, as
a byproduct of the proof of the variance formula, simple covariance formula of
the weight distribution is derived.



The use of the logarithm of the variate in the calculation of differential entropy among certain related statistical distributions

This paper demonstrates that basic statistics (mean, variance) of the
logarithm of the variate itself can be used in the calculation of differential
entropy among random variables known to be multiples and powers of a common
underlying variate. For the same set of distributions, the variance of the
differential self-information is shown also to be a function of statistics of
the logarithmic variate. Then entropy and its "variance" can be estimated using
only statistics of the logarithmic variate plus constants, without reference to
the traditional parameters of the variate.



On Term Rewriting Systems Having a Rational Derivation

Several types of term rewriting systems can be distinguished by the way their
rules overlap. In particular, we define the classes of prefix, suffix,
bottom-up and top-down systems, which generalize similar classes on words. Our
aim is to study the derivation relation of such systems (i.e. the reflexive and
transitive closure of their rewriting relation) and, if possible, to provide a
finite mechanism characterizing it. Using a notion of rational relations based
on finite graph grammars, we show that the derivation of any bottom-up,
top-down or suffix systems is rational, while it can be non recursive for
prefix systems.



The Distance Geometry of Music

We demonstrate relationships between the classic Euclidean algorithm and many
other fields of study, particularly in the context of music and distance
geometry. Specifically, we show how the structure of the Euclidean algorithm
defines a family of rhythms which encompass over forty timelines
(\emph{ostinatos}) from traditional world music. We prove that these
\emph{Euclidean rhythms} have the mathematical property that their onset
patterns are distributed as evenly as possible: they maximize the sum of the
Euclidean distances between all pairs of onsets, viewing onsets as points on a
circle. Indeed, Euclidean rhythms are the unique rhythms that maximize this
notion of \emph{evenness}. We also show that essentially all Euclidean rhythms
are \emph{deep}: each distinct distance between onsets occurs with a unique
multiplicity, and these multiplicies form an interval $1,2,...,k-1$. Finally,
we characterize all deep rhythms, showing that they form a subclass of
generated rhythms, which in turn proves a useful property called shelling. All
of our results for musical rhythms apply equally well to musical scales. In
addition, many of the problems we explore are interesting in their own right as
distance geometry problems on the circle; some of the same problems were
explored by Erd\H{o}s in the plane.



Efficiency and Nash Equilibria in a Scrip System for P2P Networks

A model of providing service in a P2P network is analyzed. It is shown that
by adding a scrip system, a mechanism that admits a reasonable Nash equilibrium
that reduces free riding can be obtained. The effect of varying the total
amount of money (scrip) in the system on efficiency (i.e., social welfare) is
analyzed, and it is shown that by maintaining the appropriate ratio between the
total amount of money and the number of agents, efficiency is maximized. The
work has implications for many online systems, not only P2P networks but also a
wide variety of online forums for which scrip systems are popular, but formal
analyses have been lacking.



Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists

We discuss the design of efficient scrip systems and develop tools for
empirically analyzing them. For those interested in the empirical study of
scrip systems, we demonstrate how characteristics of agents in a system can be
inferred from the equilibrium distribution of money. From the perspective of a
system designer, we examine the effect of the money supply on social welfare
and show that social welfare is maximized by increasing the money supply up to
the point that the system experiences a ``monetary crash,'' where money is
sufficiently devalued that no agent is willing to perform a service. We also
examine the implications of the presence of altruists and hoarders on the
performance of the system. While a small number of altruists may improve social
welfare, too many can also cause the system to experience a monetary crash,
which may be bad for social welfare. Hoarders generally decrease social welfare
but, surprisingly, they also promote system stability by helping prevent
monetary crashes. In addition, we provide new technical tools for analyzing and
computing equilibria by showing that our model exhibits strategic
complementarities, which implies that there exist equilibria in pure strategies
that can be computed efficiently.



The Battery-Discharge-Model: A Class of Stochastic Finite Automata to Simulate Multidimensional Continued Fraction Expansion

We define an infinite stochastic state machine, the Battery-Discharge-Model
(BDM), which simulates the behaviour of linear and jump complexity of the
continued fraction expansion of multidimensional formal power series, a
relevant security measure in the cryptanalysis of stream ciphers.
  We also obtain finite approximations to the infinite BDM, where polynomially
many states suffice to approximate with an exponentially small error the
probabilities and averages for linear and jump complexity of M-multisequences
of length n over the finite field F_q, for any M, n, q.



The Asymptotic Normalized Linear Complexity of Multisequences

We show that the asymptotic linear complexity of a multisequence a in
F_q^\infty that is I := liminf L_a(n)/n and S := limsup L_a(n)/n satisfy the
inequalities M/(M+1) <= S <= 1 and M(1-S) <= I <= 1-S/M, if all M sequences
have nonzero discrepancy infinitely often, and all pairs (I,S) satisfying these
conditions are met by 2^{\aleph_0} multisequences a.
  This answers an Open Problem by Dai, Imamura, and Yang.
  Keywords: Linear complexity, multisequence, Battery Discharge Model,
isometry.



Grover search algorithm

A quantum algorithm is a set of instructions for a quantum computer, however,
unlike algorithms in classical computer science their results cannot be
guaranteed. A quantum system can undergo two types of operation, measurement
and quantum state transformation, operations themselves must be unitary
(reversible). Most quantum algorithms involve a series of quantum state
transformations followed by a measurement. Currently very few quantum
algorithms are known and no general design methodology exists for their
construction.



Secure Two-party Protocols for Point Inclusion Problem

It is well known that, in theory, the general secure multi-party computation
problem is solvable using circuit evaluation protocols. However, the
communication complexity of the resulting protocols depend on the size of the
circuit that expresses the functionality to be computed and hence can be
impractical. Hence special solutions are needed for specific problems for
efficiency reasons. The point inclusion problem in computational geometry is a
special multiparty computation and has got many applications. Previous
protocols for the secure point inclusion problem are not adequate. In this
paper we modify some known solutions to the point inclusion problem in
computational geometry to the frame work of secure two-party computation.



Second-Order Type Isomorphisms Through Game Semantics

The characterization of second-order type isomorphisms is a purely
syntactical problem that we propose to study under the enlightenment of game
semantics. We study this question in the case of second-order
&#955;$\mu$-calculus, which can be seen as an extension of system F to
classical logic, and for which we de&#64257;ne a categorical framework: control
hyperdoctrines. Our game model of &#955;$\mu$-calculus is based on polymorphic
arenas (closely related to Hughes' hyperforests) which evolve during the play
(following the ideas of Murawski-Ong). We show that type isomorphisms coincide
with the "equality" on arenas associated with types. Finally we deduce the
equational characterization of type isomorphisms from this equality. We also
recover from the same model Roberto Di Cosmo's characterization of type
isomorphisms for system F. This approach leads to a geometrical comprehension
on the question of second order type isomorphisms, which can be easily extended
to some other polymorphic calculi including additional programming features.



Curry-style type Isomorphisms and Game Semantics

Curry-style system F, ie. system F with no explicit types in terms, can be
seen as a core presentation of polymorphism from the point of view of
programming languages. This paper gives a characterisation of type isomorphisms
for this language, by using a game model whose intuitions come both from the
syntax and from the game semantics universe. The model is composed of: an
untyped part to interpret terms, a notion of game to interpret types, and a
typed part to express the fact that an untyped strategy plays on a game. By
analysing isomorphisms in the model, we prove that the equational system
corresponding to type isomorphisms for Curry-style system F is the extension of
the equational system for Church-style isomorphisms with a new, non-trivial
equation: forall X.A = A[forall Y.Y/X] if X appears only positively in A.



Truecluster matching

Cluster matching by permuting cluster labels is important in many clustering
contexts such as cluster validation and cluster ensemble techniques. The
classic approach is to minimize the euclidean distance between two cluster
solutions which induces inappropriate stability in certain settings. Therefore,
we present the truematch algorithm that introduces two improvements best
explained in the crisp case. First, instead of maximizing the trace of the
cluster crosstable, we propose to maximize a chi-square transformation of this
crosstable. Thus, the trace will not be dominated by the cells with the largest
counts but by the cells with the most non-random observations, taking into
account the marginals. Second, we suggest a probabilistic component in order to
break ties and to make the matching algorithm truly random on random data. The
truematch algorithm is designed as a building block of the truecluster
framework and scales in polynomial time. First simulation results confirm that
the truematch algorithm gives more consistent truecluster results for unequal
cluster sizes. Free R software is available.



Defect-Tolerant CMOL Cell Assignment via Satisfiability

We present a CAD framework for CMOL, a hybrid CMOS/ molecular circuit
architecture. Our framework first transforms any logically synthesized circuit
based on AND/OR/NOT gates to a NOR gate circuit, and then maps the NOR gates to
CMOL. We encode the CMOL cell assignment problem as boolean conditions. The
boolean constraint is satisfiable if and only if there is a way to map all the
NOR gates to the CMOL cells. We further investigate various types of static
defects for the CMOL architecture, and propose a reconfiguration technique that
can deal with these defects through our CAD framework. This is the first
automated framework for CMOL cell assignment, and the first to model several
different CMOL static defects. Empirical results show that our approach is
efficient and scalable.



Computing Integer Powers in Floating-Point Arithmetic

We introduce two algorithms for accurately evaluating powers to a positive
integer in floating-point arithmetic, assuming a fused multiply-add (fma)
instruction is available. We show that our log-time algorithm always produce
faithfully-rounded results, discuss the possibility of getting correctly
rounded results, and show that results correctly rounded in double precision
can be obtained if extended-precision is available with the possibility to
round into double precision (with a single rounding).



PERCEVAL: a Computer-Driven System for Experimentation on Auditory and Visual Perception

Since perception tests are highly time-consuming, there is a need to automate
as many operations as possible, such as stimulus generation, procedure control,
perception testing, and data analysis. The computer-driven system we are
presenting here meets these objectives. To achieve large flexibility, the tests
are controlled by scripts. The system's core software resembles that of a
lexical-syntactic analyzer, which reads and interprets script files sent to it.
The execution sequence (trial) is modified in accordance with the commands and
data received. This type of operation provides a great deal of flexibility and
supports a wide variety of tests such as auditory-lexical decision making,
phoneme monitoring, gating, phonetic categorization, word identification, voice
quality, etc. To achieve good performance, we were careful about timing
accuracy, which is the greatest problem in computerized perception tests.



World-set Decompositions: Expressiveness and Efficient Algorithms

Uncertain information is commonplace in real-world data management scenarios.
The ability to represent large sets of possible instances (worlds) while
supporting efficient storage and processing is an important challenge in this
context. The recent formalism of world-set decompositions (WSDs) provides a
space-efficient representation for uncertain data that also supports scalable
processing. WSDs are complete for finite world-sets in that they can represent
any finite set of possible worlds. For possibly infinite world-sets, we show
that a natural generalization of WSDs precisely captures the expressive power
of c-tables. We then show that several important decision problems are
efficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we
give a polynomial-time algorithm for factorizing WSDs, i.e. an efficient
algorithm for minimizing such representations.



Mixed membership stochastic blockmodels

Observations consisting of measurements on relationships for pairs of objects
arise in many settings, such as protein interaction and gene regulatory
networks, collections of author-recipient email, and social networks. Analyzing
such data with probabilisic models can be delicate because the simple
exchangeability assumptions underlying many boilerplate models no longer hold.
In this paper, we describe a latent variable model of such data called the
mixed membership stochastic blockmodel. This model extends blockmodels for
relational data to ones which capture mixed membership latent relational
structure, thus providing an object-specific low-dimensional representation. We
develop a general variational inference algorithm for fast approximate
posterior inference. We explore applications to social and protein interaction
networks.



Loop corrections for message passing algorithms in continuous variable models

In this paper we derive the equations for Loop Corrected Belief Propagation
on a continuous variable Gaussian model. Using the exactness of the averages
for belief propagation for Gaussian models, a different way of obtaining the
covariances is found, based on Belief Propagation on cavity graphs. We discuss
the relation of this loop correction algorithm to Expectation Propagation
algorithms for the case in which the model is no longer Gaussian, but slightly
perturbed by nonlinear terms.



Modeling Epidemic Spread in Synthetic Populations - Virtual Plagues in Massively Multiplayer Online Games

A virtual plague is a process in which a behavior-affecting property spreads
among characters in a Massively Multiplayer Online Game (MMOG). The MMOG
individuals constitute a synthetic population, and the game can be seen as a
form of interactive executable model for studying disease spread, albeit of a
very special kind. To a game developer maintaining an MMOG, recognizing,
monitoring, and ultimately controlling a virtual plague is important,
regardless of how it was initiated. The prospect of using tools, methods and
theory from the field of epidemiology to do this seems natural and appealing.
We will address the feasibility of such a prospect, first by considering some
basic measures used in epidemiology, then by pointing out the differences
between real world epidemics and virtual plagues. We also suggest directions
for MMOG developer control through epidemiological modeling. Our aim is
understanding the properties of virtual plagues, rather than trying to
eliminate them or mitigate their effects, as would be in the case of real
infectious disease.



Temporal Runtime Verification using Monadic Difference Logic

In this paper we present an algorithm for performing runtime verification of
a bounded temporal logic over timed runs. The algorithm consists of three
elements. First, the bounded temporal formula to be verified is translated into
a monadic first-order logic over difference inequalities, which we call monadic
difference logic. Second, at each step of the timed run, the monadic difference
formula is modified by computing a quotient with the state and time of that
step. Third, the resulting formula is checked for being a tautology or being
unsatisfiable by a decision procedure for monadic difference logic.
  We further provide a simple decision procedure for monadic difference logic
based on the data structure Difference Decision Diagrams. The algorithm is
complete in a very strong sense on a subclass of temporal formulae
characterized as homogeneously monadic and it is approximate on other formulae.
The approximation comes from the fact that not all unsatisfiable or
tautological formulae are recognised at the earliest possible time of the
runtime verification.
  Contrary to existing approaches, the presented algorithms do not work by
syntactic rewriting but employ efficient decision structures which make them
applicable in real applications within for instance business software.



Dynamic User-Defined Similarity Searching in Semi-Structured Text Retrieval

Modern text retrieval systems often provide a similarity search utility, that
allows the user to find efficiently a fixed number k of documents in the data
set that are most similar to a given query (here a query is either a simple
sequence of keywords or the identifier of a full document found in previous
searches that is considered of interest). We consider the case of a textual
database made of semi-structured documents. Each field, in turns, is modelled
with a specific vector space. The problem is more complex when we also allow
each such vector space to have an associated user-defined dynamic weight that
influences its contribution to the overall dynamic aggregated and weighted
similarity. This dynamic problem has been tackled in a recent paper by
Singitham et al. in in VLDB 2004. Their proposed solution, which we take as
baseline, is a variant of the cluster-pruning technique that has the potential
for scaling to very large corpora of documents, and is far more efficient than
the naive exhaustive search. We devise an alternative way of embedding weights
in the data structure, coupled with a non-trivial application of a clustering
algorithm based on the furthest point first heuristic for the metric k-center
problem. The validity of our approach is demonstrated experimentally by showing
significant performance improvements over the scheme proposed in Singitham et
al. in VLDB 2004. We improve significantly tradeoffs between query time and
output quality with respect to the baseline method in Singitham et al. in in
VLDB 2004, and also with respect to a novel method by Chierichetti et al. to
appear in ACM PODS 2007. We also speed up the pre-processing time by a factor
at least thirty.



An Improved Tight Closure Algorithm for Integer Octagonal Constraints

Integer octagonal constraints (a.k.a. ``Unit Two Variables Per Inequality''
or ``UTVPI integer constraints'') constitute an interesting class of
constraints for the representation and solution of integer problems in the
fields of constraint programming and formal analysis and verification of
software and hardware systems, since they couple algorithms having polynomial
complexity with a relatively good expressive power. The main algorithms
required for the manipulation of such constraints are the satisfiability check
and the computation of the inferential closure of a set of constraints. The
latter is called `tight' closure to mark the difference with the (incomplete)
closure algorithm that does not exploit the integrality of the variables. In
this paper we present and fully justify an O(n^3) algorithm to compute the
tight closure of a set of UTVPI integer constraints.



Local Area Damage Detection in Composite Structures Using Piezoelectric Transducers

An integrated and automated smart structures approach for structural health
monitoring is presented, utilizing an array of piezoelectric transducers
attached to or embedded within the structure for both actuation and sensing.
The system actively interrogates the structure via broadband excitation of
multiple actuators across a desired frequency range. The structure's vibration
signature is then characterized by computing the transfer functions between
each actuator/sensor pair, and compared to the baseline signature. Experimental
results applying the system to local area damage detection in a MD Explorer
rotorcraft composite flexbeam are presented.



Two sources are better than one for increasing the Kolmogorov complexity of infinite sequences

The randomness rate of an infinite binary sequence is characterized by the
sequence of ratios between the Kolmogorov complexity and the length of the
initial segments of the sequence. It is known that there is no uniform
effective procedure that transforms one input sequence into another sequence
with higher randomness rate. By contrast, we display such a uniform effective
procedure having as input two independent sequences with positive but
arbitrarily small constant randomness rate. Moreover the transformation is a
truth-table reduction and the output has randomness rate arbitrarily close to
1.



A randomized algorithm for the on-line weighted bipartite matching problem

We study the on-line minimum weighted bipartite matching problem in arbitrary
metric spaces. Here, $n$ not necessary disjoint points of a metric space $M$
are given, and are to be matched on-line with $n$ points of $M$ revealed one by
one. The cost of a matching is the sum of the distances of the matched points,
and the goal is to find or approximate its minimum. The competitive ratio of
the deterministic problem is known to be $\Theta(n)$. It was conjectured that a
randomized algorithm may perform better against an oblivious adversary, namely
with an expected competitive ratio $\Theta(\log n)$. We prove a slightly weaker
result by showing a $o(\log^3 n)$ upper bound on the expected competitive
ratio. As an application the same upper bound holds for the notoriously hard
fire station problem, where $M$ is the real line.



Recursive n-gram hashing is pairwise independent, at best

Many applications use sequences of n consecutive symbols (n-grams). Hashing
these n-grams can be a performance bottleneck. For more speed, recursive hash
families compute hash values by updating previous values. We prove that
recursive hash families cannot be more than pairwise independent. While hashing
by irreducible polynomials is pairwise independent, our implementations either
run in time O(n) or use an exponential amount of memory. As a more scalable
alternative, we make hashing by cyclic polynomials pairwise independent by
ignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials
is is twice as fast as hashing by irreducible polynomials. We also show that
randomized Karp-Rabin hash families are not pairwise independent.



Towards an exact adaptive algorithm for the determinant of a rational matrix

In this paper we propose several strategies for the exact computation of the
determinant of a rational matrix. First, we use the Chinese Remaindering
Theorem and the rational reconstruction to recover the rational determinant
from its modular images. Then we show a preconditioning for the determinant
which allows us to skip the rational reconstruction process and reconstruct an
integer result. We compare those approaches with matrix preconditioning which
allow us to treat integer instead of rational matrices. This allows us to
introduce integer determinant algorithms to the rational determinant problem.
In particular, we discuss the applicability of the adaptive determinant
algorithm of [9] and compare it with the integer Chinese Remaindering scheme.
We present an analysis of the complexity of the strategies and evaluate their
experimental performance on numerous examples. This experience allows us to
develop an adaptive strategy which would choose the best solution at the run
time, depending on matrix properties. All strategies have been implemented in
LinBox linear algebra library.



Modeling Computations in a Semantic Network

Semantic network research has seen a resurgence from its early history in the
cognitive sciences with the inception of the Semantic Web initiative. The
Semantic Web effort has brought forth an array of technologies that support the
encoding, storage, and querying of the semantic network data structure at the
world stage. Currently, the popular conception of the Semantic Web is that of a
data modeling medium where real and conceptual entities are related in
semantically meaningful ways. However, new models have emerged that explicitly
encode procedural information within the semantic network substrate. With these
new technologies, the Semantic Web has evolved from a data modeling medium to a
computational medium. This article provides a classification of existing
computational modeling efforts and the requirements of supporting technologies
that will aid in the further growth of this burgeoning domain.



Symmetry Partition Sort

In this paper, we propose a useful replacement for quicksort-style utility
functions. The replacement is called Symmetry Partition Sort, which has
essentially the same principle as Proportion Extend Sort. The maximal
difference between them is that the new algorithm always places already
partially sorted inputs (used as a basis for the proportional extension) on
both ends when entering the partition routine. This is advantageous to speeding
up the partition routine. The library function based on the new algorithm is
more attractive than Psort which is a library function introduced in 2004. Its
implementation mechanism is simple. The source code is clearer. The speed is
faster, with O(n log n) performance guarantee. Both the robustness and
adaptivity are better. As a library function, it is competitive.



Many concepts and two logics of algorithmic reduction

Within the program of finding axiomatizations for various parts of
computability logic, it was proved earlier that the logic of interactive Turing
reduction is exactly the implicative fragment of Heyting's intuitionistic
calculus. That sort of reduction permits unlimited reusage of the computational
resource represented by the antecedent. An at least equally basic and natural
sort of algorithmic reduction, however, is the one that does not allow such
reusage. The present article shows that turning the logic of the first sort of
reduction into the logic of the second sort of reduction takes nothing more
than just deleting the contraction rule from its Gentzen-style axiomatization.
The first (Turing) sort of interactive reduction is also shown to come in three
natural versions. While those three versions are very different from each
other, their logical behaviors (in isolation) turn out to be indistinguishable,
with that common behavior being precisely captured by implicative
intuitionistic logic. Among the other contributions of the present article is
an informal introduction of a series of new -- finite and bounded -- versions
of recurrence operations and the associated reduction operations. An online
source on computability logic can be found at
http://www.cis.upenn.edu/~giorgi/cl.html



On the End-to-End Distortion for a Buffered Transmission over Fading Channel

In this paper, we study the end-to-end distortion/delay tradeoff for a
analogue source transmitted over a fading channel. The analogue source is
quantized and stored in a buffer until it is transmitted. There are two extreme
cases as far as buffer delay is concerned: no delay and infinite delay. We
observe that there is a significant power gain by introducing a buffer delay.
Our goal is to investigate the situation between these two extremes. Using
recently proposed \emph{effective capacity} concept, we derive a closed-form
formula for this tradeoff. For SISO case, an asymptotically tight upper bound
for our distortion-delay curve is derived, which approaches to the infinite
delay lower bound as $\mathcal{D}_\infty \exp(\frac{C}{\tau_n})$, with $\tau_n$
is the normalized delay, $C$ is a constant. For more general MIMO channel, we
computed the distortion SNR exponent -- the exponential decay rate of the
expected distortion in the high SNR regime. Numerical results demonstrate that
introduction of a small amount delay can save significant transmission power.



Applying the Z-transform for the static analysis of floating-point numerical filters

Digital linear filters are used in a variety of applications (sound
treatment, control/command, etc.), implemented in software, in hardware, or a
combination thereof. For safety-critical applications, it is necessary to bound
all variables and outputs of all filters. We give a compositional, effective
abstraction for digital linear filters expressed as block diagrams, yielding
sound, precise bounds for fixed-point or floating-point implementations of the
filters.



Multi-Agent Modeling Using Intelligent Agents in the Game of Lerpa

Game theory has many limitations implicit in its application. By utilizing
multiagent modeling, it is possible to solve a number of problems that are
unsolvable using traditional game theory. In this paper reinforcement learning
is applied to neural networks to create intelligent agents



Automatic Detection of Pulmonary Embolism using Computational Intelligence

This article describes the implementation of a system designed to
automatically detect the presence of pulmonary embolism in lung scans. These
images are firstly segmented, before alignment and feature extraction using
PCA. The neural network was trained using the Hybrid Monte Carlo method,
resulting in a committee of 250 neural networks and good results are obtained.



Submission of content to a digital object repository using a configurable workflow system

The prototype of a workflow system for the submission of content to a digital
object repository is here presented. It is based entirely on open-source
standard components and features a service-oriented architecture. The front-end
consists of Java Business Process Management (jBPM), Java Server Faces (JSF),
and Java Server Pages (JSP). A Fedora Repository and a mySQL data base
management system serve as a back-end. The communication between front-end and
back-end uses a SOAP minimal binding stub. We describe the design principles
and the construction of the prototype and discuss the possibilities and
limitations of work ow creation by administrators. The code of the prototype is
open-source and can be retrieved in the project escipub at
http://sourceforge.net



Multiplication of free random variables and the S-transform: the case of vanishing mean

This note extends Voiculescu's S-transform based analytical machinery for
free multiplicative convolution to the case where the mean of the probability
measures vanishes. We show that with the right interpretation of the
S-transform in the case of vanishing mean, the usual formula makes perfectly
good sense.



Watermark Embedding and Detection

The embedder and the detector (or decoder) are the two most important
components of the digital watermarking systems. Thus in this work, we discuss
how to design a better embedder and detector (or decoder). I first give a
summary of the prospective applications of watermarking technology and major
watermarking schemes in the literature. My review on the literature closely
centers upon how the side information is exploited at both embedders and
detectors. In Chapter 3, I explore the optimum detector or decoder according to
a particular probability distribution of the host signals. We found that the
performance of both multiplicative and additive spread spectrum schemes depends
on the shape parameter of the host signals. For spread spectrum schemes, the
performance of the detector or the decoder is reduced by the host interference.
Thus I present a new host-interference rejection technique for the
multiplicative spread spectrum schemes. Its embedding rule is tailored to the
optimum detection or decoding rule. Though the host interference rejection
schemes enjoy a big performance gain over the traditional spread spectrum
schemes, their drawbacks that it is difficult for them to be implemented with
the perceptual analysis to achieve the maximum allowable embedding level
discourage their use in real scenarios. Thus, in the last chapters of this
work, I introduce a double-sided technique to tackle this drawback. It differs
from the host interference rejection schemes in that it utilizes but does not
reject the host interference at its embedder. The perceptual analysis can be
easily implemented in our scheme to achieve the maximum allowable level of
embedding strength.



Anonymity in the Wild: Mixes on unstructured networks

As decentralized computing scenarios get ever more popular, unstructured
topologies are natural candidates to consider running mix networks upon. We
consider mix network topologies where mixes are placed on the nodes of an
unstructured network, such as social networks and scale-free random networks.
We explore the efficiency and traffic analysis resistance properties of mix
networks based on unstructured topologies as opposed to theoretically optimal
structured topologies, under high latency conditions. We consider a mix of
directed and undirected network models, as well as one real world case study --
the LiveJournal friendship network topology. Our analysis indicates that
mix-networks based on scale-free and small-world topologies have, firstly,
mix-route lengths that are roughly comparable to those in expander graphs;
second, that compromise of the most central nodes has little effect on
anonymization properties, and third, batch sizes required for warding off
intersection attacks need to be an order of magnitude higher in unstructured
networks in comparison with expander graph topologies.



Abstract numeration systems on bounded languages and multiplication by a constant

A set of integers is $S$-recognizable in an abstract numeration system $S$ if
the language made up of the representations of its elements is accepted by a
finite automaton. For abstract numeration systems built over bounded languages
with at least three letters, we show that multiplication by an integer
$\lambda\ge2$ does not preserve $S$-recognizability, meaning that there always
exists a $S$-recognizable set $X$ such that $\lambda X$ is not
$S$-recognizable. The main tool is a bijection between the representation of an
integer over a bounded language and its decomposition as a sum of binomial
coefficients with certain properties, the so-called combinatorial numeration
system.



Challenges and Opportunities of Evolutionary Robotics

Robotic hardware designs are becoming more complex as the variety and number
of on-board sensors increase and as greater computational power is provided in
ever-smaller packages on-board robots. These advances in hardware, however, do
not automatically translate into better software for controlling complex
robots. Evolutionary techniques hold the potential to solve many difficult
problems in robotics which defy simple conventional approaches, but present
many challenges as well. Numerous disciplines including artificial life,
cognitive science and neural networks, rule-based systems, behavior-based
control, genetic algorithms and other forms of evolutionary computation have
contributed to shaping the current state of evolutionary robotics. This paper
provides an overview of developments in the emerging field of evolutionary
robotics, and discusses some of the opportunities and challenges which
currently face practitioners in the field.



Virtual Sensor Based Fault Detection and Classification on a Plasma Etch Reactor

The SEMATECH sponsored J-88-E project teaming Texas Instruments with
NeuroDyne (et al.) focused on Fault Detection and Classification (FDC) on a Lam
9600 aluminum plasma etch reactor, used in the process of semiconductor
fabrication. Fault classification was accomplished by implementing a series of
virtual sensor models which used data from real sensors (Lam Station sensors,
Optical Emission Spectroscopy, and RF Monitoring) to predict recipe setpoints
and wafer state characteristics. Fault detection and classification were
performed by comparing predicted recipe and wafer state values with expected
values. Models utilized include linear PLS, Polynomial PLS, and Neural Network
PLS. Prediction of recipe setpoints based upon sensor data provides a
capability for cross-checking that the machine is maintaining the desired
setpoints. Wafer state characteristics such as Line Width Reduction and
Remaining Oxide were estimated on-line using these same process sensors (Lam,
OES, RFM). Wafer-to-wafer measurement of these characteristics in a production
setting (where typically this information may be only sparsely available, if at
all, after batch processing runs with numerous wafers have been completed)
would provide important information to the operator that the process is or is
not producing wafers within acceptable bounds of product quality. Production
yield is increased, and correspondingly per unit cost is reduced, by providing
the operator with the opportunity to adjust the process or machine before
etching more wafers.



Motivation, Design, and Ubiquity: A Discussion of Research Ethics and Computer Science

Modern society is permeated with computers, and the software that controls
them can have latent, long-term, and immediate effects that reach far beyond
the actual users of these systems. This places researchers in Computer Science
and Software Engineering in a critical position of influence and
responsibility, more than any other field because computer systems are vital
research tools for other disciplines. This essay presents several key ethical
concerns and responsibilities relating to research in computing. The goal is to
promote awareness and discussion of ethical issues among computer science
researchers. A hypothetical case study is provided, along with questions for
reflection and discussion.



Sampling Colourings of the Triangular Lattice

We show that the Glauber dynamics on proper 9-colourings of the triangular
lattice is rapidly mixing, which allows for efficient sampling. Consequently,
there is a fully polynomial randomised approximation scheme (FPRAS) for
counting proper 9-colourings of the triangular lattice. Proper colourings
correspond to configurations in the zero-temperature anti-ferromagnetic Potts
model. We show that the spin system consisting of proper 9-colourings of the
triangular lattice has strong spatial mixing. This implies that there is a
unique infinite-volume Gibbs distribution, which is an important property
studied in statistical physics. Our results build on previous work by Goldberg,
Martin and Paterson, who showed similar results for 10 colours on the
triangular lattice. Their work was preceded by Salas and Sokal's 11-colour
result. Both proofs rely on computational assistance, and so does our 9-colour
proof. We have used a randomised heuristic to guide us towards rigourous
results.



Relating two standard notions of secrecy

Two styles of definitions are usually considered to express that a security
protocol preserves the confidentiality of a data s. Reachability-based secrecy
means that s should never be disclosed while equivalence-based secrecy states
that two executions of a protocol with distinct instances for s should be
indistinguishable to an attacker. Although the second formulation ensures a
higher level of security and is closer to cryptographic notions of secrecy,
decidability results and automatic tools have mainly focused on the first
definition so far.
  This paper initiates a systematic investigation of the situations where
syntactic secrecy entails strong secrecy. We show that in the passive case,
reachability-based secrecy actually implies equivalence-based secrecy for
digital signatures, symmetric and asymmetric encryption provided that the
primitives are probabilistic. For active adversaries, we provide sufficient
(and rather tight) conditions on the protocol for this implication to hold.



A collaborative framework to exchange and share product information within a supply chain context

The new requirement for "collaboration" between multidisciplinary
collaborators induces to exchange and share adequate information on the
product, processes throughout the products' lifecycle. Thus, effective capture
of information, and also its extraction, recording, exchange, sharing, and
reuse become increasingly critical. These lead companies to adopt new improved
methodologies in managing the exchange and sharing of information. The aim of
this paper is to describe a collaborative framework system to exchange and
share information, which is based on: (i) The Product Process Collaboration
Organization model (PPCO) which defines product and process information, and
the various collaboration methods for the organizations involved in the supply
chain. (ii) Viewpoint model describes relationships between each actor and the
comprehensive Product/Process model, defining each actor's "domain of interest"
within the evolving product definition. (iii) A layer which defines the
comprehensive organization and collaboration relationships between the actors
within the supply chain. (iv) Based on the above relationships, the last layer
proposes a typology of exchanged messages. A communication method, based on
XML, is developed that supports optimal exchange/sharing of information. To
illustrate the proposed framework system, an example is presented related to
collaborative design of a new piston for an automotive engine. The focus is on
user-viewpoint integration to ensure that the adequate information is retrieved
from the PPCO.



Interpolant-Based Transition Relation Approximation

In predicate abstraction, exact image computation is problematic, requiring
in the worst case an exponential number of calls to a decision procedure. For
this reason, software model checkers typically use a weak approximation of the
image. This can result in a failure to prove a property, even given an adequate
set of predicates. We present an interpolant-based method for strengthening the
abstract transition relation in case of such failures. This approach guarantees
convergence given an adequate set of predicates, without requiring an exact
image computation. We show empirically that the method converges more rapidly
than an earlier method based on counterexample analysis.



Compressed Regression

Recent research has studied the role of sparsity in high dimensional
regression and signal reconstruction, establishing theoretical limits for
recovering sparse models from sparse data. This line of work shows that
$\ell_1$-regularized least squares regression can accurately estimate a sparse
linear model from $n$ noisy examples in $p$ dimensions, even if $p$ is much
larger than $n$. In this paper we study a variant of this problem where the
original $n$ input variables are compressed by a random linear transformation
to $m \ll n$ examples in $p$ dimensions, and establish conditions under which a
sparse linear model can be successfully recovered from the compressed data. A
primary motivation for this compression procedure is to anonymize the data and
preserve privacy by revealing little information about the original data. We
characterize the number of random projections that are required for
$\ell_1$-regularized compressed regression to identify the nonzero coefficients
in the true model with probability approaching one, a property called
``sparsistence.'' In addition, we show that $\ell_1$-regularized compressed
regression asymptotically predicts as well as an oracle linear model, a
property called ``persistence.'' Finally, we characterize the privacy
properties of the compression procedure in information-theoretic terms,
establishing upper bounds on the mutual information between the compressed and
uncompressed data that decay to zero.



Tropical Implicitization and Mixed Fiber Polytopes

The software TrIm offers implementations of tropical implicitization and
tropical elimination, as developed by Tevelev and the authors. Given a
polynomial map with generic coefficients, TrIm computes the tropical variety of
the image. When the image is a hypersurface, the output is the Newton polytope
of the defining polynomial. TrIm can thus be used to compute mixed fiber
polytopes, including secondary polytopes.



Efficient Batch Update of Unique Identifiers in a Distributed Hash Table for Resources in a Mobile Host

Resources in a distributed system can be identified using identifiers based
on random numbers. When using a distributed hash table to resolve such
identifiers to network locations, the straightforward approach is to store the
network location directly in the hash table entry associated with an
identifier. When a mobile host contains a large number of resources, this
requires that all of the associated hash table entries must be updated when its
network address changes.
  We propose an alternative approach where we store a host identifier in the
entry associated with a resource identifier and the actual network address of
the host in a separate host entry. This can drastically reduce the time
required for updating the distributed hash table when a mobile host changes its
network address. We also investigate under which circumstances our approach
should or should not be used. We evaluate and confirm the usefulness of our
approach with experiments run on top of OpenDHT.



A Novel Model of Working Set Selection for SMO Decomposition Methods

In the process of training Support Vector Machines (SVMs) by decomposition
methods, working set selection is an important technique, and some exciting
schemes were employed into this field. To improve working set selection, we
propose a new model for working set selection in sequential minimal
optimization (SMO) decomposition methods. In this model, it selects B as
working set without reselection. Some properties are given by simple proof, and
experiments demonstrate that the proposed method is in general faster than
existing methods.



Code spectrum and reliability function: Gaussian channel

A new approach for upper bounding the channel reliability function using the
code spectrum is described. It allows to treat both low and high rate cases in
a unified way. In particular, the earlier known upper bounds are improved, and
a new derivation of the sphere-packing bound is presented.



Non-Parametric Field Estimation using Randomly Deployed, Noisy, Binary Sensors

The reconstruction of a deterministic data field from binary-quantized noisy
observations of sensors which are randomly deployed over the field domain is
studied. The study focuses on the extremes of lack of deterministic control in
the sensor deployment, lack of knowledge of the noise distribution, and lack of
sensing precision and reliability. Such adverse conditions are motivated by
possible real-world scenarios where a large collection of low-cost, crudely
manufactured sensors are mass-deployed in an environment where little can be
assumed about the ambient noise. A simple estimator that reconstructs the
entire data field from these unreliable, binary-quantized, noisy observations
is proposed. Technical conditions for the almost sure and integrated mean
squared error (MSE) convergence of the estimate to the data field, as the
number of sensors tends to infinity, are derived and their implications are
discussed. For finite-dimensional, bounded-variation, and
Sobolev-differentiable function classes, specific integrated MSE decay rates
are derived. For the first and third function classes these rates are found to
be minimax order optimal with respect to infinite precision sensing and known
noise distribution.



Probabilistic Interval Temporal Logic and Duration Calculus with Infinite Intervals: Complete Proof Systems

The paper presents probabilistic extensions of interval temporal logic (ITL)
and duration calculus (DC) with infinite intervals and complete Hilbert-style
proof systems for them. The completeness results are a strong completeness
theorem for the system of probabilistic ITL with respect to an abstract
semantics and a relative completeness theorem for the system of probabilistic
DC with respect to real-time semantics. The proposed systems subsume
probabilistic real-time DC as known from the literature. A correspondence
between the proposed systems and a system of probabilistic interval temporal
logic with finite intervals and expanding modalities is established too.



Universal Quantile Estimation with Feedback in the Communication-Constrained Setting

We consider the following problem of decentralized statistical inference:
given i.i.d. samples from an unknown distribution, estimate an arbitrary
quantile subject to limits on the number of bits exchanged. We analyze a
standard fusion-based architecture, in which each of $m$ sensors transmits a
single bit to the fusion center, which in turn is permitted to send some number
$k$ bits of feedback. Supposing that each of $\nodenum$ sensors receives $n$
observations, the optimal centralized protocol yields mean-squared error
decaying as $\order(1/[n m])$. We develop and analyze the performance of
various decentralized protocols in comparison to this centralized
gold-standard. First, we describe a decentralized protocol based on $k =
\log(\nodenum)$ bits of feedback that is strongly consistent, and achieves the
same asymptotic MSE as the centralized optimum. Second, we describe and analyze
a decentralized protocol based on only a single bit ($k=1$) of feedback. For
step sizes independent of $m$, it achieves an asymptotic MSE of order
$\order[1/(n \sqrt{m})]$, whereas for step sizes decaying as $1/\sqrt{m}$, it
achieves the same $\order(1/[n m])$ decay in MSE as the centralized optimum.
Our theoretical results are complemented by simulations, illustrating the
tradeoffs between these different protocols.



Position Coding

A position coding pattern is an array of symbols in which subarrays of a
certain fixed size appear at most once. So, each subarray uniquely identifies a
location in the larger array, which means there is a bijection of some sort
from this set of subarrays to a set of coordinates. The key to Fly Pentop
Computer paper and other examples of position codes is a method to read the
subarray and then convert it to coordinates. Position coding makes use of ideas
from discrete mathematics and number theory. In this paper, we will describe
the underlying mathematics of two position codes, one being the Anoto code that
is the basis of "Fly paper". Then, we will present two new codes, one which
uses binary wavelets as part of the bijection.



Inferring the Composition of a Trader Population in a Financial Market

We discuss a method for predicting financial movements and finding pockets of
predictability in the price-series, which is built around inferring the
heterogeneity of trading strategies in a multi-agent trader population. This
work explores extensions to our previous framework (arXiv:physics/0506134).
Here we allow for more intelligent agents possessing a richer strategy set, and
we no longer constrain the estimate for the heterogeneity of the agents to a
probability space. We also introduce a scheme which allows the incorporation of
models with a wide variety of agent types, and discuss a mechanism for the
removal of bias from relevant parameters.



Families of traveling impulses and fronts in some models with cross-diffusion

An analysis of traveling wave solutions of partial differential equation
(PDE) systems with cross-diffusion is presented. The systems under study fall
in a general class of the classical Keller-Segel models to describe chemotaxis.
The analysis is conducted using the theory of the phase plane analysis of the
corresponding wave systems without a priory restrictions on the boundary
conditions of the initial PDE. Special attention is paid to families of
traveling wave solutions. Conditions for existence of front-impulse,
impulse-front, and front-front traveling wave solutions are formulated. In
particular, the simplest mathematical model is presented that has an
impulse-impulse solution; we also show that a non-isolated singular point in
the ordinary differential equation (ODE) wave system implies existence of
free-boundary fronts. The results can be used for construction and analysis of
different mathematical models describing systems with chemotaxis.



Epistemic Analysis of Strategic Games with Arbitrary Strategy Sets

We provide here an epistemic analysis of arbitrary strategic games based on
the possibility correspondences. Such an analysis calls for the use of
transfinite iterations of the corresponding operators. Our approach is based on
Tarski's Fixpoint Theorem and applies both to the notions of rationalizability
and the iterated elimination of strictly dominated strategies.



Moving Vertices to Make Drawings Plane

A straight-line drawing $\delta$ of a planar graph $G$ need not be plane, but
can be made so by moving some of the vertices. Let shift$(G,\delta)$ denote the
minimum number of vertices that need to be moved to turn $\delta$ into a plane
drawing of $G$. We show that shift$(G,\delta)$ is NP-hard to compute and to
approximate, and we give explicit bounds on shift$(G,\delta)$ when $G$ is a
tree or a general planar graph. Our hardness results extend to
1BendPointSetEmbeddability, a well-known graph-drawing problem.



Probabilistic Anonymity and Admissible Schedulers

When studying safety properties of (formal) protocol models, it is customary
to view the scheduler as an adversary: an entity trying to falsify the safety
property. We show that in the context of security protocols, and in particular
of anonymizing protocols, this gives the adversary too much power; for
instance, the contents of encrypted messages and internal computations by the
parties should be considered invisible to the adversary.
  We restrict the class of schedulers to a class of admissible schedulers which
better model adversarial behaviour. These admissible schedulers base their
decision solely on the past behaviour of the system that is visible to the
adversary.
  Using this, we propose a definition of anonymity: for all admissible
schedulers the identity of the users and the observations of the adversary are
independent stochastic variables. We also develop a proof technique for typical
cases that can be used to proof anonymity: a system is anonymous if it is
possible to `exchange' the behaviour of two users without the adversary
`noticing'.



Improved Neural Modeling of Real-World Systems Using Genetic Algorithm Based Variable Selection

Neural network models of real-world systems, such as industrial processes,
made from sensor data must often rely on incomplete data. System states may not
all be known, sensor data may be biased or noisy, and it is not often known
which sensor data may be useful for predictive modelling. Genetic algorithms
may be used to help to address this problem by determining the near optimal
subset of sensor variables most appropriate to produce good models. This paper
describes the use of genetic search to optimize variable selection to determine
inputs into the neural network model. We discuss genetic algorithm
implementation issues including data representation types and genetic operators
such as crossover and mutation. We present the use of this technique for neural
network modelling of a typical industrial application, a liquid fed ceramic
melter, and detail the results of the genetic search to optimize the neural
network model for this application.



Design, Implementation, and Cooperative Coevolution of an Autonomous/ Teleoperated Control System for a Serpentine Robotic Manipulator

Design, implementation, and machine learning issues associated with
developing a control system for a serpentine robotic manipulator are explored.
The controller developed provides autonomous control of the serpentine robotic
manipulatorduring operation of the manipulator within an enclosed environment
such as an underground storage tank. The controller algorithms make use of both
low-level joint angle control employing force/position feedback constraints,
and high-level coordinated control of end-effector positioning. This approach
has resulted in both high-level full robotic control and low-level telerobotic
control modes, and provides a high level of dexterity for the operator.



Small Worlds: Strong Clustering in Wireless Networks

Small-worlds represent efficient communication networks that obey two
distinguishing characteristics: a high clustering coefficient together with a
small characteristic path length. This paper focuses on an interesting paradox,
that removing links in a network can increase the overall clustering
coefficient. Reckful Roaming, as introduced in this paper, is a 2-localized
algorithm that takes advantage of this paradox in order to selectively remove
superfluous links, this way optimizing the clustering coefficient while still
retaining a sufficiently small characteristic path length.



Applying Test-Paradigms in a Generic Tutoring System Concept for Web-based Learning

Realizing test scenarios through a tutoring system involve questions about
architecture and didactic methods in such a system. Observing the fact that
traditional tutoring systems normally are domain-static, this paper shows
investigations for a generic domain-independent tutoring system for utilizing
test scenarios in computer-based and web-based environments. Furthermore, test
paradigms are analyzed and it is presented an approach for realizing
functionality for applying test paradigms in the presented generic tutoring
system architecture by an XML-specified language.



WACA: A Hierarchical Weighted Clustering Algorithm optimized for Mobile Hybrid Networks

Clustering techniques create hierarchal network structures, called clusters,
on an otherwise flat network. In a dynamic environment-in terms of node
mobility as well as in terms of steadily changing device parameters-the
clusterhead election process has to be re-invoked according to a suitable
update policy. Cluster re-organization causes additional message exchanges and
computational complexity and it execution has to be optimized. Our
investigations focus on the problem of minimizing clusterhead re-elections by
considering stability criteria. These criteria are based on topological
characteristics as well as on device parameters. This paper presents a weighted
clustering algorithm optimized to avoid needless clusterhead re-elections for
stable clusters in mobile ad-hoc networks. The proposed localized algorithm
deals with mobility, but does not require geographical, speed or distances
information.



Sublinear Algorithms for Approximating String Compressibility

We raise the question of approximating the compressibility of a string with
respect to a fixed compression scheme, in sublinear time. We study this
question in detail for two popular lossless compression schemes: run-length
encoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for
approximating compressibility with respect to both schemes. We also give
several lower bounds that show that our algorithms for both schemes cannot be
improved significantly.
  Our investigation of LZ yields results whose interest goes beyond the initial
questions we set out to study. In particular, we prove combinatorial structural
lemmas that relate the compressibility of a string with respect to Lempel-Ziv
to the number of distinct short substrings contained in it. In addition, we
show that approximating the compressibility with respect to LZ is related to
approximating the support size of a distribution.



On Anomalies in Annotation Systems

Today's computer-based annotation systems implement a wide range of
functionalities that often go beyond those available in traditional
paper-and-pencil annotations. Conceptually, annotation systems are based on
thoroughly investigated psycho-sociological and pedagogical learning theories.
They offer a huge diversity of annotation types that can be placed in textual
as well as in multimedia format. Additionally, annotations can be published or
shared with a group of interested parties via well-organized repositories.
Although highly sophisticated annotation systems exist both conceptually as
well as technologically, we still observe that their acceptance is somewhat
limited. In this paper, we argue that nowadays annotation systems suffer from
several fundamental problems that are inherent in the traditional
paper-and-pencil annotation paradigm. As a solution, we propose to shift the
annotation paradigm for the implementation of annotation system.



Inquiring the Potential of Evoking Small-World Properties for Self-Organizing Communication Networks

Mobile multi-hop ad hoc networks allow establishing local groups of
communicating devices in a self-organizing way. However, in a global setting
such networks fail to work properly due to network partitioning. Providing that
devices are capable of communicating both locally-e.g. using Wi-Fi or
Bluetooth-and additionally also with arbitrary remote devices-e.g. using
GSM/UMTS links-the objective is to find efficient ways of inter-linking
multiple network partitions. Tackling this problem of topology control, we
focus on the class of small-world networks that obey two distinguishing
characteristics: they have a strong local clustering while still retaining a
small average distance between two nodes. This paper reports on results gained
investigating the question if small-world properties are indicative for an
efficient link management in multiple multi-hop ad hoc network partitions.



Asynchronous games: innocence without alternation

The notion of innocent strategy was introduced by Hyland and Ong in order to
capture the interactive behaviour of lambda-terms and PCF programs. An innocent
strategy is defined as an alternating strategy with partial memory, in which
the strategy plays according to its view. Extending the definition to
non-alternating strategies is problematic, because the traditional definition
of views is based on the hypothesis that Opponent and Proponent alternate
during the interaction. Here, we take advantage of the diagrammatic
reformulation of alternating innocence in asynchronous games, in order to
provide a tentative definition of innocence in non-alternating games. The task
is interesting, and far from easy. It requires the combination of true
concurrency and game semantics in a clean and organic way, clarifying the
relationship between asynchronous games and concurrent games in the sense of
Abramsky and Melli\`es. It also requires an interactive reformulation of the
usual acyclicity criterion of linear logic, as well as a directed variant, as a
scheduling criterion.



Cointegration of the Daily Electric Power System Load and the Weather

The paper makes a thermal predictive analysis of the electric power system
security for a day ahead. This predictive analysis is set as a thermal
computation of the expected security. This computation is obtained by
cointegrating the daily electric power systen load and the weather, by finding
the daily electric power system thermodynamics and by introducing tests for
this thermodynamics. The predictive analysis made shows the electricity
consumers' wisdom.



Redesigning Computer-based Learning Environments: Evaluation as Communication

In the field of evaluation research, computer scientists live constantly upon
dilemmas and conflicting theories. As evaluation is differently perceived and
modeled among educational areas, it is not difficult to become trapped in
dilemmas, which reflects an epistemological weakness. Additionally, designing
and developing a computer-based learning scenario is not an easy task.
Advancing further, with end-users probing the system in realistic settings, is
even harder. Computer science research in evaluation faces an immense
challenge, having to cope with contributions from several conflicting and
controversial research fields. We believe that deep changes must be made in our
field if we are to advance beyond the CBT (computer-based training) learning
model and to build an adequate epistemology for this challenge. The first task
is to relocate our field by building upon recent results from philosophy,
psychology, social sciences, and engineering. In this article we locate
evaluation in respect to communication studies. Evaluation presupposes a
definition of goals to be reached, and we suggest that it is, by many means, a
silent communication between teacher and student, peers, and institutional
entities. If we accept that evaluation can be viewed as set of invisible rules
known by nobody, but somehow understood by everybody, we should add
anthropological inquiries to our research toolkit. The paper is organized
around some elements of the social communication and how they convey new
insights to evaluation research for computer and related scientists. We found
some technical limitations and offer discussions on how we relate to technology
at same time we establish expectancies and perceive others work.



A Communication Model for Adaptive Service Provisioning in Hybrid Wireless Networks

Mobile entities with wireless links are able to form a mobile ad-hoc network.
Such an infrastructureless network does not have to be administrated. However,
self-organizing principles have to be applied to deal with upcoming problems,
e.g. information dissemination. These kinds of problems are not easy to tackle,
requiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks
is arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could
eliminate the need for any fixed infrastructure, has been damped. The goal is
to overcome the limitations of pure ad-hoc networks by augmenting them with
instant Internet access, e.g. via integration of UMTS respectively GSM links.
However, this raises multiple questions at the technical as well as the
organizational level. Motivated by characteristics of small-world networks that
describe an efficient network even without central or organized design, this
paper proposes to combine mobile ad-hoc networks and infrastructured networks
to form hybrid wireless networks. One main objective is to investigate how this
approach can reduce the costs of a permanent backbone link and providing in the
same way the benefits of useful information from Internet connectivity or
service providers. For the purpose of bridging between the different types of
networks, an adequate middleware service is the focus of our investigation.
This paper shows our first steps forward to this middleware by introducing the
Injection Communication paradigm as principal concept.



Automatically Restructuring Practice Guidelines using the GEM DTD

This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present a system developed for this task. We show that it
yields good performance when applied to the analysis of French practice
guidelines.



Multimedia Content Distribution in Hybrid Wireless Networks using Weighted Clustering

Fixed infrastructured networks naturally support centralized approaches for
group management and information provisioning. Contrary to infrastructured
networks, in multi-hop ad-hoc networks each node acts as a router as well as
sender and receiver. Some applications, however, requires hierarchical
arrangements that-for practical reasons-has to be done locally and
self-organized. An additional challenge is to deal with mobility that causes
permanent network partitioning and re-organizations. Technically, these
problems can be tackled by providing additional uplinks to a backbone network,
which can be used to access resources in the Internet as well as to inter-link
multiple ad-hoc network partitions, creating a hybrid wireless network. In this
paper, we present a prototypically implemented hybrid wireless network system
optimized for multimedia content distribution. To efficiently manage the ad-hoc
communicating devices a weighted clustering algorithm is introduced. The
proposed localized algorithm deals with mobility, but does not require
geographical information or distances.



Localized Support for Injection Point Election in Hybrid Networks

Ad-hoc networks, a promising trend in wireless technology, fail to work
properly in a global setting. In most cases, self-organization and cost-free
local communication cannot compensate the need for being connected, gathering
urgent information just-in-time. Equipping mobile devices additionally with GSM
or UMTS adapters in order to communicate with arbitrary remote devices or even
a fixed network infrastructure provides an opportunity. Devices that operate as
intermediate nodes between the ad-hoc network and a reliable backbone network
are potential injection points. They allow disseminating received information
within the local neighborhood. The effectiveness of different devices to serve
as injection point differs substantially. For practical reasons the
determination of injection points should be done locally, within the ad-hoc
network partitions. We analyze different localized algorithms using at most
2-hop neighboring information. Results show that devices selected this way
spread information more efficiently through the ad-hoc network. Our results can
also be applied in order to support the election process for clusterheads in
the field of clustering mechanisms.



A taxonomic Approach to Topology Control in Ad-hoc and Wireless Networks

Topology Control (TC) aims at tuning the topology of highly dynamic networks
to provide better control over network resources and to increase the efficiency
of communication. Recently, many TC protocols have been proposed. The protocols
are designed for preserving connectivity, minimizing energy consumption,
maximizing the overall network coverage or network capacity. Each TC protocol
makes different assumptions about the network topology, environment detection
resources, and control capacities. This circumstance makes it extremely
difficult to comprehend the role and purpose of each protocol. To tackle this
situation, a taxonomy for TC protocols is presented throughout this paper.
Additionally, some TC protocols are classified based upon this taxonomy.



The multiple viewpoints as approach to information retrieval within collaborative development context

Nowadays, to achieve competitive advantage, the industrial companies are
considering that success is sustained to great product development. That is to
manage the product throughout its entire lifecycle. Achieving this goal
requires a tight collaboration between actors from a wide variety of domains,
using different software tools producing various product data types and
formats. The actors' collaboration is mainly based on the exchange /share
product information. The representation of the actors' viewpoints is the
underlying requirement of the collaborative product development. The multiple
viewpoints approach was designed to provide an organizational framework
following the actors' perspectives in the collaboration, and their
relationships. The approach acknowledges the inevitability of multiple
integration of product information as different views, promotes gathering of
actors' interest, and encourages retrieved adequate information while providing
support for integration through PLM and/or SCM collaboration. In this paper, a
multiple viewpoints representation is proposed. The product, process,
organization information models are discussed. A series of issues referring to
the viewpoints representation are discussed in detail. Based on XML standard,
taking electrical connector as an example, an application case of part of
product information modeling is stated.



Vector Precoding for Wireless MIMO Systems: A Replica Analysis

We apply the replica method to analyze vector precoding, a method to reduce
transmit power in antenna array communications. The analysis applies to a very
general class of channel matrices. The statistics of the channel matrix enter
the transmitted energy per symbol via its R-transform. We find that vector
precoding performs much better for complex than for real alphabets. As a
byproduct, we find a nonlinear precoding method with polynomial complexity that
outperforms NP-hard Tomlinson-Harashima precoding for binary modulation on
complex channels if the number of transmit antennas is slightly larger than
twice the number of receive antennas.



Collaborative product and process model: Multiple Viewpoints approach

The design and development of complex products invariably involves many
actors who have different points of view on the problem they are addressing,
the product being developed, and the process by which it is being developed.
The actors' viewpoints approach was designed to provide an organisational
framework in which these different perspectives or points of views, and their
relationships, could be explicitly gathered and formatted (by actor activity's
focus). The approach acknowledges the inevitability of multiple interpretation
of product information as different views, promotes gathering of actors'
interests, and encourages retrieved adequate information while providing
support for integration through PLM and/or SCM collaboration. In this paper, we
present our multiple viewpoints approach, and we illustrate it by an industrial
example on cyclone vessel product.



Developing a Collaborative and Autonomous Training and Learning Environment for Hybrid Wireless Networks

With larger memory capacities and the ability to link into wireless networks,
more and more students uses palmtop and handheld computers for learning
activities. However, existing software for Web-based learning is not
well-suited for such mobile devices, both due to constrained user interfaces as
well as communication effort required. A new generation of applications for the
learning domain that is explicitly designed to work on these kinds of small
mobile devices has to be developed. For this purpose, we introduce CARLA, a
cooperative learning system that is designed to act in hybrid wireless
networks. As a cooperative environment, CARLA aims at disseminating teaching
material, notes, and even components of itself through both fixed and mobile
networks to interested nodes. Due to the mobility of nodes, CARLA deals with
upcoming problems such as network partitions and synchronization of teaching
material, resource dependencies, and time constraints.



Temporal Reasoning without Transitive Tables

Representing and reasoning about qualitative temporal information is an
essential part of many artificial intelligence tasks. Lots of models have been
proposed in the litterature for representing such temporal information. All
derive from a point-based or an interval-based framework. One fundamental
reasoning task that arises in applications of these frameworks is given by the
following scheme: given possibly indefinite and incomplete knowledge of the
binary relationships between some temporal objects, find the consistent
scenarii between all these objects. All these models require transitive tables
-- or similarly inference rules-- for solving such tasks. We have defined an
alternative model, S-languages - to represent qualitative temporal information,
based on the only two relations of \emph{precedence} and \emph{simultaneity}.
In this paper, we show how this model enables to avoid transitive tables or
inference rules to handle this kind of problem.



Constructing a maximum utility slate of on-line advertisements

We present an algorithm for constructing an optimal slate of sponsored search
advertisements which respects the ordering that is the outcome of a generalized
second price auction, but which must also accommodate complicating factors such
as overall budget constraints. The algorithm is easily fast enough to use on
the fly for typical problem sizes, or as a subroutine in an overall
optimization.



Opportunistic Network Coding for Video Streaming over Wireless

In this paper, we study video streaming over wireless networks with network
coding capabilities. We build upon recent work, which demonstrated that network
coding can increase throughput over a broadcast medium, by mixing packets from
different flows into a single packet, thus increasing the information content
per transmission. Our key insight is that, when the transmitted flows are video
streams, network codes should be selected so as to maximize not only the
network throughput but also the video quality. We propose video-aware
opportunistic network coding schemes that take into account both (i) the
decodability of network codes by several receivers and (ii) the importance and
deadlines of video packets. Simulation results show that our schemes
significantly improve both video quality and throughput.



Duality and Stability Regions of Multi-rate Broadcast and Multiple Access Networks

We characterize stability regions of two-user fading Gaussian multiple access
(MAC) and broadcast (BC) networks with centralized scheduling. The data to be
transmitted to the users is encoded into codewords of fixed length. The rates
of the codewords used are restricted to a fixed set of finite cardinality. With
successive decoding and interference cancellation at the receivers, we find the
set of arrival rates that can be stabilized over the MAC and BC networks. In
MAC and BC networks with average power constraints, we observe that the duality
property that relates the MAC and BC information theoretic capacity regions
extend to their stability regions as well. In MAC and BC networks with peak
power constraints, the union of stability regions of dual MAC networks is found
to be strictly contained in the BC stability region.



Analyzing Design Process and Experiments on the AnITA Generic Tutoring System

In the field of tutoring systems, investigations have shown that there are
many tutoring systems specific to a specific domain that, because of their
static architecture, cannot be adapted to other domains. As consequence, often
neither methods nor knowledge can be reused. In addition, the knowledge
engineer must have programming skills in order to enhance and evaluate the
system. One particular challenge is to tackle these problems with the
development of a generic tutoring system. AnITA, as a stand-alone application,
has been developed and implemented particularly for this purpose. However, in
the testing phase, we discovered that this architecture did not fully match the
user's intuitive understanding of the use of a learning tool. Therefore, AnITA
has been redesigned to exclusively work as a client/server application and
renamed to AnITA2. This paper discusses the evolvements made on the AnITA
tutoring system, the goal of which is to use generic principles for system
re-use in any domain. Two experiments were conducted, and the results are
presented in this paper.



A Proof of a Recursion for Bessel Moments

We provide a proof of a conjecture in (Bailey, Borwein, Borwein, Crandall
2007) on the existence and form of linear recursions for moments of powers of
the Bessel function $K_0$.



Evolutionary Mesh Numbering: Preliminary Results

Mesh numbering is a critical issue in Finite Element Methods, as the
computational cost of one analysis is highly dependent on the order of the
nodes of the mesh. This paper presents some preliminary investigations on the
problem of mesh numbering using Evolutionary Algorithms. Three conclusions can
be drawn from these experiments. First, the results of the up-to-date method
used in all FEM softwares (Gibb's method) can be consistently improved; second,
none of the crossover operators tried so far (either general or problem
specific) proved useful; third, though the general tendency in Evolutionary
Computation seems to be the hybridization with other methods (deterministic or
heuristic), none of the presented attempt did encounter any success yet. The
good news, however, is that this algorithm allows an improvement over the
standard heuristic method between 12% and 20% for both the 1545 and 5453-nodes
meshes used as test-bed. Finally, some strange interaction between the
selection scheme and the use of problem specific mutation operator was
observed, which appeals for further investigation.



A Generic Model of Contracts for Embedded Systems

We present the mathematical foundations of the contract-based model developed
in the framework of the SPEEDS project. SPEEDS aims at developing methods and
tools to support "speculative design", a design methodology in which
distributed designers develop different aspects of the overall system, in a
concurrent but controlled way. Our generic mathematical model of contract
supports this style of development. This is achieved by focusing on behaviors,
by supporting the notion of "rich component" where diverse (functional and
non-functional) aspects of the system can be considered and combined, by
representing rich components via their set of associated contracts, and by
formalizing the whole process of component composition.



VPSPACE and a transfer theorem over the complex field

We extend the transfer theorem of [KP2007] to the complex field. That is, we
investigate the links between the class VPSPACE of families of polynomials and
the Blum-Shub-Smale model of computation over C. Roughly speaking, a family of
polynomials is in VPSPACE if its coefficients can be computed in polynomial
space. Our main result is that if (uniform, constant-free) VPSPACE families can
be evaluated efficiently then the class PAR of decision problems that can be
solved in parallel polynomial time over the complex field collapses to P. As a
result, one must first be able to show that there are VPSPACE families which
are hard to evaluate in order to separate P from NP over C, or even from PAR.



Optimal Choice of Threshold in Two Level Processor Sharing

We analyze the Two Level Processor Sharing (TLPS) scheduling discipline with
the hyper-exponential job size distribution and with the Poisson arrival
process. TLPS is a convenient model to study the benefit of the file size based
differentiation in TCP/IP networks. In the case of the hyper-exponential job
size distribution with two phases, we find a closed form analytic expression
for the expected sojourn time and an approximation for the optimal value of the
threshold that minimizes the expected sojourn time. In the case of the
hyper-exponential job size distribution with more than two phases, we derive a
tight upper bound for the expected sojourn time conditioned on the job size. We
show that when the variance of the job size distribution increases, the gain in
system performance increases and the sensitivity to the choice of the threshold
near its optimal value decreases.



Detection of Gauss-Markov Random Fields with Nearest-Neighbor Dependency

The problem of hypothesis testing against independence for a Gauss-Markov
random field (GMRF) is analyzed. Assuming an acyclic dependency graph, an
expression for the log-likelihood ratio of detection is derived. Assuming
random placement of nodes over a large region according to the Poisson or
uniform distribution and nearest-neighbor dependency graph, the error exponent
of the Neyman-Pearson detector is derived using large-deviations theory. The
error exponent is expressed as a dependency-graph functional and the limit is
evaluated through a special law of large numbers for stabilizing graph
functionals. The exponent is analyzed for different values of the variance
ratio and correlation. It is found that a more correlated GMRF has a higher
exponent at low values of the variance ratio whereas the situation is reversed
at high values of the variance ratio.



Non-Cooperative Scheduling of Multiple Bag-of-Task Applications

Multiple applications that execute concurrently on heterogeneous platforms
compete for CPU and network resources. In this paper we analyze the behavior of
$K$ non-cooperative schedulers using the optimal strategy that maximize their
efficiency while fairness is ensured at a system level ignoring applications
characteristics. We limit our study to simple single-level master-worker
platforms and to the case where each scheduler is in charge of a single
application consisting of a large number of independent tasks. The tasks of a
given application all have the same computation and communication requirements,
but these requirements can vary from one application to another. In this
context, we assume that each scheduler aims at maximizing its throughput. We
give closed-form formula of the equilibrium reached by such a system and study
its performance. We characterize the situations where this Nash equilibrium is
optimal (in the Pareto sense) and show that even though no catastrophic
situation (Braess-like paradox) can occur, such an equilibrium can be
arbitrarily bad for any classical performance measure.



Relative Strength of Strategy Elimination Procedures

We compare here the relative strength of four widely used procedures on
finite strategic games: iterated elimination of weakly/strictly dominated
strategies by a pure/mixed strategy. A complication is that none of these
procedures is based on a monotonic operator. To deal with this problem we use
'global' versions of these operators.



On the growth of components with non fixed excesses

Denote by an $l$-component a connected graph with $l$ edges more than
vertices. We prove that the expected number of creations of $(l+1)$-component,
by means of adding a new edge to an $l$-component in a randomly growing graph
with $n$ vertices, tends to 1 as $l,n$ tends to $\infty$ but with $l =
o(n^{1/4})$. We also show, under the same conditions on $l$ and $n$, that the
expected number of vertices that ever belong to an $l$-component is $\sim
(12l)^{1/3} n^{2/3}$.



Another Proof of Wright's Inequalities

We present a short way of proving the inequalities obtained by Wright in
[Journal of Graph Theory, 4: 393 - 407 (1980)] concerning the number of
connected graphs with $\ell$ edges more than vertices.



A Methodology for Efficient Space-Time Adapter Design Space Exploration: A Case Study of an Ultra Wide Band Interleaver

This paper presents a solution to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall architecture of the system is significantly affected by
communication architecture, so the designers need specifically optimized
adapters. By explicitly modeling these communications within an effective
graph-theoretic model and analysis framework, we automatically generate an
optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs
a C description of Input/Output data scheduling, and user requirements
(throughput, latency, parallelism...), and formalizes communication constraints
through a Resource Constraints Graph (RCG). The RCG properties enable an
efficient architecture space exploration in order to synthesize a STAR
component. The proposed approach has been tested to design an industrial data
mixing block example: an Ultra-Wideband interleaver.



Information Criteria and Arithmetic Codings : An Illustration on Raw Images

In this paper we give a short theoretical description of the general
predictive adaptive arithmetic coding technique. The links between this
technique and the works of J. Rissanen in the 80's, in particular the BIC
information criterion used in parametrical model selection problems, are
established. We also design lossless and lossy coding techniques of images. The
lossless technique uses a mix between fixed-length coding and arithmetic coding
and provides better compression results than those separate methods. That
technique is also seen to have an interesting application in the domain of
statistics since it gives a data-driven procedure for the non-parametrical
histogram selection problem. The lossy technique uses only predictive adaptive
arithmetic codes and shows how a good choice of the order of prediction might
lead to better results in terms of compression. We illustrate those coding
techniques on a raw grayscale image.



Modeling and analysis using hybrid Petri nets

This paper is devoted to the use of hybrid Petri nets (PNs) for modeling and
control of hybrid dynamic systems (HDS). Modeling, analysis and control of HDS
attract ever more of researchers' attention and several works have been devoted
to these topics. We consider in this paper the extensions of the PN formalism
(initially conceived for modeling and analysis of discrete event systems) in
the direction of hybrid modeling. We present, first, the continuous PN models.
These models are obtained from discrete PNs by the fluidification of the
markings. They constitute the first steps in the extension of PNs toward hybrid
modeling. Then, we present two hybrid PN models, which differ in the class of
HDS they can deal with. The first one is used for deterministic HDS modeling,
whereas the second one can deal with HDS with nondeterministic behavior.
Keywords: Hybrid dynamic systems; D-elementary hybrid Petri nets; Hybrid
automata; Controller synthesis



MacWilliams Identity for Codes with the Rank Metric

The MacWilliams identity, which relates the weight distribution of a code to
the weight distribution of its dual code, is useful in determining the weight
distribution of codes. In this paper, we derive the MacWilliams identity for
linear codes with the rank metric, and our identity has a different form than
that by Delsarte. Using our MacWilliams identity, we also derive related
identities for rank metric codes. These identities parallel the binomial and
power moment identities derived for codes with the Hamming metric.



FreeBSD Mandatory Access Control Usage for Implementing Enterprise Security Policies

FreeBSD was one of the first widely deployed free operating systems to
provide mandatory access control. It supports a number of classic MAC models.
This tutorial paper addresses exploiting this implementation to enforce typical
enterprise security policies of varying complexities.



Le travail collaboratif dans le cadre d'un projet architectural

The analysis of the practices and the tendencies of the users at the time of
the search for information on Internet makes it possible to highlight several
points. The search for information becomes powerful after knowledge of the
typology of the various systems of research. This typology supports the
adoption of a methodology of research which one can characterize by pull
systems, intelligent agents, etc. In addition, the importance of the structure
of the electronic document, correctly elaborated in advance, will support a
higher relevance ratio to find information. In our article, the problems turn
around the study of the behavior of the users in situation of search for
information, as well as the constitution of a pole of documentary resources
within a framework of an architectural project. It is noted that the evolution
of the documentary resources is related to information technologies.



How to measure efficiency?

In the context of applied game theory in networking environments, a number of
concepts have been proposed to measure both efficiency and optimality of
resource allocations, the most famous certainly being the price of anarchy and
the Jain index. Yet, very few have tried to question these measures and compare
them one to another, in a general framework, which is the aim of the present
article.



FIPA-based Interoperable Agent Mobility Proposal

This paper presents a proposal for a flexible agent mobility architecture
based on IEEE-FIPA standards and intended to be one of them. This proposal is a
first step towards interoperable mobility mechanisms, which are needed for
future agent migration between different kinds of platforms. Our proposal is
presented as a flexible and robust architecture that has been successfully
implemented in the JADE and AgentScape platforms. It is based on an open set of
protocols, allowing new protocols and future improvements to be accommodated in
the architecture. With this proposal we demonstrate that a standard
architecture for agent mobility capable of supporting several agent platforms
can be defined and implemented.



Towards understanding and modelling office daily life

Measuring and modeling human behavior is a very complex task. In this paper
we present our initial thoughts on modeling and automatic recognition of some
human activities in an office. We argue that to successfully model human
activities, we need to consider both individual behavior and group dynamics. To
demonstrate these theoretical approaches, we introduce an experimental system
for analyzing everyday activity in our office.



Information-theoretic security without an honest majority

We present six multiparty protocols with information-theoretic security that
tolerate an arbitrary number of corrupt participants. All protocols assume
pairwise authentic private channels and a broadcast channel (in a single case,
we require a simultaneous broadcast channel). We give protocols for veto, vote,
anonymous bit transmission, collision detection, notification and anonymous
message transmission. Not assuming an honest majority, in most cases, a single
corrupt participant can make the protocol abort. All protocols achieve
functionality never obtained before without the use of either computational
assumptions or of an honest majority.



On the Performance Evaluation of Encounter-based Worm Interactions Based on Node Characteristics

An encounter-based network is a frequently disconnected wireless ad-hoc
network requiring nearby neighbors to store and forward data utilizing mobility
and encounters over time. Using traditional approaches such as gateways or
firewalls for deterring worm propagation in encounter-based networks is
inappropriate. Because this type of network is highly dynamic and has no
specific boundary, a distributed counter-worm mechanism is needed. We propose
models for the worm interaction approach that relies upon automated beneficial
worm generation to alleviate problems of worm propagation in such networks. We
study and analyze the impact of key mobile node characteristics including node
cooperation, immunization, on-off behavior on the worm propagations and
interactions. We validate our proposed model using extensive simulations. We
also find that, in addition to immunization, cooperation can reduce the level
of worm infection. Furthermore, on-off behavior linearly impacts only timing
aspect but not the overall infection. Using realistic mobile network
measurements, we find that encounters are non-uniform, the trends are
consistent with the model but the magnitudes are drastically different.
Immunization seems to be the most effective in such scenarios. These findings
provide insight that we hope would aid to develop counter-worm protocols in
future encounter-based networks.



Power Allocation for Discrete-Input Delay-Limited Fading Channels

We consider power allocation algorithms for fixed-rate transmission over
Nakagami-m non-ergodic block-fading channels with perfect transmitter and
receiver channel state information and discrete input signal constellations,
under both short- and long-term power constraints. Optimal power allocation
schemes are shown to be direct applications of previous results in the
literature. We show that the SNR exponent of the optimal short-term scheme is
given by m times the Singleton bound. We also illustrate the significant gains
available by employing long-term power constraints. In particular, we analyze
the optimal long-term solution, showing that zero outage can be achieved
provided that the corresponding short-term SNR exponent with the same system
parameters is strictly greater than one. Conversely, if the short-term SNR
exponent is smaller than one, we show that zero outage cannot be achieved. In
this case, we derive the corresponding long-term SNR exponent as a function of
the Singleton bound. Due to the nature of the expressions involved, the
complexity of optimal schemes may be prohibitive for system implementation. We
therefore propose simple sub-optimal power allocation schemes whose outage
probability performance is very close to the minimum outage probability
obtained by optimal schemes. We also show the applicability of these techniques
to practical systems employing orthogonal frequency division multiplexing.



Critique of Feinstein's Proof that P is not Equal to NP

We examine a proof by Craig Alan Feinstein that P is not equal to NP. We
present counterexamples to claims made in his paper and expose a flaw in the
methodology he uses to make his assertions. The fault in his argument is the
incorrect use of reduction. Feinstein makes incorrect assumptions about the
complexity of a problem based on the fact that there is a more complex problem
that can be used to solve it. His paper introduces the terminology "imaginary
processor" to describe how it is possible to beat the brute force reduction he
offers to solve the Subset-Sum problem. The claims made in the paper would not
be validly established even were imaginary processors to exist.



Getting started in probabilistic graphical models

Probabilistic graphical models (PGMs) have become a popular tool for
computational analysis of biological data in a variety of domains. But, what
exactly are they and how do they work? How can we use PGMs to discover patterns
that are biologically relevant? And to what extent can PGMs help us formulate
new hypotheses that are testable at the bench? This note sketches out some
answers and illustrates the main ideas behind the statistical approach to
biological pattern discovery.



Building Portable Thread Schedulers for Hierarchical Multiprocessors: the BubbleSched Framework

Exploiting full computational power of current more and more hierarchical
multiprocessor machines requires a very careful distribution of threads and
data among the underlying non-uniform architecture. Unfortunately, most
operating systems only provide a poor scheduling API that does not allow
applications to transmit valuable scheduling hints to the system. In a previous
paper, we showed that using a bubble-based thread scheduler can significantly
improve applications' performance in a portable way. However, since
multithreaded applications have various scheduling requirements, there is no
universal scheduler that could meet all these needs. In this paper, we present
a framework that allows scheduling experts to implement and experiment with
customized thread schedulers. It provides a powerful API for dynamically
distributing bubbles among the machine in a high-level, portable, and efficient
way. Several examples show how experts can then develop, debug and tune their
own portable bubble schedulers.



An Efficient OpenMP Runtime System for Hierarchical Arch

Exploiting the full computational power of always deeper hierarchical
multiprocessor machines requires a very careful distribution of threads and
data among the underlying non-uniform architecture. The emergence of multi-core
chips and NUMA machines makes it important to minimize the number of remote
memory accesses, to favor cache affinities, and to guarantee fast completion of
synchronization steps. By using the BubbleSched platform as a threading backend
for the GOMP OpenMP compiler, we are able to easily transpose affinities of
thread teams into scheduling hints using abstractions called bubbles. We then
propose a scheduling strategy suited to nested OpenMP parallelism. The
resulting preliminary performance evaluations show an important improvement of
the speedup on a typical NAS OpenMP benchmark application.



A Finite Semantics of Simply-Typed Lambda Terms for Infinite Runs of<br> Automata

Model checking properties are often described by means of finite automata.
Any particular such automaton divides the set of infinite trees into finitely
many classes, according to which state has an infinite run. Building the full
type hierarchy upon this interpretation of the base type gives a finite
semantics for simply-typed lambda-trees.
  A calculus based on this semantics is proven sound and complete. In
particular, for regular infinite lambda-trees it is decidable whether a given
automaton has a run or not. As regular lambda-trees are precisely recursion
schemes, this decidability result holds for arbitrary recursion schemes of
arbitrary level, without any syntactical restriction.



Efficient Multidimensional Data Redistribution for Resizable Parallel Computations

Traditional parallel schedulers running on cluster supercomputers support
only static scheduling, where the number of processors allocated to an
application remains fixed throughout the execution of the job. This results in
under-utilization of idle system resources thereby decreasing overall system
throughput. In our research, we have developed a prototype framework called
ReSHAPE, which supports dynamic resizing of parallel MPI applications executing
on distributed memory platforms. The resizing library in ReSHAPE includes
support for releasing and acquiring processors and efficiently redistributing
application state to a new set of processors. In this paper, we derive an
algorithm for redistributing two-dimensional block-cyclic arrays from $P$ to
$Q$ processors, organized as 2-D processor grids. The algorithm ensures a
contention-free communication schedule for data redistribution if $P_r \leq
Q_r$ and $P_c \leq Q_c$. In other cases, the algorithm implements circular row
and column shifts on the communication schedule to minimize node contention.



Stability of boundary measures

We introduce the boundary measure at scale r of a compact subset of the
n-dimensional Euclidean space. We show how it can be computed for point clouds
and suggest these measures can be used for feature detection. The main
contribution of this work is the proof a quantitative stability theorem for
boundary measures using tools of convex analysis and geometric measure theory.
As a corollary we obtain a stability result for Federer's curvature measures of
a compact, allowing to compute them from point-cloud approximations of the
compact.



Dualheap Selection Algorithm: Efficient, Inherently Parallel and Somewhat Mysterious

An inherently parallel algorithm is proposed that efficiently performs
selection: finding the K-th largest member of a set of N members. Selection is
a common component of many more complex algorithms and therefore is a widely
studied problem.
  Not much is new in the proposed dualheap selection algorithm: the heap data
structure is from J.W.J.Williams, the bottom-up heap construction is from R.W.
Floyd, and the concept of a two heap data structure is from J.W.J. Williams and
D.E. Knuth. The algorithm's novelty is limited to a few relatively minor
implementation twists: 1) the two heaps are oriented with their roots at the
partition values rather than at the minimum and maximum values, 2)the coding of
one of the heaps (the heap of smaller values) employs negative indexing, and 3)
the exchange phase of the algorithm is similar to a bottom-up heap
construction, but navigates the heap with a post-order tree traversal.
  When run on a single processor, the dualheap selection algorithm's
performance is competitive with quickselect with median estimation, a common
variant of C.A.R. Hoare's quicksort algorithm. When run on parallel processors,
the dualheap selection algorithm is superior due to its subtasks that are
easily partitioned and innately balanced.



Resource control of object-oriented programs

A sup-interpretation is a tool which provides an upper bound on the size of a
value computed by some symbol of a program. Sup-interpretations have shown
their interest to deal with the complexity of first order functional programs.
For instance, they allow to characterize all the functions bitwise computable
in Alogtime. This paper is an attempt to adapt the framework of
sup-interpretations to a fragment of oriented-object programs, including
distinct encodings of numbers through the use of constructor symbols, loop and
while constructs and non recursive methods with side effects. We give a
criterion, called brotherly criterion, which ensures that each brotherly
program computes objects whose size is polynomially bounded by the inputs
sizes.



Space-time coding techniques with bit-interleaved coded modulations for MIMO block-fading channels

The space-time bit-interleaved coded modulation (ST-BICM) is an efficient
technique to obtain high diversity and coding gain on a block-fading MIMO
channel. Its maximum-likelihood (ML) performance is computed under ideal
interleaving conditions, which enables a global optimization taking into
account channel coding. Thanks to a diversity upperbound derived from the
Singleton bound, an appropriate choice of the time dimension of the space-time
coding is possible, which maximizes diversity while minimizing complexity.
Based on the analysis, an optimized interleaver and a set of linear precoders,
called dispersive nucleo algebraic (DNA) precoders are proposed. The proposed
precoders have good performance with respect to the state of the art and exist
for any number of transmit antennas and any time dimension. With turbo codes,
they exhibit a frame error rate which does not increase with frame length.



Pricing American Options for Jump Diffusions by Iterating Optimal Stopping Problems for Diffusions

We approximate the price of the American put for jump diffusions by a
sequence of functions, which are computed iteratively. This sequence converges
to the price function uniformly and exponentially fast. Each element of the
approximating sequence solves an optimal stopping problem for geometric
Brownian motion, and can be numerically computed using the classical finite
difference methods. We prove the convergence of this numerical scheme and
present examples to illustrate its performance.



Interference and Outage in Clustered Wireless Ad Hoc Networks

In the analysis of large random wireless networks, the underlying node
distribution is almost ubiquitously assumed to be the homogeneous Poisson point
process. In this paper, the node locations are assumed to form a Poisson
clustered process on the plane. We derive the distributional properties of the
interference and provide upper and lower bounds for its CCDF. We consider the
probability of successful transmission in an interference limited channel when
fading is modeled as Rayleigh. We provide a numerically integrable expression
for the outage probability and closed-form upper and lower bounds.We show that
when the transmitter-receiver distance is large, the success probability is
greater than that of a Poisson arrangement. These results characterize the
performance of the system under geographical or MAC-induced clustering. We
obtain the maximum intensity of transmitting nodes for a given outage
constraint, i.e., the transmission capacity (of this spatial arrangement) and
show that it is equal to that of a Poisson arrangement of nodes. For the
analysis, techniques from stochastic geometry are used, in particular the
probability generating functional of Poisson cluster processes, the Palm
characterization of Poisson cluster processes and the Campbell-Mecke theorem.



Progresses in the Analysis of Stochastic 2D Cellular Automata: a Study of Asynchronous 2D Minority

Cellular automata are often used to model systems in physics, social
sciences, biology that are inherently asynchronous. Over the past 20 years,
studies have demonstrated that the behavior of cellular automata drastically
changed under asynchronous updates. Still, the few mathematical analyses of
asynchronism focus on one-dimensional probabilistic cellular automata, either
on single examples or on specific classes. As for other classic dynamical
systems in physics, extending known methods from one- to two-dimensional
systems is a long lasting challenging problem.
  In this paper, we address the problem of analysing an apparently simple 2D
asynchronous cellular automaton: 2D Minority where each cell, when fired,
updates to the minority state of its neighborhood. Our experiments reveal that
in spite of its simplicity, the minority rule exhibits a quite complex response
to asynchronism. By focusing on the fully asynchronous regime, we are however
able to describe completely the asymptotic behavior of this dynamics as long as
the initial configuration satisfies some natural constraints. Besides these
technical results, we have strong reasons to believe that our techniques
relying on defining an energy function from the transition table of the
automaton may be extended to the wider class of threshold automata.



Analysis of Inter-Domain Traffic Correlations: Random Matrix Theory Approach

The traffic behavior of University of Louisville network with the
interconnected backbone routers and the number of Virtual Local Area Network
(VLAN) subnets is investigated using the Random Matrix Theory (RMT) approach.
We employ the system of equal interval time series of traffic counts at all
router to router and router to subnet connections as a representation of the
inter-VLAN traffic. The cross-correlation matrix C of the traffic rate changes
between different traffic time series is calculated and tested against
null-hypothesis of random interactions.
  The majority of the eigenvalues \lambda_{i} of matrix C fall within the
bounds predicted by the RMT for the eigenvalues of random correlation matrices.
The distribution of eigenvalues and eigenvectors outside of the RMT bounds
displays prominent and systematic deviations from the RMT predictions.
Moreover, these deviations are stable in time.
  The method we use provides a unique possibility to accomplish three
concurrent tasks of traffic analysis. The method verifies the uncongested state
of the network, by establishing the profile of random interactions. It
recognizes the system-specific large-scale interactions, by establishing the
profile of stable in time non-random interactions. Finally, by looking into the
eigenstatistics we are able to detect and allocate anomalies of network traffic
interactions.



Abstract machines for dialogue games

The notion of abstract Boehm tree has arisen as an operationally-oriented
distillation of works on game semantics, and has been investigated in two
papers. This paper revisits the notion, providing more syntactic support and
more examples (like call-by-value evaluation) illustrating the generality of
the underlying computing device. Precise correspondences between various
formulations of the evaluation mechanism of abstract Boehm trees are
established.



A new lower bound on the independence number of a graph

For a given connected graph G on n vertices and m edges, we prove that its
independence number is at least (2m+n+2-sqrt(sqr(2m+n+2)-16sqr(n)))/8.



Decisive Markov Chains

We consider qualitative and quantitative verification problems for
infinite-state Markov chains. We call a Markov chain decisive w.r.t. a given
set of target states F if it almost certainly eventually reaches either F or a
state from which F can no longer be reached. While all finite Markov chains are
trivially decisive (for every set F), this also holds for many classes of
infinite Markov chains. Infinite Markov chains which contain a finite attractor
are decisive w.r.t. every set F. In particular, this holds for probabilistic
lossy channel systems (PLCS). Furthermore, all globally coarse Markov chains
are decisive. This class includes probabilistic vector addition systems (PVASS)
and probabilistic noisy Turing machines (PNTM). We consider both safety and
liveness problems for decisive Markov chains, i.e., the probabilities that a
given set of states F is eventually reached or reached infinitely often,
respectively. 1. We express the qualitative problems in abstract terms for
decisive Markov chains, and show an almost complete picture of its decidability
for PLCS, PVASS and PNTM. 2. We also show that the path enumeration algorithm
of Iyer and Narasimha terminates for decisive Markov chains and can thus be
used to solve the approximate quantitative safety problem. A modified variant
of this algorithm solves the approximate quantitative liveness problem. 3.
Finally, we show that the exact probability of (repeatedly) reaching F cannot
be effectively expressed (in a uniform way) in Tarski-algebra for either PLCS,
PVASS or (P)NTM.



Randomness Extraction via Delta-Biased Masking in the Presence of a Quantum Attacker

Randomness extraction is of fundamental importance for information-theoretic
cryptography. It allows to transform a raw key about which an attacker has some
limited knowledge into a fully secure random key, on which the attacker has
essentially no information. Up to date, only very few randomness-extraction
techniques are known to work against an attacker holding quantum information on
the raw key. This is very much in contrast to the classical (non-quantum)
setting, which is much better understood and for which a vast amount of
different techniques are known and proven to work.
  We prove a new randomness-extraction technique, which is known to work in the
classical setting, to be secure against a quantum attacker as well. Randomness
extraction is done by XOR'ing a so-called delta-biased mask to the raw key. Our
result allows to extend the classical applications of this extractor to the
quantum setting. We discuss the following two applications. We show how to
encrypt a long message with a short key, information-theoretically secure
against a quantum attacker, provided that the attacker has enough quantum
uncertainty on the message. This generalizes the concept of entropically-secure
encryption to the case of a quantum attacker. As second application, we show
how to do error-correction without leaking partial information to a quantum
attacker. Such a technique is useful in settings where the raw key may contain
errors, since standard error-correction techniques may provide the attacker
with information on, say, a secret key that was used to obtain the raw key.



Algorithms for Omega-Regular Games with Imperfect Information

We study observation-based strategies for two-player turn-based games on
graphs with omega-regular objectives. An observation-based strategy relies on
imperfect information about the history of a play, namely, on the past sequence
of observations. Such games occur in the synthesis of a controller that does
not see the private state of the plant. Our main results are twofold. First, we
give a fixed-point algorithm for computing the set of states from which a
player can win with a deterministic observation-based strategy for any
omega-regular objective. The fixed point is computed in the lattice of
antichains of state sets. This algorithm has the advantages of being directed
by the objective and of avoiding an explicit subset construction on the game
graph. Second, we give an algorithm for computing the set of states from which
a player can win with probability 1 with a randomized observation-based
strategy for a Buechi objective. This set is of interest because in the absence
of perfect information, randomized strategies are more powerful than
deterministic ones. We show that our algorithms are optimal by proving matching
lower bounds.



The Complexity of Determining Existence a Hamiltonian Cycle is $O(n^3)$

The Hamiltonian cycle problem in digraph is mapped into a matching cover
bipartite graph. Based on this mapping, it is proved that determining existence
a Hamiltonian cycle in graph is $O(n^3)$.



A Design Methodology for Space-Time Adapter

This paper presents a solution to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall architecture of the system is significantly affected by
communication architecture, so the designers need specifically optimized
adapters. By explicitly modeling these communications within an effective
graph-theoretic model and analysis framework, we automatically generate an
optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs
a C description of Input/Output data scheduling, and user requirements
(throughput, latency, parallelism...), and formalizes communication constraints
through a Resource Constraints Graph (RCG). The RCG properties enable an
efficient architecture space exploration in order to synthesize a STAR
component. The proposed approach has been tested to design an industrial data
mixing block example: an Ultra-Wideband interleaver.



Abstract Storage Devices

A quantum storage device differs radically from a conventional physical
storage device. Its state can be set to any value in a certain (infinite) state
space, but in general every possible read operation yields only partial
information about the stored state.
  The purpose of this paper is to initiate the study of a combinatorial
abstraction, called abstract storage device (ASD), which models deterministic
storage devices with the property that only partial information about the state
can be read, but that there is a degree of freedom as to which partial
information should be retrieved.
  This concept leads to a number of interesting problems which we address, like
the reduction of one device to another device, the equivalence of devices,
direct products of devices, as well as the factorization of a device into
primitive devices. We prove that every ASD has an equivalent ASD with minimal
number of states and of possible read operations. Also, we prove that the
reducibility problem for ASD's is NP-complete, that the equivalence problem is
at least as hard as the graph isomorphism problem, and that the factorization
into binary-output devices (if it exists) is unique.



A Survey of Unix Init Schemes

In most modern operating systems, init (as in "initialization") is the
program launched by the kernel at boot time. It runs as a daemon and typically
has PID 1. Init is responsible for spawning all other processes and scavenging
zombies. It is also responsible for reboot and shutdown operations. This
document describes existing solutions that implement the init process and/or
init scripts in Unix-like systems. These solutions range from the legacy and
still-in-use BSD and SystemV schemes, to recent and promising schemes from
Ubuntu, Apple, Sun and independent developers. Our goal is to highlight their
focus and compare their sets of features.



Dirty-paper Coding without Channel Information at the Transmitter and Imperfect Estimation at the Receiver

In this paper, we examine the effects of imperfect channel estimation at the
receiver and no channel knowledge at the transmitter on the capacity of the
fading Costa's channel with channel state information non-causally known at the
transmitter. We derive the optimal Dirty-paper coding (DPC) scheme and its
corresponding achievable rates with the assumption of Gaussian inputs. Our
results, for uncorrelated Rayleigh fading, provide intuitive insights on the
impact of the channel estimate and the channel characteristics (e.g. SNR,
fading process, channel training) on the achievable rates. These are useful in
practical scenarios of multiuser wireless communications (e.g. Broadcast
Channels) and information embedding applications (e.g. robust watermarking). We
also studied optimal training design adapted to each application. We provide
numerical results for a single-user fading Costa's channel with
maximum-likehood (ML) channel estimation. These illustrate an interesting
practical trade-off between the amount of training and its impact to the
interference cancellation performance using DPC scheme.



Extraction d'entit\'es dans des collections \'evolutives

The goal of our work is to use a set of reports and extract named entities,
in our case the names of Industrial or Academic partners. Starting with an
initial list of entities, we use a first set of documents to identify syntactic
patterns that are then validated in a supervised learning phase on a set of
annotated documents. The complete collection is then explored. This approach is
similar to the ones used in data extraction from semi-structured documents
(wrappers) and do not need any linguistic resources neither a large set for
training. As our collection of documents would evolve over years, we hope that
the performance of the extraction would improve with the increased size of the
training set.



On the Outage Capacity of a Practical Decoder Using Channel Estimation Accuracy

The optimal decoder achieving the outage capacity under imperfect channel
estimation is investigated. First, by searching into the family of nearest
neighbor decoders, which can be easily implemented on most practical coded
modulation systems, we derive a decoding metric that minimizes the average of
the transmission error probability over all channel estimation errors. This
metric, for arbitrary memoryless channels, achieves the capacity of a composite
(more noisy) channel. Next, according to the notion of estimation-induced
outage capacity (EIO capacity) introduced in our previous work, we characterize
maximal achievable information rates associated to the proposed decoder. The
performance of the proposed decoding metric over uncorrelated Rayleigh fading
MIMO channels is compared to both the classical mismatched maximum-likelihood
(ML) decoder and the theoretical limits given by the EIO capacity (i.e. the
best decoder in presence of channel estimation errors). Numerical results show
that the derived metric provides significant gains, in terms of achievable
information rates and bit error rate (BER), in a bit interleaved coded
modulation (BICM) framework, without introducing any additional decoding
complexity.



M\'ethodologie de mod\'elisation et d'impl\'ementation d'adaptateurs spatio-temporels

The re-use of pre-designed blocks is a well-known concept of the software
development. This technique has been applied to System-on-Chip (SoC) design
whose complexity and heterogeneity are growing. The re-use is made thanks to
high level components, called virtual components (IP), available in more or
less flexible forms. These components are dedicated blocks: digital signal
processing (DCT, FFT), telecommunications (Viterbi, TurboCodes),... These
blocks rest on a model of fixed architecture with very few degrees of
personalization. This rigidity is particularly true for the communication
interface whose orders of acquisition and production of data, the temporal
behavior and protocols of exchanges are fixed. The successful integration of
such an IP requires that the designer (1) synchronizes the components (2)
converts the protocols between "incompatible" blocks (3) temporizes the data to
guarantee the temporal constraints and the order of the data. This phase
remains however very manual and source of errors. Our approach proposes a
formal modeling, based on an original Ressource Compatibility Graph. The
synthesis flow is based on a set of transformations of the initial graph to
lead to an interface architecture allowing the space-time adaptation of the
data exchanges between several components.



Cache Analysis of Non-uniform Distribution Sorting Algorithms

We analyse the average-case cache performance of distribution sorting
algorithms in the case when keys are independently but not necessarily
uniformly distributed. The analysis is for both `in-place' and `out-of-place'
distribution sorting algorithms and is more accurate than the analysis
presented in \cite{RRESA00}. In particular, this new analysis yields tighter
upper and lower bounds when the keys are drawn from a uniform distribution.
  We use this analysis to tune the performance of the integer sorting algorithm
MSB radix sort when it is used to sort independent uniform floating-point
numbers (floats). Our tuned MSB radix sort algorithm comfortably outperforms a
cache-tuned implementations of bucketsort \cite{RR99} and Quicksort when
sorting uniform floats from $[0, 1)$.



Variations on Kak's Three Stage Quantum Cryptography Protocol

This paper introduces a variation on Kak's three-stage quanutm key
distribution protocol which allows for defence against the man in the middle
attack. In addition, we introduce a new protocol, which also offers similar
resiliance against such an attack.



Dualheap Sort Algorithm: An Inherently Parallel Generalization of Heapsort

A generalization of the heapsort algorithm is proposed. At the expense of
about 50% more comparison and move operations for typical cases, the dualheap
sort algorithm offers several advantages over heapsort: improved cache
performance, better performance if the input happens to be already sorted, and
easier parallel implementations.



Capacity Scaling for MIMO Two-Way Relaying

A multiple input multiple output (MIMO) two-way relay channel is considered,
where two sources want to exchange messages with each other using multiple
relay nodes, and both the sources and relay nodes are equipped with multiple
antennas. Both the sources are assumed to have equal number of antennas and
have perfect channel state information (CSI) for all the channels of the MIMO
two-way relay channel, whereas, each relay node is either assumed to have CSI
for its transmit and receive channel (the coherent case) or no CSI for any of
the channels (the non-coherent case). The main results in this paper are on the
scaling behavior of the capacity region of the MIMO two-way relay channel with
increasing number of relay nodes. In the coherent case, the capacity region of
the MIMO two-way relay channel is shown to scale linearly with the number of
antennas at source nodes and logarithmically with the number of relay nodes. In
the non-coherent case, the capacity region is shown to scale linearly with the
number of antennas at the source nodes and logarithmically with the signal to
noise ratio.



Reducing the Error Floor

We discuss how the loop calculus approach of [Chertkov, Chernyak '06],
enhanced by the pseudo-codeword search algorithm of [Chertkov, Stepanov '06]
and the facet-guessing idea from [Dimakis, Wainwright '06], improves decoding
of graph based codes in the error-floor domain. The utility of the new, Linear
Programming based, decoding is demonstrated via analysis and simulations of the
model $[155,64,20]$ code.



Outage Behavior of Discrete Memoryless Channels Under Channel Estimation Errors

Classically, communication systems are designed assuming perfect channel
state information at the receiver and/or transmitter. However, in many
practical situations, only an estimate of the channel is available that differs
from the true channel. We address this channel mismatch scenario by using the
notion of estimation-induced outage capacity, for which we provide an
associated coding theorem and its strong converse, assuming a discrete
memoryless channel. We illustrate our ideas via numerical simulations for
transmissions over Ricean fading channels under a quality of service (QoS)
constraint using rate-limited feedback channel and maximum likelihood (ML)
channel estimation. Our results provide intuitive insights on the impact of the
channel estimate and the channel characteristics (SNR, Ricean K-factor,
training sequence length, feedback rate, etc.) on the mean outage capacity.



Remote laboratories: new technology and standard based architecture

E-Laboratories are important components of e- learning environments,
especially in scientific and technical disciplines. First widespread E-Labs
consisted in proposing simulations of real systems (virtual labs), as building
remote labs (remote control of real systems) was difficult by lack of
industrial standards and common protocols. Nowadays, robotics and automation
technologies make easier the interfacing of systems with computers. In this
frame, many researchers (such as those mentioned in [1]) focus on how to set up
such a remote control. But, only a few of them deal with the educational point
of view of the problem. This paper outlines our current research and reflection
about remote laboratory modelling.



A Generic Deployment Framework for Grid Computing and Distributed Applications

Deployment of distributed applications on large systems, and especially on
grid infrastructures, becomes a more and more complex task. Grid users spend a
lot of time to prepare, install and configure middleware and application
binaries on nodes, and eventually start their applications. The problem is that
the deployment process is composed of many heterogeneous tasks that have to be
orchestrated in a specific correct order. As a consequence, the automatization
of the deployment process is currently very difficult to reach. To address this
problem, we propose in this paper a generic deployment framework allowing to
automatize the execution of heterogeneous tasks composing the whole deployment
process. Our approach is based on a reification as software components of all
required deployment mechanisms or existing tools. Grid users only have to
describe the configuration to deploy in a simple natural language instead of
programming or scripting how the deployment process is executed. As a toy
example, this framework is used to deploy CORBA component-based applications
and OpenCCM middleware on one thousand nodes of the French Grid5000
infrastructure.



Application of a design space exploration tool to enhance interleaver generation

This paper presents a methodology to efficiently explore the design space of
communication adapters. In most digital signal processing (DSP) applications,
the overall performance of the system is significantly affected by
communication architectures, as a consequence the designers need specifically
optimized adapters. By explicitly modeling these communications within an
effective graph-theoretic model and analysis framework, we automatically
generate an optimized architecture, named Space-Time AdapteR (STAR). Our design
flow inputs a C description of Input/Output data scheduling, and user
requirements (throughput, latency, parallelism...), and formalizes
communication constraints through a Resource Constraints Graph (RCG). Design
space exploration is then performed through associated tools, to synthesize a
STAR component under time-to-market constraints. The proposed approach has been
tested to design an industrial data mixing block example: an Ultra-Wideband
interleaver.



N-Body Simulations on GPUs

Commercial graphics processors (GPUs) have high compute capacity at very low
cost, which makes them attractive for general purpose scientific computing. In
this paper we show how graphics processors can be used for N-body simulations
to obtain improvements in performance over current generation CPUs. We have
developed a highly optimized algorithm for performing the O(N^2) force
calculations that constitute the major part of stellar and molecular dynamics
simulations. In some of the calculations, we achieve sustained performance of
nearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to
specialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the
cost. Furthermore, the wide availability of GPUs has significant implications
for cluster computing and distributed computing efforts like Folding@Home.



On the Performance of Joint Fingerprint Embedding and Decryption Scheme

Till now, few work has been done to analyze the performances of joint
fingerprint embedding and decryption schemes. In this paper, the security of
the joint fingerprint embedding and decryption scheme proposed by Kundur et al.
is analyzed and improved. The analyses include the security against
unauthorized customer, the security against authorized customer, the
relationship between security and robustness, the relationship between
secu-rity and imperceptibility and the perceptual security. Based these
analyses, some means are proposed to strengthen the system, such as multi-key
encryp-tion and DC coefficient encryption. The method can be used to analyze
other JFD schemes. It is expected to provide valuable information to design JFD
schemes.



Group Testing with Random Pools: optimal two-stage algorithms

We study Probabilistic Group Testing of a set of N items each of which is
defective with probability p. We focus on the double limit of small defect
probability, p<<1, and large number of variables, N>>1, taking either p->0
after $N\to\infty$ or $p=1/N^{\beta}$ with $\beta\in(0,1/2)$. In both settings
the optimal number of tests which are required to identify with certainty the
defectives via a two-stage procedure, $\bar T(N,p)$, is known to scale as
$Np|\log p|$. Here we determine the sharp asymptotic value of $\bar
T(N,p)/(Np|\log p|)$ and construct a class of two-stage algorithms over which
this optimal value is attained. This is done by choosing a proper bipartite
regular graph (of tests and variable nodes) for the first stage of the
detection. Furthermore we prove that this optimal value is also attained on
average over a random bipartite graph where all variables have the same degree,
while the tests have Poisson-distributed degrees. Finally, we improve the
existing upper and lower bound for the optimal number of tests in the case
$p=1/N^{\beta}$ with $\beta\in[1/2,1)$.



Closed-Form Density of States and Localization Length for a Non-Hermitian Disordered System

We calculate the Lyapunov exponent for the non-Hermitian Zakharov-Shabat
eigenvalue problem corresponding to the attractive non-linear Schroedinger
equation with a Gaussian random pulse as initial value function. Using an
extension of the Thouless formula to non-Hermitian random operators, we
calculate the corresponding average density of states. We analyze two cases,
one with circularly symmetric complex Gaussian pulses and the other with real
Gaussian pulses. We discuss the implications in the context of the information
transmission through non-linear optical fibers.



EasyVoice: Integrating voice synthesis with Skype

This paper presents EasyVoice, a system that integrates voice synthesis with
Skype. EasyVoice allows a person with voice disabilities to talk with another
person located anywhere in the world, removing an important obstacle that
affect these people during a phone or VoIP-based conversation.



WiFi Epidemiology: Can Your Neighbors' Router Make Yours Sick?

In densely populated urban areas WiFi routers form a tightly interconnected
proximity network that can be exploited as a substrate for the spreading of
malware able to launch massive fraudulent attack and affect entire urban areas
WiFi networks. In this paper we consider several scenarios for the deployment
of malware that spreads solely over the wireless channel of major urban areas
in the US. We develop an epidemiological model that takes into consideration
prevalent security flaws on these routers. The spread of such a contagion is
simulated on real-world data for geo-referenced wireless routers. We uncover a
major weakness of WiFi networks in that most of the simulated scenarios show
tens of thousands of routers infected in as little time as two weeks, with the
majority of the infections occurring in the first 24 to 48 hours. We indicate
possible containment and prevention measure to limit the eventual harm of such
an attack.



Une s\'emantique observationnelle du mod\`ele des bo\^ites pour la r\'esolution de programmes logiques (version \'etendue)

This report specifies an observational semantics and gives an original
presentation of the Byrd's box model. The approach accounts for the semantics
of Prolog tracers independently of a particular implementation. Traces are, in
general, considered as rather obscure and difficult to use. The proposed formal
presentation of a trace constitutes a simple and pedagogical approach for
teaching Prolog or for implementing Prolog tracers. It constitutes a form of
declarative specification for the tracers. Our approach highlights qualities of
the box model which made its success, but also its drawbacks and limits. As a
matter of fact, the presented semantics is only one example to illustrate
general problems relating to tracers and observing processes. Observing
processes know, from observed processes, only their traces. The issue is then
to be able to reconstitute by the sole analysis of the trace the main part of
the observed process, and if possible, without any loss of information.



A solution for actors' viewpoints representation with collaborative product development

As product complexity and marketing competition increase, a collaborative
product development is necessary for companies which develop high quality
products in short lead-times. To support product actors from different fields,
disciplines, and locations, wishing to exchange and share information, the
representation of the actors' viewpoints is the underlying requirement of the
collaborative product development. The actors' viewpoints approach was designed
to provide an organisational framework following the actors' perspectives in
the collaboration, and their relationships, could be explicitly gathered and
formatted. The approach acknowledges the inevitability of multiple integration
of product information as different views, promotes gathering of actors'
interests, and encourages retrieved adequate information while providing
support for integration through PLM and/or SCM collaboration. In this paper, a
solution for neutral viewpoints representation is proposed. The product,
process, and organisation information models are seriatim discussed. A series
of issues referring to the viewpoints representation are discussed in detail.
Based on XML standard, taking cyclone vessel as an example, an application case
of part of product information modelling is stated.



Asymptotic Analysis of General Multiuser Detectors in MIMO DS-CDMA Channels

We analyze a MIMO DS-CDMA channel with a general multiuser detector including
a nonlinear multiuser detector, using the replica method. In the many-user,
limit the MIMO DS-CDMA channel with the multiuser detector is decoupled into a
bank of single-user SIMO Gaussian channels if a spatial spreading scheme is
employed. On the other hand, it is decoupled into a bank of single-user MIMO
Gaussian channels if a spatial spreading scheme is not employed. The spectral
efficiency of the MIMO DS-CDMA channel with the spatial spreading scheme is
comparable with that of the MIMO DS-CDMA channel using an optimal space-time
block code without the spatial spreading scheme. In the case of the QPSK data
modulation scheme the spectral efficiency of the MIMO DS-CDMA channel with the
MMSE detector shows {\it waterfall} behavior and is very close to the
corresponding sum capacity when the system load is just below the transition
point of the {\it waterfall} behavior. Our result implies that the performance
of a multiuser detector taking the data modulation scheme into consideration
can be far superior to that of linear multiuser detectors.



A tutorial on conformal prediction

Conformal prediction uses past experience to determine precise levels of
confidence in new predictions. Given an error probability $\epsilon$, together
with a method that makes a prediction $\hat{y}$ of a label $y$, it produces a
set of labels, typically containing $\hat{y}$, that also contains $y$ with
probability $1-\epsilon$. Conformal prediction can be applied to any method for
producing $\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge
regression, etc.
  Conformal prediction is designed for an on-line setting in which labels are
predicted successively, each one being revealed before the next is predicted.
The most novel and valuable feature of conformal prediction is that if the
successive examples are sampled independently from the same distribution, then
the successive predictions will be right $1-\epsilon$ of the time, even though
they are based on an accumulating dataset rather than on independent datasets.
  In addition to the model under which successive examples are sampled
independently, other on-line compression models can also use conformal
prediction. The widely used Gaussian linear model is one of these.
  This tutorial presents a self-contained account of the theory of conformal
prediction and works through several numerical examples. A more comprehensive
treatment of the topic is provided in "Algorithmic Learning in a Random World",
by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).



Lower bounds on the minimum average distance of binary codes

New lower bounds on the minimum average Hamming distance of binary codes are
derived. The bounds are obtained using linear programming approach.



A simple generalization of the ElGamal cryptosystem to non-abelian groups II

This is a study of the MOR cryptosystem using the special linear group over
finite fields. The automorphism group of the special linear group is analyzed
for this purpose. At our current state of knowledge, I show that the MOR
cryptosystem has better security than the ElGamal cryptosystem over finite
fields.



A Sequent Calculus for Modelling Interferences

A logic calculus is presented that is a conservative extension of linear
logic. The motivation beneath this work concerns lazy evaluation, true
concurrency and interferences in proof search. The calculus includes two new
connectives to deal with multisequent structures and has the cut-elimination
property. Extensions are proposed that give first results concerning our
objectives.



Optimal Replica Placement in Tree Networks with QoS and Bandwidth Constraints and the Closest Allocation Policy

This paper deals with the replica placement problem on fully homogeneous tree
networks known as the Replica Placement optimization problem. The client
requests are known beforehand, while the number and location of the servers are
to be determined. We investigate the latter problem using the Closest access
policy when adding QoS and bandwidth constraints. We propose an optimal
algorithm in two passes using dynamic programming.



On Canonical Forms of Complete Problems via First-order Projections

The class of problems complete for NP via first-order reductions is known to
be characterized by existential second-order sentences of a fixed form. All
such sentences are built around the so-called generalized IS-form of the
sentence that defines Independent-Set. This result can also be understood as
that every sentence that defines a NP-complete problem P can be decomposed in
two disjuncts such that the first one characterizes a fragment of P as hard as
Independent-Set and the second the rest of P. That is, a decomposition that
divides every such sentence into a quotient and residue modulo Independent-Set.
  In this paper, we show that this result can be generalized over a wide
collection of complexity classes, including the so-called nice classes.
Moreover, we show that such decomposition can be done for any complete problem
with respect to the given class, and that two such decompositions are
non-equivalent in general. Interestingly, our results are based on simple and
well-known properties of first-order reductions.ow that this result can be
generalized over a wide collection of complexity classes, including the
so-called nice classes. Moreover, we show that such decomposition can be done
for any complete problem with respect to the given class, and that two such
decompositions are non-equivalent in general. Interestingly, our results are
based on simple and well-known properties of first-order reductions.



The Impact of Channel Feedback on Opportunistic Relay Selection for Hybrid-ARQ in Wireless Networks

This paper presents a decentralized relay selection protocol for a dense
wireless network and describes channel feedback strategies that improve its
performance. The proposed selection protocol supports hybrid
automatic-repeat-request transmission where relays forward parity information
to the destination in the event of a decoding error. Channel feedback is
employed for refining the relay selection process and for selecting an
appropriate transmission mode in a proposed adaptive modulation transmission
framework. An approximation of the throughput of the proposed adaptive
modulation strategy is presented, and the dependence of the throughput on
system parameters such as the relay contention probability and the adaptive
modulation switching point is illustrated via maximization of this
approximation. Simulations show that the throughput of the proposed selection
strategy is comparable to that yielded by a centralized selection approach that
relies on geographic information.



NP by means of lifts and shadows

We show that every NP problem is polynomially equivalent to a simple
combinatorial problem: the membership problem for a special class of digraphs.
These classes are defined by means of shadows (projections) and by finitely
many forbidden colored (lifted) subgraphs. Our characterization is motivated by
the analysis of syntactical subclasses with the full computational power of NP,
which were first studied by Feder and Vardi.
  Our approach applies to many combinatorial problems and it induces the
characterization of coloring problems (CSP) defined by means of shadows. This
turns out to be related to homomorphism dualities. We prove that a class of
digraphs (relational structures) defined by finitely many forbidden colored
subgraphs (i.e. lifted substructures) is a CSP class if and only if all the the
forbidden structures are homomorphically equivalent to trees. We show a
surprising richness of coloring problems when restricted to most frequent graph
classes. Using results of Ne\v{s}et\v{r}il and Ossona de Mendez for bounded
expansion classes (which include bounded degree and proper minor closed
classes) we prove that the restriction of every class defined as the shadow of
finitely many colored subgraphs equals to the restriction of a coloring (CSP)
class.



Hypocomputation

Hypercomputational formal theories will, clearly, be both structurally and
foundationally different from the formal theories underpinning computational
theories. However, many of the maps that might guide us into this strange realm
have been lost. So little work has been done recently in the area of
metamathematics, and so many of the previous results have been folded into
other theories, that we are in danger of loosing an appreciation of the broader
structure of formal theories. As an aid to those looking to develop
hypercomputational theories, we will briefly survey the known landmarks both
inside and outside the borders of computational theory. We will not focus in
this paper on why the structure of formal theory looks the way it does. Instead
we will focus on what this structure looks like, moving from hypocomputational,
through traditional computational theories, and then beyond to
hypercomputational theories.



Tight Bounds on the Average Length, Entropy, and Redundancy of Anti-Uniform Huffman Codes

In this paper we consider the class of anti-uniform Huffman codes and derive
tight lower and upper bounds on the average length, entropy, and redundancy of
such codes in terms of the alphabet size of the source. The Fibonacci
distributions are introduced which play a fundamental role in AUH codes. It is
shown that such distributions maximize the average length and the entropy of
the code for a given alphabet size. Another previously known bound on the
entropy for given average length follows immediately from our results.



Approximately-Universal Space-Time Codes for the Parallel, Multi-Block and Cooperative-Dynamic-Decode-and-Forward Channels

Explicit codes are constructed that achieve the diversity-multiplexing gain
tradeoff of the cooperative-relay channel under the dynamic decode-and-forward
protocol for any network size and for all numbers of transmit and receive
antennas at the relays.
  A particularly simple code construction that makes use of the Alamouti code
as a basic building block is provided for the single relay case.
  Along the way, we prove that space-time codes previously constructed in the
literature for the block-fading and parallel channels are approximately
universal, i.e., they achieve the DMT for any fading distribution. It is shown
how approximate universality of these codes leads to the first DMT-optimum code
construction for the general, MIMO-OFDM channel.



There Exist some Omega-Powers of Any Borel Rank

Omega-powers of finitary languages are languages of infinite words
(omega-languages) in the form V^omega, where V is a finitary language over a
finite alphabet X. They appear very naturally in the characterizaton of regular
or context-free omega-languages. Since the set of infinite words over a finite
alphabet X can be equipped with the usual Cantor topology, the question of the
topological complexity of omega-powers of finitary languages naturally arises
and has been posed by Niwinski (1990), Simonnet (1992) and Staiger (1997). It
has been recently proved that for each integer n > 0, there exist some
omega-powers of context free languages which are Pi^0_n-complete Borel sets,
that there exists a context free language L such that L^omega is analytic but
not Borel, and that there exists a finitary language V such that V^omega is a
Borel set of infinite rank. But it was still unknown which could be the
possible infinite Borel ranks of omega-powers. We fill this gap here, proving
the following very surprising result which shows that omega-powers exhibit a
great topological complexity: for each non-null countable ordinal alpha, there
exist some Sigma^0_alpha-complete omega-powers, and some Pi^0_alpha-complete
omega-powers.



stdchk: A Checkpoint Storage System for Desktop Grid Computing

Checkpointing is an indispensable technique to provide fault tolerance for
long-running high-throughput applications like those running on desktop grids.
This paper argues that a dedicated checkpoint storage system, optimized to
operate in these environments, can offer multiple benefits: reduce the load on
a traditional file system, offer high-performance through specialization, and,
finally, optimize data management by taking into account checkpoint application
semantics. Such a storage system can present a unifying abstraction to
checkpoint operations, while hiding the fact that there are no dedicated
resources to store the checkpoint data. We prototype stdchk, a checkpoint
storage system that uses scavenged disk space from participating desktops to
build a low-cost storage system, offering a traditional file system interface
for easy integration with applications. This paper presents the stdchk
architecture, key performance optimizations, support for incremental
checkpointing, and increased data availability. Our evaluation confirms that
the stdchk approach is viable in a desktop grid setting and offers a low cost
storage system with desirable performance characteristics: high write
throughput and reduced storage space and network effort to save checkpoint
images.



Experimental Algorithm for the Maximum Independent Set Problem

We develop an experimental algorithm for the exact solving of the maximum
independent set problem. The algorithm consecutively finds the maximal
independent sets of vertices in an arbitrary undirected graph such that the
next such set contains more elements than the preceding one. For this purpose,
we use a technique, developed by Ford and Fulkerson for the finite partially
ordered sets, in particular, their method for partition of a poset into the
minimum number of chains with finding the maximum antichain. In the process of
solving, a special digraph is constructed, and a conjecture is formulated
concerning properties of such digraph. This allows to offer of the solution
algorithm. Its theoretical estimation of running time equals to is $O(n^{8})$,
where $n$ is the number of graph vertices. The offered algorithm was tested by
a program on random graphs. The testing the confirms correctness of the
algorithm.



A Collection of Definitions of Intelligence

This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is.



Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers Taking Values in R^Q

Bounds on the risk play a crucial role in statistical learning theory. They
usually involve as capacity measure of the model studied the VC dimension or
one of its extensions. In classification, such "VC dimensions" exist for models
taking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations
appropriate for the missing case, the one of models with values in R^Q. This
provides us with a new guaranteed risk for M-SVMs which appears superior to the
existing one.



Optimal Constellations for the Low SNR Noncoherent MIMO Block Rayleigh Fading Channel

Reliable communication over the discrete-input/continuous-output noncoherent
multiple-input multiple-output (MIMO) Rayleigh block fading channel is
considered when the signal-to-noise ratio (SNR) per degree of freedom is low.
Two key problems are posed and solved to obtain the optimum discrete input. In
both problems, the average and peak power per space-time slot of the input
constellation are constrained. In the first one, the peak power to average
power ratio (PPAPR) of the input constellation is held fixed, while in the
second problem, the peak power is fixed independently of the average power. In
the first PPAPR-constrained problem, the mutual information, which grows as
O(SNR^2), is maximized up to second order in SNR. In the second
peak-constrained problem, where the mutual information behaves as O(SNR), the
structure of constellations that are optimal up to first order, or
equivalently, that minimize energy/bit, are explicitly characterized.
Furthermore, among constellations that are first-order optimal, those that
maximize the mutual information up to second order, or equivalently, the
wideband slope, are characterized. In both PPAPR-constrained and
peak-constrained problems, the optimal constellations are obtained in
closed-form as solutions to non-convex optimizations, and interestingly, they
are found to be identical. Due to its special structure, the common solution is
referred to as Space Time Orthogonal Rank one Modulation, or STORM. In both
problems, it is seen that STORM provides a sharp characterization of the
behavior of noncoherent MIMO capacity.



Order-Invariant MSO is Stronger than Counting MSO in the Finite

We compare the expressiveness of two extensions of monadic second-order logic
(MSO) over the class of finite structures. The first, counting monadic
second-order logic (CMSO), extends MSO with first-order modulo-counting
quantifiers, allowing the expression of queries like ``the number of elements
in the structure is even''. The second extension allows the use of an
additional binary predicate, not contained in the signature of the queried
structure, that must be interpreted as an arbitrary linear order on its
universe, obtaining order-invariant MSO.
  While it is straightforward that every CMSO formula can be translated into an
equivalent order-invariant MSO formula, the converse had not yet been settled.
Courcelle showed that for restricted classes of structures both order-invariant
MSO and CMSO are equally expressive, but conjectured that, in general,
order-invariant MSO is stronger than CMSO.
  We affirm this conjecture by presenting a class of structures that is
order-invariantly definable in MSO but not definable in CMSO.



Pruning Processes and a New Characterization of Convex Geometries

We provide a new characterization of convex geometries via a multivariate
version of an identity that was originally proved by Maneva, Mossel and
Wainwright for certain combinatorial objects arising in the context of the
k-SAT problem. We thus highlight the connection between various
characterizations of convex geometries and a family of removal processes
studied in the literature on random structures.



Secure Nested Codes for Type II Wiretap Channels

This paper considers the problem of secure coding design for a type II
wiretap channel, where the main channel is noiseless and the eavesdropper
channel is a general binary-input symmetric-output memoryless channel. The
proposed secure error-correcting code has a nested code structure. Two secure
nested coding schemes are studied for a type II Gaussian wiretap channel. The
nesting is based on cosets of a good code sequence for the first scheme and on
cosets of the dual of a good code sequence for the second scheme. In each case,
the corresponding achievable rate-equivocation pair is derived based on the
threshold behavior of good code sequences. The two secure coding schemes
together establish an achievable rate-equivocation region, which almost covers
the secrecy capacity-equivocation region in this case study. The proposed
secure coding scheme is extended to a type II binary symmetric wiretap channel.
A new achievable perfect secrecy rate, which improves upon the previously
reported result by Thangaraj et al., is derived for this channel.



Multiple Access Channels with Generalized Feedback and Confidential Messages

This paper considers the problem of secret communication over a multiple
access channel with generalized feedback. Two trusted users send independent
confidential messages to an intended receiver, in the presence of a passive
eavesdropper. In this setting, an active cooperation between two trusted users
is enabled through using channel feedback in order to improve the communication
efficiency. Based on rate-splitting and decode-and-forward strategies,
achievable secrecy rate regions are derived for both discrete memoryless and
Gaussian channels. Results show that channel feedback improves the achievable
secrecy rates.



Dynamic Exploration of Networks: from general principles to the traceroute process

Dynamical processes taking place on real networks define on them evolving
subnetworks whose topology is not necessarily the same of the underlying one.
We investigate the problem of determining the emerging degree distribution,
focusing on a class of tree-like processes, such as those used to explore the
Internet's topology. A general theory based on mean-field arguments is
proposed, both for single-source and multiple-source cases, and applied to the
specific example of the traceroute exploration of networks. Our results provide
a qualitative improvement in the understanding of dynamical sampling and of the
interplay between dynamics and topology in large networks like the Internet.



Java Components Vulnerabilities - An Experimental Classification Targeted at the OSGi Platform

The OSGi Platform finds a growing interest in two different applications
domains: embedded systems, and applications servers. However, the security
properties of this platform are hardly studied, which is likely to hinder its
use in production systems. This is all the more important that the dynamic
aspect of OSGi-based applications, that can be extended at runtime, make them
vulnerable to malicious code injection. We therefore perform a systematic audit
of the OSGi platform so as to build a vulnerability catalog that intends to
reference OSGi Vulnerabilities originating in the Core Specification, and in
behaviors related to the use of the Java language. Standard Services are not
considered. To support this audit, a Semi-formal Vulnerability Pattern is
defined, that enables to uniquely characterize fundamental properties for each
vulnerability, to include verbose description in the pattern, to reference
known security protections, and to track the implementation status of the
proof-of-concept OSGi Bundles that exploit the vulnerability. Based on the
analysis of the catalog, a robust OSGi Platform is built, and recommendations
are made to enhance the OSGi Specifications.



Design of optimal convolutional codes for joint decoding of correlated sources in wireless sensor networks

We consider a wireless sensors network scenario where two nodes detect
correlated sources and deliver them to a central collector via a wireless link.
Differently from the Slepian-Wolf approach to distributed source coding, in the
proposed scenario the sensing nodes do not perform any pre-compression of the
sensed data. Original data are instead independently encoded by means of
low-complexity convolutional codes. The decoder performs joint decoding with
the aim of exploiting the inherent correlation between the transmitted sources.
Complexity at the decoder is kept low thanks to the use of an iterative joint
decoding scheme, where the output of each decoder is fed to the other decoder's
input as a-priori information. For such scheme, we derive a novel analytical
framework for evaluating an upper bound of joint-detection packet error
probability and for deriving the optimum coding scheme. Experimental results
confirm the validity of the analytical framework, and show that recursive codes
allow a noticeable performance gain with respect to non-recursive coding
schemes. Moreover, the proposed recursive coding scheme allows to approach the
ideal Slepian-Wolf scheme performance in AWGN channel, and to clearly
outperform it over fading channels on account of diversity gain due to
correlation of information.



Opportunistic Scheduling and Beamforming for MIMO-SDMA Downlink Systems with Linear Combining

Opportunistic scheduling and beamforming schemes are proposed for multiuser
MIMO-SDMA downlink systems with linear combining in this work. Signals received
from all antennas of each mobile terminal (MT) are linearly combined to improve
the {\em effective} signal-to-noise-interference ratios (SINRs). By exploiting
limited feedback on the effective SINRs, the base station (BS) schedules
simultaneous data transmission on multiple beams to the MTs with the largest
effective SINRs. Utilizing the extreme value theory, we derive the asymptotic
system throughputs and scaling laws for the proposed scheduling and beamforming
schemes with different linear combining techniques. Computer simulations
confirm that the proposed schemes can substantially improve the system
throughput.



Minimum Sum Edge Colorings of Multicycles

In the minimum sum edge coloring problem, we aim to assign natural numbers to
edges of a graph, so that adjacent edges receive different numbers, and the sum
of the numbers assigned to the edges is minimum. The {\em chromatic edge
strength} of a graph is the minimum number of colors required in a minimum sum
edge coloring of this graph. We study the case of multicycles, defined as
cycles with parallel edges, and give a closed-form expression for the chromatic
edge strength of a multicycle, thereby extending a theorem due to Berge. It is
shown that the minimum sum can be achieved with a number of colors equal to the
chromatic index. We also propose simple algorithms for finding a minimum sum
edge coloring of a multicycle. Finally, these results are generalized to a
large family of minimum cost coloring problems.



Approximations of Lovasz extensions and their induced interaction index

The Lovasz extension of a pseudo-Boolean function $f : \{0,1\}^n \to R$ is
defined on each simplex of the standard triangulation of $[0,1]^n$ as the
unique affine function $\hat f : [0,1]^n \to R$ that interpolates $f$ at the
$n+1$ vertices of the simplex. Its degree is that of the unique multilinear
polynomial that expresses $f$. In this paper we investigate the least squares
approximation problem of an arbitrary Lovasz extension $\hat f$ by Lovasz
extensions of (at most) a specified degree. We derive explicit expressions of
these approximations. The corresponding approximation problem for
pseudo-Boolean functions was investigated by Hammer and Holzman (1992) and then
solved explicitly by Grabisch, Marichal, and Roubens (2000), giving rise to an
alternative definition of Banzhaf interaction index. Similarly we introduce a
new interaction index from approximations of $\hat f$ and we present some of
its properties. It turns out that its corresponding power index identifies with
the power index introduced by Grabisch and Labreuche (2001).



Bid Optimization for Internet Graphical Ad Auction Systems via Special Ordered Sets

This paper describes an optimization model for setting bid levels for certain
types of advertisements on web pages. This model is non-convex, but we are able
to obtain optimal or near-optimal solutions rapidly using branch and cut
open-source software. The financial benefits obtained using the prototype
system have been substantial.



A Comparison of Push and Pull Techniques for Ajax

Ajax applications are designed to have high user interactivity and low
user-perceived latency. Real-time dynamic web data such as news headlines,
stock tickers, and auction updates need to be propagated to the users as soon
as possible. However, Ajax still suffers from the limitations of the Web's
request/response architecture which prevents servers from pushing real-time
dynamic web data. Such applications usually use a pull style to obtain the
latest updates, where the client actively requests the changes based on a
predefined interval. It is possible to overcome this limitation by adopting a
push style of interaction where the server broadcasts data when a change occurs
on the server side. Both these options have their own trade-offs. This paper
explores the fundamental limits of browser-based applications and analyzes push
solutions for Ajax technology. It also shows the results of an empirical study
comparing push and pull.



End-to-End Available Bandwidth Measurement Tools : A Comparative Evaluation of Performances

In recent years, there has been a strong interest in measuring the available
bandwidth of network paths. Several methods and techniques have been proposed
and various measurement tools have been developed and evaluated. However, there
have been few comparative studies with regards to the actual performance of
these tools. This paper presents a study of available bandwidth measurement
techniques and undertakes a comparative analysis in terms of accuracy,
intrusiveness and response time of active probing tools. Finally, measurement
errors and the uncertainty of the tools are analysed and overall conclusions
made.



Multi-criteria scheduling of pipeline workflows

Mapping workflow applications onto parallel platforms is a challenging
problem, even for simple application patterns such as pipeline graphs. Several
antagonist criteria should be optimized, such as throughput and latency (or a
combination). In this paper, we study the complexity of the bi-criteria mapping
problem for pipeline graphs on communication homogeneous platforms. In
particular, we assess the complexity of the well-known chains-to-chains problem
for different-speed processors, which turns out to be NP-hard. We provide
several efficient polynomial bi-criteria heuristics, and their relative
performance is evaluated through extensive simulations.



Self-Stabilizing Wavelets and r-Hops Coordination

We introduce a simple tool called the wavelet (or, r-wavelet) scheme.
Wavelets deals with coordination among processes which are at most r hops away
of each other. We present a selfstabilizing solution for this scheme. Our
solution requires no underlying structure and works in arbritrary anonymous
networks, i.e., no process identifier is required. Moreover, our solution works
under any (even unfair) daemon. Next, we use the wavelet scheme to design
self-stabilizing layer clocks. We show that they provide an efficient device in
the design of local coordination problems at distance r, i.e., r-barrier
synchronization and r-local resource allocation (LRA) such as r-local mutual
exclusion (LME), r-group mutual exclusion (GME), and r-Reader/Writers. Some
solutions to the r-LRA problem (e.g., r-LME) also provide transformers to
transform algorithms written assuming any r-central daemon into algorithms
working with any distributed daemon.



Encounter-based worms: Analysis and Defense

Encounter-based network is a frequently-disconnected wireless ad-hoc network
requiring immediate neighbors to store and forward aggregated data for
information disseminations. Using traditional approaches such as gateways or
firewalls for deterring worm propagation in encounter-based networks is
inappropriate. We propose the worm interaction approach that relies upon
automated beneficial worm generation aiming to alleviate problems of worm
propagations in such networks. To understand the dynamic of worm interactions
and its performance, we mathematically model worm interactions based on major
worm interaction factors including worm interaction types, network
characteristics, and node characteristics using ordinary differential equations
and analyze their effects on our proposed metrics. We validate our proposed
model using extensive synthetic and trace-driven simulations. We find that, all
worm interaction factors significantly affect the pattern of worm propagations.
For example, immunization linearly decreases the infection of susceptible nodes
while on-off behavior only impacts the duration of infection. Using realistic
mobile network measurements, we find that encounters are bursty, multi-group
and non-uniform. The trends from the trace-driven simulations are consistent
with the model, in general. Immunization and timely deployment seem to be the
most effective to counter the worm attacks in such scenarios while cooperation
may help in a specific case. These findings provide insight that we hope would
aid to develop counter-worm protocols in future encounter-based networks.



Scheduling multiple divisible loads on a linear processor network

Min, Veeravalli, and Barlas have recently proposed strategies to minimize the
overall execution time of one or several divisible loads on a heterogeneous
linear network, using one or more installments. We show on a very simple
example that their approach does not always produce a solution and that, when
it does, the solution is often suboptimal. We also show how to find an optimal
schedule for any instance, once the number of installments per load is given.
Then, we formally state that any optimal schedule has an infinite number of
installments under a linear cost model as the one assumed in the original
papers. Therefore, such a cost model cannot be used to design practical
multi-installment strategies. Finally, through extensive simulations we
confirmed that the best solution is always produced by the linear programming
approach, while solutions of the original papers can be far away from the
optimal.



PSPACE Bounds for Rank-1 Modal Logics

For lack of general algorithmic methods that apply to wide classes of logics,
establishing a complexity bound for a given modal logic is often a laborious
task. The present work is a step towards a general theory of the complexity of
modal logics. Our main result is that all rank-1 logics enjoy a shallow model
property and thus are, under mild assumptions on the format of their
axiomatisation, in PSPACE. This leads to a unified derivation of tight
PSPACE-bounds for a number of logics including K, KD, coalition logic, graded
modal logic, majority logic, and probabilistic modal logic. Our generic
algorithm moreover finds tableau proofs that witness pleasant proof-theoretic
properties including a weak subformula property. This generality is made
possible by a coalgebraic semantics, which conveniently abstracts from the
details of a given model class and thus allows covering a broad range of logics
in a uniform way.



Getting More From Your Multicore: Exploiting OpenMP From An Open Source Numerical Scripting Language

We introduce SLIRP, a module generator for the S-Lang numerical scripting
language, with a focus on its vectorization capabilities. We demonstrate how
both SLIRP and S-Lang were easily adapted to exploit the inherent parallelism
of high-level mathematical languages with OpenMP, allowing general users to
employ tightly-coupled multiprocessors in scriptable research calculations
while requiring no special knowledge of parallel programming. Motivated by
examples in the ISIS astrophysical modeling & analysis tool, performance
figures are presented for several machine and compiler configurations,
demonstrating beneficial speedups for real-world operations.



Some Quantitative Aspects of Fractional Computability

Motivated by results on generic-case complexity in group theory, we apply the
ideas of effective Baire category and effective measure theory to study
complexity classes of functions which are "fractionally computable" by a
partial algorithm. For this purpose it is crucial to specify an allowable
effective density, $\delta$, of convergence for a partial algorithm. The set
$\mathcal{FC}(\delta)$ consists of all total functions $ f: \Sigma^\ast \to
\{0,1 \}$ where $\Sigma$ is a finite alphabet with $|\Sigma| \ge 2$ which are
"fractionally computable at density $\delta$". The space $\mathcal{FC}(\delta)
$ is effectively of the second category while any fractional complexity class,
defined using $\delta$ and any computable bound $\beta$ with respect to an
abstract Blum complexity measure, is effectively meager. A remarkable result of
Kautz and Miltersen shows that relative to an algorithmically random oracle
$A$, the relativized class $\mathcal{NP}^A$ does not have effective polynomial
measure zero in $\mathcal{E}^A$, the relativization of strict exponential time.
We define the class $\mathcal{UFP}^A$ of all languages which are fractionally
decidable in polynomial time at ``a uniform rate'' by algorithms with an oracle
for $A$. We show that this class does have effective polynomial measure zero in
$\mathcal{E}^A$ for every oracle $A$. Thus relaxing the requirement of
polynomial time decidability to hold only for a fraction of possible inputs
does not compensate for the power of nondeterminism in the case of random
oracles.



Radix Sorting With No Extra Space

It is well known that n integers in the range [1,n^c] can be sorted in O(n)
time in the RAM model using radix sorting. More generally, integers in any
range [1,U] can be sorted in O(n sqrt{loglog n}) time. However, these
algorithms use O(n) words of extra memory. Is this necessary?
  We present a simple, stable, integer sorting algorithm for words of size
O(log n), which works in O(n) time and uses only O(1) words of extra memory on
a RAM model. This is the integer sorting case most useful in practice. We
extend this result with same bounds to the case when the keys are read-only,
which is of theoretical interest. Another interesting question is the case of
arbitrary c. Here we present a black-box transformation from any RAM sorting
algorithm to a sorting algorithm which uses only O(1) extra space and has the
same running time. This settles the complexity of in-place sorting in terms of
the complexity of sorting.



The Domino Problem of the Hyperbolic Plane Is Undecidable

In this paper, we prove that the general tiling problem of the hyperbolic
plane is undecidable by proving a slightly stronger version using only a
regular polygon as the basic shape of the tiles. The problem was raised by a
paper of Raphael Robinson in 1971, in his famous simplified proof that the
general tiling problem is undecidable for the Euclidean plane, initially proved
by Robert Berger in 1966.



Hilbert++ Manual

We present here an installation guide, a hand-on mini-tutorial through
examples, and the theoretical foundations of the Hilbert++ code.



Heuristics for Network Coding in Wireless Networks

Multicast is a central challenge for emerging multi-hop wireless
architectures such as wireless mesh networks, because of its substantial cost
in terms of bandwidth. In this report, we study one specific case of multicast:
broadcasting, sending data from one source to all nodes, in a multi-hop
wireless network. The broadcast we focus on is based on network coding, a
promising avenue for reducing cost; previous work of ours showed that the
performance of network coding with simple heuristics is asymptotically optimal:
each transmission is beneficial to nearly every receiver. This is for
homogenous and large networks of the plan. But for small, sparse or for
inhomogeneous networks, some additional heuristics are required. This report
proposes such additional new heuristics (for selecting rates) for broadcasting
with network coding. Our heuristics are intended to use only simple local
topology information. We detail the logic of the heuristics, and with
experimental results, we illustrate the behavior of the heuristics, and
demonstrate their excellent performance.



User driven applications - new design paradigm

Programs for complicated engineering and scientific tasks always have to deal
with a problem of showing numerous graphical results. The limits of the screen
space and often opposite requirements from different users are the cause of the
infinite discussions between designers and users, but the source of this
ongoing conflict is not in the level of interface design, but in the basic
principle of current graphical output: user may change some views and details,
but in general the output view is absolutely defined and fixed by the
developer. Author was working for several years on the algorithm that will
allow eliminating this problem thus allowing stepping from designer-driven
applications to user-driven. Such type of applications in which user is
deciding what, when and how to show on the screen, is the dream of scientists
and engineers working on the analysis of the most complicated tasks. The new
paradigm is based on movable and resizable graphics, and such type of graphics
can be widely used not only for scientific and engineering applications.



Unison as a Self-Stabilizing Wave Stream Algorithm in Asynchronous Anonymous Networks

How to pass from local to global scales in anonymous networks? How to
organize a selfstabilizing propagation of information with feedback. From the
Angluin impossibility results, we cannot elect a leader in a general anonymous
network. Thus, it is impossible to build a rooted spanning tree. Many problems
can only be solved by probabilistic methods. In this paper we show how to use
Unison to design a self-stabilizing barrier synchronization in an anonymous
network. We show that the commuication structure of this barrier
synchronization designs a self-stabilizing wave-stream, or pipelining wave, in
anonymous networks. We introduce two variants of Wave: the strong waves and the
wavelets. A strong wave can be used to solve the idempotent r-operator
parametrized computation problem. A wavelet deals with k-distance computation.
We show how to use Unison to design a self-stabilizing wave stream, a
self-stabilizing strong wave stream and a self-stabilizing wavelet stream.



Theory of Finite or Infinite Trees Revisited

We present in this paper a first-order axiomatization of an extended theory
$T$ of finite or infinite trees, built on a signature containing an infinite
set of function symbols and a relation $\fini(t)$ which enables to distinguish
between finite or infinite trees. We show that $T$ has at least one model and
prove its completeness by giving not only a decision procedure, but a full
first-order constraint solver which gives clear and explicit solutions for any
first-order constraint satisfaction problem in $T$. The solver is given in the
form of 16 rewriting rules which transform any first-order constraint $\phi$
into an equivalent disjunction $\phi$ of simple formulas such that $\phi$ is
either the formula $\true$ or the formula $\false$ or a formula having at least
one free variable, being equivalent neither to $\true$ nor to $\false$ and
where the solutions of the free variables are expressed in a clear and explicit
way. The correctness of our rules implies the completeness of $T$. We also
describe an implementation of our algorithm in CHR (Constraint Handling Rules)
and compare the performance with an implementation in C++ and that of a recent
decision procedure for decomposable theories.



A Robust Linguistic Platform for Efficient and Domain specific Web Content Analysis

Web semantic access in specific domains calls for specialized search engines
with enhanced semantic querying and indexing capacities, which pertain both to
information retrieval (IR) and to information extraction (IE). A rich
linguistic analysis is required either to identify the relevant semantic units
to index and weight them according to linguistic specific statistical
distribution, or as the basis of an information extraction process. Recent
developments make Natural Language Processing (NLP) techniques reliable enough
to process large collections of documents and to enrich them with semantic
annotations. This paper focuses on the design and the development of a text
processing platform, Ogmios, which has been developed in the ALVIS project. The
Ogmios platform exploits existing NLP modules and resources, which may be tuned
to specific domains and produces linguistically annotated documents. We show
how the three constraints of genericity, domain semantic awareness and
performance can be handled all together.



2-State 3-Symbol Universal Turing Machines Do Not Exist

In this brief note, we give a simple information-theoretic proof that 2-state
3-symbol universal Turing machines cannot possibly exist, unless one loosens
the definition of "universal".



Non-atomic Games for Multi-User Systems

In this contribution, the performance of a multi-user system is analyzed in
the context of frequency selective fading channels. Using game theoretic tools,
a useful framework is provided in order to determine the optimal power
allocation when users know only their own channel (while perfect channel state
information is assumed at the base station). We consider the realistic case of
frequency selective channels for uplink CDMA. This scenario illustrates the
case of decentralized schemes, where limited information on the network is
available at the terminal. Various receivers are considered, namely the Matched
filter, the MMSE filter and the optimum filter. The goal of this paper is to
derive simple expressions for the non-cooperative Nash equilibrium as the
number of mobiles becomes large and the spreading length increases. To that end
two asymptotic methodologies are combined. The first is asymptotic random
matrix theory which allows us to obtain explicit expressions of the impact of
all other mobiles on any given tagged mobile. The second is the theory of
non-atomic games which computes good approximations of the Nash equilibrium as
the number of mobiles grows.



Location and Spectral Estimation of Weak Wave Packets on Noise Background

The method of location and spectral estimation of weak signals on a noise
background is being considered. The method is based on the optimized on order
and noise dispersion autoregressive model of a sought signal. A new approach of
model order determination is being offered. Available estimation of the noise
dispersion is close to the real one. The optimized model allows to define
function of empirical data spectral and dynamic features changes. The analysis
of the signal as dynamic invariant in respect of the linear shift
transformation yields the function of model consistency. Use of these both
functions enables to detect short-time and nonstationary wave packets at signal
to noise ratio as from -20 dB and above.



Selection Relaying at Low Signal to Noise Ratios

Performance of cooperative diversity schemes at Low Signal to Noise Ratios
(LSNR) was recently studied by Avestimehr et. al. [1] who emphasized the
importance of diversity gain over multiplexing gain at low SNRs. It has also
been pointed out that continuous energy transfer to the channel is necessary
for achieving the max-flow min-cut bound at LSNR. Motivated by this we propose
the use of Selection Decode and Forward (SDF) at LSNR and analyze its
performance in terms of the outage probability. We also propose an energy
optimization scheme which further brings down the outage probability.



Directed Feedback Vertex Set is Fixed-Parameter Tractable

We resolve positively a long standing open question regarding the
fixed-parameter tractability of the parameterized Directed Feedback Vertex Set
problem. In particular, we propose an algorithm which solves this problem in
$O(8^kk!*poly(n))$.



A Generalized Sampling Theorem for Frequency Localized Signals

A generalized sampling theorem for frequency localized signals is presented.
The generalization in the proposed model of sampling is twofold: (1) It applies
to various prefilters effecting a "soft" bandlimitation, (2) an approximate
reconstruction from sample values rather than a perfect one is obtained (though
the former might be "practically perfect" in many cases). For an arbitrary
finite-energy signal the frequency localization is performed by a prefilter
realizing a crosscorrelation with a function of prescribed properties. The
range of the filter, the so-called localization space, is described in some
detail. Regular sampling is applied and a reconstruction formula is given. For
the reconstruction error a general error estimate is derived and connections
between a critical sampling interval and notions of "soft bandwidth" for the
prefilter are indicated. Examples based on the sinc-function, Gaussian
functions and B-splines are discussed.



Interference Alignment and the Degrees of Freedom for the K User Interference Channel

While the best known outerbound for the K user interference channel states
that there cannot be more than K/2 degrees of freedom, it has been conjectured
that in general the constant interference channel with any number of users has
only one degree of freedom. In this paper, we explore the spatial degrees of
freedom per orthogonal time and frequency dimension for the K user wireless
interference channel where the channel coefficients take distinct values across
frequency slots but are fixed in time. We answer five closely related
questions. First, we show that K/2 degrees of freedom can be achieved by
channel design, i.e. if the nodes are allowed to choose the best constant,
finite and nonzero channel coefficient values. Second, we show that if channel
coefficients can not be controlled by the nodes but are selected by nature,
i.e., randomly drawn from a continuous distribution, the total number of
spatial degrees of freedom for the K user interference channel is almost surely
K/2 per orthogonal time and frequency dimension. Thus, only half the spatial
degrees of freedom are lost due to distributed processing of transmitted and
received signals on the interference channel. Third, we show that interference
alignment and zero forcing suffice to achieve all the degrees of freedom in all
cases. Fourth, we show that the degrees of freedom $D$ directly lead to an
$\mathcal{O}(1)$ capacity characterization of the form
$C(SNR)=D\log(1+SNR)+\mathcal{O}(1)$ for the multiple access channel, the
broadcast channel, the 2 user interference channel, the 2 user MIMO X channel
and the 3 user interference channel with M>1 antennas at each node. Fifth, we
characterize the degree of freedom benefits from cognitive sharing of messages
on the 3 user interference channel.



Pricing Options on Defaultable Stocks

In this note, we develop stock option price approximations for a model which
takes both the risk o default and the stochastic volatility into account. We
also let the intensity of defaults be influenced by the volatility. We show
that it might be possible to infer the risk neutral default intensity from the
stock option prices. Our option price approximation has a rich implied
volatility surface structure and fits the data implied volatility well. Our
calibration exercise shows that an effective hazard rate from bonds issued by a
company can be used to explain the implied volatility skew of the implied
volatility of the option prices issued by the same company.



Performance Analysis of Publish/Subscribe Systems

The Desktop Grid offers solutions to overcome several challenges and to
answer increasingly needs of scientific computing. Its technology consists
mainly in exploiting resources, geographically dispersed, to treat complex
applications needing big power of calculation and/or important storage
capacity. However, as resources number increases, the need for scalability,
self-organisation, dynamic reconfigurations, decentralisation and performance
becomes more and more essential. Since such properties are exhibited by P2P
systems, the convergence of grid computing and P2P computing seems natural. In
this context, this paper evaluates the scalability and performance of P2P tools
for discovering and registering services. Three protocols are used for this
purpose: Bonjour, Avahi and Free-Pastry. We have studied the behaviour of
theses protocols related to two criteria: the elapsed time for registrations
services and the needed time to discover new services. Our aim is to analyse
these results in order to choose the best protocol we can use in order to
create a decentralised middleware for desktop grid.



Robust Audio Watermarking Against the D/A and A/D conversions

Audio watermarking has played an important role in multimedia security. In
many applications using audio watermarking, D/A and A/D conversions (denoted by
DA/AD in this paper) are often involved. In previous works, however, the
robustness issue of audio watermarking against the DA/AD conversions has not
drawn sufficient attention yet. In our extensive investigation, it has been
found that the degradation of a watermarked audio signal caused by the DA/AD
conversions manifests itself mainly in terms of wave magnitude distortion and
linear temporal scaling, making the watermark extraction failed. Accordingly, a
DWT-based audio watermarking algorithm robust against the DA/AD conversions is
proposed in this paper. To resist the magnitude distortion, the relative energy
relationships among different groups of the DWT coefficients in the
low-frequency sub-band are utilized in watermark embedding by adaptively
controlling the embedding strength. Furthermore, the resynchronization is
designed to cope with the linear temporal scaling. The time-frequency
localization characteristics of DWT are exploited to save the computational
load in the resynchronization. Consequently, the proposed audio watermarking
algorithm is robust against the DA/AD conversions, other common audio
processing manipulations, and the attacks in StirMark Benchmark for Audio,
which has been verified by experiments.



The $k$-anonymity Problem is Hard

The problem of publishing personal data without giving up privacy is becoming
increasingly important. An interesting formalization recently proposed is the
k-anonymity. This approach requires that the rows in a table are clustered in
sets of size at least k and that all the rows in a cluster become the same
tuple, after the suppression of some records. The natural optimization problem,
where the goal is to minimize the number of suppressed entries, is known to be
NP-hard when the values are over a ternary alphabet, k = 3 and the rows length
is unbounded. In this paper we give a lower bound on the approximation factor
that any polynomial-time algorithm can achive on two restrictions of the
problem,namely (i) when the records values are over a binary alphabet and k =
3, and (ii) when the records have length at most 8 and k = 4, showing that
these restrictions of the problem are APX-hard.



Assisted Problem Solving and Decompositions of Finite Automata

A study of assisted problem solving formalized via decompositions of
deterministic finite automata is initiated. The landscape of new types of
decompositions of finite automata this study uncovered is presented. Languages
with various degrees of decomposability between undecomposable and perfectly
decomposable are shown to exist.



Optimal Strategies for Gaussian Jamming in Block-Fading Channels under Delay and Power Constraints

Without assuming any knowledge on source's codebook and its output signals,
we formulate a Gaussian jamming problem in block fading channels as a
two-player zero sum game. The outage probability is adopted as an objective
function, over which transmitter aims at minimization and jammer aims at
maximization by selecting their power control strategies. Optimal power control
strategies for each player are obtained under both short-term and long-term
power constraints. For the latter case, we first prove the non-existence of a
Nash equilibrium, and then provide a complete solution for both maxmin and
minimax problems. Numerical results demonstrate a sharp difference between the
outage probabilities of the minimax and maxmin solutions.



Physical Network Coding in Two-Way Wireless Relay Channels

It has recently been recognized that the wireless networks represent a
fertile ground for devising communication modes based on network coding. A
particularly suitable application of the network coding arises for the two--way
relay channels, where two nodes communicate with each other assisted by using a
third, relay node. Such a scenario enables application of \emph{physical
network coding}, where the network coding is either done (a) jointly with the
channel coding or (b) through physical combining of the communication flows
over the multiple access channel. In this paper we first group the existing
schemes for physical network coding into two generic schemes, termed 3--step
and 2--step scheme, respectively. We investigate the conditions for
maximization of the two--way rate for each individual scheme: (1) the
Decode--and--Forward (DF) 3--step schemes (2) three different schemes with two
steps: Amplify--and--Forward (AF), JDF and Denoise--and--Forward (DNF). While
the DNF scheme has a potential to offer the best two--way rate, the most
interesting result of the paper is that, for some SNR configurations of the
source--relay links, JDF yields identical maximal two--way rate as the upper
bound on the rate for DNF.



Blind Estimation of Multiple Carrier Frequency Offsets

Multiple carrier-frequency offsets (CFO) arise in a distributed antenna
system, where data are transmitted simultaneously from multiple antennas. In
such systems the received signal contains multiple CFOs due to mismatch between
the local oscillators of transmitters and receiver. This results in a
time-varying rotation of the data constellation, which needs to be compensated
for at the receiver before symbol recovery. This paper proposes a new approach
for blind CFO estimation and symbol recovery. The received base-band signal is
over-sampled, and its polyphase components are used to formulate a virtual
Multiple-Input Multiple-Output (MIMO) problem. By applying blind MIMO system
estimation techniques, the system response is estimated and used to
subsequently transform the multiple CFOs estimation problem into many
independent single CFO estimation problems. Furthermore, an initial estimate of
the CFO is obtained from the phase of the MIMO system response. The Cramer-Rao
Lower bound is also derived, and the large sample performance of the proposed
estimator is compared to the bound.



Fractional Power Control for Decentralized Wireless Networks

We consider a new approach to power control in decentralized wireless
networks, termed fractional power control (FPC). Transmission power is chosen
as the current channel quality raised to an exponent -s, where s is a constant
between 0 and 1. The choices s = 1 and s = 0 correspond to the familiar cases
of channel inversion and constant power transmission, respectively. Choosing s
in (0,1) allows all intermediate policies between these two extremes to be
evaluated, and we see that usually neither extreme is ideal. We derive
closed-form approximations for the outage probability relative to a target SINR
in a decentralized (ad hoc or unlicensed) network as well as for the resulting
transmission capacity, which is the number of users/m^2 that can achieve this
SINR on average. Using these approximations, which are quite accurate over
typical system parameter values, we prove that using an exponent of 1/2
minimizes the outage probability, meaning that the inverse square root of the
channel strength is a sensible transmit power scaling for networks with a
relatively low density of interferers. We also show numerically that this
choice of s is robust to a wide range of variations in the network parameters.
Intuitively, s=1/2 balances between helping disadvantaged users while making
sure they do not flood the network with interference.



Precoding for the AWGN Channel with Discrete Interference

For a state-dependent DMC with input alphabet $\mathcal{X}$ and state
alphabet $\mathcal{S}$ where the i.i.d. state sequence is known causally at the
transmitter, it is shown that by using at most
$|\mathcal{X}||\mathcal{S}|-|\mathcal{S}|+1$ out of
$|\mathcal{X}|^{|\mathcal{S}|}$ input symbols of the Shannon's
\emph{associated} channel, the capacity is achievable. As an example of
state-dependent channels with side information at the transmitter, $M$-ary
signal transmission over AWGN channel with additive $Q$-ary interference where
the sequence of i.i.d. interference symbols is known causally at the
transmitter is considered. For the special case where the Gaussian noise power
is zero, a sufficient condition, which is independent of interference, is given
for the capacity to be $\log_2 M$ bits per channel use. The problem of
maximization of the transmission rate under the constraint that the channel
input given any current interference symbol is uniformly distributed over the
channel input alphabet is investigated. For this setting, the general structure
of a communication system with optimal precoding is proposed.



The Role of Time in the Creation of Knowledge

This paper I assume that in humans the creation of knowledge depends on a
discrete time, or stage, sequential decision-making process subjected to a
stochastic, information transmitting environment. For each time-stage, this
environment randomly transmits Shannon type information-packets to the
decision-maker, who examines each of them for relevancy and then determines his
optimal choices. Using this set of relevant information-packets, the
decision-maker adapts, over time, to the stochastic nature of his environment,
and optimizes the subjective expected rate-of-growth of knowledge. The
decision-maker's optimal actions, lead to a decision function that involves,
over time, his view of the subjective entropy of the environmental process and
other important parameters at each time-stage of the process. Using this model
of human behavior, one could create psychometric experiments using computer
simulation and real decision-makers, to play programmed games to measure the
resulting human performance.



Location-Aided Fast Distributed Consensus in Wireless Networks

Existing works on distributed consensus explore linear iterations based on
reversible Markov chains, which contribute to the slow convergence of the
algorithms. It has been observed that by overcoming the diffusive behavior of
reversible chains, certain nonreversible chains lifted from reversible ones mix
substantially faster than the original chains. In this paper, we investigate
the idea of accelerating distributed consensus via lifting Markov chains, and
propose a class of Location-Aided Distributed Averaging (LADA) algorithms for
wireless networks, where nodes' coarse location information is used to
construct nonreversible chains that facilitate distributed computing and
cooperative processing. First, two general pseudo-algorithms are presented to
illustrate the notion of distributed averaging through chain-lifting. These
pseudo-algorithms are then respectively instantiated through one LADA algorithm
on grid networks, and one on general wireless networks. For a $k\times k$ grid
network, the proposed LADA algorithm achieves an $\epsilon$-averaging time of
$O(k\log(\epsilon^{-1}))$. Based on this algorithm, in a wireless network with
transmission range $r$, an $\epsilon$-averaging time of
$O(r^{-1}\log(\epsilon^{-1}))$ can be attained through a centralized algorithm.
Subsequently, we present a fully-distributed LADA algorithm for wireless
networks, which utilizes only the direction information of neighbors to
construct nonreversible chains. It is shown that this distributed LADA
algorithm achieves the same scaling law in averaging time as the centralized
scheme. Finally, we propose a cluster-based LADA (C-LADA) algorithm, which,
requiring no central coordination, provides the additional benefit of reduced
message complexity compared with the distributed LADA algorithm.



Phase space methods and psychoacoustic models in lossy transform coding

I present a method for lossy transform coding of digital audio that uses the
Weyl symbol calculus for constructing the encoding and decoding transformation.
The method establishes a direct connection between a time-frequency
representation of the signal dependent threshold of masked noise and the
encode/decode pair. The formalism also offers a time-frequency measure of
perceptual entropy.



Weighted Popular Matchings

We study the problem of assigning jobs to applicants. Each applicant has a
weight and provides a preference list ranking a subset of the jobs. A matching
M is popular if there is no other matching M' such that the weight of the
applicants who prefer M' over M exceeds the weight of those who prefer M over
M'. This paper gives efficient algorithms to find a popular matching if one
exists.



From Royal Road to Epistatic Road for Variable Length Evolution Algorithm

Although there are some real world applications where the use of variable
length representation (VLR) in Evolutionary Algorithm is natural and suitable,
an academic framework is lacking for such representations. In this work we
propose a family of tunable fitness landscapes based on VLR of genotypes. The
fitness landscapes we propose possess a tunable degree of both neutrality and
epistasis; they are inspired, on the one hand by the Royal Road fitness
landscapes, and the other hand by the NK fitness landscapes. So these
landscapes offer a scale of continuity from Royal Road functions, with
neutrality and no epistasis, to landscapes with a large amount of epistasis and
no redundancy. To gain insight into these fitness landscapes, we first use
standard tools such as adaptive walks and correlation length. Second, we
evaluate the performances of evolutionary algorithms on these landscapes for
various values of the neutral and the epistatic parameters; the results allow
us to correlate the performances with the expected degrees of neutrality and
epistasis.



Determinacy in a synchronous pi-calculus

The S-pi-calculus is a synchronous pi-calculus which is based on the SL
model. The latter is a relaxation of the Esterel model where the reaction to
the absence of a signal within an instant can only happen at the next instant.
In the present work, we present and characterise a compositional semantics of
the S-pi-calculus based on suitable notions of labelled transition system and
bisimulation. Based on this semantic framework, we explore the notion of
determinacy and the related one of (local) confluence.



On a Non-Context-Free Extension of PDL

Over the last 25 years, a lot of work has been done on seeking for decidable
non-regular extensions of Propositional Dynamic Logic (PDL). Only recently, an
expressive extension of PDL, allowing visibly pushdown automata (VPAs) as a
formalism to describe programs, was introduced and proven to have a
satisfiability problem complete for deterministic double exponential time.
Lately, the VPA formalism was extended to so called k-phase multi-stack visibly
pushdown automata (k-MVPAs). Similarly to VPAs, it has been shown that the
language of k-MVPAs have desirable effective closure properties and that the
emptiness problem is decidable. On the occasion of introducing k-MVPAs, it has
been asked whether the extension of PDL with k-MVPAs still leads to a decidable
logic. This question is answered negatively here. We prove that already for the
extension of PDL with 2-phase MVPAs with two stacks satisfiability becomes
\Sigma_1^1-complete.



Optimal Linear Precoding Strategies for Wideband Non-Cooperative Systems based on Game Theory-Part I: Nash Equilibria

In this two-parts paper we propose a decentralized strategy, based on a
game-theoretic formulation, to find out the optimal precoding/multiplexing
matrices for a multipoint-to-multipoint communication system composed of a set
of wideband links sharing the same physical resources, i.e., time and
bandwidth. We assume, as optimality criterion, the achievement of a Nash
equilibrium and consider two alternative optimization problems: 1) the
competitive maximization of mutual information on each link, given constraints
on the transmit power and on the spectral mask imposed by the radio spectrum
regulatory bodies; and 2) the competitive maximization of the transmission
rate, using finite order constellations, under the same constraints as above,
plus a constraint on the average error probability. In Part I of the paper, we
start by showing that the solution set of both noncooperative games is always
nonempty and contains only pure strategies. Then, we prove that the optimal
precoding/multiplexing scheme for both games leads to a channel diagonalizing
structure, so that both matrix-valued problems can be recast in a simpler
unified vector power control game, with no performance penalty. Thus, we study
this simpler game and derive sufficient conditions ensuring the uniqueness of
the Nash equilibrium. Interestingly, although derived under stronger
constraints, incorporating for example spectral mask constraints, our
uniqueness conditions have broader validity than previously known conditions.
Finally, we assess the goodness of the proposed decentralized strategy by
comparing its performance with the performance of a Pareto-optimal centralized
scheme. To reach the Nash equilibria of the game, in Part II, we propose
alternative distributed algorithms, along with their convergence conditions.



Unfolding Orthogonal Terrains

It is shown that every orthogonal terrain, i.e., an orthogonal (right-angled)
polyhedron based on a rectangle that meets every vertical line in a segment,
has a grid unfolding: its surface may be unfolded to a single non-overlapping
piece by cutting along grid edges defined by coordinate planes through every
vertex.



Where are Bottlenecks in NK Fitness Landscapes?

Usually the offspring-parent fitness correlation is used to visualize and
analyze some caracteristics of fitness landscapes such as evolvability. In this
paper, we introduce a more general representation of this correlation, the
Fitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels
in landscape that cause local search process to slow down. For a local search
heuristic such as hill-climbing or simulated annealing, FC allows to visualize
bottleneck and neutrality of landscapes. To confirm the relevance of the FC
representation we show where the bottlenecks are in the well-know NK fitness
landscape and also how to use neutrality information from the FC to combine
some neutral operator with local search heuristic.



Scuba Search : when selection meets innovation

We proposed a new search heuristic using the scuba diving metaphor. This
approach is based on the concept of evolvability and tends to exploit
neutrality in fitness landscape. Despite the fact that natural evolution does
not directly select for evolvability, the basic idea behind the scuba search
heuristic is to explicitly push the evolvability to increase. The search
process switches between two phases: Conquest-of-the-Waters and
Invasion-of-the-Land. A comparative study of the new algorithm and standard
local search heuristics on the NKq-landscapes has shown advantage and limit of
the scuba search. To enlighten qualitative differences between neutral search
processes, the space is changed into a connected graph to visualize the
pathways that the search is likely to follow.



Another view of the Gaussian algorithm

We introduce here a rewrite system in the group of unimodular matrices,
\emph{i.e.}, matrices with integer entries and with determinant equal to $\pm
1$. We use this rewrite system to precisely characterize the mechanism of the
Gaussian algorithm, that finds shortest vectors in a two--dimensional lattice
given by any basis. Putting together the algorithmic of lattice reduction and
the rewrite system theory, we propose a new worst--case analysis of the
Gaussian algorithm. There is already an optimal worst--case bound for some
variant of the Gaussian algorithm due to Vall\'ee \cite {ValGaussRevisit}. She
used essentially geometric considerations. Our analysis generalizes her result
to the case of the usual Gaussian algorithm. An interesting point in our work
is its possible (but not easy) generalization to the same problem in higher
dimensions, in order to exhibit a tight upper-bound for the number of
iterations of LLL--like reduction algorithms in the worst case. Moreover, our
method seems to work for analyzing other families of algorithms. As an
illustration, the analysis of sorting algorithms are briefly developed in the
last section of the paper.



Dial a Ride from k-forest

The k-forest problem is a common generalization of both the k-MST and the
dense-$k$-subgraph problems. Formally, given a metric space on $n$ vertices
$V$, with $m$ demand pairs $\subseteq V \times V$ and a ``target'' $k\le m$,
the goal is to find a minimum cost subgraph that connects at least $k$ demand
pairs. In this paper, we give an $O(\min\{\sqrt{n},\sqrt{k}\})$-approximation
algorithm for $k$-forest, improving on the previous best ratio of
$O(n^{2/3}\log n)$ by Segev & Segev.
  We then apply our algorithm for k-forest to obtain approximation algorithms
for several Dial-a-Ride problems. The basic Dial-a-Ride problem is the
following: given an $n$ point metric space with $m$ objects each with its own
source and destination, and a vehicle capable of carrying at most $k$ objects
at any time, find the minimum length tour that uses this vehicle to move each
object from its source to destination. We prove that an $\alpha$-approximation
algorithm for the $k$-forest problem implies an
$O(\alpha\cdot\log^2n)$-approximation algorithm for Dial-a-Ride. Using our
results for $k$-forest, we get an $O(\min\{\sqrt{n},\sqrt{k}\}\cdot\log^2 n)$-
approximation algorithm for Dial-a-Ride. The only previous result known for
Dial-a-Ride was an $O(\sqrt{k}\log n)$-approximation by Charikar &
Raghavachari; our results give a different proof of a similar approximation
guarantee--in fact, when the vehicle capacity $k$ is large, we give a slight
improvement on their results.



Sphere Lower Bound for Rotated Lattice Constellations in Fading Channels

We study the error probability performance of rotated lattice constellations
in frequency-flat Nakagami-$m$ block-fading channels. In particular, we use the
sphere lower bound on the underlying infinite lattice as a performance
benchmark. We show that the sphere lower bound has full diversity. We observe
that optimally rotated lattices with largest known minimum product distance
perform very close to the lower bound, while the ensemble of random rotations
is shown to lack diversity and perform far from it.



How to use the Scuba Diving metaphor to solve problem with neutrality ?

We proposed a new search heuristic using the scuba diving metaphor. This
approach is based on the concept of evolvability and tends to exploit
neutrality which exists in many real-world problems. Despite the fact that
natural evolution does not directly select for evolvability, the basic idea
behind the scuba search heuristic is to explicitly push evolvability to
increase. A comparative study of the scuba algorithm and standard local search
heuristics has shown the advantage and the limitation of the scuba search. In
order to tune neutrality, we use the NKq fitness landscapes and a family of
travelling salesman problems (TSP) where cities are randomly placed on a
lattice and where travel distance between cities is computed with the Manhattan
metric. In this last problem the amount of neutrality varies with the city
concentration on the grid ; assuming the concentration below one, this TSP
reasonably remains a NP-hard problem.



Clustering and Feature Selection using Sparse Principal Component Analysis

In this paper, we study the application of sparse principal component
analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks
sparse factors, or linear combinations of the data variables, explaining a
maximum amount of variance in the data while having only a limited number of
nonzero coefficients. PCA is often used as a simple clustering technique and
sparse factors allow us here to interpret the clusters in terms of a reduced
set of variables. We begin with a brief introduction and motivation on sparse
PCA and detail our implementation of the algorithm in d'Aspremont et al.
(2005). We then apply these results to some classic clustering and feature
selection problems arising in biology.



Model Selection Through Sparse Maximum Likelihood Estimation

We consider the problem of estimating the parameters of a Gaussian or binary
distribution in such a way that the resulting undirected graphical model is
sparse. Our approach is to solve a maximum likelihood problem with an added
l_1-norm penalty term. The problem as formulated is convex but the memory
requirements and complexity of existing interior point methods are prohibitive
for problems with more than tens of nodes. We present two new algorithms for
solving problems with at least a thousand nodes in the Gaussian case. Our first
algorithm uses block coordinate descent, and can be interpreted as recursive
l_1-norm penalized regression. Our second algorithm, based on Nesterov's first
order method, yields a complexity estimate with a better dependence on problem
size than existing interior point methods. Using a log determinant relaxation
of the log partition function (Wainwright & Jordan (2006)), we show that these
same algorithms can be used to solve an approximate sparse maximum likelihood
problem for the binary case. We test our algorithms on synthetic data, as well
as on gene expression and senate voting records data.



Optimal Solutions for Sparse Principal Component Analysis

Given a sample covariance matrix, we examine the problem of maximizing the
variance explained by a linear combination of the input variables while
constraining the number of nonzero coefficients in this combination. This is
known as sparse principal component analysis and has a wide array of
applications in machine learning and engineering. We formulate a new
semidefinite relaxation to this problem and derive a greedy algorithm that
computes a full set of good solutions for all target numbers of non zero
coefficients, with total complexity O(n^3), where n is the number of variables.
We then use the same relaxation to derive sufficient conditions for global
optimality of a solution, which can be tested in O(n^3) per pattern. We discuss
applications in subset selection and sparse recovery and show on artificial
examples and biological data that our algorithm does provide globally optimal
solutions in many cases.



Workspace Analysis of the Parallel Module of the VERNE Machine

The paper addresses geometric aspects of a spatial three-degree-of-freedom
parallel module, which is the parallel module of a hybrid serial-parallel
5-axis machine tool. This parallel module consists of a moving platform that is
connected to a fixed base by three non-identical legs. Each leg is made up of
one prismatic and two pairs of spherical joint, which are connected in a way
that the combined effects of the three legs lead to an over-constrained
mechanism with complex motion. This motion is defined as a simultaneous
combination of rotation and translation. A method for computing the complete
workspace of the VERNE parallel module for various tool lengths is presented.
An algorithm describing this method is also introduced.



A Multi Interface Grid Discovery System

Discovery Systems (DS) can be considered as entry points for global loosely
coupled distributed systems. An efficient Discovery System in essence increases
the performance, reliability and decision making capability of distributed
systems. With the rapid increase in scale of distributed applications, existing
solutions for discovery systems are fast becoming either obsolete or incapable
of handling such complexity. They are particularly ineffective when handling
service lifetimes and providing up-to-date information, poor at enabling
dynamic service access and they can also impose unwanted restrictions on
interfaces to widely available information repositories. In this paper we
present essential the design characteristics, an implementation and a
performance analysis for a discovery system capable of overcoming these
deficiencies in large, globally distributed environments.



Mobile Computing in Physics Analysis - An Indicator for eScience

This paper presents the design and implementation of a Grid-enabled physics
analysis environment for handheld and other resource-limited computing devices
as one example of the use of mobile devices in eScience. Handheld devices offer
great potential because they provide ubiquitous access to data and
round-the-clock connectivity over wireless links. Our solution aims to provide
users of handheld devices the capability to launch heavy computational tasks on
computational and data Grids, monitor the jobs status during execution, and
retrieve results after job completion. Users carry their jobs on their handheld
devices in the form of executables (and associated libraries). Users can
transparently view the status of their jobs and get back their outputs without
having to know where they are being executed. In this way, our system is able
to act as a high-throughput computing environment where devices ranging from
powerful desktop machines to small handhelds can employ the power of the Grid.
The results shown in this paper are readily applicable to the wider eScience
community.



DIANA Scheduling Hierarchies for Optimizing Bulk Job Scheduling

The use of meta-schedulers for resource management in large-scale distributed
systems often leads to a hierarchy of schedulers. In this paper, we discuss why
existing meta-scheduling hierarchies are sometimes not sufficient for Grid
systems due to their inability to re-organise jobs already scheduled locally.
Such a job re-organisation is required to adapt to evolving loads which are
common in heavily used Grid infrastructures. We propose a peer-to-peer
scheduling model and evaluate it using case studies and mathematical modelling.
We detail the DIANA (Data Intensive and Network Aware) scheduling algorithm and
its queue management system for coping with the load distribution and for
supporting bulk job scheduling. We demonstrate that such a system is beneficial
for dynamic, distributed and self-organizing resource management and can assist
in optimizing load or job distribution in complex Grid infrastructures.



A process algebra based framework for promise theory

We present a process algebra based approach to formalize the interactions of
computing devices such as the representation of policies and the resolution of
conflicts. As an example we specify how promises may be used in coming to an
agreement regarding a simple though practical transportation problem.



Semantic Information Retrieval from Distributed Heterogeneous Data Sources

Information retrieval from distributed heterogeneous data sources remains a
challenging issue. As the number of data sources increases more intelligent
retrieval techniques, focusing on information content and semantics, are
required. Currently ontologies are being widely used for managing semantic
knowledge, especially in the field of bioinformatics. In this paper we describe
an ontology assisted system that allows users to query distributed
heterogeneous data sources by hiding details like location, information
structure, access pattern and semantic structure of the data. Our goal is to
provide an integrated view on biomedical information sources for the
Health-e-Child project with the aim to overcome the lack of sufficient
semantic-based reformulation techniques for querying distributed data sources.
In particular, this paper examines the problem of query reformulation across
biomedical data sources, based on merged ontologies and the underlying
heterogeneous descriptions of the respective data sources.



Experiences of Engineering Grid-Based Medical Software

Objectives: Grid-based technologies are emerging as potential solutions for
managing and collaborating distributed resources in the biomedical domain. Few
examples exist, however, of successful implementations of Grid-enabled medical
systems and even fewer have been deployed for evaluation in practice. The
objective of this paper is to evaluate the use in clinical practice of a
Grid-based imaging prototype and to establish directions for engineering future
medical Grid developments and their subsequent deployment. Method: The
MammoGrid project has deployed a prototype system for clinicians using the Grid
as its information infrastructure. To assist in the specification of the system
requirements (and for the first time in healthgrid applications), use-case
modelling has been carried out in close collaboration with clinicians and
radiologists who had no prior experience of this modelling technique. A
critical qualitative and, where possible, quantitative analysis of the
MammoGrid prototype is presented leading to a set of recommendations from the
delivery of the first deployed Grid-based medical imaging application. Results:
We report critically on the application of software engineering techniques in
the specification and implementation of the MammoGrid project and show that
use-case modelling is a suitable vehicle for representing medical requirements
and for communicating effectively with the clinical community. This paper also
discusses the practical advantages and limitations of applying the Grid to
real-life clinical applications and presents the consequent lessons learned.



Managing Separation of Concerns in Grid Applications Through Architectural Model Transformations

Grids enable the aggregation, virtualization and sharing of massive
heterogeneous and geographically dispersed resources, using files, applications
and storage devices, to solve computation and data intensive problems, across
institutions and countries via temporary collaborations called virtual
organizations (VO). Most implementations result in complex superposition of
software layers, often delivering low quality of service and quality of
applications. As a consequence, Grid-based applications design and development
is increasingly complex, and the use of most classical engineering practices is
unsuccessful. Not only is the development of such applications a
time-consuming, error prone and expensive task, but also the resulting
applications are often hard-coded for specific Grid configurations, platforms
and infra-structures. Having neither guidelines nor rules in the design of a
Grid-based application is a paradox since there are many existing architectural
approaches for distributed computing, which could ease and promote rigorous
engineering methods based on the re-use of software components. It is our
belief that ad-hoc and semi-formal engineer-ing approaches, in current use, are
insufficient to tackle tomorrows Grid develop-ments requirements. Because
Grid-based applications address multi-disciplinary and complex domains (health,
military, scientific computation), their engineering requires rigor and
control. This paper therefore advocates a formal model-driven engineering
process and corresponding design framework and tools for building the next
generation of Grids.



PhantomOS: A Next Generation Grid Operating System

Grid Computing has made substantial advances in the past decade; these are
primarily due to the adoption of standardized Grid middleware. However Grid
computing has not yet become pervasive because of some barriers that we believe
have been caused by the adoption of middleware centric approaches. These
barriers include: scant support for major types of applications such as
interactive applications; lack of flexible, autonomic and scalable Grid
architectures; lack of plug-and-play Grid computing and, most importantly, no
straightforward way to setup and administer Grids. PhantomOS is a project which
aims to address many of these barriers. Its goal is the creation of a user
friendly pervasive Grid computing platform that facilitates the rapid
deployment and easy maintenance of Grids whilst providing support for major
types of applications on Grids of almost any topology. In this paper we present
the detailed system architecture and an overview of its implementation.



The Requirements for Ontologies in Medical Data Integration: A Case Study

Evidence-based medicine is critically dependent on three sources of
information: a medical knowledge base, the patients medical record and
knowledge of available resources, including where appropriate, clinical
protocols. Patient data is often scattered in a variety of databases and may,
in a distributed model, be held across several disparate repositories.
Consequently addressing the needs of an evidence-based medicine community
presents issues of biomedical data integration, clinical interpretation and
knowledge management. This paper outlines how the Health-e-Child project has
approached the challenge of requirements specification for (bio-) medical data
integration, from the level of cellular data, through disease to that of
patient and population. The approach is illuminated through the requirements
elicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three
diseases being studied in the EC-funded Health-e-Child project.



p-Adic Degeneracy of the Genetic Code

Degeneracy of the genetic code is a biological way to minimize effects of the
undesirable mutation changes. Degeneration has a natural description on the
5-adic space of 64 codons $\mathcal{C}_5 (64) = \{n_0 + n_1 5 + n_2 5^2
  : n_i = 1, 2, 3, 4 \} ,$ where $n_i$ are digits related to nucleotides as
follows: C = 1, A = 2, T = U = 3, G = 4. The smallest 5-adic distance between
codons joins them into 16 quadruplets, which under 2-adic distance decay into
32 doublets. p-Adically close codons are assigned to one of 20 amino acids,
which are building blocks of proteins, or code termination of protein
synthesis. We shown that genetic code multiplets are made of the p-adic nearest
codons.



Performance of Linear Field Reconstruction Techniques with Noise and Uncertain Sensor Locations

We consider a wireless sensor network, sampling a bandlimited field,
described by a limited number of harmonics. Sensor nodes are irregularly
deployed over the area of interest or subject to random motion; in addition
sensors measurements are affected by noise. Our goal is to obtain a high
quality reconstruction of the field, with the mean square error (MSE) of the
estimate as performance metric. In particular, we analytically derive the
performance of several reconstruction/estimation techniques based on linear
filtering. For each technique, we obtain the MSE, as well as its asymptotic
expression in the case where the field number of harmonics and the number of
sensors grow to infinity, while their ratio is kept constant. Through numerical
simulations, we show the validity of the asymptotic analysis, even for a small
number of sensors. We provide some novel guidelines for the design of sensor
networks when many parameters, such as field bandwidth, number of sensors,
reconstruction quality, sensor motion characteristics, and noise level of the
measures, have to be traded off.



A New Family of Unitary Space-Time Codes with a Fast Parallel Sphere Decoder Algorithm

In this paper we propose a new design criterion and a new class of unitary
signal constellations for differential space-time modulation for
multiple-antenna systems over Rayleigh flat-fading channels with unknown fading
coefficients. Extensive simulations show that the new codes have significantly
better performance than existing codes. We have compared the performance of our
codes with differential detection schemes using orthogonal design, Cayley
differential codes, fixed-point-free group codes and product of groups and for
the same bit error rate, our codes allow smaller signal to noise ratio by as
much as 10 dB. The design of the new codes is accomplished in a systematic way
through the optimization of a performance index that closely describes the bit
error rate as a function of the signal to noise ratio. The new performance
index is computationally simple and we have derived analytical expressions for
its gradient with respect to constellation parameters. Decoding of the proposed
constellations is reduced to a set of one-dimensional closest point problems
that we solve using parallel sphere decoder algorithms. This decoding strategy
can also improve efficiency of existing codes.



Very fast watermarking by reversible contrast mapping

Reversible contrast mapping (RCM) is a simple integer transform that applies
to pairs of pixels. For some pairs of pixels, RCM is invertible, even if the
least significant bits (LSBs) of the transformed pixels are lost. The data
space occupied by the LSBs is suitable for data hiding. The embedded
information bit-rates of the proposed spatial domain reversible watermarking
scheme are close to the highest bit-rates reported so far. The scheme does not
need additional data compression, and, in terms of mathematical complexity, it
appears to be the lowest complexity one proposed up to now. A very fast lookup
table implementation is proposed. Robustness against cropping can be ensured as
well.



A New Generalization of Chebyshev Inequality for Random Vectors

In this article, we derive a new generalization of Chebyshev inequality for
random vectors. We demonstrate that the new generalization is much less
conservative than the classical generalization.



The Cyborg Astrobiologist: Porting from a wearable computer to the Astrobiology Phone-cam

We have used a simple camera phone to significantly improve an `exploration
system' for astrobiology and geology. This camera phone will make it much
easier to develop and test computer-vision algorithms for future planetary
exploration. We envision that the `Astrobiology Phone-cam' exploration system
can be fruitfully used in other problem domains as well.



On the Minimum Number of Transmissions in Single-Hop Wireless Coding Networks

The advent of network coding presents promising opportunities in many areas
of communication and networking. It has been recently shown that network coding
technique can significantly increase the overall throughput of wireless
networks by taking advantage of their broadcast nature. In wireless networks,
each transmitted packet is broadcasted within a certain area and can be
overheard by the neighboring nodes. When a node needs to transmit packets, it
employs the opportunistic coding approach that uses the knowledge of what the
node's neighbors have heard in order to reduce the number of transmissions.
With this approach, each transmitted packet is a linear combination of the
original packets over a certain finite field.
  In this paper, we focus on the fundamental problem of finding the optimal
encoding for the broadcasted packets that minimizes the overall number of
transmissions. We show that this problem is NP-complete over GF(2) and
establish several fundamental properties of the optimal solution. We also
propose a simple heuristic solution for the problem based on graph coloring and
present some empirical results for random settings.



Scheduling in Data Intensive and Network Aware (DIANA) Grid Environments

In Grids scheduling decisions are often made on the basis of jobs being
either data or computation intensive: in data intensive situations jobs may be
pushed to the data and in computation intensive situations data may be pulled
to the jobs. This kind of scheduling, in which there is no consideration of
network characteristics, can lead to performance degradation in a Grid
environment and may result in large processing queues and job execution delays
due to site overloads. In this paper we describe a Data Intensive and Network
Aware (DIANA) meta-scheduling approach, which takes into account data,
processing power and network characteristics when making scheduling decisions
across multiple sites. Through a practical implementation on a Grid testbed, we
demonstrate that queue and execution times of data-intensive jobs can be
significantly improved when we introduce our proposed DIANA scheduler. The
basic scheduling decisions are dictated by a weighting factor for each
potential target location which is a calculated function of network
characteristics, processing cycles and data location and size. The job
scheduler provides a global ranking of the computing resources and then selects
an optimal one on the basis of this overall access and execution cost. The
DIANA approach considers the Grid as a combination of active network elements
and takes network characteristics as a first class criterion in the scheduling
decision matrix along with computation and data. The scheduler can then make
informed decisions by taking into account the changing state of the network,
locality and size of the data and the pool of available processing cycles.



Optimal Linear Precoding Strategies for Wideband Non-Cooperative Systems based on Game Theory-Part II: Algorithms

In this two-part paper, we address the problem of finding the optimal
precoding/multiplexing scheme for a set of non-cooperative links sharing the
same physical resources, e.g., time and bandwidth. We consider two alternative
optimization problems: P.1) the maximization of mutual information on each
link, given constraints on the transmit power and spectral mask; and P.2) the
maximization of the transmission rate on each link, using finite order
constellations, under the same constraints as in P.1, plus a constraint on the
maximum average error probability on each link. Aiming at finding decentralized
strategies, we adopted as optimality criterion the achievement of a Nash
equilibrium and thus we formulated both problems P.1 and P.2 as strategic
noncooperative (matrix-valued) games. In Part I of this two-part paper, after
deriving the optimal structure of the linear transceivers for both games, we
provided a unified set of sufficient conditions that guarantee the uniqueness
of the Nash equilibrium. In this Part II, we focus on the achievement of the
equilibrium and propose alternative distributed iterative algorithms that solve
both games. Specifically, the new proposed algorithms are the following: 1) the
sequential and simultaneous iterative waterfilling based algorithms,
incorporating spectral mask constraints; 2) the sequential and simultaneous
gradient projection based algorithms, establishing an interesting link with
variational inequality problems. Our main contribution is to provide sufficient
conditions for the global convergence of all the proposed algorithms which,
although derived under stronger constraints, incorporating for example spectral
mask constraints, have a broader validity than the convergence conditions known
in the current literature for the sequential iterative waterfilling algorithm.



Risk Analysis in Robust Control -- Making the Case for Probabilistic Robust Control

This paper offers a critical view of the "worst-case" approach that is the
cornerstone of robust control design. It is our contention that a blind
acceptance of worst-case scenarios may lead to designs that are actually more
dangerous than designs based on probabilistic techniques with a built-in risk
factor. The real issue is one of modeling. If one accepts that no mathematical
model of uncertainties is perfect then a probabilistic approach can lead to
more reliable control even if it cannot guarantee stability for all possible
cases. Our presentation is based on case analysis. We first establish that
worst-case is not necessarily "all-encompassing." In fact, we show that for
some uncertain control problems to have a conventional robust control solution
it is necessary to make assumptions that leave out some feasible cases. Once we
establish that point, we argue that it is not uncommon for the risk of
unaccounted cases in worst-case design to be greater than that of the accepted
risk in a probabilistic approach. With an example, we quantify the risks and
show that worst-case can be significantly more risky. Finally, we join our
analysis with existing results on computational complexity and probabilistic
robustness to argue that the deterministic worst-case analysis is not
necessarily the better tool.



Are there Hilbert-style Pure Type Systems?

For many a natural deduction style logic there is a Hilbert-style logic that
is equivalent to it in that it has the same theorems (i.e. valid judgements
with empty contexts). For intuitionistic logic, the axioms of the equivalent
Hilbert-style logic can be propositions which are also known as the types of
the combinators I, K and S. Hilbert-style versions of illative combinatory
logic have formulations with axioms that are actual type statements for I, K
and S. As pure type systems (PTSs)are, in a sense, equivalent to systems of
illative combinatory logic, it might be thought that Hilbert-style PTSs (HPTSs)
could be based in a similar way. This paper shows that some PTSs have very
trivial equivalent HPTSs, with only the axioms as theorems and that for many
PTSs no equivalent HPTS can exist. Most commonly used PTSs belong to these two
classes. For some PTSs however, including lambda* and the PTS at the basis of
the proof assistant Coq, there is a nontrivial equivalent HPTS, with axioms
that are type statements for I, K and S.



The Nash Equilibrium Revisited: Chaos and Complexity Hidden in Simplicity

The Nash Equilibrium is a much discussed, deceptively complex, method for the
analysis of non-cooperative games. If one reads many of the commonly available
definitions the description of the Nash Equilibrium is deceptively simple in
appearance. Modern research has discovered a number of new and important
complex properties of the Nash Equilibrium, some of which remain as
contemporary conundrums of extraordinary difficulty and complexity. Among the
recently discovered features which the Nash Equilibrium exhibits under various
conditions are heteroclinic Hamiltonian dynamics, a very complex asymptotic
structure in the context of two-player bi-matrix games and a number of
computationally complex or computationally intractable features in other
settings. This paper reviews those findings and then suggests how they may
inform various market prediction strategies.



Segmentation and Context of Literary and Musical Sequences

We test a segmentation algorithm, based on the calculation of the
Jensen-Shannon divergence between probability distributions, to two symbolic
sequences of literary and musical origin. The first sequence represents the
successive appearance of characters in a theatrical play, and the second
represents the succession of tones from the twelve-tone scale in a keyboard
sonata. The algorithm divides the sequences into segments of maximal
compositional divergence between them. For the play, these segments are related
to changes in the frequency of appearance of different characters and in the
geographical setting of the action. For the sonata, the segments correspond to
tonal domains and reveal in detail the characteristic tonal progression of such
kind of musical composition.



Spectrum Sensing in Cognitive Radios Based on Multiple Cyclic Frequencies

Cognitive radios sense the radio spectrum in order to find unused frequency
bands and use them in an agile manner. Transmission by the primary user must be
detected reliably even in the low signal-to-noise ratio (SNR) regime and in the
face of shadowing and fading. Communication signals are typically
cyclostationary, and have many periodic statistical properties related to the
symbol rate, the coding and modulation schemes as well as the guard periods,
for example. These properties can be exploited in designing a detector, and for
distinguishing between the primary and secondary users' signals. In this paper,
a generalized likelihood ratio test (GLRT) for detecting the presence of
cyclostationarity using multiple cyclic frequencies is proposed. Distributed
decision making is employed by combining the quantized local test statistics
from many secondary users. User cooperation allows for mitigating the effects
of shadowing and provides a larger footprint for the cognitive radio system.
Simulation examples demonstrate the resulting performance gains in the low SNR
regime and the benefits of cooperative detection.



Theorem proving support in programming language semantics

We describe several views of the semantics of a simple programming language
as formal documents in the calculus of inductive constructions that can be
verified by the Coq proof system. Covered aspects are natural semantics,
denotational semantics, axiomatic semantics, and abstract interpretation.
Descriptions as recursive functions are also provided whenever suitable, thus
yielding a a verification condition generator and a static analyser that can be
run inside the theorem prover for use in reflective proofs. Extraction of an
interpreter from the denotational semantics is also described. All different
aspects are formally proved sound with respect to the natural semantics
specification.



Resource Allocation for Wireless Fading Relay Channels: Max-Min Solution

As a basic information-theoretic model for fading relay channels, the
parallel relay channel is first studied, for which lower and upper bounds on
the capacity are derived. For the parallel relay channel with degraded
subchannels, the capacity is established, and is further demonstrated via the
Gaussian case, for which the synchronized and asynchronized capacities are
obtained. The capacity achieving power allocation at the source and relay nodes
among the subchannels is characterized. The fading relay channel is then
studied, for which resource allocations that maximize the achievable rates are
obtained for both the full-duplex and half-duplex cases. Capacities are
established for fading relay channels that satisfy certain conditions.



When Network Coding and Dirty Paper Coding meet in a Cooperative Ad Hoc Network

We develop and analyze new cooperative strategies for ad hoc networks that
are more spectrally efficient than classical DF cooperative protocols. Using
analog network coding, our strategies preserve the practical half-duplex
assumption but relax the orthogonality constraint. The introduction of
interference due to non-orthogonality is mitigated thanks to precoding, in
particular Dirty Paper coding. Combined with smart power allocation, our
cooperation strategies allow to save time and lead to more efficient use of
bandwidth and to improved network throughput with respect to classical RDF/PDF.



On Cognitive Interference Networks

We study the high-power asymptotic behavior of the sum-rate capacity of
multi-user interference networks with an equal number of transmitters and
receivers. We assume that each transmitter is cognizant of the message it
wishes to convey to its corresponding receiver and also of the messages that a
subset of the other transmitters wish to send. The receivers are assumed not to
be able to cooperate in any way so that they must base their decision on the
signal they receive only. We focus on the network's pre-log, which is defined
as the limiting ratio of the sum-rate capacity to half the logarithm of the
transmitted power. We present both upper and lower bounds on the network's
pre-log. The lower bounds are based on a linear partial-cancellation scheme
which entails linearly transforming Gaussian codebooks so as to eliminate the
interference in a subset of the receivers. Inter alias, the bounds give a
complete characterization of the networks and side-information settings that
result in a full pre-log, i.e., in a pre-log that is equal to the number of
transmitters (and receivers) as well as a complete characterization of networks
whose pre-log is equal to the full pre-log minus one. They also fully
characterize networks where the full pre-log can only be achieved if each
transmitter knows the messages of all users, i.e., when the side-information is
"full".



The star trellis decoding of Reed-Solomon codes

The new method for Reed-Solomon codes decoding is introduced. The method is
based on the star trellis decoding of the binary image of Reed-Solomon codes.



Noisy Sorting Without Resampling

In this paper we study noisy sorting without re-sampling. In this problem
there is an unknown order $a_{\pi(1)} < ... < a_{\pi(n)}$ where $\pi$ is a
permutation on $n$ elements. The input is the status of $n \choose 2$ queries
of the form $q(a_i,x_j)$, where $q(a_i,a_j) = +$ with probability at least
$1/2+\ga$ if $\pi(i) > \pi(j)$ for all pairs $i \neq j$, where $\ga > 0$ is a
constant and $q(a_i,a_j) = -q(a_j,a_i)$ for all $i$ and $j$. It is assumed that
the errors are independent. Given the status of the queries the goal is to find
the maximum likelihood order. In other words, the goal is find a permutation
$\sigma$ that minimizes the number of pairs $\sigma(i) > \sigma(j)$ where
$q(\sigma(i),\sigma(j)) = -$. The problem so defined is the feedback arc set
problem on distributions of inputs, each of which is a tournament obtained as a
noisy perturbations of a linear order. Note that when $\ga < 1/2$ and $n$ is
large, it is impossible to recover the original order $\pi$.
  It is known that the weighted feedback are set problem on tournaments is
NP-hard in general. Here we present an algorithm of running time
$n^{O(\gamma^{-4})}$ and sampling complexity $O_{\gamma}(n \log n)$ that with
high probability solves the noisy sorting without re-sampling problem. We also
show that if $a_{\sigma(1)},a_{\sigma(2)},...,a_{\sigma(n)}$ is an optimal
solution of the problem then it is ``close'' to the original order. More
formally, with high probability it holds that $\sum_i |\sigma(i) - \pi(i)| =
\Theta(n)$ and $\max_i |\sigma(i) - \pi(i)| = \Theta(\log n)$.
  Our results are of interest in applications to ranking, such as ranking in
sports, or ranking of search items based on comparisons by experts.



Exploration via design and the cost of uncertainty in keyword auctions

We present a deterministic exploration mechanism for sponsored search
auctions, which enables the auctioneer to learn the relevance scores of
advertisers, and allows advertisers to estimate the true value of clicks
generated at the auction site. This exploratory mechanism deviates only
minimally from the mechanism being currently used by Google and Yahoo! in the
sense that it retains the same pricing rule, similar ranking scheme, as well
as, similar mathematical structure of payoffs. In particular, the estimations
of the relevance scores and true-values are achieved by providing a chance to
lower ranked advertisers to obtain better slots. This allows the search engine
to potentially test a new pool of advertisers, and correspondingly, enables new
advertisers to estimate the value of clicks/leads generated via the auction.
Both these quantities are unknown a priori, and their knowledge is necessary
for the auction to operate efficiently. We show that such an exploration policy
can be incorporated without any significant loss in revenue for the auctioneer.
We compare the revenue of the new mechanism to that of the standard mechanism
at their corresponding symmetric Nash equilibria and compute the cost of
uncertainty, which is defined as the relative loss in expected revenue per
impression. We also bound the loss in efficiency, as well as, in user
experience due to exploration, under the same solution concept (i.e. SNE). Thus
the proposed exploration mechanism learns the relevance scores while
incorporating the incentive constraints from the advertisers who are selfish
and are trying to maximize their own profits, and therefore, the exploration is
essentially achieved via mechanism design. We also discuss variations of the
new mechanism such as truthful implementations.



For-profit mediators in sponsored search advertising

A mediator is a well-known construct in game theory, and is an entity that
plays on behalf of some of the agents who choose to use its services, while the
rest of the agents participate in the game directly. We initiate a game
theoretic study of sponsored search auctions, such as those used by Google and
Yahoo!, involving {\em incentive driven} mediators. We refer to such mediators
as {\em for-profit} mediators, so as to distinguish them from mediators
introduced in prior work, who have no monetary incentives, and are driven by
the altruistic goal of implementing certain desired outcomes. We show that in
our model, (i) players/advertisers can improve their payoffs by choosing to use
the services of the mediator, compared to directly participating in the
auction; (ii) the mediator can obtain monetary benefit by managing the
advertising burden of its group of advertisers; and (iii) the payoffs of the
mediator and the advertisers it plays for are compatible with the incentive
constraints from the advertisers who do dot use its services. A simple
intuition behind the above result comes from the observation that the mediator
has more information about and more control over the bid profile than any
individual advertiser, allowing her to reduce the payments made to the
auctioneer, while still maintaining incentive constraints. Further, our results
indicate that there are significant opportunities for diversification in the
internet economy and we should expect it to continue to develop richer
structure, with room for different types of agents to coexist.



Projection semantics for rigid loops

A rigid loop is a for-loop with a counter not accessible to the loop body or
any other part of a program. Special instructions for rigid loops are
introduced on top of the syntax of the program algebra PGA. Two different
semantic projections are provided and proven equivalent. One of these is taken
to have definitional status on the basis of two criteria: `normative semantic
adequacy' and `indicative algorithmic adequacy'.



High-resolution distributed sampling of bandlimited fields with low-precision sensors

The problem of sampling a discrete-time sequence of spatially bandlimited
fields with a bounded dynamic range, in a distributed,
communication-constrained, processing environment is addressed. A central unit,
having access to the data gathered by a dense network of fixed-precision
sensors, operating under stringent inter-node communication constraints, is
required to reconstruct the field snapshots to maximum accuracy. Both
deterministic and stochastic field models are considered. For stochastic
fields, results are established in the almost-sure sense. The feasibility of
having a flexible tradeoff between the oversampling rate (sensor density) and
the analog-to-digital converter (ADC) precision, while achieving an exponential
accuracy in the number of bits per Nyquist-interval per snapshot is
demonstrated. This exposes an underlying ``conservation of bits'' principle:
the bit-budget per Nyquist-interval per snapshot (the rate) can be distributed
along the amplitude axis (sensor-precision) and space (sensor density) in an
almost arbitrary discrete-valued manner, while retaining the same (exponential)
distortion-rate characteristics. Achievable information scaling laws for field
reconstruction over a bounded region are also derived: With N one-bit sensors
per Nyquist-interval, $\Theta(\log N)$ Nyquist-intervals, and total network
bitrate $R_{net} = \Theta((\log N)^2)$ (per-sensor bitrate $\Theta((\log
N)/N)$), the maximum pointwise distortion goes to zero as $D = O((\log N)^2/N)$
or $D = O(R_{net} 2^{-\beta \sqrt{R_{net}}})$. This is shown to be possible
with only nearest-neighbor communication, distributed coding, and appropriate
interpolation algorithms. For a fixed, nonzero target distortion, the number of
fixed-precision sensors and the network rate needed is always finite.



The Effect of Noise Correlation in AF Relay Networks

In wireless relay networks, noise at the relays can be correlated possibly
due to common interference or noise propagation from preceding hops. In this
work we consider a parallel relay network with noise correlation. For the relay
strategy of amplify-and-forward (AF), we determine the optimal rate maximizing
relay gains when correlation knowledge is available at the relays. The effect
of correlation on the performance of the relay networks is analyzed for the
cases where full knowledge of correlation is available at the relays and when
there is no knowledge about the correlation structure. Interestingly we find
that, on the average, noise correlation is beneficial regardless of whether the
relays know the noise covariance matrix or not. However, the knowledge of
correlation can greatly improve the performance. Typically, the performance
improvement from correlation knowledge increases with the relay power and the
number of relays. With perfect correlation knowledge the system is capable of
canceling interference if the number of interferers is less than the number of
relays.
  For a dual-hop multiple access parallel network, we obtain closed form
expressions for the maximum sum-rate and the optimal relay strategy. The relay
optimization for networks with three hops is also considered. For any relay
gains for the first stage relays, this represents a parallel relay network with
correlated noise. Based on the result of two hop networks with noise
correlation, we propose an algorithm for solving the relay optimization problem
for three-hop networks.



Delayed Correlations in Inter-Domain Network Traffic

To observe the evolution of network traffic correlations we analyze the
eigenvalue spectra and eigenvectors statistics of delayed correlation matrices
of network traffic counts time series. Delayed correlation matrix D is composed
of the correlations between one variable in the multivariable time series and
another at a time delay \tau . Inverse participation ratio (IPR) of
eigenvectors of D deviates substantially from the IPR of eigenvectors of the
equal time correlation matrix C. We relate this finding to the localization and
discuss its importance for network congestion control. The time-lagged
correlation pattern between network time series is preserved over a long time,
up to 100\tau, where \tau=300 sec. The largest eigenvalue \lambda_{max} of D
and the corresponding IPR oscillate with two characteristic periods of 3\tau
and 6\tau . The existence of delayed correlations between network time series
fits well into the long range dependence (LRD) property of the network traffic.
  The ability to monitor and control the long memory processes is crucial since
they impact the network performance. Injecting the random traffic counts
between non-randomly correlated time series, we were able to break the picture
of periodicity of \lambda_{max}. In addition, we investigated influence of the
periodic injections on both largest eigenvalue and the IPR, and addressed
relevance of these indicators for the LRD and self-similarity of the network
traffic.



Better Algorithms and Bounds for Directed Maximum Leaf Problems

The {\sc Directed Maximum Leaf Out-Branching} problem is to find an
out-branching (i.e. a rooted oriented spanning tree) in a given digraph with
the maximum number of leaves. In this paper, we improve known parameterized
algorithms and combinatorial bounds on the number of leaves in out-branchings.
We show that
  \begin{itemize} \item every strongly connected digraph $D$ of order $n$ with
minimum in-degree at least 3 has an out-branching with at least $(n/4)^{1/3}-1$
leaves; \item if a strongly connected digraph $D$ does not contain an
out-branching with $k$ leaves, then the pathwidth of its underlying graph is
$O(k\log k)$; \item it can be decided in time $2^{O(k\log^2 k)}\cdot n^{O(1)}$
whether a strongly connected digraph on $n$ vertices has an out-branching with
at least $k$ leaves. \end{itemize}
  All improvements use properties of extremal structures obtained after
applying local search and of some out-branching decompositions.



Worst-Case Interactive Communication and Enhancing Sensor Network Lifetime

We are concerned with the problem of maximizing the worst-case lifetime of a
data-gathering wireless sensor network consisting of a set of sensor nodes
directly communicating with a base-station.We propose to solve this problem by
modeling sensor node and base-station communication as the interactive
communication between multiple correlated informants (sensor nodes) and a
recipient (base-station). We provide practical and scalable interactive
communication protocols for data gathering in sensor networks and demonstrate
their efficiency compared to traditional approaches.
  In this paper, we first develop a formalism to address the problem of
worst-case interactive communication between a set of multiple correlated
informants and a recipient. We realize that there can be different objectives
to achieve in such a communication scenario and compute the optimal number of
messages and bits exchanged to realize these objectives. Then, we propose to
adapt these results in the context of single-hop data-gathering sensor
networks. Finally, based on this proposed formalism, we propose a clustering
based communication protocol for large sensor networks and demonstrate its
superiority over a traditional clustering protocol.



Logic, Design & Organization of PTVD-SHAM; A Parallel Time Varying & Data Super-helical Access Memory

This paper encompasses a super helical memory system's design, 'Boolean logic
& image-logic' as a theoretical concept of an invention-model to 'store
time-data' in terms of anticipating the best memory location ever for
data/time. A waterfall effect is deemed to assist the process of
potential-difference output-switch into diverse logic states in quantum dot
computational methods via utilizing coiled carbon nanotubes (CCNTs) and carbon
nanotube field effect transistors (CNFETs). A 'quantum confinement' is thus
derived for a flow of particles in a categorized quantum well substrate with a
normalized capacitance rectifying high B-field flux into electromagnetic
induction. Multi-access of coherent sequences of 'qubit addressing' is gained
in any magnitude as pre-defined for the orientation of array displacement.
Briefly, Gaussian curvature of k<0 is debated in aim of specifying the 2D
electron gas characteristics in scenarios where data is stored in short
intervals versus long ones e.g. when k'>(k<0) for greater CCNT diameters,
space-time continuum is folded by chance for the particle. This benefits from
Maxwell-Lorentz theory in Minkowski's space-time viewpoint alike to crystal
oscillators for precise data timing purposes and radar systems e.g., time
varying self-clocking devices in diverse geographic locations. This application
could also be optional for data depository versus extraction, in the best
supercomputer system's locations, autonomously. For best performance in
minimizing current limiting mechanisms including electromigration, a multilevel
metallization and implant process forming elevated sources/drains for the
circuit's staircase pyramidal construction, is discussed accordingly.



Expressing an NP-Complete Problem as the Solvability of a Polynomial Equation

We demonstrate a polynomial approach to express the decision version of the
directed Hamiltonian Cycle Problem (HCP), which is NP-Complete, as the
Solvability of a Polynomial Equation with a constant number of variables,
within a bounded real space. We first introduce four new Theorems for a set of
periodic Functions with irrational periods, based on which we then use a
trigonometric substitution, to show how the HCP can be expressed as the
Solvability of a single polynomial Equation with a constant number of
variables. The feasible solution of each of these variables is bounded within
two real numbers. We point out what future work is necessary to prove that
P=NP.



Singular curves and cusp points in the joint space of 3-RPR parallel manipulators

This paper investigates the singular curves in two-dimensional slices of the
joint space of a family of planar parallel manipulators. It focuses on special
points, referred to as cusp points, which may appear on these curves. Cusp
points play an important role in the kinematic behavior of parallel
manipulators since they make possible a nonsingular change of assembly mode.
The purpose of this study is twofold. First, it reviews an important previous
work, which, to the authors' knowledge, has never been exploited yet. Second,
it determines the cusp points in any two-dimensional slice of the joint space.
First results show that the number of cusp points may vary from zero to eight.
This work finds applications in both design and trajectory planning.



Graph-Based Decoding in the Presence of ISI

We propose an approximation of maximum-likelihood detection in ISI channels
based on linear programming or message passing. We convert the detection
problem into a binary decoding problem, which can be easily combined with LDPC
decoding. We show that, for a certain class of channels and in the absence of
coding, the proposed technique provides the exact ML solution without an
exponential complexity in the size of channel memory, while for some other
channels, this method has a non-diminishing probability of failure as SNR
increases. Some analysis is provided for the error events of the proposed
technique under linear programming.



Building Decision Procedures in the Calculus of Inductive Constructions

It is commonly agreed that the success of future proof assistants will rely
on their ability to incorporate computations within deduction in order to mimic
the mathematician when replacing the proof of a proposition P by the proof of
an equivalent proposition P' obtained from P thanks to possibly complex
calculations. In this paper, we investigate a new version of the calculus of
inductive constructions which incorporates arbitrary decision procedures into
deduction via the conversion rule of the calculus. The novelty of the problem
in the context of the calculus of inductive constructions lies in the fact that
the computation mechanism varies along proof-checking: goals are sent to the
decision procedure together with the set of user hypotheses available from the
current context. Our main result shows that this extension of the calculus of
constructions does not compromise its main properties: confluence, subject
reduction, strong normalization and consistency are all preserved.



Espaces de repr\'esentation multidimensionnels d\'edi\'es \`a la visualisation

In decision-support systems, the visual component is important for On Line
Analysis Processing (OLAP). In this paper, we propose a new approach that faces
the visualization problem due to data sparsity. We use the results of a
Multiple Correspondence Analysis (MCA) to reduce the negative effect of
sparsity by organizing differently data cube cells. Our approach does not
reduce sparsity, however it tries to build relevant representation spaces where
facts are efficiently gathered. In order to evaluate our approach, we propose
an homogeneity criterion based on geometric neighborhood of cells. The obtained
experimental results have shown the efficiency of our method.



Efficient supervised learning in networks with binary synapses

Recent experimental studies indicate that synaptic changes induced by
neuronal activity are discrete jumps between a small number of stable states.
Learning in systems with discrete synapses is known to be a computationally
hard problem. Here, we study a neurobiologically plausible on-line learning
algorithm that derives from Belief Propagation algorithms. We show that it
performs remarkably well in a model neuron with binary synapses, and a finite
number of `hidden' states per synapse, that has to learn a random
classification task. Such system is able to learn a number of associations
close to the theoretical limit, in time which is sublinear in system size. This
is to our knowledge the first on-line algorithm that is able to achieve
efficiently a finite number of patterns learned per binary synapse.
Furthermore, we show that performance is optimal for a finite number of hidden
states which becomes very small for sparse coding. The algorithm is similar to
the standard `perceptron' learning algorithm, with an additional rule for
synaptic transitions which occur only if a currently presented pattern is
`barely correct'. In this case, the synaptic changes are meta-plastic only
(change in hidden states and not in actual synaptic state), stabilizing the
synapse in its current state. Finally, we show that a system with two visible
states and K hidden states is much more robust to noise than a system with K
visible states. We suggest this rule is sufficiently simple to be easily
implemented by neurobiological systems or in hardware.



Un index de jointure pour les entrep\^ots de donn\'ees XML

XML data warehouses form an interesting basis for decision-support
applications that exploit heterogeneous data from multiple sources. However,
XML-native database systems currently bear limited performances and it is
necessary to research ways to optimize them. In this paper, we propose a new
index that is specifically adapted to the multidimensional architecture of XML
warehouses and eliminates join operations, while preserving the information
contained in the original warehouse. A theoretical study and experimental
results demonstrate the efficiency of our index, even when queries are complex.



S\'election simultan\'ee d'index et de vues mat\'erialis\'ees

Indices and materialized views are physical structures that accelerate data
access in data warehouses. However, these data structures generate some
maintenance overhead. They also share the same storage space. The existing
studies about index and materialized view selection consider these structures
separately. In this paper, we adopt the opposite stance and couple index and
materialized view selection to take into account the interactions between them
and achieve an efficient storage space sharing. We develop cost models that
evaluate the respective benefit of indexing and view materialization. These
cost models are then exploited by a greedy algorithm to select a relevant
configuration of indices and materialized views. Experimental results show that
our strategy performs better than the independent selection of indices and
materialized views.



Report on Generic Case Complexity

This article is a short introduction to generic case complexity, which is a
recently developed way of measuring the difficulty of a computational problem
while ignoring atypical behavior on a small set of inputs. Generic case
complexity applies to both recursively solvable and recursively unsolvable
problems.



Computability Closure: Ten Years Later

The notion of computability closure has been introduced for proving the
termination of higher-order rewriting with first-order matching by Jean-Pierre
Jouannaud and Mitsuhiro Okada in a 1997 draft which later served as a basis for
the author's PhD. In this paper, we show how this notion can also be used for
dealing with beta-normalized rewriting with matching modulo beta-eta (on
patterns \`a la Miller), rewriting with matching modulo some equational theory,
and higher-order data types (types with constructors having functional
recursive arguments). Finally, we show how the computability closure can easily
be turned into a reduction ordering which, in the higher-order case, contains
Jean-Pierre Jouannaud and Albert Rubio's higher-order recursive path ordering
and, in the first-order case, is equal to the usual first-order recursive path
ordering.



Sequential products in effect categories

A new categorical framework is provided for dealing with multiple arguments
in a programming language with effects, for example in a language with
imperative features. Like related frameworks (Monads, Arrows, Freyd
categories), we distinguish two kinds of functions. In addition, we also
distinguish two kinds of equations. Then, we are able to define a kind of
product, that generalizes the usual categorical product. This yields a powerful
tool for deriving many results about languages with effects.



Clusters, Graphs, and Networks for Analysing Internet-Web-Supported Communication within a Virtual Community

The proposal is to use clusters, graphs and networks as models in order to
analyse the Web structure. Clusters, graphs and networks provide knowledge
representation and organization. Clusters were generated by co-site analysis.
The sample is a set of academic Web sites from the countries belonging to the
European Union. These clusters are here revisited from the point of view of
graph theory and social network analysis. This is a quantitative and structural
analysis. In fact, the Internet is a computer network that connects people and
organizations. Thus we may consider it to be a social network. The set of Web
academic sites represents an empirical social network, and is viewed as a
virtual community. The network structural properties are here analysed applying
together cluster analysis, graph theory and social network analysis.



Secrecy Capacity Region of Fading Broadcast Channels

The fading broadcast channel with confidential messages (BCC) is
investigated, where a source node has common information for two receivers
(receivers 1 and 2), and has confidential information intended only for
receiver 1. The confidential information needs to be kept as secret as possible
from receiver 2. The channel state information (CSI) is assumed to be known at
both the transmitter and the receivers. The secrecy capacity region is first
established for the parallel Gaussian BCC, and the optimal source power
allocations that achieve the boundary of the secrecy capacity region are
derived. In particular, the secrecy capacity region is established for the
Gaussian case of the Csiszar-Korner BCC model. The secrecy capacity results are
then applied to give the ergodic secrecy capacity region for the fading BCC.



IRVO: an Interaction Model for designing Collaborative Mixed Reality systems

This paper presents an interaction model adapted to mixed reality
environments known as IRVO (Interacting with Real and Virtual Objects). IRVO
aims at modeling the interaction between one or more users and the Mixed
Reality system by representing explicitly the objects and tools involved and
their relationship. IRVO covers the design phase of the life cycle and models
the intended use of the system. In a first part, we present a brief review of
related HCI models. The second part is devoted to the IRVO model, its notation
and some examples. In the third part, we present how IRVO is used for designing
applications and in particular we show how this model can be integrated in a
Model-Based Approach (CoCSys) which is currently designed at our lab.



Fast computing of velocity field for flows in industrial burners and pumps

In this work we present a technique of fast numerical computation for
solutions of Navier-Stokes equations in the case of flows of industrial
interest. At first the partial differential equations are translated into a set
of nonlinear ordinary differential equations using the geometrical shape of the
domain where the flow is developing, then these ODEs are numerically resolved
using a set of computations distributed among the available processors. We
present some results from simulations on a parallel hardware architecture using
native multithreads software and simulating a shared-memory or a
distributed-memory environment.



Random subgroups and analysis of the length-based and quotient attacks

In this paper we discuss generic properties of "random subgroups" of a given
group G. It turns out that in many groups G (even in most exotic of them) the
random subgroups have a simple algebraic structure and they "sit" inside G in a
very particular way. This gives a strong mathematical foundation for
cryptanalysis of several group-based cryptosystems and indicates on how to
chose "strong keys". To illustrate our technique we analyze the
Anshel-Anshel-Goldfeld (AAG) cryptosystem and give a mathematical explanation
of recent success of some heuristic length-based attacks on it. Furthermore, we
design and analyze a new type of attacks, which we term the quotient attacks.
Mathematical methods we develop here also indicate how one can try to choose
"parameters" in AAG to foil the attacks.



Properties of polynomial bases used in a line-surface intersection algorithm

In [5], Srijuntongsiri and Vavasis propose the "Kantorovich-Test Subdivision
algorithm", or KTS, which is an algorithm for finding all zeros of a polynomial
system in a bounded region of the plane. This algorithm can be used to find the
intersections between a line and a surface. The main features of KTS are that
it can operate on polynomials represented in any basis that satisfies certain
conditions and that its efficiency has an upper bound that depends only on the
conditioning of the problem and the choice of the basis representing the
polynomial system.
  This article explores in detail the dependence of the efficiency of the KTS
algorithm on the choice of basis. Three bases are considered: the power, the
Bernstein, and the Chebyshev bases. These three bases satisfy the basis
properties required by KTS. Theoretically, Chebyshev case has the smallest
upper bound on its running time. The computational results, however, do not
show that Chebyshev case performs better than the other two.



Programming Telepathy: Implementing Quantum Non-Locality Games

Quantum pseudo-telepathy is an intriguing phenomenon which results from the
application of quantum information theory to communication complexity. To
demonstrate this phenomenon researchers in the field of quantum communication
complexity devised a number of quantum non-locality games. The setting of these
games is as follows: the players are separated so that no communication between
them is possible and are given a certain computational task. When the players
have access to a quantum resource called entanglement, they can accomplish the
task: something that is impossible in a classical setting. To an observer who
is unfamiliar with the laws of quantum mechanics it seems that the players
employ some sort of telepathy; that is, they somehow exchange information
without sharing a communication channel. This paper provides a formal framework
for specifying, implementing, and analysing quantum non-locality games.



Sorting and Selection in Posets

Classical problems of sorting and searching assume an underlying linear
ordering of the objects being compared. In this paper, we study a more general
setting, in which some pairs of objects are incomparable. This generalization
is relevant in applications related to rankings in sports, college admissions,
or conference submissions. It also has potential applications in biology, such
as comparing the evolutionary fitness of different strains of bacteria, or
understanding input-output relations among a set of metabolic reactions or the
causal influences among a set of interacting genes or proteins. Our results
improve and extend results from two decades ago of Faigle and Tur\'{a}n.
  A measure of complexity of a partially ordered set (poset) is its width. Our
algorithms obtain information about a poset by queries that compare two
elements. We present an algorithm that sorts, i.e. completely identifies, a
width w poset of size n and has query complexity O(wn + nlog(n)), which is
within a constant factor of the information-theoretic lower bound. We also show
that a variant of Mergesort has query complexity O(wn(log(n/w))) and total
complexity O((w^2)nlog(n/w)). Faigle and Tur\'{a}n have shown that the sorting
problem has query complexity O(wn(log(n/w))) but did not address its total
complexity.
  For the related problem of determining the minimal elements of a poset, we
give efficient deterministic and randomized algorithms with O(wn) query and
total complexity, along with matching lower bounds for the query complexity up
to a factor of 2. We generalize these results to the k-selection problem of
determining the elements of height at most k. We also derive upper bounds on
the total complexity of some other problems of a similar flavor.



An Architecture Framework for Complex Data Warehouses

Nowadays, many decision support applications need to exploit data that are
not only numerical or symbolic, but also multimedia, multistructure,
multisource, multimodal, and/or multiversion. We term such data complex data.
Managing and analyzing complex data involves a lot of different issues
regarding their structure, storage and processing, and metadata are a key
element in all these processes. Such problems have been addressed by classical
data warehousing (i.e., applied to "simple" data). However, data warehousing
approaches need to be adapted for complex data. In this paper, we first propose
a precise, though open, definition of complex data. Then we present a general
architecture framework for warehousing complex data. This architecture heavily
relies on metadata and domain-related knowledge, and rests on the XML language,
which helps storing data, metadata and domain-specific knowledge altogether,
and facilitates communication between the various warehousing processes.



Data Mining-based Materialized View and Index Selection in Data Warehouses

Materialized views and indexes are physical structures for accelerating data
access that are casually used in data warehouses. However, these data
structures generate some maintenance overhead. They also share the same storage
space. Most existing studies about materialized view and index selection
consider these structures separately. In this paper, we adopt the opposite
stance and couple materialized view and index selection to take view-index
interactions into account and achieve efficient storage space sharing.
Candidate materialized views and indexes are selected through a data mining
process. We also exploit cost models that evaluate the respective benefit of
indexing and view materialization, and help select a relevant configuration of
indexes and materialized views among the candidates. Experimental results show
that our strategy performs better than an independent selection of materialized
views and indexes.



Autonomy with regard to an Attribute

This paper presents a model of autonomy called autonomy with regard to an
attribute applicable to cognitive and not cognitive artificial agents. Three
criteria (global / partial, social / nonsocial, absolute / relative) are
defined and used to describe the main characteristics of this type of autonomy.
A software agent autonomous with regard to the mobility illustrates a possible
implementation of this model.



A Tight Lower Bound to the Outage Probability of Discrete-Input Block-Fading Channels

In this correspondence, we propose a tight lower bound to the outage
probability of discrete-input Nakagami-m block-fading channels. The approach
permits an efficient method for numerical evaluation of the bound, providing an
additional tool for system design. The optimal rate-diversity trade-off for the
Nakagami-m block-fading channel is also derived and a tight upper bound is
obtained for the optimal coding gain constant.



Cactus Framework: Black Holes to Gamma Ray Bursts

Gamma Ray Bursts (GRBs) are intense narrowly-beamed flashes of gamma-rays of
cosmological origin. They are among the most scientifically interesting
astrophysical systems, and the riddle concerning their central engines and
emission mechanisms is one of the most complex and challenging problems of
astrophysics today. In this article we outline our petascale approach to the
GRB problem and discuss the computational toolkits and numerical codes that are
currently in use and that will be scaled up to run on emerging petaflop scale
computing platforms in the near future.
  Petascale computing will require additional ingredients over conventional
parallelism. We consider some of the challenges which will be caused by future
petascale architectures, and discuss our plans for the future development of
the Cactus framework and its applications to meet these challenges in order to
profit from these new architectures.



The Trade-offs with Space Time Cube Representation of Spatiotemporal Patterns

Space time cube representation is an information visualization technique
where spatiotemporal data points are mapped into a cube. Fast and correct
analysis of such information is important in for instance geospatial and social
visualization applications. Information visualization researchers have
previously argued that space time cube representation is beneficial in
revealing complex spatiotemporal patterns in a dataset to users. The argument
is based on the fact that both time and spatial information are displayed
simultaneously to users, an effect difficult to achieve in other
representations. However, to our knowledge the actual usefulness of space time
cube representation in conveying complex spatiotemporal patterns to users has
not been empirically validated. To fill this gap we report on a
between-subjects experiment comparing novice users error rates and response
times when answering a set of questions using either space time cube or a
baseline 2D representation. For some simple questions the error rates were
lower when using the baseline representation. For complex questions where the
participants needed an overall understanding of the spatiotemporal structure of
the dataset, the space time cube representation resulted in on average twice as
fast response times with no difference in error rates compared to the baseline.
These results provide an empirical foundation for the hypothesis that space
time cube representation benefits users when analyzing complex spatiotemporal
patterns.



Interface groups and financial transfer architectures

Analytic execution architectures have been proposed by the same authors as a
means to conceptualize the cooperation between heterogeneous collectives of
components such as programs, threads, states and services. Interface groups
have been proposed as a means to formalize interface information concerning
analytic execution architectures. These concepts are adapted to organization
architectures with a focus on financial transfers. Interface groups (and
monoids) now provide a technique to combine interface elements into interfaces
with the flexibility to distinguish between directions of flow dependent on
entity naming.
  The main principle exploiting interface groups is that when composing a
closed system of a collection of interacting components, the sum of their
interfaces must vanish in the interface group modulo reflection. This certainly
matters for financial transfer interfaces.
  As an example of this, we specify an interface group and within it some
specific interfaces concerning the financial transfer architecture for a part
of our local academic organization.
  Financial transfer interface groups arise as a special case of more general
service architecture interfaces.



Fast and Simple Relational Processing of Uncertain Data

This paper introduces U-relations, a succinct and purely relational
representation system for uncertain databases. U-relations support
attribute-level uncertainty using vertical partitioning. If we consider
positive relational algebra extended by an operation for computing possible
answers, a query on the logical level can be translated into, and evaluated as,
a single relational algebra query on the U-relation representation. The
translation scheme essentially preserves the size of the query in terms of
number of operations and, in particular, number of joins. Standard techniques
employed in off-the-shelf relational database management systems are effective
for optimizing and processing queries on U-relations. In our experiments we
show that query evaluation on U-relations scales to large amounts of data with
high degrees of uncertainty.



Sampling Algorithms and Coresets for Lp Regression

The Lp regression problem takes as input a matrix $A \in \Real^{n \times d}$,
a vector $b \in \Real^n$, and a number $p \in [1,\infty)$, and it returns as
output a number ${\cal Z}$ and a vector $x_{opt} \in \Real^d$ such that ${\cal
Z} = \min_{x \in \Real^d} ||Ax -b||_p = ||Ax_{opt}-b||_p$. In this paper, we
construct coresets and obtain an efficient two-stage sampling-based
approximation algorithm for the very overconstrained ($n \gg d$) version of
this classical problem, for all $p \in [1, \infty)$. The first stage of our
algorithm non-uniformly samples $\hat{r}_1 = O(36^p d^{\max\{p/2+1, p\}+1})$
rows of $A$ and the corresponding elements of $b$, and then it solves the Lp
regression problem on the sample; we prove this is an 8-approximation. The
second stage of our algorithm uses the output of the first stage to resample
$\hat{r}_1/\epsilon^2$ constraints, and then it solves the Lp regression
problem on the new sample; we prove this is a $(1+\epsilon)$-approximation. Our
algorithm unifies, improves upon, and extends the existing algorithms for
special cases of Lp regression, namely $p = 1,2$. In course of proving our
result, we develop two concepts--well-conditioned bases and subspace-preserving
sampling--that are of independent interest.



Sources of Superlinearity in Davenport-Schinzel Sequences

A generalized Davenport-Schinzel sequence is one over a finite alphabet that
contains no subsequences isomorphic to a fixed forbidden subsequence. One of
the fundamental problems in this area is bounding (asymptotically) the maximum
length of such sequences. Following Klazar, let Ex(\sigma,n) be the maximum
length of a sequence over an alphabet of size n avoiding subsequences
isomorphic to \sigma. It has been proved that for every \sigma, Ex(\sigma,n) is
either linear or very close to linear; in particular it is O(n
2^{\alpha(n)^{O(1)}}), where \alpha is the inverse-Ackermann function and O(1)
depends on \sigma. However, very little is known about the properties of \sigma
that induce superlinearity of \Ex(\sigma,n).
  In this paper we exhibit an infinite family of independent superlinear
forbidden subsequences. To be specific, we show that there are 17 prototypical
superlinear forbidden subsequences, some of which can be made arbitrarily long
through a simple padding operation. Perhaps the most novel part of our
constructions is a new succinct code for representing superlinear forbidden
subsequences.



Numerical Calculation With Arbitrary Precision

The vast use of computers on scientific numerical computation makes the
awareness of the limited precision that these machines are able to provide us
an essential matter. A limited and insufficient precision allied to the
truncation and rounding errors may induce the user to incorrect interpretation
of his/hers answer. In this work, we have developed a computational package to
minimize this kind of error by offering arbitrary precision numbers and
calculation. This is very important in Physics where we can work with numbers
too small and too big simultaneously.



On slow-fading non-separable correlation MIMO systems

In a frequency selective slow-fading channel in a MIMO system, the channel
matrix is of the form of a block matrix. We propose a method to calculate the
limit of the eigenvalue distribution of block matrices if the size of the
blocks tends to infinity. We will also calculate the asymptotic eigenvalue
distribution of $HH^*$, where the entries of $H$ are jointly Gaussian, with a
correlation of the form $E[h_{pj}\bar h_{qk}]= \sum_{s=1}^t
\Psi^{(s)}_{jk}\hat\Psi^{(s)}_{pq}$ (where $t$ is fixed and does not increase
with the size of the matrix). We will use an operator-valued free probability
approach to achieve this goal. Using this method, we derive a system of
equations, which can be solved numerically to compute the desired eigenvalue
distribution.



Understanding the Properties of the BitTorrent Overlay

In this paper, we conduct extensive simulations to understand the properties
of the overlay generated by BitTorrent. We start by analyzing how the overlay
properties impact the efficiency of BitTorrent. We focus on the average peer
set size (i.e., average number of neighbors), the time for a peer to reach its
maximum peer set size, and the diameter of the overlay. In particular, we show
that the later a peer arrives in a torrent, the longer it takes to reach its
maximum peer set size. Then, we evaluate the impact of the maximum peer set
size, the maximum number of outgoing connections per peer, and the number of
NATed peers on the overlay properties. We show that BitTorrent generates a
robust overlay, but that this overlay is not a random graph. In particular, the
connectivity of a peer to its neighbors depends on its arriving order in the
torrent. We also show that a large number of NATed peers significantly
compromise the robustness of the overlay to attacks. Finally, we evaluate the
impact of peer exchange on the overlay properties, and we show that it
generates a chain-like overlay with a large diameter, which will adversely
impact the efficiency of large torrents.



The Kinematics of Manipulators Built From Closed Planar Mechanisms

The paper discusses the kinematics of manipulators builts of planar closed
kinematic chains. A special kinematic scheme is extracted from the array of
these mechanisms that looks the most promising for the creation of different
types of robotic manipulators. The structural features of this manipulator
determine a number of its original properties that essentially simplify its
control. These features allow the main control problems to be effectively
overcome by application of the simple kinematic problems. The workspace and
singular configurations of a basic planar manipulator are studied. By using a
graphic simulation method, motions of the designed mechanism are examined. A
prototype of this mechanism was implemented to verify the proposed approach.



On the Degrees of Freedom in Cognitive Radio Channels

After receiving useful peer comments, we would like to withdraw this paper.



Generalized Solution Concepts in Games with Possibly Unaware Players

Most work in game theory assumes that players are perfect reasoners and have
common knowledge of all significant aspects of the game. In earlier work, we
proposed a framework for representing and analyzing games with possibly unaware
players, and suggested a generalization of Nash equilibrium appropriate for
games with unaware players that we called generalized Nash equilibrium. Here,
we use this framework to analyze other solution concepts that have been
considered in the game-theory literature, with a focus on sequential
equilibrium. We also provide some insight into the notion of generalized Nash
equilibrium by proving that it is closely related to the notion of
rationalizability when we restrict the analysis to games in normal form and no
unawareness is involved.



Throughput Scaling Laws for Wireless Networks with Fading Channels

A network of n communication links, operating over a shared wireless channel,
is considered. Fading is assumed to be the dominant factor affecting the
strength of the channels between transmitter and receiver terminals. It is
assumed that each link can be active and transmit with a constant power P or
remain silent. The objective is to maximize the throughput over the selection
of active links. By deriving an upper bound and a lower bound, it is shown that
in the case of Rayleigh fading (i) the maximum throughput scales like $\log n$
(ii) the maximum throughput is achievable in a distributed fashion. The upper
bound is obtained using probabilistic methods, where the key point is to upper
bound the throughput of any random set of active links by a chi-squared random
variable. To obtain the lower bound, a decentralized link activation strategy
is proposed and analyzed.



Removing Manually-Generated Boilerplate from Electronic Texts: Experiments with Project Gutenberg e-Books

Collaborative work on unstructured or semi-structured documents, such as in
literature corpora or source code, often involves agreed upon templates
containing metadata. These templates are not consistent across users and over
time. Rule-based parsing of these templates is expensive to maintain and tends
to fail as new documents are added. Statistical techniques based on frequent
occurrences have the potential to identify automatically a large fraction of
the templates, thus reducing the burden on the programmers. We investigate the
case of the Project Gutenberg corpus, where most documents are in ASCII format
with preambles and epilogues that are often copied and pasted or manually
typed. We show that a statistical approach can solve most cases though some
documents require knowledge of English. We also survey various technical
solutions that make our approach applicable to large data sets.



A note on minimal matching covered graphs

A graph is called matching covered if for its every edge there is a maximum
matching containing it. It is shown that minimal matching covered graphs
contain a perfect matching.



On trees with a maximum proper partial 0-1 coloring containing a maximum matching

I prove that in a tree in which the distance between any two endpoints is
even, there is a maximum proper partial 0-1 coloring such that the edges
colored by 0 form a maximum matching.



Bandlimited Field Reconstruction for Wireless Sensor Networks

Wireless sensor networks are often used for environmental monitoring
applications. In this context sampling and reconstruction of a physical field
is one of the most important problems to solve. We focus on a bandlimited field
and find under which conditions on the network topology the reconstruction of
the field is successful, with a given probability. We review irregular sampling
theory, and analyze the problem using random matrix theory. We show that even a
very irregular spatial distribution of sensors may lead to a successful signal
reconstruction, provided that the number of collected samples is large enough
with respect to the field bandwidth. Furthermore, we give the basis to
analytically determine the probability of successful field reconstruction.



Moveability and Collision Analysis for Fully-Parallel Manipulators

The aim of this paper is to characterize the moveability of fully-parallel
manipulators in the presence of obstacles. Fully parallel manipulators are used
in applications where accuracy, stiffness or high speeds and accelerations are
required \cite{Merlet:97}. However, one of its main drawbacks is a relatively
small workspace compared to the one of serial manipulators. This is due mainly
to the existence of potential internal collisions, and the existence of
singularities. In this paper, the notion of free aspect is defined which
permits to exhibit domains of the workspace and the joint space free of
singularity and collision. The main application of this study is the
moveability analysis in the workspace of the manipulator as well as
path-planning, control and design.



A Normalizing Intuitionistic Set Theory with Inaccessible Sets

We propose a set theory strong enough to interpret powerful type theories
underlying proof assistants such as LEGO and also possibly Coq, which at the
same time enables program extraction from its constructive proofs. For this
purpose, we axiomatize an impredicative constructive version of
Zermelo-Fraenkel set theory IZF with Replacement and $\omega$-many
inaccessibles, which we call \izfio. Our axiomatization utilizes set terms, an
inductive definition of inaccessible sets and the mutually recursive nature of
equality and membership relations. It allows us to define a weakly-normalizing
typed lambda calculus corresponding to proofs in \izfio according to the
Curry-Howard isomorphism principle. We use realizability to prove the
normalization theorem, which provides a basis for program extraction
capability.



Working Modes and Aspects in Fully-Parallel Manipulator

The aim of this paper is to characterize the notion of aspect in the
workspace and in the joint space for parallel manipulators. In opposite to the
serial manipulators, the parallel manipulators can admit not only multiple
inverse kinematic solutions, but also multiple direct kinematic solutions. The
notion of aspect introduced for serial manipulators in [Borrel 86], and
redefined for parallel manipulators with only one inverse kinematic solution in
[Wenger 1997], is redefined for general fully parallel manipulators. Two
Jacobian matrices appear in the kinematic relations between the joint-rate and
the Cartesian-velocity vectors, which are called the "inverse kinematics" and
the "direct kinematics" matrices. The study of these matrices allow to
respectively define the parallel and the serial singularities. The notion of
working modes is introduced to separate inverse kinematic solutions. Thus, we
can find out domains of the workspace and the joint space exempt of
singularity. Application of this study is the moveability analysis in the
workspace of the manipulator as well as path-planing and control. This study is
illustrated in this paper with a RR-RRR planar parallel manipulator.



On the error exponent of variable-length block-coding schemes over finite-state Markov channels with feedback

The error exponent of Markov channels with feedback is studied in the
variable-length block-coding setting. Burnashev's classic result is extended
and a single letter characterization for the reliability function of
finite-state Markov channels is presented, under the assumption that the
channel state is causally observed both at the transmitter and at the receiver
side. Tools from stochastic control theory are used in order to treat channels
with intersymbol interference. In particular the convex analytical approach to
Markov decision processes is adopted to handle problems with stopping time
horizons arising from variable-length coding schemes.



The Isoconditioning Loci of A Class of Closed-Chain Manipulators

The subject of this paper is a special class of closed-chain manipulators.
First, we analyze a family of two-degree-of-freedom (dof) five-bar planar
linkages. Two Jacobian matrices appear in the kinematic relations between the
joint-rate and the Cartesian-velocity vectors, which are called the ``inverse
kinematics" and the "direct kinematics" matrices. It is shown that the loci of
points of the workspace where the condition number of the direct-kinematics
matrix remains constant, i.e., the isoconditioning loci, are the coupler points
of the four-bar linkage obtained upon locking the middle joint of the linkage.
Furthermore, if the line of centers of the two actuated revolutes is used as
the axis of a third actuated revolute, then a three-dof hybrid manipulator is
obtained. The isoconditioning loci of this manipulator are surfaces of
revolution generated by the isoconditioning curves of the two-dof manipulator,
whose axis of symmetry is that of the third actuated revolute.



Workspace and Assembly modes in Fully-Parallel Manipulators : A Descriptive Study

The goal of this paper is to explain, using a typical example, the
distribution of the different assembly modes in the workspace and their
effective role in the execution of trajectories. The singular and non-singular
changes of assembly mode are described and compared to each other. The
non-singular change of assembly mode is more deeply analysed and discussed in
the context of trajectory planning. In particular, it is shown that, according
to the location of the initial and final configurations with respect to the
uniqueness domains in the workspace, there are three different cases to
consider before planning a linking trajectory.



Conception Isotropique D'Une Morphologie Parall\`Ele : Application \`a L'Usinage

The aim of this paper is the isotropic design of a hybrid morphology
dedicated to 3-axis machining applications. It is necessary to ensure the
feasibility of continuous, singularity-free trajectories, as well as a good
manipulability in position and velocity. We want to propose an alternative
design to conventional serial machine-tools. We compare a serial PPP
machine-tool (three prismatic orthogonal axes) with a hybrid architecture which
we optimize only the first two axes. The critrerion used for the optimization
is the conditioning of the Jacobian matrices. The optimum, namely isotropy, can
be obtained which provides our architecture with excellent manipulability
properties.



A distributed Approach for Access and Visibility Task under Ergonomic Constraints with a Manikin in a Virtual Reality Environment

This paper presents a new method, based on a multi-agent system and on
digital mock-up technology, to assess an efficient path planner for a manikin
for access and visibility task under ergonomic constraints. In order to solve
this problem, the human operator is integrated in the process optimization to
contribute to a global perception of the environment. This operator cooperates,
in real-time, with several automatic local elementary agents. The result of
this work validates solutions brought by digital mock-up and that can be
applied to simulate maintenance task.



A Training based Distributed Non-Coherent Space-Time Coding Strategy

Unitary space-time modulation is known to be an efficient means to
communicate over non-coherent Multiple Input Multiple Output (MIMO) channels.
In this letter, differential unitary space-time coding and non-coherent
space-time coding for the training based approach of Kim and Tarokh are
addressed. For this approach, necessary and sufficient conditions for
multi-group decodability are derived in a simple way assuming a Generalized
Likelihood Ratio Test receiver and a unitary codebook. Extending Kim and
Tarokh's approach for colocated MIMO systems, a novel training based approach
to distributed non-coherent space-time coding for wireless relay networks is
proposed. An explicit construction of two-group decodable distributed
non-coherent space-time codes achieving full cooperative diversity for all even
number of relays is provided.



On complexity of special maximum matchings constructing

For bipartite graphs the NP-completeness is proved for the problem of
existence of maximum matching which removal leads to a graph with given
lower(upper)bound for the cardinality of its maximum matching.



Splay Trees, Davenport-Schinzel Sequences, and the Deque Conjecture

We introduce a new technique to bound the asymptotic performance of splay
trees. The basic idea is to transcribe, in an indirect fashion, the rotations
performed by the splay tree as a Davenport-Schinzel sequence S, none of whose
subsequences are isomorphic to fixed forbidden subsequence. We direct this
technique towards Tarjan's deque conjecture and prove that n deque operations
require O(n alpha^*(n)) time, where alpha^*(n) is the minimum number of
applications of the inverse-Ackermann function mapping n to a constant. We are
optimistic that this approach could be directed towards other open conjectures
on splay trees such as the traversal and split conjectures.



Benefit of Delay on the Diversity-Multiplexing Tradeoffs of MIMO Channels with Partial CSI

This paper re-examines the well-known fundamental tradeoffs between rate and
reliability for the multi-antenna, block Rayleigh fading channel in the high
signal to noise ratio (SNR) regime when (i) the transmitter has access to
(noiseless) one bit per coherence-interval of causal channel state information
(CSI) and (ii) soft decoding delays together with worst-case delay guarantees
are acceptable. A key finding of this work is that substantial improvements in
reliability can be realized with a very short expected delay and a slightly
longer (but bounded) worst-case decoding delay guarantee in communication
systems where the transmitter has access to even one bit per coherence interval
of causal CSI. While similar in spirit to the recent work on communication
systems based on automatic repeat requests (ARQ) where decoding failure is
known at the transmitter and leads to re-transmission, here transmit
side-information is purely based on CSI. The findings reported here also lend
further support to an emerging understanding that decoding delay (related to
throughput) and codeword blocklength (related to coding complexity and delays)
are distinctly different design parameters which can be tuned to control
reliability.



Design of Multistage Decimation Filters Using Cyclotomic Polynomials: Optimization and Design Issues

This paper focuses on the design of multiplier-less decimation filters
suitable for oversampled digital signals. The aim is twofold. On one hand, it
proposes an optimization framework for the design of constituent decimation
filters in a general multistage decimation architecture. The basic building
blocks embedded in the proposed filters belong, for a simple reason, to the
class of cyclotomic polynomials (CPs): the first 104 CPs have a z-transfer
function whose coefficients are simply {-1,0,+1}. On the other hand, the paper
provides a bunch of useful techniques, most of which stemming from some key
properties of CPs, for designing the proposed filters in a variety of
architectures. Both recursive and non-recursive architectures are discussed by
focusing on a specific decimation filter obtained as a result of the
optimization algorithm.
  Design guidelines are provided with the aim to simplify the design of the
constituent decimation filters in the multistage chain.



Mod\'elisation Dynamique d'un Robot Parall\`ele \`a 3-DDL : l'Orthoglide

In this article, we propose a method for calculation of the inverse and
direct dynamic models of the Orthoglide, a parallel robot with threedegrees of
freedom in translation. These models are calculated starting from the elements
of the dynamic model of the kinematic chain structure and equations of
Newton-Euler applied to the platform. These models are obtained in explicit
form having an interesting physical interpretation.



Word statistics in Blogs and RSS feeds: Towards empirical universal evidence

We focus on the statistics of word occurrences and of the waiting times
between such occurrences in Blogs. Due to the heterogeneity of words'
frequencies, the empirical analysis is performed by studying classes of
"frequently-equivalent" words, i.e. by grouping words depending on their
frequencies. Two limiting cases are considered: the dilute limit, i.e. for
those words that are used less than once a day, and the dense limit for
frequent words. In both cases, extreme events occur more frequently than
expected from the Poisson hypothesis. These deviations from Poisson statistics
reveal non-trivial time correlations between events that are associated with
bursts of activities. The distribution of waiting times is shown to behave like
a stretched exponential and to have the same shape for different sets of words
sharing a common frequency, thereby revealing universal features.



Degeneracy study of the forward kinematics of planar 3-RPR parallel manipulators

This paper investigates two situations in which the forward kinematics of
planar 3-RPR parallel manipulators degenerates. These situations have not been
addressed before. The first degeneracy arises when the three input joint
variables r1, r2 and r3 satisfy a certain relationship. This degeneracy yields
a double root of the characteristic polynomial in t, which could be erroneously
interpreted as two coalesce assembly modes. But, unlike what arises in
non-degenerate cases, this double root yields two sets of solutions for the
position coordinates (x, y) of the platform. In the second situation, we show
that the forward kinematics degenerates over the whole joint space if the base
and platform triangles are congruent and the platform triangle is rotated by
180 deg about one of its sides. For these "degenerate" manipulators, which are
defined here for the first time, the forward kinematics is reduced to the
solution of a 3rd-degree polynomial and a quadratics in sequence. Such
manipulators constitute, in turn, a new family of analytic planar manipulators
that would be more suitable for industrial applications.



Kinematic Analysis of a Family of 3R Manipulators

The workspace topologies of a family of 3-revolute (3R) positioning
manipulators are enumerated. The workspace is characterized in a half-cross
section by the singular curves. The workspace topology is defined by the number
of cusps that appear on these singular curves. The design parameters space is
shown to be divided into five domains where all manipulators have the same
number of cusps. Each separating surface is given as an explicit expression in
the DH-parameters. As an application of this work, we provide a necessary and
sufficient condition for a 3R orthogonal manipulator to be cuspidal, i.e. to
change posture without meeting a singularity. This condition is set as an
explicit expression in the DH parameters.



The Computation of All 4R Serial Spherical Wrists With an Isotropic Architecture

A spherical wrist of the serial type with n revolute (R) joints is said to be
isotropic if it can attain a posture whereby the singular values of its
Jacobian matrix are all equal to sqrt(n/3). What isotropy brings about is
robustness to manufacturing, assembly, and measurement errors, thereby
guaranteeing a maximum orientation accuracy. In this paper we investigate the
existence of redundant isotropic architectures, which should add to the
dexterity of the wrist under design by virtue of its extra degree of freedom.
The problem formulation, for, leads to a system of eight quadratic equations
with eight unknowns. The Bezout number of this system is thus 2^8=256, its BKK
bound being 192. However, the actual number of solutions is shown to be 32. We
list all solutions of the foregoing algebraic problem. All these solutions are
real, but distinct solutions do not necessarily lead to distinct manipulators.
Upon discarding those algebraic solutions that yield no new wrists, we end up
with exactly eight distinct architectures, the eight corresponding manipulators
being displayed at their isotropic postures.



A design oriented study for 3R Orthogonal Manipulators With Geometric Simplifications

This paper proposes a method to calculate the largest Regular Dextrous
Workspace (RDW) of some types of three-revolute orthogonal manipulators that
have at least one of their DH parameters equal to zero. Then a new performance
index based on the RDW is introduced, the isocontours of this index are plotted
in the parameter space of the interesting types of manipulators and finally an
inspection of the domains of the parameter spaces is conducted in order to
identify the better manipulator architectures. The RDW is a part of the
workspace whose shape is regular (cube, cylinder) and the performances
(conditioning index) are bounded inside. The groups of 3R orthogonal
manipulators studied have interesting kinematic properties such as, a
well-connected workspace that is fully reachable with four inverse kinematic
solutions and that does not contain any void. This study is of high interest
for the design of alternative manipulator geometries.



Separable convex optimization problems with linear ascending constraints

Separable convex optimization problems with linear ascending inequality and
equality constraints are addressed in this paper. Under an ordering condition
on the slopes of the functions at the origin, an algorithm that determines the
optimum point in a finite number of steps is described. The optimum value is
shown to be monotone with respect to a partial order on the constraint
parameters. Moreover, the optimum value is convex with respect to these
parameters. Examples motivated by optimizations for communication systems are
used to illustrate the algorithm.



Design of a Spherical Wrist with Parallel Architecture: Application to Vertebrae of an Eel Robot

The design of a spherical wrist with parallel architecture is the object of
this article. This study is part of a larger project, which aims to design and
to build an eel robot for inspection of immersed piping. The kinematic analysis
of the mechanism is presented first to characterize the singular configurations
as well as the isotropic configurations. We add the design constraints related
to the application, such as (i) the compactness of the mechanism, (ii) the
symmetry of the elements in order to ensure static and dynamic balance and
(iii) the possibility of the mechanism to fill the elliptic form of the ell
sections.



Passive Control Architecture for Virtual Humans

In the present paper, we introduce a new control architecture aimed at
driving virtual humans in interaction with virtual environments, by motion
capture. It brings decoupling of functionalities, and also of stability thanks
to passivity. We show projections can break passivity, and thus must be used
carefully. Our control scheme enables task space and internal control, contact,
and joint limits management. Thanks to passivity, it can be easily extended.
Besides, we introduce a new tool as for manikin's control, which makes it able
to build passive projections, so as to guide the virtual manikin when sharp
movements are needed.



An Integrated Crosscutting Concern Migration Strategy and its Application to JHotDraw

In this paper we propose a systematic strategy for migrating crosscutting
concerns in existing object-oriented systems to aspect-based solutions. The
proposed strategy consists of four steps: mining, exploration, documentation
and refactoring of crosscutting concerns. We discuss in detail a new approach
to aspect refactoring that is fully integrated with our strategy, and apply the
whole strategy to an object-oriented system, namely the JHotDraw framework. The
result of this migration is made available as an open-source project, which is
the largest aspect refactoring available to date. We report on our experiences
with conducting this case study and reflect on the success and challenges of
the migration process, as well as on the feasibility of automatic aspect
refactoring.



Worm Epidemics in Wireless Adhoc Networks

A dramatic increase in the number of computing devices with wireless
communication capability has resulted in the emergence of a new class of
computer worms which specifically target such devices. The most striking
feature of these worms is that they do not require Internet connectivity for
their propagation but can spread directly from device to device using a
short-range radio communication technology, such as WiFi or Bluetooth. In this
paper, we develop a new model for epidemic spreading of these worms and
investigate their spreading in wireless ad hoc networks via extensive Monte
Carlo simulations. Our studies show that the threshold behaviour and dynamics
of worm epidemics in these networks are greatly affected by a combination of
spatial and temporal correlations which characterize these networks, and are
significantly different from the previously studied epidemics in the Internet.



Two polynomial algorithms for special maximum matching constructing in trees

For an arbitrary tree we investigate the problems of constructing a maximum
matching which minimizes or maximizes the cardinality of a maximum matching of
the graph obtained from original one by its removal and present corresponding
polynomial algorithms.



A Cultural Market Model

Social interactions and personal tastes shape our consumption behavior of
cultural products. In this study, we present a computational model of a
cultural market and we aim to analyze the behavior of the consumer population
as an emergent phenomena. Our results suggest that the final market shares of
cultural products dramatically depend on consumer heterogeneity and social
interaction pressure. Furthermore, the relation between the resulting market
shares and social interaction is robust with respect to a wide range of
variation in the parameter values and the type of topology.



Memory efficient scheduling of Strassen-Winograd's matrix multiplication algorithm

We propose several new schedules for Strassen-Winograd's matrix
multiplication algorithm, they reduce the extra memory allocation requirements
by three different means: by introducing a few pre-additions, by overwriting
the input matrices, or by using a first recursive level of classical
multiplication. In particular, we show two fully in-place schedules: one having
the same number of operations, if the input matrices can be overwritten; the
other one, slightly increasing the constant of the leading term of the
complexity, if the input matrices are read-only. Many of these schedules have
been found by an implementation of an exhaustive search algorithm based on a
pebble game.



Tripartitions do not always discriminate phylogenetic networks

Phylogenetic networks are a generalization of phylogenetic trees that allow
for the representation of non-treelike evolutionary events, like recombination,
hybridization, or lateral gene transfer. In a recent series of papers devoted
to the study of reconstructibility of phylogenetic networks, Moret, Nakhleh,
Warnow and collaborators introduced the so-called {tripartition metric for
phylogenetic networks. In this paper we show that, in fact, this tripartition
metric does not satisfy the separation axiom of distances (zero distance means
isomorphism, or, in a more relaxed version, zero distance means
indistinguishability in some specific sense) in any of the subclasses of
phylogenetic networks where it is claimed to do so. We also present a subclass
of phylogenetic networks whose members can be singled out by means of their
sets of tripartitions (or even clusters), and hence where the latter can be
used to define a meaningful metric.



Pricing Asian Options for Jump Diffusions

We construct a sequence of functions that uniformly converge (on compact
sets) to the price of Asian option, which is written on a stock whose dynamics
follows a jump diffusion, exponentially fast. Each of the element in this
sequence solves a parabolic partial differen- tial equation (not an
integro-differential equation). As a result we obtain a fast numerical
approximation scheme whose accuracy versus speed characteristics can be
controlled. We analyze the performance of our numerical algorithm on several
examples.



On the Polyphase Decomposition for Design of Generalized Comb Decimation Filters

Generalized comb filters (GCFs) are efficient anti-aliasing decimation
filters with improved selectivity and quantization noise (QN) rejection
performance around the so called folding bands with respect to classical comb
filters.
  In this paper, we address the design of GCF filters by proposing an efficient
partial polyphase architecture with the aim to reduce the data rate as much as
possible after the Sigma-Delta A/D conversion. We propose a mathematical
framework in order to completely characterize the dependence of the frequency
response of GCFs on the quantization of the multipliers embedded in the
proposed filter architecture. This analysis paves the way to the design of
multiplier-less decimation architectures.
  We also derive the impulse response of a sample 3rd order GCF filter used as
a reference scheme throughout the paper.



Multiple-Description Lattice Vector Quantization

In this thesis, we construct and analyze multiple-description codes based on
lattice vector quantization.



Mixed Integer Linear Programming For Exact Finite-Horizon Planning In Decentralized Pomdps

We consider the problem of finding an n-agent joint-policy for the optimal
finite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem
of very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new
mathematical programming approach for the problem. Our approach is based on two
ideas: First, we represent each agent's policy in the sequence-form and not in
the tree-form, thereby obtaining a very compact representation of the set of
joint-policies. Second, using this compact representation, we solve this
problem as an instance of combinatorial optimization for which we formulate a
mixed integer linear program (MILP). The optimal solution of the MILP directly
yields an optimal joint-policy for the Dec-Pomdp. Computational experience
shows that formulating and solving the MILP requires significantly less time to
solve benchmark Dec-Pomdp problems than existing algorithms. For example, the
multi-agent tiger problem for horizon 4 is solved in 72 secs with the MILP
whereas existing algorithms require several hours to solve it.



Rate and Power Allocation for Discrete-Rate Link Adaptation

Link adaptation, in particular adaptive coded modulation (ACM), is a
promising tool for bandwidth-efficient transmission in a fading environment.
The main motivation behind employing ACM schemes is to improve the spectral
efficiency of wireless communication systems. In this paper, using a finite
number of capacity achieving component codes, we propose new transmission
schemes employing constant power transmission, as well as discrete and
continuous power adaptation, for slowly varying flat-fading channels.
  We show that the proposed transmission schemes can achieve throughputs close
to the Shannon limits of flat-fading channels using only a small number of
codes. Specifically, using a fully discrete scheme with just four codes, each
associated with four power levels, we achieve a spectral efficiency within 1 dB
of the continuous-rate continuous-power Shannon capacity. Furthermore, when
restricted to a fixed number of codes, the introduction of power adaptation has
significant gains with respect to ASE and probability of no transmission
compared to a constant power scheme.



A Characterisation of First-Order Constraint Satisfaction Problems

We describe simple algebraic and combinatorial characterisations of finite
relational core structures admitting finitely many obstructions. As a
consequence, we show that it is decidable to determine whether a constraint
satisfaction problem is first-order definable: we show the general problem to
be NP-complete, and give a polynomial-time algorithm in the case of cores. A
slight modification of this algorithm provides, for first-order definable
CSP's, a simple poly-time algorithm to produce a solution when one exists. As
an application of our algebraic characterisation of first order CSP's, we
describe a large family of L-complete CSP's.



Multi-physics Extension of OpenFMO Framework

OpenFMO framework, an open-source software (OSS) platform for Fragment
Molecular Orbital (FMO) method, is extended to multi-physics simulations (MPS).
After reviewing the several FMO implementations on distributed computer
environments, the subsequent development planning corresponding to MPS is
presented. It is discussed which should be selected as a scientific software,
lightweight and reconfigurable form or large and self-contained form.



A fixed point iteration for computing the matrix logarithm

In various areas of applied numerics, the problem of calculating the
logarithm of a matrix A emerges. Since series expansions of the logarithm
usually do not converge well for matrices far away from the identity, the
standard numerical method calculates successive square roots. In this article,
a new algorithm is presented that relies on the computation of successive
matrix exponentials. Convergence of the method is demonstrated for a large
class of initial matrices and favorable choices of the initial matrix are
discussed.



Animation of virtual mannequins, robot-like simulation or motion captures

In order to optimize the costs and time of design of the new products while
improving their quality, concurrent engineering is based on the digital model
of these products, the numerical model. However, in order to be able to avoid
definitively physical model, old support of the design, without loss of
information, new tools must be available. Especially, a tool making it possible
to check simply and quickly the maintainability of complex mechanical sets
using the numerical model is necessary. Since one decade, our team works on the
creation of tool for the generation and the analysis of trajectories of virtual
mannequins. The simulation of human tasks can be carried out either by
robot-like simulation or by simulation by motion capture. This paper presents
some results on the both two methods. The first method is based on a
multi-agent system and on a digital mock-up technology, to assess an efficient
path planner for a manikin or a robot for access and visibility task taking
into account ergonomic constraints or joint and mechanical limits. In order to
solve this problem, the human operator is integrated in the process
optimization to contribute to a global perception of the environment. This
operator cooperates, in real-time, with several automatic local elementary
agents. In the case of the second approach, we worked with the CEA and EADS/CCR
to solve the constraints related to the evolution of human virtual in its
environment on the basis of data resulting from motion capture system. An
approach using of the virtual guides was developed to allow to the user the
realization of precise trajectory in absence of force feedback. The result of
this work validates solutions through the digital mock-up; it can be applied to
simulate maintenability and mountability tasks.



A Framework to Illustrate Kinematic Behavior of Mechanisms by Haptic Feedback

The kinematic properties of mechanisms are well known by the researchers and
teachers. The theory based on the study of Jacobian matrices allows us to
explain, for example, the singular configuration. However, in many cases, the
physical sense of such properties is difficult to explain to students. The aim
of this article is to use haptic feedback to render to the user the
signification of different kinematic indices. The framework uses a Phantom Omni
and a serial and parallel mechanism with two degrees of freedom. The
end-effector of both mechanisms can be moved either by classical mouse, or
Phantom Omni with or without feedback.



On the ergodic sum-rate performance of CDD in multi-user systems

The main focus of space-time coding design and analysis for MIMO systems has
been so far focused on single-user systems. For single-user systems, transmit
diversity schemes suffer a loss in spectral efficiency if the receiver is
equipped with more than one antenna, making them unsuitable for high rate
transmission. One such transmit diversity scheme is the cyclic delay diversity
code (CDD). The advantage of CDD over other diversity schemes such as
orthogonal space-time block codes (OSTBC) is that a code rate of one and delay
optimality are achieved independent of the number of transmit antennas. In this
work we analyze the ergodic rate of a multi-user multiple access channel (MAC)
with each user applying such a cyclic delay diversity (CDD) code. We derive
closed form expressions for the ergodic sum-rate of multi-user CDD and compare
it with the sum-capacity. We study the ergodic rate region and show that in
contrast to what is conventionally known regarding the single-user case,
transmit diversity schemes are viable candidates for high rate transmission in
multi-user systems. Finally, our theoretical findings are illustrated by
numerical simulation results.



Distributed Compression and Multiparty Squashed Entanglement

We study a protocol in which many parties use quantum communication to
transfer a shared state to a receiver without communicating with each other.
This protocol is a multiparty version of the fully quantum Slepian-Wolf
protocol for two senders and arises through the repeated application of the
two-sender protocol. We describe bounds on the achievable rate region for the
distributed compression problem. The inner bound arises by expressing the
achievable rate region for our protocol in terms of its vertices and extreme
rays and, equivalently, in terms of facet inequalities. We also prove an outer
bound on all possible rates for distributed compression based on the multiparty
squashed entanglement, a measure of multiparty entanglement.



A Comparative Study between Two Three-DOF Parallel Kinematic Machines using Kinetostatic Criteria and Interval Analysis

This paper addresses the workspace analysis of two 3-DOF translational
parallel mechanisms designed for machining applications. The two machines
features three fixed linear joints. The joint axes of the first machine are
orthogonal whereas these of the second are parallel. In both cases, the mobile
platform moves in the Cartesian $x-y-z$ space with fixed orientation. The
workspace analysis is conducted on the basis of prescribed kinetostatic
performances. Interval analysis based methods are used to compute the dextrous
workspace and the largest cube enclosed in this workspace.



Multimedia Capacity Analysis of the IEEE 802.11e Contention-based Infrastructure Basic Service Set

We first propose a simple mathematical analysis framework for the Enhanced
Distributed Channel Access (EDCA) function of the recently ratified IEEE
802.11e standard. Our analysis considers the fact that the distributed random
access systems exhibit cyclic behavior. The proposed model is valid for
arbitrary assignments of AC-specific Arbitration Interframe Space (AIFS) values
and Contention Window (CW) sizes and is the first that considers an arbitrary
distribution of active Access Categories (ACs) at the stations. Validating the
theoretical results via extensive simulations, we show that the proposed
analysis accurately captures the EDCA saturation performance. Next, we propose
a framework for multimedia capacity analysis of the EDCA function. We calculate
an accurate station- and AC-specific queue utilization ratio by appropriately
weighing the service time predictions of the cycle time model for different
number of active stations. Based on the calculated queue utilization ratio, we
design a simple model-based admission control scheme. We show that the proposed
call admission control algorithm maintains satisfactory user-perceived quality
for coexisting voice and video connections in an infrastructure BSS and does
not present over- or under-admission problems of previously proposed models in
the literature.



The Virtual Manufacturing concept: Scope, Socio-Economic Aspects and Future Trends

The research area "Virtual Manufacturing (VM)'' is the use of information
technology and computer simulation to model real world manufacturing processes
for the purpose of analysing and understanding them. As automation technologies
such as CAD/CAM have substantially shortened the time required to design
products, Virtual Manufacturing will have a similar effect on the manufacturing
phase thanks to the modelling, simulation and optimisation of the product and
the processes involved in its fabrication. After a description of Virtual
Manufacturing (definitions and scope), we present some socio-economic factors
of VM and finaly some "hot topics'' for the future are proposed.



A Classification of 3R Orthogonal Manipulators by the Topology of their Workspace

A classification of a family of 3-revolute (3R) positining manipulators is
established. This classification is based on the topology of their workspace.
The workspace is characterized in a half-cross section by the singular curves.
The workspace topology is defined by the number of cusps and nodes that appear
on these singular curves. The design parameters space is shown to be divided
into nine domains of distinct workspace topologies, in which all manipulators
have similar global kinematic properties. Each separating surface is given as
an explicit expression in the DH-parameters.



OA@MPS - a colourful view

The open access agenda of the Max Planck Society, initiator of the Berlin
Declaration, envisions the support of both the green way and the golden way to
open access. For the implementation of the green way the Max Planck Society
through its newly established unit (Max Planck Digital Library) follows the
idea of providing a centralized technical platform for publications and a local
support for editorial issues. With regard to the golden way, the Max Planck
Society fosters the development of open access publication models and
experiments new publishing concepts like the Living Reviews journals.



Covering a line segment with variable radius discs

The paper addresses the problem of locating sensors with a circular field of
view so that a given line segment is under full surveillance, which is termed
as the Disc Covering Problem on a Line. The cost of each sensor includes a
fixed component, and a variable component that is proportional to the
field-of-view area. When only one type of sensor or, in general, one type of
disc, is available, then a simple polynomial algorithm solves the problem. When
there are different types of sensors in terms of fixed and variable costs, the
problem becomes NP-hard. A branch-and-bound algorithm as well as an efficient
heuristic are developed. The heuristic very often obtains the optimal solution
as shown in extensive computational testing.



Robust Hypothesis Testing with a Relative Entropy Tolerance

This paper considers the design of a minimax test for two hypotheses where
the actual probability densities of the observations are located in
neighborhoods obtained by placing a bound on the relative entropy between
actual and nominal densities. The minimax problem admits a saddle point which
is characterized. The robust test applies a nonlinear transformation which
flattens the nominal likelihood ratio in the vicinity of one. Results are
illustrated by considering the transmission of binary data in the presence of
additive noise.



Instrumented Collective Learning Situations (ICLS): the Gap between Theoretical Research and Observed Practices

According to socio-constructivism approach, collective situations are
promoted to favor learning in classroom, at a distance or in a blended
educational context. So, many Information and Communication Technologies (ICT)
are provided to teachers but there are no clear studies about the way they are
used and perceived. Our research is based on the hypothesis that practices of
educational actors (instructional designers and tutors) are far away from
theoretical results of research in education technologies. In this paper, we
consider a precise kind of situation: Instrumented Collective Learning
Situations (ICLS). By a survey on 13 fields in higher education in France,
Switzerland and Canada, we present how ICLS are designed and how teachers used
them. Conclusions give an indication on the gap between the way information
technologies are prescribed and the way they are actually used and perceived by
teachers.



Building a Cooperative Communications System

In this paper, we present the results from over-the-air experiments of a
complete implementation of an amplify and forward cooperative communications
system. Our custom OFDM-based physical layer uses a distributed version of the
Alamouti block code, where the relay sends one branch of Alamouti encoded
symbols. First we show analytically and experimentally that amplify and forward
protocols are unaffected by carrier frequency offsets at the relay. This result
allows us to use a conventional Alamouti receiver without change for the
distributed relay system. Our full system implementation shows gains up to
5.5dB in peak power constrained networks. Thus, we can conclusively state that
even the simplest form of relaying can lead to significant gains in practical
implementations.



Situations d'apprentissage collectives instrument\'ees : \'etude de pratiques dans l'enseignement sup\'erieur

Currently, educational platforms propose many tools of communication,
production, labour division or collective work management in order to support
collective activities. But it is not guaranteed that actors (instructional
designers, tutors or learner) are really using them. Our work, describe
characteristics of instrumented learning situations (ICLS) in the higher
education. Our intention is to determine: if ICLS are really existing; which
form they take (in terms of scenario, tools, type of activity...) ; if
recommendations resulting from research tasks are taken into account by
instructional designers and if the instructional designer prescribed activities
are really follow by learners or tutors? To answer these questions, we have
made a survey about ICLS actors uses.



Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms

This work aims at optimizing injection networks, which consist in adding a
set of long-range links (called bypass links) in mobile multi-hop ad hoc
networks so as to improve connectivity and overcome network partitioning. To
this end, we rely on small-world network properties, that comprise a high
clustering coefficient and a low characteristic path length. We investigate the
use of two genetic algorithms (generational and steady-state) to optimize three
instances of this topology control problem and present results that show
initial evidence of their capacity to solve it.



p-Adic Modelling of the Genome and the Genetic Code

The present paper is devoted to foundations of p-adic modelling in genomics.
Considering nucleotides, codons, DNA and RNA sequences, amino acids, and
proteins as information systems, we have formulated the corresponding p-adic
formalisms for their investigations. Each of these systems has its
characteristic prime number used for construction of the related information
space. Relevance of this approach is illustrated by some examples. In
particular, it is shown that degeneration of the genetic code is a p-adic
phenomenon. We have also put forward a hypothesis on evolution of the genetic
code assuming that primitive code was based on single nucleotides and
chronologically first four amino acids. This formalism of p-adic genomic
information systems can be implemented in computer programs and applied to
various concrete cases.



Universal Reinforcement Learning

We consider an agent interacting with an unmodeled environment. At each time,
the agent makes an observation, takes an action, and incurs a cost. Its actions
can influence future observations and costs. The goal is to minimize the
long-term average cost. We propose a novel algorithm, known as the active LZ
algorithm, for optimal control based on ideas from the Lempel-Ziv scheme for
universal data compression and prediction. We establish that, under the active
LZ algorithm, if there exists an integer $K$ such that the future is
conditionally independent of the past given a window of $K$ consecutive actions
and observations, then the average cost converges to the optimum. Experimental
results involving the game of Rock-Paper-Scissors illustrate merits of the
algorithm.



Channel Capacity Estimation using Free Probability Theory

In many channel measurement applications, one needs to estimate some
characteristics of the channels based on a limited set of measurements. This is
mainly due to the highly time varying characteristics of the channel. In this
contribution, it will be shown how free probability can be used for channel
capacity estimation in MIMO systems. Free probability has already been applied
in various application fields such as digital communications, nuclear physics
and mathematical finance, and has been shown to be an invaluable tool for
describing the asymptotic behaviour of many large-dimensional systems. In
particular, using the concept of free deconvolution, we provide an
asymptotically (w.r.t. the number of observations) unbiased capacity estimator
for MIMO channels impaired with noise called the free probability based
estimator. Another estimator, called the Gaussian matrix mean based estimator,
is also introduced by slightly modifying the free probability based estimator.
This estimator is shown to give unbiased estimation of the moments of the
channel matrix for any number of observations. Also, the estimator has this
property when we extend to MIMO channels with phase off-set and frequency
drift, for which no estimator has been provided so far in the literature. It is
also shown that both the free probability based and the Gaussian matrix mean
based estimator are asymptotically unbiased capacity estimators as the number
of transmit antennas go to infinity, regardless of whether phase off-set and
frequency drift are present. The limitations in the two estimators are also
explained. Simulations are run to assess the performance of the estimators for
a low number of antennas and samples to confirm the usefulness of the
asymptotic results.



Inductive Definition and Domain Theoretic Properties of Fully Abstract

A construction of fully abstract typed models for PCF and PCF^+ (i.e., PCF +
"parallel conditional function"), respectively, is presented. It is based on
general notions of sequential computational strategies and wittingly consistent
non-deterministic strategies introduced by the author in the seventies.
Although these notions of strategies are old, the definition of the fully
abstract models is new, in that it is given level-by-level in the finite type
hierarchy. To prove full abstraction and non-dcpo domain theoretic properties
of these models, a theory of computational strategies is developed. This is
also an alternative and, in a sense, an analogue to the later game strategy
semantics approaches of Abramsky, Jagadeesan, and Malacaria; Hyland and Ong;
and Nickau. In both cases of PCF and PCF^+ there are definable universal
(surjective) functionals from numerical functions to any given type,
respectively, which also makes each of these models unique up to isomorphism.
Although such models are non-omega-complete and therefore not continuous in the
traditional terminology, they are also proved to be sequentially complete (a
weakened form of omega-completeness), "naturally" continuous (with respect to
existing directed "pointwise", or "natural" lubs) and also "naturally"
omega-algebraic and "naturally" bounded complete -- appropriate generalisation
of the ordinary notions of domain theory to the case of non-dcpos.



Stacked OSTBC: Error Performance and Rate Analysis

It is well known, that the Alamouti scheme is the only space-time code from
orthogonal design achieving the capacity of a multiple-input multiple-output
(MIMO) wireless communication system with n_T=2 transmit antennas and n_R=1
receive antenna. In this work, we propose the n-times stacked Alamouti scheme
for n_T=2n transmit antennas and show that this scheme achieves the capacity in
the case of n_R=1 receive antenna. This result may regarded as an extension of
the Alamouti case. For the more general case of more than one receive antenna,
we show that if the number of transmit antennas is higher than the number of
receive antennas we achieve a high portion of the capacity with this scheme.
Further, we show that the MIMO capacity is at most twice the rate achieved with
the proposed scheme for all SNR. We derive lower and upper bounds for the rate
achieved with this scheme and compare it with upper and lower bounds for the
capacity. In addition to the capacity analysis based on the assumption of a
coherent channel, we analyze the error rate performance of the stacked OSTBC
with the optimal ML detector and with the suboptimal lattice-reduction (LR)
aided zero-forcing detector. We compare the error rate performance of the
stacked OSTBC with spatial multiplexing (SM) and full-diversity achieving
schemes. Finally, we illustrate the theoretical results by numerical
simulations.



Kinematic and stiffness analysis of the Orthoglide, a PKM with simple, regular workspace and homogeneous performances

The Orthoglide is a Delta-type PKM dedicated to 3-axis rapid machining
applications that was originally developed at IRCCyN in 2000-2001 to meet the
advantages of both serial 3-axis machines (regular workspace and homogeneous
performances) and parallel kinematic architectures (good dynamic performances
and stiffness). This machine has three fixed parallel linear joints that are
mounted orthogonally. The geometric parameters of the Orthoglide were defined
as function of the size of a prescribed cubic Cartesian workspace that is free
of singularities and internal collision. The interesting features of the
Orthoglide are a regular Cartesian workspace shape, uniform performances in all
directions and good compactness. In this paper, a new method is proposed to
analyze the stiffness of overconstrained Delta-type manipulators, such as the
Orthoglide. The Orthoglide is then benchmarked according to geometric,
kinematic and stiffness criteria: workspace to footprint ratio, velocity and
force transmission factors, sensitivity to geometric errors, torsional
stiffness and translational stiffness.



Neutrality and Many-Valued Logics

In this book, we consider various many-valued logics: standard, linear,
hyperbolic, parabolic, non-Archimedean, p-adic, interval, neutrosophic, etc. We
survey also results which show the tree different proof-theoretic frameworks
for many-valued logics, e.g. frameworks of the following deductive calculi:
Hilbert's style, sequent, and hypersequent. We present a general way that
allows to construct systematically analytic calculi for a large family of
non-Archimedean many-valued logics: hyperrational-valued, hyperreal-valued, and
p-adic valued logics characterized by a special format of semantics with an
appropriate rejection of Archimedes' axiom. These logics are built as different
extensions of standard many-valued logics (namely, Lukasiewicz's, Goedel's,
Product, and Post's logics). The informal sense of Archimedes' axiom is that
anything can be measured by a ruler. Also logical multiple-validity without
Archimedes' axiom consists in that the set of truth values is infinite and it
is not well-founded and well-ordered. On the base of non-Archimedean valued
logics, we construct non-Archimedean valued interval neutrosophic logic INL by
which we can describe neutrality phenomena.



Estimation of Small s-t Reliabilities in Acyclic Networks

In the classical s-t network reliability problem a fixed network G is given
including two designated vertices s and t (called terminals). The edges are
subject to independent random failure, and the task is to compute the
probability that s and t are connected in the resulting network, which is known
to be #P-complete. In this paper we are interested in approximating the s-t
reliability in case of a directed acyclic original network G. We introduce and
analyze a specialized version of the Monte-Carlo algorithm given by Karp and
Luby. For the case of uniform edge failure probabilities, we give a worst-case
bound on the number of samples that have to be drawn to obtain an epsilon-delta
approximation, being sharper than the original upper bound. We also derive a
variance reduction of the estimator which reduces the expected number of
iterations to perform to achieve the desired accuracy when applied in
conjunction with different stopping rules. Initial computational results on two
types of random networks (directed acyclic Delaunay graphs and a slightly
modified version of a classical random graph) with up to one million vertices
are presented. These results show the advantage of the introduced Monte-Carlo
approach compared to direct simulation when small reliabilities have to be
estimated and demonstrate its applicability on large-scale instances.



RS-232 Led Board

This article demonstrates how to develop a Microchip PIC16F84 based device
that supports RS-232 interface with PC. Circuit (LED Board) design and software
development will be discussed. PicBasic Pro Compiler from microEngineering
Labs, Inc. is used for PIC programming. Development of LED Board Control
Console using C/C++ is also briefly discussed. The project requires basic work
experience with Microchip PICs, serial communication and programming.



Decentralized sequential change detection using physical layer fusion

The problem of decentralized sequential detection with conditionally
independent observations is studied. The sensors form a star topology with a
central node called fusion center as the hub. The sensors make noisy
observations of a parameter that changes from an initial state to a final state
at a random time where the random change time has a geometric distribution. The
sensors amplify and forward the observations over a wireless Gaussian multiple
access channel and operate under either a power constraint or an energy
constraint. The optimal transmission strategy at each stage is shown to be the
one that maximizes a certain Ali-Silvey distance between the distributions for
the hypotheses before and after the change. Simulations demonstrate that the
proposed analog technique has lower detection delays when compared with
existing schemes. Simulations further demonstrate that the energy-constrained
formulation enables better use of the total available energy than the
power-constrained formulation in the change detection problem.



Autonomous tools for Grid management, monitoring and optimization

We outline design and lines of development of autonomous tools for the
computing Grid management, monitoring and optimization. The management is
proposed to be based on the notion of utility. Grid optimization is considered
to be application-oriented. A generic Grid simulator is proposed as an
optimization tool for Grid structure and functionality.



International Standard for a Linguistic Annotation Framework

This paper describes the Linguistic Annotation Framework under development
within ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to
serve as a basis for harmonizing existing language resources as well as
developing new ones.



A Formal Model of Dictionary Structure and Content

We show that a general model of lexical information conforms to an abstract
model that reflects the hierarchy of information found in a typical dictionary
entry. We show that this model can be mapped into a well-formed XML document,
and how the XSL transformation language can be used to implement a semantics
defined over the abstract model to enable extraction and manipulation of the
information in any format.



Statistical mechanical analysis of the linear vector channel in digital communication

A statistical mechanical framework to analyze linear vector channel models in
digital wireless communication is proposed for a large system. The framework is
a generalization of that proposed for code-division multiple-access systems in
Europhys. Lett. 76 (2006) 1193 and enables the analysis of the system in which
the elements of the channel transfer matrix are statistically correlated with
each other. The significance of the proposed scheme is demonstrated by
assessing the performance of an existing model of multi-input multi-output
communication systems.



Consistency of the group Lasso and multiple kernel learning

We consider the least-square regression problem with regularization by a
block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger
than one. This problem, referred to as the group Lasso, extends the usual
regularization by the 1-norm where all spaces have dimension one, where it is
commonly referred to as the Lasso. In this paper, we study the asymptotic model
consistency of the group Lasso. We derive necessary and sufficient conditions
for the consistency of group Lasso under practical assumptions, such as model
misspecification. When the linear predictors and Euclidean norms are replaced
by functions and reproducing kernel Hilbert norms, the problem is usually
referred to as multiple kernel learning and is commonly used for learning from
heterogeneous data sources and for non linear variable selection. Using tools
from functional analysis, and in particular covariance operators, we extend the
consistency results to this infinite dimensional case and also propose an
adaptive scheme to obtain a consistent model estimate, even when the necessary
condition required for the non adaptive scheme is not satisfied.



Faster subsequence recognition in compressed strings

Computation on compressed strings is one of the key approaches to processing
massive data sets. We consider local subsequence recognition problems on
strings compressed by straight-line programs (SLP), which is closely related to
Lempel--Ziv compression. For an SLP-compressed text of length $\bar m$, and an
uncompressed pattern of length $n$, C{\'e}gielski et al. gave an algorithm for
local subsequence recognition running in time $O(\bar mn^2 \log n)$. We improve
the running time to $O(\bar mn^{1.5})$. Our algorithm can also be used to
compute the longest common subsequence between a compressed text and an
uncompressed pattern in time $O(\bar mn^{1.5})$; the same problem with a
compressed pattern is known to be NP-hard.



Faster exon assembly by sparse spliced alignment

Assembling a gene from candidate exons is an important problem in
computational biology. Among the most successful approaches to this problem is
\emph{spliced alignment}, proposed by Gelfand et al., which scores different
candidate exon chains within a DNA sequence of length $m$ by comparing them to
a known related gene sequence of length n, $m = \Theta(n)$. Gelfand et al.\
gave an algorithm for spliced alignment running in time O(n^3). Kent et al.\
considered sparse spliced alignment, where the number of candidate exons is
O(n), and proposed an algorithm for this problem running in time O(n^{2.5}). We
improve on this result, by proposing an algorithm for sparse spliced alignment
running in time O(n^{2.25}). Our approach is based on a new framework of
\emph{quasi-local string comparison}.



A Knowledge-Based Analysis of Global Function Computation

Consider a distributed system N in which each agent has an input value and
each communication link has a weight. Given a global function, that is, a
function f whose value depends on the whole network, the goal is for every
agent to eventually compute the value f(N). We call this problem global
function computation. Various solutions for instances of this problem, such as
Boolean function computation, leader election, (minimum) spanning tree
construction, and network determination, have been proposed, each under
particular assumptions about what processors know about the system and how this
knowledge can be acquired. We give a necessary and sufficient condition for the
problem to be solvable that generalizes a number of well-known results. We then
provide a knowledge-based (kb) program (like those of Fagin, Halpern, Moses,
and Vardi) that solves global function computation whenever possible. Finally,
we improve the message overhead inherent in our initial kb program by giving a
counterfactual belief-based program that also solves the global function
computation whenever possible, but where agents send messages only when they
believe it is necessary to do so. The latter program is shown to be implemented
by a number of well-known algorithms for solving leader election.



Zero-automatic queues and product form

We introduce and study a new model: 0-automatic queues. Roughly, 0-automatic
queues are characterized by a special buffering mechanism evolving like a
random walk on some infinite group or monoid. The salient result is that all
stable 0-automatic queues have a product form stationary distribution and a
Poisson output process. When considering the two simplest and extremal cases of
0-automatic queues, we recover the simple M/M/1 queue, and Gelenbe's G-queue
with positive and negative customers.



A Generalized Information Formula as the Bridge between Shannon and Popper

A generalized information formula related to logical probability and fuzzy
set is deduced from the classical information formula. The new information
measure accords with to Popper's criterion for knowledge evolution very much.
In comparison with square error criterion, the information criterion does not
only reflect error of a proposition, but also reflects the particularity of the
event described by the proposition. It gives a proposition with less logical
probability higher evaluation. The paper introduces how to select a prediction
or sentence from many for forecasts and language translations according to the
generalized information criterion. It also introduces the rate fidelity theory,
which comes from the improvement of the rate distortion theory in the classical
information theory by replacing distortion (i.e. average error) criterion with
the generalized mutual information criterion, for data compression and
communication efficiency. Some interesting conclusions are obtained from the
rate-fidelity function in relation to image communication. It also discusses
how to improve Popper's theory.



Lattices for Distributed Source Coding: Jointly Gaussian Sources and Reconstruction of a Linear Function

Consider a pair of correlated Gaussian sources (X1,X2). Two separate encoders
observe the two components and communicate compressed versions of their
observations to a common decoder. The decoder is interested in reconstructing a
linear combination of X1 and X2 to within a mean-square distortion of D. We
obtain an inner bound to the optimal rate-distortion region for this problem. A
portion of this inner bound is achieved by a scheme that reconstructs the
linear function directly rather than reconstructing the individual components
X1 and X2 first. This results in a better rate region for certain parameter
values. Our coding scheme relies on lattice coding techniques in contrast to
more prevalent random coding arguments used to demonstrate achievable rate
regions in information theory. We then consider the case of linear
reconstruction of K sources and provide an inner bound to the optimal
rate-distortion region. Some parts of the inner bound are achieved using the
following coding structure: lattice vector quantization followed by
"correlated" lattice-structured binning.



Separable and Low-Rank Continuous Games

In this paper, we study nonzero-sum separable games, which are continuous
games whose payoffs take a sum-of-products form. Included in this subclass are
all finite games and polynomial games. We investigate the structure of
equilibria in separable games. We show that these games admit finitely
supported Nash equilibria. Motivated by the bounds on the supports of mixed
equilibria in two-player finite games in terms of the ranks of the payoff
matrices, we define the notion of the rank of an n-player continuous game and
use this to provide bounds on the cardinality of the support of equilibrium
strategies. We present a general characterization theorem that states that a
continuous game has finite rank if and only if it is separable. Using our rank
results, we present an efficient algorithm for computing approximate equilibria
of two-player separable games with fixed strategy spaces in time polynomial in
the rank of the game.



Quantum Algorithms for Learning and Testing Juntas

In this article we develop quantum algorithms for learning and testing
juntas, i.e. Boolean functions which depend only on an unknown set of k out of
n input variables. Our aim is to develop efficient algorithms:
  - whose sample complexity has no dependence on n, the dimension of the domain
the Boolean functions are defined over;
  - with no access to any classical or quantum membership ("black-box")
queries. Instead, our algorithms use only classical examples generated
uniformly at random and fixed quantum superpositions of such classical
examples;
  - which require only a few quantum examples but possibly many classical
random examples (which are considered quite "cheap" relative to quantum
examples).
  Our quantum algorithms are based on a subroutine FS which enables sampling
according to the Fourier spectrum of f; the FS subroutine was used in earlier
work of Bshouty and Jackson on quantum learning. Our results are as follows:
  - We give an algorithm for testing k-juntas to accuracy $\epsilon$ that uses
$O(k/\epsilon)$ quantum examples. This improves on the number of examples used
by the best known classical algorithm.
  - We establish the following lower bound: any FS-based k-junta testing
algorithm requires $\Omega(\sqrt{k})$ queries.
  - We give an algorithm for learning $k$-juntas to accuracy $\epsilon$ that
uses $O(\epsilon^{-1} k\log k)$ quantum examples and $O(2^k \log(1/\epsilon))$
random examples. We show that this learning algorithms is close to optimal by
giving a related lower bound.



A Bayesian Framework for Combining Valuation Estimates

Obtaining more accurate equity value estimates is the starting point for
stock selection, value-based indexing in a noisy market, and beating benchmark
indices through tactical style rotation. Unfortunately, discounted cash flow,
method of comparables, and fundamental analysis typically yield discrepant
valuation estimates. Moreover, the valuation estimates typically disagree with
market price. Can one form a superior valuation estimate by averaging over the
individual estimates, including market price? This article suggests a Bayesian
framework for combining two or more estimates into a superior valuation
estimate. The framework justifies the common practice of averaging over several
estimates to arrive at a final point estimate.



Workspace and Kinematic Analysis of the VERNE machine

This paper describes the workspace and the inverse and direct kinematic
analysis of the VERNE machine, a serial/parallel 5-axis machine tool designed
by Fatronik for IRCCyN. This machine is composed of a three-degree-of-freedom
(DOF) parallel module and a two-DOF serial tilting table. The parallel module
consists of a moving platform that is connected to a fixed base by three
non-identical legs. This feature involves (i) a simultaneous combination of
rotation and translation for the moving platform, which is balanced by the
tilting table and (ii) workspace whose shape and volume vary as a function of
the tool length. This paper summarizes results obtained in the context of the
European projects NEXT ("Next Generation of Productions Systems").



Upper bound of loss probability in an OFDMA system with randomly located users

For OFDMA systems, we find a rough but easily computed upper bound for the
probability of loosing communications by insufficient number of sub-channels on
downlink. We consider as random the positions of receiving users in the system
as well as the number of sub-channels dedicated to each one. We use recent
results of the theory of point processes which reduce our calculations to the
first and second moments of the total required number of sub-carriers.



e-Science initiatives in Venezuela

Within the context of the nascent e-Science infrastructure in Venezuela, we
describe several web-based scientific applications developed at the Centro
Nacional de Calculo Cientifico Universidad de Los Andes (CeCalCULA), Merida,
and at the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas.
The different strategies that have been followed for implementing quantum
chemistry and atomic physics applications are presented. We also briefly
discuss a damage portal based on dynamic, nonlinear, finite elements of lumped
damage mechanics and a biomedical portal developed within the framework of the
\textit{E-Infrastructure shared between Europe and Latin America} (EELA)
initiative for searching common sequences and inferring their functions in
parasitic diseases such as leishmaniasis, chagas and malaria.



The Kinetostatic Optimization of a Novel Prismatic Drive

The design of a mechanical transmission taking into account the transmitted
forces is reported in this paper. This transmission is based on Slide-o-Cam, a
cam mechanism with multiple rollers mounted on a common translating follower.
The design of Slide-o-Cam, a transmission intended to produce a sliding motion
from a turning drive, or vice versa, was reported elsewhere. This transmission
provides pure-rolling motion, thereby reducing the friction of rack-and-pinions
and linear drives. The pressure angle is a suitable performance index for this
transmission because it determines the amount of force transmitted to the load
vs. that transmitted to the machine frame. To assess the transmission
capability of the mechanism, the Hertz formula is introduced to calculate the
stresses on the rollers and on the cams. The final transmission is intended to
replace the current ball-screws in the Orthoglide, a three-DOF parallel robot
for the production of translational motions, currently under development for
machining applications at Ecole Centrale de Nantes.



Mumford dendrograms

An effective $p$-adic encoding of dendrograms is presented through an
explicit embedding into the Bruhat-Tits tree for a $p$-adic number field. This
field depends on the number of children of a vertex and is a finite extension
of the field of $p$-adic numbers. It is shown that fixing $p$-adic
representatives of the residue field allows a natural way of encoding strings
by identifying a given alphabet with such representatives. A simple $p$-adic
hierarchic classification algorithm is derived for $p$-adic numbers, and is
applied to strings over finite alphabets. Examples of DNA coding are presented
and discussed. Finally, new geometric and combinatorial invariants of time
series of $p$-adic dendrograms are developped.



A Six Degree-Of-Freedom Haptic Device Based On The Orthoglide And A Hybrid Agile Eye

This paper is devoted to the kinematic design of a new six degree-of-freedom
haptic device using two parallel mechanisms. The first one, called orthoglide,
provides the translation motions and the second one, called agile eye, produces
the rotational motions. These two motions are decoupled to simplify the direct
and inverse kinematics, as it is needed for real-time control. To reduce the
inertial load, the motors are fixed on the base and a transmission with two
universal joints is used to transmit the rotational motions from the base to
the end-effector. Two alternative wrists are proposed (i), the agile eye with
three degrees of freedom or (ii) a hybrid wrist made by the assembly of a
two-dof agile eye with a rotary motor. The last one is optimized to increase
its stiffness and to decrease the number of moving parts.



Analyse Comparative des Manipulateurs 3R \`a Axes Orthogonaux

A family of 3R orthogonal manipulators without offset on the third body can
be divided into exactly nine workspace topologies. The workspace is
characterized in a half-cross section by the singular curves. The workspace
topology is defined by the number of cusps and nodes that appear on these
singular curves. Based on this classification, we evaluate theses manipulators
by the condition number related to the joint space and the proportion of the
region with four inverse kinematic solutions compared to a sphere containing
all the workspace. This second performance number is in relation with the
workspace. We determine finally le topology of workspace to which belong
manipulators having the best performance number values.



An Exhaustive Study of the Workspace Topologies of all 3R Orthogonal Manipulators with Geometric Simplifications

This paper analyses the workspace of the three-revolute orthogonal
manipulators that have at least one of their DH parameters equal to zero. These
manipulators are classified into different groups with similar kinematic
properties. The classification criteria are based on the topology of the
workspace. Each group is evaluated according to interesting kinematic
properties such as the size of the workspace subregion reachable with four
inverse kinematic solutions, the existence and the size of voids, and the size
of the regions of feasible paths in the workspace.



Practical Approach to Knowledge-based Question Answering with Natural Language Understanding and Advanced Reasoning

This research hypothesized that a practical approach in the form of a
solution framework known as Natural Language Understanding and Reasoning for
Intelligence (NaLURI), which combines full-discourse natural language
understanding, powerful representation formalism capable of exploiting
ontological information and reasoning approach with advanced features, will
solve the following problems without compromising practicality factors: 1)
restriction on the nature of question and response, and 2) limitation to scale
across domains and to real-life natural language text.



Integration of a Balanced Virtual Manikin in a Virtual Reality Platform aimed at Virtual Prototyping

The work presented here is aimed at introducing a virtual human controller in
a virtual prototyping framework. After a brief introduction describing the
problem solved in the paper, we describe the interest as for digital humans in
the context of concurrent engineering. This leads us to draw a control
architecture enabling to drive virtual humans in a real-time immersed way, and
to interact with the product, through motion capture. Unfortunately, we show
this control scheme can lead to unfeasible movements because of the lack of
balance control. Introducing such a controller is a problem that was never
addressed in the context of real-time. We propose an implementation of a
balance controller, that we insert into the previously described control
scheme. Next section is dedicated to show the results we obtained. Finally, we
propose a virtual reality platform into which the digital character controller
is integrated.



Balanced Virtual Humans Interacting with their Environment

The animation of human avatars seems very successful; the computer graphics
industry shows outstanding results in films everyday, the game industry
achieves exploits... Nevertheless, the animation and control processes of such
manikins are very painful. It takes days to a specialist to build such animated
sequences, and it is not adaptive to any type of modifications. Our main
purpose is the virtual human for engineering, especially virtual prototyping.
As for this domain of activity, such amounts of time are prohibitive.



Virtual reality: A human centered tool for improving Manufacturing

Manufacturing is using Virtual Reality tools to enhance the product life
cycle. Their definitions are still in flux and it is necessary to define their
connections. Thus, firstly, we will introduce more closely some definitions
where we will find that, if the Virtual manufacturing concepts originate from
machining operations and evolve in this manufacturing area, there exist a lot
of applications in different fields such as casting, forging, sheet
metalworking and robotics (mechanisms). From the recent projects in Europe or
in USA, we notice that the human perception or the simulation of mannequin is
more and more needed in both fields. In this context, we have isolated some
applications as ergonomic studies, assembly and maintenance simulation, design
or training where the virtual reality tools can be applied. Thus, we find out a
family of applications where the virtual reality tools give the engineers the
main role in the optimization process. We will illustrate our paper by several
examples where virtual reality interfaces are used and combined with
optimization tools as multi-agent systems.



A New Six Degree-of-Freedom Haptic Device based on the Orthoglide and the Agile Eye

The aim of this paper is to present a new six degree-of-freedom (dof) haptic
device using two parallel mechanisms. The first one, called orthoglide,
provides the translation motions and the second one produces the rotational
motions. These two motions are decoupled to simplify the direct and inverse
kinematics, as it is needed for real-times control. To reduce the inertial
load, the motors are fixed on the base and a transmission with two universal
joints is used to transmit the rotational motions from the base to the
end-effector. The main feature of the orthoglide and of the agile eye mechanism
is the existence of an isotropic configuration. The length of the legs and the
range limits of the orthoglide are optimized to have homogeneous performance
throughout the Cartesian workspace, which has a nearly cubic workspace. These
properties permit to have a high stiffness throughout the workspace and
workspace limits that are easily understandable by the user.



L'orthoglide : une machine-outil rapide d'architecture parall\`ele isotrope

This article presents the Orthoglide project. The purpose of this project is
the realization of a prototype of machine tool to three degrees of translation.
The characteristic of this machine is a parallel kinematic architecture
optimized to obtain a compact workspace with homogeneous performance. For that,
the principal criterion of design which was used is the isotropy.



An exploratory study of Google Scholar

The paper discusses and analyzes the scientific search service Google Scholar
(GS). The focus is on an exploratory study which investigates the coverage of
scientific serials in GS. The study shows deficiencies in the coverage and
up-to-dateness of the GS index. Furthermore, the study points up which Web
servers are the most important data providers for this search service and which
information sources are highly represented. We can show that there is a
relatively large gap in Google Scholars coverage of German literature as well
as weaknesses in the accessibility of Open Access content.
  Keywords: Search engines, Digital libraries, Worldwide Web, Serials,
Electronic journals



The effect of fading, channel inversion, and threshold scheduling on ad hoc networks

This paper addresses three issues in the field of ad hoc network capacity:
the impact of i)channel fading, ii) channel inversion power control, and iii)
threshold-based scheduling on capacity. Channel inversion and threshold
scheduling may be viewed as simple ways to exploit channel state information
(CSI) without requiring cooperation across transmitters. We use the
transmission capacity (TC) as our metric, defined as the maximum spatial
intensity of successful simultaneous transmissions subject to a constraint on
the outage probability (OP). By assuming the nodes are located on the infinite
plane according to a Poisson process, we are able to employ tools from
stochastic geometry to obtain asymptotically tight bounds on the distribution
of the signal-to-interference (SIR) level, yielding in turn tight bounds on the
OP (relative to a given SIR threshold) and the TC. We demonstrate that in the
absence of CSI, fading can significantly reduce the TC and somewhat
surprisingly, channel inversion only makes matters worse. We develop a
threshold-based transmission rule where transmitters are active only if the
channel to their receiver is acceptably strong, obtain expressions for the
optimal threshold, and show that this simple, fully distributed scheme can
significantly reduce the effect of fading.



Semi-local string comparison: algorithmic techniques and applications

A classical measure of string comparison is given by the longest common
subsequence (LCS) problem on a pair of strings. We consider its generalisation,
called the semi-local LCS problem, which arises naturally in many
string-related problems. The semi-local LCS problem asks for the LCS scores for
each of the input strings against every substring of the other input string,
and for every prefix of each input string against every suffix of the other
input string. Such a comparison pattern provides a much more detailed picture
of string similarity than a single LCS score; it also arises naturally in many
string-related problems. In fact, the semi-local LCS problem turns out to be
fundamental for string comparison, providing a powerful and flexible
alternative to classical dynamic programming. It is especially useful when the
input to a string comparison problem may not be available all at once: for
example, comparison of dynamically changing strings; comparison of compressed
strings; parallel string comparison. The same approach can also be applied to
permutation strings, providing efficient solutions for local versions of the
longest increasing subsequence (LIS) problem, and for the problem of computing
a maximum clique in a circle graph. Furthermore, the semi-local LCS problem
turns out to have surprising connections in a few seemingly unrelated fields,
such as computational geometry and algebra of semigroups. This work is devoted
to exploring the structure of the semi-local LCS problem, its efficient
solutions, and its applications in string comparison and other related areas,
including computational molecular biology.



Constant-degree graph expansions that preserve the treewidth

Many hard algorithmic problems dealing with graphs, circuits, formulas and
constraints admit polynomial-time upper bounds if the underlying graph has
small treewidth. The same problems often encourage reducing the maximal degree
of vertices to simplify theoretical arguments or address practical concerns.
Such degree reduction can be performed through a sequence of splittings of
vertices, resulting in an _expansion_ of the original graph. We observe that
the treewidth of a graph may increase dramatically if the splittings are not
performed carefully. In this context we address the following natural question:
is it possible to reduce the maximum degree to a constant without substantially
increasing the treewidth?
  Our work answers the above question affirmatively. We prove that any simple
undirected graph G=(V, E) admits an expansion G'=(V', E') with the maximum
degree <= 3 and treewidth(G') <= treewidth(G)+1. Furthermore, such an expansion
will have no more than 2|E|+|V| vertices and 3|E| edges; it can be computed
efficiently from a tree-decomposition of G. We also construct a family of
examples for which the increase by 1 in treewidth cannot be avoided.



The Review and Analysis of Human Computer Interaction (HCI) Principles

The History of HCI is briefly reviewed together with three HCI models and
structure including CSCW, CSCL and CSCR. It is shown that a number of
authorities consider HCI to be a fragmented discipline with no agreed set of
unifying design principles. An analysis of usability criteria based upon
citation frequency of authors is performed in order to discover the eight most
recognised HCI principles.



A Comparative Study of Parallel Kinematic Architectures for Machining Applications

Parallel kinematic mechanisms are interesting alternative designs for
machining applications. Three 2-DOF parallel mechanism architectures dedicated
to machining applications are studied in this paper. The three mechanisms have
two constant length struts gliding along fixed linear actuated joints with
different relative orientation. The comparative study is conducted on the basis
of a same prescribed Cartesian workspace for the three mechanisms. The common
desired workspace properties are a rectangular shape and given kinetostatic
performances. The machine size of each resulting design is used as a
comparative criterion. The 2-DOF machine mechanisms analyzed in this paper can
be extended to 3-axis machines by adding a third joint.



Kinematic Analysis of a New Parallel Machine Tool: the Orthoglide

This paper describes a new parallel kinematic architecture for machining
applications: the orthoglide. This machine features three fixed parallel linear
joints which are mounted orthogonally and a mobile platform which moves in the
Cartesian x-y-z space with fixed orientation. The main interest of the
orthoglide is that it takes benefit from the advantages of the popular PPP
serial machines (regular Cartesian workspace shape and uniform performances) as
well as from the parallel kinematic arrangement of the links (less inertia and
better dynamic performances), which makes the orthoglide well suited to
high-speed machining applications. Possible extension of the orthoglide to
5-axis machining is also investigated.



Understanding the Characteristics of Internet Short Video Sharing: YouTube as a Case Study

Established in 2005, YouTube has become the most successful Internet site
providing a new generation of short video sharing service. Today, YouTube alone
comprises approximately 20% of all HTTP traffic, or nearly 10% of all traffic
on the Internet. Understanding the features of YouTube and similar video
sharing sites is thus crucial to their sustainable development and to network
traffic engineering. In this paper, using traces crawled in a 3-month period,
we present an in-depth and systematic measurement study on the characteristics
of YouTube videos. We find that YouTube videos have noticeably different
statistics compared to traditional streaming videos, ranging from length and
access pattern, to their active life span, ratings, and comments. The series of
datasets also allows us to identify the growth trend of this fast evolving
Internet site in various aspects, which has seldom been explored before. We
also look closely at the social networking aspect of YouTube, as this is a key
driving force toward its success. In particular, we find that the links to
related videos generated by uploaders' choices form a small-world network. This
suggests that the videos have strong correlations with each other, and creates
opportunities for developing novel caching or peer-to-peer distribution schemes
to efficiently deliver videos to end users.



Products of irreducible random matrices in the (Max,+) Algebra

We consider the recursive equation ``x(n+1)=A(n)x(n)'' where x(n+1) and x(n)
are column vectors of size k and where A(n) is an irreducible random matrix of
size k x k. The matrix-vector multiplication in the (max,+) algebra is defined
by (A(n)x(n))_i= max_j [ A(n)_{ij} +x(n)_j ]. This type of equation can be used
to represent the evolution of Stochastic Event Graphs which include cyclic
Jackson Networks, some manufacturing models and models with general blocking
(such as Kanban). Let us assume that the sequence (A(n))_n is i.i.d or more
generally stationary and ergodic. The main result of the paper states that the
system couples in finite time with a unique stationary regime if and only if
there exists a set of matrices C such that P {A(0) in C} > 0, and the matrices
in C have a unique periodic regime.



The Computation of All 4R Serial Spherical Wrists With an Isotropic Architecture

A spherical wrist of the serial type is said to be isotropic if it can attain
a posture whereby the singular values of its Jacobian matrix are all identical
and nonzero. What isotropy brings about is robustness to manufacturing,
assembly, and measurement errors, thereby guaranteeing a maximum orientation
accuracy. In this paper we investigate the existence of redundant isotropic
architectures, which should add to the dexterity of the wrist under design by
virtue of its extra degree of freedom. The problem formulation leads to a
system of eight quadratic equations with eight unknowns. The Bezout number of
this system is thus 2^8 = 256, its BKK bound being 192. However, the actual
number of solutions is shown to be 32. We list all solutions of the foregoing
algebraic problem. All these solutions are real, but distinct solutions do not
necessarily lead to distinct manipulators. Upon discarding those algebraic
solutions that yield no new wrists, we end up with exactly eight distinct
architectures, the eight corresponding manipulators being displayed at their
isotropic posture.



GCP: Gossip-based Code Propagation for Large-scale Mobile Wireless Sensor Networks

Wireless sensor networks (WSN) have recently received an increasing interest.
They are now expected to be deployed for long periods of time, thus requiring
software updates. Updating the software code automatically on a huge number of
sensors is a tremendous task, as ''by hand'' updates can obviously not be
considered, especially when all participating sensors are embedded on mobile
entities. In this paper, we investigate an approach to automatically update
software in mobile sensor-based application when no localization mechanism is
available. We leverage the peer-to-peer cooperation paradigm to achieve a good
trade-off between reliability and scalability of code propagation. More
specifically, we present the design and evaluation of GCP ({\emph Gossip-based
Code Propagation}), a distributed software update algorithm for mobile wireless
sensor networks. GCP relies on two different mechanisms (piggy-backing and
forwarding control) to improve significantly the load balance without
sacrificing on the propagation speed. We compare GCP against traditional
dissemination approaches. Simulation results based on both synthetic and
realistic workloads show that GCP achieves a good convergence speed while
balancing the load evenly between sensors.



Further Comments on "Residue-to-Binary Converters Based on New Chinese Remainder Theorems"

Ananda Mohan suggested that the first New Chinese Remainder Theorem
introduced by Wang can be derived from the constructive proof of the well-known
Chinese Remainder Theorem (CRT) and claimed that Wang's approach is the same as
the one proposed earlier by Huang. Ananda Mohan's proof is however erroneous
and we show here that Wang's New CRT I is a rewriting of an algorithm
previously sketched by Hitz and Kaltofen.



Recent Advances in Solving the Protein Threading Problem

The fold recognition methods are promissing tools for capturing the structure
of a protein by its amino acid residues sequence but their use is still
restricted by the needs of huge computational resources and suitable efficient
algorithms as well. In the recent version of FROST (Fold Recognition Oriented
Search Tool) package the most efficient algorithm for solving the Protein
Threading Problem (PTP) is implemented due to the strong collaboration between
the SYMBIOSE group in IRISA and MIG in Jouy-en-Josas. In this paper, we present
the diverse components of FROST, emphasizing on the recent advances in
formulating and solving new versions of the PTP and on the way of solving on a
computer cluster a million of instances in a easonable time.



Bijective Faithful Translations among Default Logics

In this article, we study translations between variants of defaults logics
such that the extensions of the theories that are the input and the output of
the translation are in a bijective correspondence. We assume that a translation
can introduce new variables and that the result of translating a theory can
either be produced in time polynomial in the size of the theory or its output
is polynomial in that size; we however restrict to the case in which the
original theory has extensions. This study fills a gap between two previous
pieces of work, one studying bijective translations among restrictions of
default logics, and the other one studying non-bijective translations between
default logics variants.



Interactive Small-Step Algorithms I: Axiomatization

In earlier work, the Abstract State Machine Thesis -- that arbitrary
algorithms are behaviorally equivalent to abstract state machines -- was
established for several classes of algorithms, including ordinary, interactive,
small-step algorithms. This was accomplished on the basis of axiomatizations of
these classes of algorithms. Here we extend the axiomatization and, in a
companion paper, the proof, to cover interactive small-step algorithms that are
not necessarily ordinary. This means that the algorithms (1) can complete a
step without necessarily waiting for replies to all queries from that step and
(2) can use not only the environment's replies but also the order in which the
replies were received.



Interactive Small-Step Algorithms II: Abstract State Machines and the<br> Characterization Theorem

In earlier work, the Abstract State Machine Thesis -- that arbitrary
algorithms are behaviorally equivalent to abstract state machines -- was
established for several classes of algorithms, including ordinary, interactive,
small-step algorithms. This was accomplished on the basis of axiomatizations of
these classes of algorithms. In Part I (Interactive Small-Step Algorithms I:
Axiomatization), the axiomatization was extended to cover interactive
small-step algorithms that are not necessarily ordinary. This means that the
algorithms (1) can complete a step without necessarily waiting for replies to
all queries from that step and (2) can use not only the environment's replies
but also the order in which the replies were received. In order to prove the
thesis for algorithms of this generality, we extend here the definition of
abstract state machines to incorporate explicit attention to the relative
timing of replies and to the possible absence of replies. We prove the
characterization theorem for extended abstract state machines with respect to
general algorithms as axiomatized in Part I.



How to be correct, lazy and efficient ?

This paper is an introduction to Lambdix, a lazy Lisp interpreter implemented
at the Research Laboratory of Paris XI University (Laboratoire de Recherche en
Informatique, Orsay). Lambdix was devised in the course of an investigation
into the relationship between the semantics of programming languages and their
implementation; it was used to demonstrate that in the Lisp domain, semantic
correctness is consistent with efficiency, contrary to what has often been
claimed. The first part of the paper is an overview of well-known semantic
difficulties encountered by Lisp as well as an informal presentation of
Lambdix; it is shown that the difficulties which Lisp encouters do not arise in
Lambdix. The second part is about efficiency in implementation models. It
explains why Lambdix is better suited for lazy evaluation than previous models.
The section ends by giving comparative execution time tables.



Plotkin construction: rank and kernel

Given two binary codes of length n, using Plotkin construction we obtain a
code of length 2n. The construction works for linear and nonlinear codes. For
the linear case, it is straightforward to see that the dimension of the final
code is the sum of the dimensions of the starting codes. For nonlinear codes,
the rank and the dimension of the kernel are standard mesures of linearity. In
this report, we prove that both parameters are also the sum of the
corresponding ones of the starting codes.



Use of a $d$-Constraint During LDPC Decoding in a Bliss Scheme

Bliss schemes of a run length limited (RLL) codec in combination with an LDPC
codec, generate LDPC parity bits over a systematic sequence of RLL channel bits
that are inherently redundant as they satisfy e.g. a $d=1$ minimum run length
constraint. That is the subsequences consisting of runs of length $d=1$, viz.
$...010...$ and $...101...$, cannot occur. We propose to use this redundancy
during LDPC decoding in a Bliss scheme by introducing additional $d$-constraint
nodes in the factor graph used by the LDPC decoder. The messages sent from
these new nodes to the variable or codeword bit nodes exert a ``force'' on the
resulting soft-bit vector coming out of the LDPC decoding that give it a
tendency to comply with the $d$-constraints. This way, we can significantly
reduce the probability of decoding error.



Closed form solutions for symmetric water filling games

We study power control in optimization and game frameworks. In the
optimization framework there is a single decision maker who assigns network
resources and in the game framework users share the network resources according
to Nash equilibrium. The solution of these problems is based on so-called
water-filling technique, which in turn uses bisection method for solution of
non-linear equations for Lagrange multiplies. Here we provide a closed form
solution to the water-filling problem, which allows us to solve it in a finite
number of operations. Also, we produce a closed form solution for the Nash
equilibrium in symmetric Gaussian interference game with an arbitrary number of
users. Even though the game is symmetric, there is an intrinsic hierarchical
structure induced by the quantity of the resources available to the users. We
use this hierarchical structure to perform a successive reduction of the game.
In addition, to its mathematical beauty, the explicit solution allows one to
study limiting cases when the crosstalk coefficient is either small or large.
We provide an alternative simple proof of the convergence of the Iterative
Water Filling Algorithm. Furthermore, it turns out that the convergence of
Iterative Water Filling Algorithm slows down when the crosstalk coefficient is
large. Using the closed form solution, we can avoid this problem. Finally, we
compare the non-cooperative approach with the cooperative approach and show
that the non-cooperative approach results in a more fair resource distribution.



Four-Group Decodable Space-Time Block Codes

Two new rate-one full-diversity space-time block codes (STBC) are proposed.
They are characterized by the \emph{lowest decoding complexity} among the known
rate-one STBC, arising due to the complete separability of the transmitted
symbols into four groups for maximum likelihood detection. The first and the
second codes are delay-optimal if the number of transmit antennas is a power of
2 and even, respectively. The exact pair-wise error probability is derived to
allow for the performance optimization of the two codes. Compared with existing
low-decoding complexity STBC, the two new codes offer several advantages such
as higher code rate, lower encoding/decoding delay and complexity, lower
peak-to-average power ratio, and better performance.



Learning Probabilistic Models of Word Sense Disambiguation

This dissertation presents several new methods of supervised and unsupervised
learning of word sense disambiguation models. The supervised methods focus on
performing model searches through a space of probabilistic models, and the
unsupervised methods rely on the use of Gibbs Sampling and the Expectation
Maximization (EM) algorithm. In both the supervised and unsupervised case, the
Naive Bayesian model is found to perform well. An explanation for this success
is presented in terms of learning rates and bias-variance decompositions.



Clifford Algebra of the Vector Space of Conics for decision boundary Hyperplanes in m-Euclidean Space

In this paper we embed $m$-dimensional Euclidean space in the geometric
algebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of
incidence in the geometric algebra to generalize the notion of separator to a
decision boundary hyperconic in the Clifford algebra of hyperconic sections
denoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear
perceptron or the spherical perceptron in conformal geometry and introduce the
more general conic perceptron, namely the {elliptical perceptron}. Using
Clifford duality a vector orthogonal to the decision boundary hyperplane is
determined. Experimental results are shown in 2-dimensional Euclidean space
where we separate data that are naturally separated by some typical plane conic
separators by this procedure. This procedure is more general in the sense that
it is independent of the dimension of the input data and hence we can speak of
the hyperconic elliptic perceptron.



One-way Hash Function Based on Neural Network

A hash function is constructed based on a three-layer neural network. The
three neuron-layers are used to realize data confusion, diffusion and
compression respectively, and the multi-block hash mode is presented to support
the plaintext with variable length. Theoretical analysis and experimental
results show that this hash function is one-way, with high key sensitivity and
plaintext sensitivity, and secure against birthday attacks or
meet-in-the-middle attacks. Additionally, the neural network's property makes
it practical to realize in a parallel way. These properties make it a suitable
choice for data signature or authentication.



Geometrical derivation of the Boltzmann factor

We show that the Boltzmann factor has a geometrical origin. Its derivation
follows from the microcanonical picture. The Maxwell-Boltzmann distribution or
the wealth distribution in human society are some direct applications of this
new interpretation.



Chain of Separable Binary Goppa Codes and their Minimal Distance

It is shown that subclasses of separable binary Goppa codes, $\Gamma(L,G)$ -
codes, with $L=\{\alpha \in GF(2^{2l}):G(\alpha)\neq 0 \}$ and special Goppa
polynomials G(x) can be presented as a chain of embedded codes. The true
minimal distance has been obtained for all codes of the chain.



Queues, stores, and tableaux

Consider the single server queue with an infinite buffer and a FIFO
discipline, either of type M/M/1 or Geom/Geom/1. Denote by A the arrival
process and by s the services. Assume the stability condition to be satisfied.
Denote by D the departure process in equilibrium and by r the time spent by the
customers at the very back of the queue. We prove that (D,r) has the same law
as (A,s) which is an extension of the classical Burke Theorem. In fact, r can
be viewed as the departures from a dual storage model. This duality between the
two models also appears when studying the transient behavior of a tandem by
means of the RSK algorithm: the first and last row of the resulting
semi-standard Young tableau are respectively the last instant of departure in
the queue and the total number of departures in the store.



Services within a busy period of an M/M/1 queue and Dyck paths

We analyze the service times of customers in a stable M/M/1 queue in
equilibrium depending on their position in a busy period. We give the law of
the service of a customer at the beginning, at the end, or in the middle of the
busy period. It enables as a by-product to prove that the process of instants
of beginning of services is not Poisson. We then proceed to a more precise
analysis. We consider a family of polynomial generating series associated with
Dyck paths of length 2n and we show that they provide the correlation function
of the successive services in a busy period with (n+1) customers.



Multiuser Successive Refinement and Multiple Description Coding

We consider the multiuser successive refinement (MSR) problem, where the
users are connected to a central server via links with different noiseless
capacities, and each user wishes to reconstruct in a successive-refinement
fashion. An achievable region is given for the two-user two-layer case and it
provides the complete rate-distortion region for the Gaussian source under the
MSE distortion measure. The key observation is that this problem includes the
multiple description (MD) problem (with two descriptions) as a subsystem, and
the techniques useful in the MD problem can be extended to this case. We show
that the coding scheme based on the universality of random binning is
sub-optimal, because multiple Gaussian side informations only at the decoders
do incur performance loss, in contrast to the case of single side information
at the decoder. We further show that unlike the single user case, when there
are multiple users, the loss of performance by a multistage coding approach can
be unbounded for the Gaussian source. The result suggests that in such a
setting, the benefit of using successive refinement is not likely to justify
the accompanying performance loss. The MSR problem is also related to the
source coding problem where each decoder has its individual side information,
while the encoder has the complete set of the side informations. The MSR
problem further includes several variations of the MD problem, for which the
specialization of the general result is investigated and the implication is
discussed.



Parsimony Principles for Software Components and Metalanguages

Software is a communication system. The usual topic of communication is
program behavior, as encoded by programs. Domain-specific libraries are
codebooks, domain-specific languages are coding schemes, and so forth. To turn
metaphor into method, we adapt toolsfrom information theory--the study of
efficient communication--to probe the efficiency with which languages and
libraries let us communicate programs. In previous work we developed an
information-theoretic analysis of software reuse in problem domains. This new
paper uses information theory to analyze tradeoffs in the design of components,
generators, and metalanguages. We seek answers to two questions: (1) How can we
judge whether a component is over- or under-generalized? Drawing on minimum
description length principles, we propose that the best component yields the
most succinct representation of the use cases. (2) If we view a programming
language as an assemblage of metalanguages, each providing a complementary
style of abstraction, how can these metalanguages aid or hinder us in
efficiently describing software? We describe a complex triangle of interactions
between the power of an abstraction mechanism, the amount of reuse it enables,
and the cognitive difficulty of its use.



Reductionism, emergence, and levels of abstractions

Can there be independent higher level laws of nature if everything is
reducible to the fundamental laws of physics? The computer science notion of
level of abstraction explains why there can -- illustrating how computational
thinking can solve one of philosophy's most vexing problems.



Complexity of Propositional Proofs under a Promise

We study -- within the framework of propositional proof complexity -- the
problem of certifying unsatisfiability of CNF formulas under the promise that
any satisfiable formula has many satisfying assignments, where ``many'' stands
for an explicitly specified function $\Lam$ in the number of variables $n$. To
this end, we develop propositional proof systems under different measures of
promises (that is, different $\Lam$) as extensions of resolution. This is done
by augmenting resolution with axioms that, roughly, can eliminate sets of truth
assignments defined by Boolean circuits. We then investigate the complexity of
such systems, obtaining an exponential separation in the average-case between
resolution under different size promises:
  1. Resolution has polynomial-size refutations for all unsatisfiable 3CNF
formulas when the promise is $\eps\cd2^n$, for any constant $0<\eps<1$.
  2. There are no sub-exponential size resolution refutations for random 3CNF
formulas, when the promise is $2^{\delta n}$ (and the number of clauses is
$o(n^{3/2})$), for any constant $0<\delta<1$.



Star Unfolding Convex Polyhedra via Quasigeodesic Loops

We extend the notion of star unfolding to be based on a quasigeodesic loop Q
rather than on a point. This gives a new general method to unfold the surface
of any convex polyhedron P to a simple (non-overlapping), planar polygon: cut
along one shortest path from each vertex of P to Q, and cut all but one segment
of Q.



A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network

In this paper, we employ Probabilistic Neural Network (PNN) with image and
data processing techniques to implement a general purpose automated leaf
recognition algorithm. 12 leaf features are extracted and orthogonalized into 5
principal variables which consist the input vector of the PNN. The PNN is
trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater
than 90%. Compared with other approaches, our algorithm is an accurate
artificial intelligence approach which is fast in execution and easy in
implementation.



A note on equipartition

The problem of the existence of an equi-partition of a curve in $\R^n$ has
recently been raised in the context of computational geometry. The problem is
to show that for a (continuous) curve $\Gamma : [0,1] \to \R^n$ and for any
positive integer N, there exist points $t_0=0<t_1<...<t_{N-1}<1=t_N$, such that
$d(\Gamma(t_{i-1}),\Gamma(t_i))=d(\Gamma(t_{i}),\Gamma(t_{i+1}))$ for all
$i=1,...,N$, where d is a metric or even a semi-metric (a weaker notion) on
$\R^n$. We show here that the existence of such points, in a broader context,
is a consequence of Brower's fixed point theorem.



Spatial Aggregation: Data Model and Implementation

Data aggregation in Geographic Information Systems (GIS) is only marginally
present in commercial systems nowadays, mostly through ad-hoc solutions. In
this paper, we first present a formal model for representing spatial data. This
model integrates geographic data and information contained in data warehouses
external to the GIS. We define the notion of geometric aggregation, a general
framework for aggregate queries in a GIS setting. We also identify the class of
summable queries, which can be efficiently evaluated by precomputing the
overlay of two or more of the thematic layers involved in the query. We also
sketch a language, denoted GISOLAP-QL, for expressing queries that involve GIS
and OLAP features. In addition, we introduce Piet, an implementation of our
proposal, that makes use of overlay precomputation for answering spatial
queries (aggregate or not). Our experimental evaluation showed that for a
certain class of geometric queries with or without aggregation, overlay
precomputation outperforms R-tree-based techniques. Finally, as a particular
application of our proposal, we study topological queries.



Embedded Rank Distance Codes for ISI channels

Designs for transmit alphabet constrained space-time codes naturally lead to
questions about the design of rank distance codes. Recently, diversity embedded
multi-level space-time codes for flat fading channels have been designed from
sets of binary matrices with rank distance guarantees over the binary field by
mapping them onto QAM and PSK constellations. In this paper we demonstrate that
diversity embedded space-time codes for fading Inter-Symbol Interference (ISI)
channels can be designed with provable rank distance guarantees. As a corollary
we obtain an asymptotic characterization of the fixed transmit alphabet
rate-diversity trade-off for multiple antenna fading ISI channels. The key idea
is to construct and analyze properties of binary matrices with a particular
structure induced by ISI channels.



Linear-programming Decoding of Non-binary Linear Codes

We develop a framework for linear-programming (LP) decoding of non-binary
linear codes over rings. We prove that the resulting LP decoder has the
`maximum likelihood certificate' property, and we show that the decoder output
is the lowest cost pseudocodeword. Equivalence between pseudocodewords of the
linear program and pseudocodewords of graph covers is proved. LP decoding
performance is illustrated for the (11,6,5) ternary Golay code with ternary PSK
modulation over AWGN, and in this case it is shown that the LP decoder
performance is comparable to codeword-error-rate-optimum hard-decision based
decoding.



Blocking a transition in a Free Choice net and what it tells about its throughput

In a live and bounded Free Choice Petri net, pick a non-conflicting
transition. Then there exists a unique reachable marking in which no transition
is enabled except the selected one. For a routed live and bounded Free Choice
net, this property is true for any transition of the net. Consider now a live
and bounded stochastic routed Free Choice net, and assume that the routings and
the firing times are independent and identically distributed. Using the above
results, we prove the existence of asymptotic firing throughputs for all
transitions in the net. Furthermore the vector of the throughputs at the
different transitions is explicitly computable up to a multiplicative constant.



Separation Logic for Small-step Cminor

Cminor is a mid-level imperative programming language; there are
proved-correct optimizing compilers from C to Cminor and from Cminor to machine
language. We have redesigned Cminor so that it is suitable for Hoare Logic
reasoning and we have designed a Separation Logic for Cminor. In this paper, we
give a small-step semantics (instead of the big-step of the proved-correct
compiler) that is motivated by the need to support future concurrent
extensions. We detail a machine-checked proof of soundness of our Separation
Logic. This is the first large-scale machine-checked proof of a Separation
Logic w.r.t. a small-step semantics. The work presented in this paper has been
carried out in the Coq proof assistant. It is a first step towards an
environment in which concurrent Cminor programs can be verified using
Separation Logic and also compiled by a proved-correct compiler with formal
end-to-end correctness guarantees.



On sparse representations of linear operators and the approximation of matrix products

Thus far, sparse representations have been exploited largely in the context
of robustly estimating functions in a noisy environment from a few
measurements. In this context, the existence of a basis in which the signal
class under consideration is sparse is used to decrease the number of necessary
measurements while controlling the approximation error. In this paper, we
instead focus on applications in numerical analysis, by way of sparse
representations of linear operators with the objective of minimizing the number
of operations needed to perform basic operations (here, multiplication) on
these operators. We represent a linear operator by a sum of rank-one operators,
and show how a sparse representation that guarantees a low approximation error
for the product can be obtained from analyzing an induced quadratic form. This
construction in turn yields new algorithms for computing approximate matrix
products.



Small weakly universal Turing machines

We give small universal Turing machines with state-symbol pairs of (6, 2),
(3, 3) and (2, 4). These machines are weakly universal, which means that they
have an infinitely repeated word to the left of their input and another to the
right. They simulate Rule 110 and are currently the smallest known weakly
universal Turing machines.



Competitive minimax universal decoding for several ensembles of random codes

Universally achievable error exponents pertaining to certain families of
channels (most notably, discrete memoryless channels (DMC's)), and various
ensembles of random codes, are studied by combining the competitive minimax
approach, proposed by Feder and Merhav, with Chernoff bound and Gallager's
techniques for the analysis of error exponents. In particular, we derive a
single--letter expression for the largest, universally achievable fraction
$\xi$ of the optimum error exponent pertaining to the optimum ML decoding.
Moreover, a simpler single--letter expression for a lower bound to $\xi$ is
presented. To demonstrate the tightness of this lower bound, we use it to show
that $\xi=1$, for the binary symmetric channel (BSC), when the random coding
distribution is uniform over: (i) all codes (of a given rate), and (ii) all
linear codes, in agreement with well--known results. We also show that $\xi=1$
for the uniform ensemble of systematic linear codes, and for that of
time--varying convolutional codes in the bit-error--rate sense. For the latter
case, we also show how the corresponding universal decoder can be efficiently
implemented using a slightly modified version of the Viterbi algorithm which em
employs two trellises.



On Throughput Scaling of Wireless Networks: Effect of Node Density and Propagation Model

This paper derives a lower bound to the per-node throughput achievable by a
wireless network when n source-destination pairs are randomly distributed
throughout a disk of radius $n^\gamma$, $ \gamma \geq 0$, propagation is
modeled by attenuation of the form $1/(1+d)^\alpha$, $\alpha >2$, and
successful transmission occurs at a fixed rate W when received signal to noise
and interference ratio is greater than some threshold $\beta$, and at rate 0
otherwise. The lower bound has the form $n^{1-\gamma}$ when $\gamma < 1/2$, and
$(n \ln n)^{-1/2}$ when $\gamma \geq 1/2$. The methods are similar to, but
somewhat simpler than, those in the seminal paper by Gupta and Kumar.



Image Authentication Based on Neural Networks

Neural network has been attracting more and more researchers since the past
decades. The properties, such as parameter sensitivity, random similarity,
learning ability, etc., make it suitable for information protection, such as
data encryption, data authentication, intrusion detection, etc. In this paper,
by investigating neural networks' properties, the low-cost authentication
method based on neural networks is proposed and used to authenticate images or
videos. The authentication method can detect whether the images or videos are
modified maliciously. Firstly, this chapter introduces neural networks'
properties, such as parameter sensitivity, random similarity, diffusion
property, confusion property, one-way property, etc. Secondly, the chapter
gives an introduction to neural network based protection methods. Thirdly, an
image or video authentication scheme based on neural networks is presented, and
its performances, including security, robustness and efficiency, are analyzed.
Finally, conclusions are drawn, and some open issues in this field are
presented.



On the Complexity of the Interlace Polynomial

We consider the two-variable interlace polynomial introduced by Arratia,
Bollobas and Sorkin (2004). We develop graph transformations which allow us to
derive point-to-point reductions for the interlace polynomial. Exploiting these
reductions we obtain new results concerning the computational complexity of
evaluating the interlace polynomial at a fixed point. Regarding exact
evaluation, we prove that the interlace polynomial is #P-hard to evaluate at
every point of the plane, except on one line, where it is trivially polynomial
time computable, and four lines, where the complexity is still open. This
solves a problem posed by Arratia, Bollobas and Sorkin (2004). In particular,
three specializations of the two-variable interlace polynomial, the
vertex-nullity interlace polynomial, the vertex-rank interlace polynomial and
the independent set polynomial, are almost everywhere #P-hard to evaluate, too.
For the independent set polynomial, our reductions allow us to prove that it is
even hard to approximate at any point except at 0.



Note on edge-colored graphs and digraphs without properly colored cycles

We study the following two functions: d(n,c) and $\vec{d}(n,c)$; d(n,c)
($\vec{d}(n,c)$) is the minimum number k such that every c-edge-colored
undirected (directed) graph of order n and minimum monochromatic degree
(out-degree) at least k has a properly colored cycle. Abouelaoualim et al.
(2007) stated a conjecture which implies that d(n,c)=1. Using a recursive
construction of c-edge-colored graphs with minimum monochromatic degree p and
without properly colored cycles, we show that $d(n,c)\ge {1 \over c}(\log_cn
-\log_c\log_cn)$ and, thus, the conjecture does not hold. In particular, this
inequality significantly improves a lower bound on $\vec{d}(n,2)$ obtained by
Gutin, Sudakov and Yeo in 1998.



Side-information Scalable Source Coding

The problem of side-information scalable (SI-scalable) source coding is
considered in this work, where the encoder constructs a progressive
description, such that the receiver with high quality side information will be
able to truncate the bitstream and reconstruct in the rate distortion sense,
while the receiver with low quality side information will have to receive
further data in order to decode. We provide inner and outer bounds for general
discrete memoryless sources. The achievable region is shown to be tight for the
case that either of the decoders requires a lossless reconstruction, as well as
the case with degraded deterministic distortion measures. Furthermore we show
that the gap between the achievable region and the outer bounds can be bounded
by a constant when square error distortion measure is used. The notion of
perfectly scalable coding is introduced as both the stages operate on the
Wyner-Ziv bound, and necessary and sufficient conditions are given for sources
satisfying a mild support condition. Using SI-scalable coding and successive
refinement Wyner-Ziv coding as basic building blocks, a complete
characterization is provided for the important quadratic Gaussian source with
multiple jointly Gaussian side-informations, where the side information quality
does not have to be monotonic along the scalable coding order. Partial result
is provided for the doubly symmetric binary source with Hamming distortion when
the worse side information is a constant, for which one of the outer bound is
strictly tighter than the other one.



Nonlinear Matroid Optimization and Experimental Design

We study the problem of optimizing nonlinear objective functions over
matroids presented by oracles or explicitly. Such functions can be interpreted
as the balancing of multi-criteria optimization. We provide a combinatorial
polynomial time algorithm for arbitrary oracle-presented matroids, that makes
repeated use of matroid intersection, and an algebraic algorithm for vectorial
matroids.
  Our work is partly motivated by applications to minimum-aberration
model-fitting in experimental design in statistics, which we discuss and
demonstrate in detail.



Comments on the Reliability of Lawson and Hanson's Linear Distance Programming Algorithm: Subroutine LDP

This brief paper: (1) Discusses strategies to generate random test cases that
can be used to extensively test any Linear Distance Program (LDP) software. (2)
Gives three numerical examples of input cases generated by this strategy that
cause problems in the Lawson and Hanson LDP module. (3) Proposes, as a standard
matter of acceptable implementation procedures, that (unless it is done
internally in the software itself, but, in general, this seems to be much rarer
than one would expect) all users should test the returned output from any LDP
module for self-consistency since it incurs only a small amount of added
computational overhead and it is not hard to do.



Communication under Strong Asynchronism

We consider asynchronous communication over point-to-point discrete
memoryless channels. The transmitter starts sending one block codeword at an
instant that is uniformly distributed within a certain time period, which
represents the level of asynchronism. The receiver, by means of a sequential
decoder, must isolate the message without knowing when the codeword
transmission starts but being cognizant of the asynchronism level A. We are
interested in how quickly can the receiver isolate the sent message,
particularly in the regime where A is exponentially larger than the codeword
length N, which we refer to as `strong asynchronism.'
  This model of sparse communication may represent the situation of a sensor
that remains idle most of the time and, only occasionally, transmits
information to a remote base station which needs to quickly take action.
  The first result shows that vanishing error probability can be guaranteed as
N tends to infinity while A grows as Exp(N*k) if and only if k does not exceed
the `synchronization threshold,' a constant that admits a simple closed form
expression, and is at least as large as the capacity of the synchronized
channel. The second result is the characterization of a set of achievable
strictly positive rates in the regime where A is exponential in N, and where
the rate is defined with respect to the expected delay between the time
information starts being emitted until the time the receiver makes a decision.
  As an application of the first result we consider antipodal signaling over a
Gaussian channel and derive a simple necessary condition between A, N, and SNR
for achieving reliable communication.



Difference Equations in Massive Higher Order Calculations

The calculation of massive 2--loop operator matrix elements, required for the
higher order Wilson coefficients for heavy flavor production in deeply
inelastic scattering, leads to new types of multiple infinite sums over
harmonic sums and related functions, which depend on the Mellin parameter $N$.
We report on the solution of these sums through higher order difference
equations using the summation package {\tt Sigma}.



Periodic complementary sets of binary sequences

Let PCS_p^N denote a set of p binary sequences of length N such that the sum
of their periodic auto-correlation functions is a delta-function. In the 1990,
Boemer and Antweiler addressed the problem of constructing such sequences. They
presented a table covering the range p <= 12, N <= 50 and showing in which
cases it was known at that time whether such sequences exist, do not exist, or
the question of existence is undecided. The number of undecided cases was
rather large.
  Subsequently the number of undecided cases was reduced to 26 by the author.
In the present note, several cyclic difference families are constructed and
used to obtain new sets of periodic binary sequences. Thereby the original
problem of Boemer and Antweiler is completely solved.



Virtual screening with support vector machines and structure kernels

Support vector machines and kernel methods have recently gained considerable
attention in chemoinformatics. They offer generally good performance for
problems of supervised classification or regression, and provide a flexible and
computationally efficient framework to include relevant information and prior
knowledge about the data and problems to be handled. In particular, with kernel
methods molecules do not need to be represented and stored explicitly as
vectors or fingerprints, but only to be compared to each other through a
comparison function technically called a kernel. While classical kernels can be
used to compare vector or fingerprint representations of molecules, completely
new kernels were developed in the recent years to directly compare the 2D or 3D
structures of molecules, without the need for an explicit vectorization step
through the extraction of molecular descriptors. While still in their infancy,
these approaches have already demonstrated their relevance on several toxicity
prediction and structure-activity relationship problems.



A Note on Shortest Developments

De Vrijer has presented a proof of the finite developments theorem which, in
addition to showing that all developments are finite, gives an effective
reduction strategy computing longest developments as well as a simple formula
computing their length.
  We show that by applying a rather simple and intuitive principle of duality
to de Vrijer's approach one arrives at a proof that some developments are
finite which in addition yields an effective reduction strategy computing
shortest developments as well as a simple formula computing their length. The
duality fails for general beta-reduction.
  Our results simplify previous work by Khasidashvili.



Multisource Bayesian sequential change detection

Suppose that local characteristics of several independent compound Poisson
and Wiener processes change suddenly and simultaneously at some unobservable
disorder time. The problem is to detect the disorder time as quickly as
possible after it happens and minimize the rate of false alarms at the same
time. These problems arise, for example, from managing product quality in
manufacturing systems and preventing the spread of infectious diseases. The
promptness and accuracy of detection rules improve greatly if multiple
independent information sources are available. Earlier work on sequential
change detection in continuous time does not provide optimal rules for
situations in which several marked count data and continuously changing signals
are simultaneously observable. In this paper, optimal Bayesian sequential
detection rules are developed for such problems when the marked count data is
in the form of independent compound Poisson processes, and the continuously
changing signals form a multi-dimensional Wiener process. An auxiliary optimal
stopping problem for a jump-diffusion process is solved by transforming it
first into a sequence of optimal stopping problems for a pure diffusion by
means of a jump operator. This method is new and can be very useful in other
applications as well, because it allows the use of the powerful optimal
stopping theory for diffusions.



Distributing the Kalman Filter for Large-Scale Systems

This paper derives a \emph{distributed} Kalman filter to estimate a sparsely
connected, large-scale, $n-$dimensional, dynamical system monitored by a
network of $N$ sensors. Local Kalman filters are implemented on the
($n_l-$dimensional, where $n_l\ll n$) sub-systems that are obtained after
spatially decomposing the large-scale system. The resulting sub-systems
overlap, which along with an assimilation procedure on the local Kalman
filters, preserve an $L$th order Gauss-Markovian structure of the centralized
error processes. The information loss due to the $L$th order Gauss-Markovian
approximation is controllable as it can be characterized by a divergence that
decreases as $L\uparrow$. The order of the approximation, $L$, leads to a lower
bound on the dimension of the sub-systems, hence, providing a criterion for
sub-system selection. The assimilation procedure is carried out on the local
error covariances with a distributed iterate collapse inversion (DICI)
algorithm that we introduce. The DICI algorithm computes the (approximated)
centralized Riccati and Lyapunov equations iteratively with only local
communication and low-order computation. We fuse the observations that are
common among the local Kalman filters using bipartite fusion graphs and
consensus averaging algorithms. The proposed algorithm achieves full
distribution of the Kalman filter that is coherent with the centralized Kalman
filter with an $L$th order Gaussian-Markovian structure on the centralized
error processes. Nowhere storage, communication, or computation of
$n-$dimensional vectors and matrices is needed; only $n_l \ll n$ dimensional
vectors and matrices are communicated or used in the computation at the
sensors.



Capacity Region of the Finite-State Multiple Access Channel with and without Feedback

The capacity region of the Finite-State Multiple Access Channel (FS-MAC) with
feedback that may be an arbitrary time-invariant function of the channel output
samples is considered. We characterize both an inner and an outer bound for
this region, using Masseys's directed information. These bounds are shown to
coincide, and hence yield the capacity region, of FS-MACs where the state
process is stationary and ergodic and not affected by the inputs.
  Though `multi-letter' in general, our results yield explicit conclusions when
applied to specific scenarios of interest. E.g., our results allow us to:
  - Identify a large class of FS-MACs, that includes the additive mod-2 noise
MAC where the noise may have memory, for which feedback does not enlarge the
capacity region.
  - Deduce that, for a general FS-MAC with states that are not affected by the
input, if the capacity (region) without feedback is zero, then so is the
capacity (region) with feedback.
  - Deduce that the capacity region of a MAC that can be decomposed into a
`multiplexer' concatenated by a point-to-point channel (with, without, or with
partial feedback), the capacity region is given by $\sum_{m} R_m \leq C$, where
C is the capacity of the point to point channel and m indexes the encoders.
Moreover, we show that for this family of channels source-channel coding
separation holds.



The Local Fractal Properties of the Financial Time Series on the Polish Stock Exchange Market

We investigate the local fractal properties of the financial time series
based on the evolution of the Warsaw Stock Exchange Index (WIG) connected with
the largest developing financial market in Europe. Calculating the local Hurst
exponent for the WIG time series we find an interesting dependence between the
behavior of the local fractal properties of the WIG time series and the crashes
appearance on the financial market.



Why the relational data model can be considered as a formal basis for group operations in object-oriented systems

Relational data model defines a specification of a type "relation". However,
its simplicity does not mean that the system implementing this model must
operate with structures having the same simplicity. We consider two principles
allowing create a system which combines object-oriented paradigm (OOP) and
relational data model (RDM) in one framework. The first principle -- "complex
data in encapsulated domains" -- is well known from The Third Manifesto by Date
and Darwen. The second principle --"data complexity in names"-- is the basis
for a system where data are described as complex objects and uniquely
represented as a set of relations. Names of these relations and names of their
attributes are combinations of names entered in specifications of the complex
objects. Below, we consider the main properties of such a system.



Diversity of MIMO Multihop Relay Channels

We consider slow fading relay channels with a single multi-antenna
source-destination terminal pair. The source signal arrives at the destination
via N hops through N-1 layers of relays. We analyze the diversity of such
channels with fixed network size at high SNR. In the clustered case where the
relays within the same layer can have full cooperation, the cooperative
decode-and-forward (DF) scheme is shown to be optimal in terms of the
diversity-multiplexing tradeoff (DMT). The upper bound on the DMT, the cut-set
bound, is attained. In the non-clustered case, we show that the naive
amplify-and-forward (AF) scheme has the maximum multiplexing gain of the
channel but is suboptimal in diversity, as compared to the cut-set bound. To
improve the diversity, space-time relay processing is introduced through the
parallel partition of the multihop channel. The idea is to let the source
signal go through K different "AF paths" in the multihop channel. This parallel
AF scheme creates a parallel channel in the time domain and has the maximum
diversity if the partition is properly designed. Since this scheme does not
achieve the maximum multiplexing gain in general, we propose a flip-and-forward
(FF) scheme that is built from the parallel AF scheme. It is shown that the FF
scheme achieves both the maximum diversity and multiplexing gains in a
distributed multihop channel of arbitrary size. In order to realize the DMT
promised by the relaying strategies, approximately universal coding schemes are
also proposed.



Virtual Manufacturing : Tools for improving Design and Production

The research area "Virtual Manufacturing" can be defined as an integrated
manufacturing environment which can enhance one or several levels of decision
and control in manufacturing process. Several domains can be addressed: Product
and Process Design, Process and Production Planning, Machine Tool, Robot and
Manufacturing System. As automation technologies such as CAD/CAM have
substantially shortened the time required to design products, Virtual
Manufacturing will have a similar effect on the manufacturing phase thanks to
the modelling, simulation and optimisation of the product and the processes
involved in its fabrication.



A preliminary analysis on metaheuristics methods applied to the Haplotype Inference Problem

Haplotype Inference is a challenging problem in bioinformatics that consists
in inferring the basic genetic constitution of diploid organisms on the basis
of their genotype. This information allows researchers to perform association
studies for the genetic variants involved in diseases and the individual
responses to therapeutic agents.
  A notable approach to the problem is to encode it as a combinatorial problem
(under certain hypotheses, such as the pure parsimony criterion) and to solve
it using off-the-shelf combinatorial optimization techniques. The main methods
applied to Haplotype Inference are either simple greedy heuristic or exact
methods (Integer Linear Programming, Semidefinite Programming, SAT encoding)
that, at present, are adequate only for moderate size instances.
  We believe that metaheuristic and hybrid approaches could provide a better
scalability. Moreover, metaheuristics can be very easily combined with problem
specific heuristics and they can also be integrated with tree-based search
techniques, thus providing a promising framework for hybrid systems in which a
good trade-off between effectiveness and efficiency can be reached.
  In this paper we illustrate a feasibility study of the approach and discuss
some relevant design issues, such as modeling and design of approximate solvers
that combine constructive heuristics, local search-based improvement strategies
and learning mechanisms. Besides the relevance of the Haplotype Inference
problem itself, this preliminary analysis is also an interesting case study
because the formulation of the problem poses some challenges in modeling and
hybrid metaheuristic solver design that can be generalized to other problems.



Quasi-stationary distributions as centrality measures of reducible graphs

Random walk can be used as a centrality measure of a directed graph. However,
if the graph is reducible the random walk will be absorbed in some subset of
nodes and will never visit the rest of the graph. In Google PageRank the
problem was solved by introduction of uniform random jumps with some
probability. Up to the present, there is no clear criterion for the choice this
parameter. We propose to use parameter-free centrality measure which is based
on the notion of quasi-stationary distribution. Specifically we suggest four
quasi-stationary based centrality measures, analyze them and conclude that they
produce approximately the same ranking. The new centrality measures can be
applied in spam detection to detect ``link farms'' and in image search to find
photo albums.



Efficient Divide-and-Conquer Implementations Of Symmetric FSAs

A deterministic finite-state automaton (FSA) is an abstract sequential
machine that reads the symbols comprising an input word one at a time. An FSA
is symmetric if its output is independent of the order in which the input
symbols are read, i.e., if the output is invariant under permutations of the
input. We show how to convert a symmetric FSA A into an automaton-like
divide-and-conquer process whose intermediate results are no larger than the
size of A's memory. In comparison, a similar result for general FSA's has been
long known via functional composition, but entails an exponential increase in
memory size. The new result has applications to parallel processing and
symmetric FSA networks.



An Application of Chromatic Prototypes

This paper has been withdrawn.



Complementary algorithms for graphs and percolation

A pair of complementary algorithms are presented. One of the pair is a fast
method for connecting graphs with an edge. The other is a fast method for
removing edges from a graph. Both algorithms employ the same tree based graph
representation and so, in concert, can arbitrarily modify any graph. Since the
clusters of a percolation model may be described as simple connected graphs, an
efficient Monte Carlo scheme can be constructed that uses the algorithms to
sweep the occupation probability back and forth between two turning points.
This approach concentrates computational sampling time within a region of
interest. A high precision value of pc = 0.59274603(9) was thus obtained, by
Mersenne twister, for the two dimensional square site percolation threshold.



Public Cluster : parallel machine with multi-block approach

We introduce a new approach to enable an open and public parallel machine
which is accessible for multi users with multi jobs belong to different blocks
running at the same time. The concept is required especially for parallel
machines which are dedicated for public use as implemented at the LIPI Public
Cluster. We have deployed the simplest technique by running multi daemons of
parallel processing engine with different configuration files specified for
each user assigned to access the system, and also developed an integrated
system to fully control and monitor the whole system over web. A brief
performance analysis is also given for Message Parsing Interface (MPI) engine.
It is shown that the proposed approach is quite reliable and affect the whole
performances only slightly.



Introducing OPTO : Portal for Optical Communities in Indonesia

Since January 1, 2005 we have launched "OPTO" Portal, a website dedicated to
optical communities in Indonesia. The address of this portal is
http://www.opto.lipi.go.id and is self-supporting managed and not for
commercial purposes. Our aims in launching this portal are to benefit Internet
facility in increasing the communities' scientific activity; to provide an
online reference in Indonesian language for optics-based science and technology
subjects; as well as to pioneer the communities' online activities with real
impacts and benefits for our society. We will describe in the paper the
features of this portal that can be utilized by all individuals or members of
optical communities to store and share information and to build networks or
partnership as well. We realized that this portal is still not popular and most
of our aims are still not reached. This conference should be a good place for
all of us to collaborate to properly utilize this portal for the advantages to
the optical communities in Indonesia and our society at large.



Open and Free Cluster for Public

We introduce the LIPI Public Cluster, the first parallel machine facility
fully open for public and for free in Indonesia and surrounding countries. In
this paper, we focus on explaining our globally new concept on open cluster,
and how to realize and manage it to meet the users needs. We show that after 2
years trial running and several upgradings, the Public Cluster performs well
and is able to fulfil all requirements as expected.



Real-time control and monitoring system for LIPI's Public Cluster

We have developed a monitoring and control system for LIPI's Public Cluster.
The system consists of microcontrollers and full web-based user interfaces for
daily operation. It is argued that, due to its special natures, the cluster
requires fully dedicated and self developed control and monitoring system. We
discuss the implementation of using parallel port and dedicated
micro-controller for this purpose. We also show that integrating such systems
enables an autonomous control system based on the real time monitoring, for
instance an autonomous power supply control based on the actual temperature,
etc.



Resource Allocation in Public Cluster with Extended Optimization Algorithm

We introduce an optimization algorithm for resource allocation in the LIPI
Public Cluster to optimize its usage according to incoming requests from users.
The tool is an extended and modified genetic algorithm developed to match
specific natures of public cluster. We present a detail analysis of
optimization, and compare the results with the exact calculation. We show that
it would be very useful and could realize an automatic decision making system
for public clusters.



ADS-Directory Services for Mobile Ad-Hoc Networks Based on an Information Market Model

Ubiquitous computing based on small mobile devices using wireless
communication links is becoming very attractive. The computational power and
storage capacities provided allow the execution of sophisticated applications.
Due to the fact that sharing of information is a central problem for
distributed applications, the development of self organizing middleware
services providing high level interfaces for information managing is essential.
ADS is a directory service for mobile ad-hoc networks dealing with local and
nearby information as well as providing access to distant information. The
approach discussed throughout this paper is based upon the concept of
information markets.



ADS as Information Management Service in an M-Learning Environment

Leveraging the potential power of even small handheld devices able to
communicate wirelessly requires dedicated support. In particular, collaborative
applications need sophisticated assistance in terms of querying and exchanging
different kinds of data. Using a concrete example from the domain of mobile
learning, the general need for information dissemination is motivated.
Subsequently, and driven by infrastructural conditions, realization strategies
of an appropriate middleware service are discussed.



Auction-Based Distributed Resource Allocation for Cooperation Transmission in Wireless Networks

Cooperative transmission can greatly improve communication system performance
by taking advantage of the broadcast nature of wireless channels. Most previous
work on resource allocation for cooperation transmission is based on
centralized control. In this paper, we propose two share auction mechanisms,
the SNR auction and the power auction, to distributively coordinate the
resource allocation among users. We prove the existence, uniqueness and
effectiveness of the auction results. In particular, the SNR auction leads to a
fair resource allocation among users, and the power auction achieves a solution
that is close to the efficient allocation.



Structure or Noise?

We show how rate-distortion theory provides a mechanism for automated theory
building by naturally distinguishing between regularity and randomness. We
start from the simple principle that model variables should, as much as
possible, render the future and past conditionally independent. From this, we
construct an objective function for model making whose extrema embody the
trade-off between a model's structural complexity and its predictive power. The
solutions correspond to a hierarchy of models that, at each level of
complexity, achieve optimal predictive power at minimal cost. In the limit of
maximal prediction the resulting optimal model identifies a process's intrinsic
organization by extracting the underlying causal states. In this limit, the
model's complexity is given by the statistical complexity, which is known to be
minimal for achieving maximum prediction. Examples show how theory building can
profit from analyzing a process's causal compressibility, which is reflected in
the optimal models' rate-distortion curve--the process's characteristic for
optimally balancing structure and noise at different levels of representation.



Network synchronizability analysis: the theory of subgraphs and complementary graphs

In this paper, subgraphs and complementary graphs are used to analyze the
network synchronizability. Some sharp and attainable bounds are provided for
the eigenratio of the network structural matrix, which characterizes the
network synchronizability, especially when the network's corresponding graph
has cycles, chains, bipartite graphs or product graphs as its subgraphs.



Reconstruction of Protein-Protein Interaction Pathways by Mining Subject-Verb-Objects Intermediates

The exponential increase in publication rate of new articles is limiting
access of researchers to relevant literature. This has prompted the use of text
mining tools to extract key biological information. Previous studies have
reported extensive modification of existing generic text processors to process
biological text. However, this requirement for modification had not been
examined. In this study, we have constructed Muscorian, using MontyLingua, a
generic text processor. It uses a two-layered generalization-specialization
paradigm previously proposed where text was generically processed to a suitable
intermediate format before domain-specific data extraction techniques are
applied at the specialization layer. Evaluation using a corpus and experts
indicated 86-90% precision and approximately 30% recall in extracting
protein-protein interactions, which was comparable to previous studies using
either specialized biological text processing tools or modified existing tools.
Our study had also demonstrated the flexibility of the two-layered
generalization-specialization paradigm by using the same generalization layer
for two specialized information extraction tasks.



Virtual Environments for Training: From Individual Learning to Collaboration with Humanoids

The next generation of virtual environments for training is oriented towards
collaborative aspects. Therefore, we have decided to enhance our platform for
virtual training environments, adding collaboration opportunities and
integrating humanoids. In this paper we put forward a model of humanoid that
suits both virtual humans and representations of real users, according to
collaborative training activities. We suggest adaptations to the scenario model
of our platform making it possible to write collaborative procedures. We
introduce a mechanism of action selection made up of a global repartition and
an individual choice. These models are currently being integrated and validated
in GVT, a virtual training tool for maintenance of military equipments,
developed in collaboration with the French company NEXTER-Group.



Edit and verify

Automated theorem provers are used in extended static checking, where they
are the performance bottleneck. Extended static checkers are run typically
after incremental changes to the code. We propose to exploit this usage pattern
to improve performance. We present two approaches of how to do so and a full
solution.



Characterising Web Site Link Structure

The topological structures of the Internet and the Web have received
considerable attention. However, there has been little research on the
topological properties of individual web sites. In this paper, we consider
whether web sites (as opposed to the entire Web) exhibit structural
similarities. To do so, we exhaustively crawled 18 web sites as diverse as
governmental departments, commercial companies and university departments in
different countries. These web sites consisted of as little as a few thousand
pages to millions of pages. Statistical analysis of these 18 sites revealed
that the internal link structure of the web sites are significantly different
when measured with first and second-order topological properties, i.e.
properties based on the connectivity of an individual or a pairs of nodes.
However, examination of a third-order topological property that consider the
connectivity between three nodes that form a triangle, revealed a strong
correspondence across web sites, suggestive of an invariant. Comparison with
the Web, the AS Internet, and a citation network, showed that this third-order
property is not shared across other types of networks. Nor is the property
exhibited in generative network models such as that of Barabasi and Albert.



Cooperative Beamforming for Wireless Ad Hoc Networks

Via collaborative beamforming, nodes in a wireless network are able to
transmit a common message over long distances in an energy efficient fashion.
However, the process of making available the same message to all collaborating
nodes introduces delays. In this paper, a MAC-PHY cross-layer scheme is
proposed that enables collaborative beamforming at significantly reduced
collaboration overhead. It consists of two phases. In the first phase, nodes
transmit locally in a random access time-slotted fashion. Simultaneous
transmissions from multiple source nodes are viewed as linear mixtures of all
transmitted packets. In the second phase, a set of collaborating nodes, acting
as a distributed antenna system, beamform the received analog waveform to one
or more faraway destinations. This step requires multiplication of the received
analog waveform by a complex weight, which is independently computed by each
cooperating node, and which allows packets bound to the same destination to add
coherently at the destination node. Assuming that each node has access to
location information, the proposed scheme can achieve high throughput, which in
certain cases exceeds one. An analysis of the symbol error probability
corresponding to the proposed scheme is provided.



Cooperative game theory and the Gaussian interference channel

In this paper we discuss the use of cooperative game theory for analyzing
interference channels. We extend our previous work, to games with N players as
well as frequency selective channels and joint TDM/FDM strategies.
  We show that the Nash bargaining solution can be computed using convex
optimization techniques. We also show that the same results are applicable to
interference channels where only statistical knowledge of the channel is
available. Moreover, for the special case of two players $2\times K$ frequency
selective channel (with K frequency bins) we provide an $O(K \log_2 K)$
complexity algorithm for computing the Nash bargaining solution under mask
constraint and using joint FDM/TDM strategies. Simulation results are also
provided.



Relations between random coding exponents and the statistical physics of random codes

The partition function pertaining to finite--temperature decoding of a
(typical) randomly chosen code is known to have three types of behavior,
corresponding to three phases in the plane of rate vs. temperature: the {\it
ferromagnetic phase}, corresponding to correct decoding, the {\it paramagnetic
phase}, of complete disorder, which is dominated by exponentially many
incorrect codewords, and the {\it glassy phase} (or the condensed phase), where
the system is frozen at minimum energy and dominated by subexponentially many
incorrect codewords. We show that the statistical physics associated with the
two latter phases are intimately related to random coding exponents. In
particular, the exponent associated with the probability of correct decoding at
rates above capacity is directly related to the free energy in the glassy
phase, and the exponent associated with probability of error (the error
exponent) at rates below capacity, is strongly related to the free energy in
the paramagnetic phase. In fact, we derive alternative expressions of these
exponents in terms of the corresponding free energies, and make an attempt to
obtain some insights from these expressions. Finally, as a side result, we also
compare the phase diagram associated with a simple finite-temperature universal
decoder for discrete memoryless channels, to that of the finite--temperature
decoder that is aware of the channel statistics.



A Portal Analysis for the Design of a Collaborative Research Environment for Students and Supervisors (CRESS) within the CSCR Domain

In a previous paper the CSCR domain was defined. Here this is taken to the
next stage where we consider the design of a particular Collaborative Research
Environment to support Students and Supervisors CRESS. Following the CSCR
structure a preliminary design for CRESS has been established and a portal
framework analysis is undertaken in order to determine the most appropriate set
of tools for its implementation.



Permutation Decoding and the Stopping Redundancy Hierarchy of Cyclic and Extended Cyclic Codes

We introduce the notion of the stopping redundancy hierarchy of a linear
block code as a measure of the trade-off between performance and complexity of
iterative decoding for the binary erasure channel. We derive lower and upper
bounds for the stopping redundancy hierarchy via Lovasz's Local Lemma and
Bonferroni-type inequalities, and specialize them for codes with cyclic
parity-check matrices. Based on the observed properties of parity-check
matrices with good stopping redundancy characteristics, we develop a novel
decoding technique, termed automorphism group decoding, that combines iterative
message passing and permutation decoding. We also present bounds on the
smallest number of permutations of an automorphism group decoder needed to
correct any set of erasures up to a prescribed size. Simulation results
demonstrate that for a large number of algebraic codes, the performance of the
new decoding method is close to that of maximum likelihood decoding.



On the Self-stabilization of Mobile Robots in Graphs

Self-stabilization is a versatile technique to withstand any transient fault
in a distributed system. Mobile robots (or agents) are one of the emerging
trends in distributed computing as they mimic autonomous biologic entities. The
contribution of this paper is threefold. First, we present a new model for
studying mobile entities in networks subject to transient faults. Our model
differs from the classical robot model because robots have constraints about
the paths they are allowed to follow, and from the classical agent model
because the number of agents remains fixed throughout the execution of the
protocol. Second, in this model, we study the possibility of designing
self-stabilizing algorithms when those algorithms are run by mobile robots (or
agents) evolving on a graph. We concentrate on the core building blocks of
robot and agents problems: naming and leader election. Not surprisingly, when
no constraints are given on the network graph topology and local execution
model, both problems are impossible to solve. Finally, using minimal hypothesis
with respect to impossibility results, we provide deterministic and
probabilistic solutions to both problems, and show equivalence of these
problems by an algorithmic reduction mechanism.



Modeling Visual Information Processing in Brain: A Computer Vision Point of View and Approach

We live in the Information Age, and information has become a critically
important component of our life. The success of the Internet made huge amounts
of it easily available and accessible to everyone. To keep the flow of this
information manageable, means for its faultless circulation and effective
handling have become urgently required. Considerable research efforts are
dedicated today to address this necessity, but they are seriously hampered by
the lack of a common agreement about "What is information?" In particular, what
is "visual information" - human's primary input from the surrounding world. The
problem is further aggravated by a long-lasting stance borrowed from the
biological vision research that assumes human-like information processing as an
enigmatic mix of perceptual and cognitive vision faculties. I am trying to find
a remedy for this bizarre situation. Relying on a new definition of
"information", which can be derived from Kolmogorov's compexity theory and
Chaitin's notion of algorithmic information, I propose a unifying framework for
visual information processing, which explicitly accounts for the perceptual and
cognitive image processing peculiarities. I believe that this framework will be
useful to overcome the difficulties that are impeding our attempts to develop
the right model of human-like intelligent image processing.



Nodally 3-connected planar graphs and convex combination mappings

A convex combination mapping of a planar graph is a plane mapping in which
the external vertices are mapped to the corners of a convex polygon and every
internal vertex is a proper weighted average of its neighbours. If a planar
graph is nodally 3-connected or triangulated then every such mapping is an
embedding (Tutte, Floater).
  We give a simple characterisation of nodally 3-connected planar graphs, and
generalise the above result to any planar graph which admits any convex
embedding.



Near Optimal Broadcast with Network Coding in Large Sensor Networks

We study efficient broadcasting for wireless sensor networks, with network
coding. We address this issue for homogeneous sensor networks in the plane. Our
results are based on a simple principle (IREN/IRON), which sets the same rate
on most of the nodes (wireless links) of the network. With this rate selection,
we give a value of the maximum achievable broadcast rate of the source: our
central result is a proof of the value of the min-cut for such networks, viewed
as hypergraphs. Our metric for efficiency is the number of transmissions
necessary to transmit one packet from the source to every destination: we show
that IREN/IRON achieves near optimality for large networks; that is,
asymptotically, nearly every transmission brings new information from the
source to the receiver. As a consequence, network coding asymptotically
outperforms any scheme that does not use network coding.



From symmetry break to Poisson point process in 2D Voronoi tessellations: the generic nature of hexagons

We bridge the properties of the regular square and honeycomb Voronoi
tessellations of the plane to those of the Poisson-Voronoi case, thus analyzing
in a common framework symmetry-break processes and the approach to uniformly
random distributions of tessellation-generating points. We consider ensemble
simulations of tessellations generated by points whose regular positions are
perturbed through a Gaussian noise controlled by the parameter alpha. We study
the number of sides, the area, and the perimeter of the Voronoi cells. For
alpha>0, hexagons are the most common class of cells, and 2-parameter gamma
distributions describe well the statistics of the geometrical characteristics.
The symmetry break due to noise destroys the square tessellation, whereas the
honeycomb hexagonal tessellation is very stable and all Voronoi cells are
hexagon for small but finite noise with alpha<0.1. For a moderate amount of
Gaussian noise, memory of the specific unperturbed tessellation is lost,
because the statistics of the two perturbed tessellations is indistinguishable.
When alpha>2, results converge to those of Poisson-Voronoi tessellations. The
properties of n-sided cells change with alpha until the Poisson-Voronoi limit
is reached for alpha>2. The Desch law for perimeters is confirmed to be not
valid and a square root dependence on n is established. The ensemble mean of
the cells area and perimeter restricted to the hexagonal cells coincides with
the full ensemble mean; this might imply that the number of sides acts as a
thermodynamic state variable fluctuating about n=6; this reinforces the idea
that hexagons, beyond their ubiquitous numerical prominence, can be taken as
generic polygons in 2D Voronoi tessellations.



A Formulation of the Channel Capacity of Multiple-Access Channel

The necessary and sufficient condition of the channel capacity is rigorously
formulated for the N-user discrete memoryless multiple-access channel (MAC).
The essence of the formulation is to invoke an {\em elementary} MAC where sizes
of input alphabets are not greater than the size of output alphabet. The main
objective is to demonstrate that the channel capacity of an MAC is achieved by
an elementary MAC included in the original MAC. The proof is quite
straightforward by the very definition of the elementary MAC. Moreover it is
proved that the Kuhn-Tucker conditions of the elementary MAC are strictly
sufficient and obviously necessary for the channel capacity. The latter proof
requires some steps such that for the elementary MAC every solution of the
Kuhn-Tucker conditions reveals itself as local maximum on the domain of all
possible input probability distributions and then it achieves the channel
capacity. As a result, in respect of the channel capacity, the MAC in general
can be regarded as an aggregate of a finite number of elementary MAC's.



An Interval Analysis Based Study for the Design and the Comparison of 3-DOF Parallel Kinematic Machines

This paper addresses an interval analysis based study that is applied to the
design and the comparison of 3-DOF parallel kinematic machines. Two design
criteria are used, (i) a regular workspace shape and, (ii) a kinetostatic
performance index that needs to be as homogeneous as possible throughout the
workspace. The interval analysis based method takes these two criteria into
account: on the basis of prescribed kinetostatic performances, the workspace is
analysed to find out the largest regular dextrous workspace enclosed in the
Cartesian workspace. An algorithm describing this method is introduced. Two
3-DOF translational parallel mechanisms designed for machining applications are
compared using this method. The first machine features three fixed linear
joints which are mounted orthogonally and the second one features three linear
joints which are mounted in parallel. In both cases, the mobile platform moves
in the Cartesian x-y-z space with fixed orientation.



Nearly MDS expander codes with reduced alphabet size

Recently, Roth and Skachek proposed two methods for constructing nearly
maximum-distance separable (MDS) expander codes. We show that through the
simple modification of using mixed-alphabet codes derived from MDS codes as
constituent codes in their code designs, one can obtain nearly MDS codes of
significantly smaller alphabet size, albeit at the expense of a (very slight)
reduction in code rate.



A variant of the Recoil Growth algorithm to generate multi-polymer systems

The Recoil Growth algorithm, proposed in 1999 by Consta et al., is one of the
most efficient algorithm available in the literature to sample from a
multi-polymer system. Such problems are closely related to the generation of
self-avoiding paths. In this paper, we study a variant of the original Recoil
Growth algorithm, where we constrain the generation of a new polymer to take
place on a specific class of graphs. This makes it possible to make a fine
trade-off between computational cost and success rate. We moreover give a
simple proof for a lower bound on the irreducibility of this new algorithm,
which applies to the original algorithm as well.



A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts and their Usage

The large-scale analysis of scholarly artifact usage is constrained primarily
by current practices in usage data archiving, privacy issues concerned with the
dissemination of usage data, and the lack of a practical ontology for modeling
the usage domain. As a remedy to the third constraint, this article presents a
scholarly ontology that was engineered to represent those classes for which
large-scale bibliographic and usage data exists, supports usage research, and
whose instantiation is scalable to the order of 50 million articles along with
their associated artifacts (e.g. authors and journals) and an accompanying 1
billion usage events. The real world instantiation of the presented abstract
ontology is a semantic network model of the scholarly community which lends the
scholarly process to statistical analysis and computational support. We present
the ontology, discuss its instantiation, and provide some example inference
rules for calculating various scholarly artifact metrics.



Diversity-Multiplexing Tradeoff of Asynchronous Cooperative Diversity in Wireless Networks

Synchronization of relay nodes is an important and critical issue in
exploiting cooperative diversity in wireless networks. In this paper, two
asynchronous cooperative diversity schemes are proposed, namely, distributed
delay diversity and asynchronous space-time coded cooperative diversity
schemes. In terms of the overall diversity-multiplexing (DM) tradeoff function,
we show that the proposed independent coding based distributed delay diversity
and asynchronous space-time coded cooperative diversity schemes achieve the
same performance as the synchronous space-time coded approach which requires an
accurate symbol-level timing synchronization to ensure signals arriving at the
destination from different relay nodes are perfectly synchronized. This
demonstrates diversity order is maintained even at the presence of asynchronism
between relay node. Moreover, when all relay nodes succeed in decoding the
source information, the asynchronous space-time coded approach is capable of
achieving better DM-tradeoff than synchronous schemes and performs equivalently
to transmitting information through a parallel fading channel as far as the
DM-tradeoff is concerned. Our results suggest the benefits of fully exploiting
the space-time degrees of freedom in multiple antenna systems by employing
asynchronous space-time codes even in a frequency flat fading channel. In
addition, it is shown asynchronous space-time coded systems are able to achieve
higher mutual information than synchronous space-time coded systems for any
finite signal-to-noise-ratio (SNR) when properly selected baseband waveforms
are employed.



A Deterministic Sub-linear Time Sparse Fourier Algorithm via Non-adaptive Compressed Sensing Methods

We study the problem of estimating the best B term Fourier representation for
a given frequency-sparse signal (i.e., vector) $\textbf{A}$ of length $N \gg
B$. More explicitly, we investigate how to deterministically identify B of the
largest magnitude frequencies of $\hat{\textbf{A}}$, and estimate their
coefficients, in polynomial$(B,\log N)$ time. Randomized sub-linear time
algorithms which have a small (controllable) probability of failure for each
processed signal exist for solving this problem. However, for failure
intolerant applications such as those involving mission-critical hardware
designed to process many signals over a long lifetime, deterministic algorithms
with no probability of failure are highly desirable. In this paper we build on
the deterministic Compressed Sensing results of Cormode and Muthukrishnan (CM)
\cite{CMDetCS3,CMDetCS1,CMDetCS2} in order to develop the first known
deterministic sub-linear time sparse Fourier Transform algorithm suitable for
failure intolerant applications. Furthermore, in the process of developing our
new Fourier algorithm, we present a simplified deterministic Compressed Sensing
algorithm which improves on CM's algebraic compressibility results while
simultaneously maintaining their results concerning exponential decay.



Cost-minimising strategies for data labelling : optimal stopping and active learning

Supervised learning deals with the inference of a distribution over an output
or label space $\CY$ conditioned on points in an observation space $\CX$, given
a training dataset $D$ of pairs in $\CX \times \CY$. However, in a lot of
applications of interest, acquisition of large amounts of observations is easy,
while the process of generating labels is time-consuming or costly. One way to
deal with this problem is {\em active} learning, where points to be labelled
are selected with the aim of creating a model with better performance than that
of an model trained on an equal number of randomly sampled points. In this
paper, we instead propose to deal with the labelling cost directly: The
learning goal is defined as the minimisation of a cost which is a function of
the expected model performance and the total cost of the labels used. This
allows the development of general strategies and specific algorithms for (a)
optimal stopping, where the expected cost dictates whether label acquisition
should continue (b) empirical evaluation, where the cost is used as a
performance metric for a given combination of inference, stopping and sampling
methods. Though the main focus of the paper is optimal stopping, we also aim to
provide the background for further developments and discussion in the related
field of active learning.



A Matrix Ring Description for Cyclic Convolutional Codes

In this paper, we study convolutional codes with a specific cyclic structure.
By definition, these codes are left ideals in a certain skew polynomial ring.
Using that the skew polynomial ring is isomorphic to a matrix ring we can
describe the algebraic parameters of the codes in a more accessible way. We
show that the existence of such codes with given algebraic parameters can be
reduced to the solvability of a modified rook problem. It is our strong belief
that the rook problem is always solvable, and we present solutions in
particular cases.



Physical limits of inference

I show that physical devices that perform observation, prediction, or
recollection share an underlying mathematical structure. I call devices with
that structure "inference devices". I present a set of existence and
impossibility results concerning inference devices. These results hold
independent of the precise physical laws governing our universe. In a limited
sense, the impossibility results establish that Laplace was wrong to claim that
even in a classical, non-chaotic universe the future can be unerringly
predicted, given sufficient knowledge of the present. Alternatively, these
impossibility results can be viewed as a non-quantum mechanical "uncertainty
principle". Next I explore the close connections between the mathematics of
inference devices and of Turing Machines. In particular, the impossibility
results for inference devices are similar to the Halting theorem for TM's.
Furthermore, one can define an analog of Universal TM's (UTM's) for inference
devices. I call those analogs "strong inference devices". I use strong
inference devices to define the "inference complexity" of an inference task,
which is the analog of the Kolmogorov complexity of computing a string. However
no universe can contain more than one strong inference device. So whereas the
Kolmogorov complexity of a string is arbitrary up to specification of the UTM,
there is no such arbitrariness in the inference complexity of an inference
task. I end by discussing the philosophical implications of these results,
e.g., for whether the universe "is" a computer.



Achievable Outage Rates with Improved Decoding of Bicm Multiband Ofdm Under Channel Estimation Errors

We consider the decoding of bit interleaved coded modulation (BICM) applied
to multiband OFDM for practical scenarios where only a noisy (possibly very
bad) estimate of the channel is available at the receiver. First, a decoding
metric based on the channel it a posteriori probability density, conditioned on
the channel estimate is derived and used for decoding BICM multiband OFDM.
Then, we characterize the limits of reliable information rates in terms of the
maximal achievable outage rates associated to the proposed metric. We also
compare our results with the outage rates of a system using a theoretical
decoder. Our results are useful for designing a communication system where a
prescribed quality of service (QoS), in terms of achievable target rates with
small error probability, must be satisfied even in the presence of imperfect
channel estimation. Numerical results over both realistic UWB and theoretical
Rayleigh fading channels show that the proposed method provides significant
gain in terms of BER and outage rates compared to the classical mismatched
detector, without introducing any additional complexity.



On Optimal Turbo Decoding of Wideband MIMO-OFDM Systems Under Imperfect Channel State Information

We consider the decoding of bit interleaved coded modulation (BICM) applied
to both multiband and MIMO OFDM systems for typical scenarios where only a
noisy (possibly very bad) estimate of the channel is provided by sending a
limited number of pilot symbols. First, by using a Bayesian framework involving
the channel a posteriori density, we adopt a practical decoding metric that is
robust to the presence of channel estimation errors. Then this metric is used
in the demapping part of BICM multiband and MIMO OFDM receivers. We also
compare our results with the performance of a mismatched decoder that replaces
the channel by its estimate in the decoding metric. Numerical results over both
realistic UWB and theoretical Rayleigh fading channels show that the proposed
method provides significant gain in terms of bit error rate compared to the
classical mismatched detector, without introducing any additional complexity.



Wavelet Based Semi-blind Channel Estimation For Multiband OFDM

This paper introduces an expectation-maximization (EM) algorithm within a
wavelet domain Bayesian framework for semi-blind channel estimation of
multiband OFDM based UWB communications. A prior distribution is chosen for the
wavelet coefficients of the unknown channel impulse response in order to model
a sparseness property of the wavelet representation. This prior yields, in
maximum a posteriori estimation, a thresholding rule within the EM algorithm.
We particularly focus on reducing the number of estimated parameters by
iteratively discarding ``unsignificant'' wavelet coefficients from the
estimation process. Simulation results using UWB channels issued from both
models and measurements show that under sparsity conditions, the proposed
algorithm outperforms pilot based channel estimation in terms of mean square
error and bit error rate and enhances the estimation accuracy with less
computational complexity than traditional semi-blind methods.



MIMO-OFDM Optimal Decoding and Achievable Information Rates Under Imperfect Channel Estimation

Optimal decoding of bit interleaved coded modulation (BICM) MIMO-OFDM where
an imperfect channel estimate is available at the receiver is investigated.
First, by using a Bayesian approach involving the channel a posteriori density,
we derive a practical decoding metric for general memoryless channels that is
robust to the presence of channel estimation errors. Then, we evaluate the
outage rates achieved by a decoder that uses our proposed metric. The
performance of the proposed decoder is compared to the classical mismatched
decoder and a theoretical decoder defined as the best decoder in the presence
of imperfect channel estimation. Numerical results over Rayleigh block fading
MIMO-OFDM channels show that the proposed decoder outperforms mismatched
decoding in terms of bit error rate and outage capacity without introducing any
additional complexity.



Valid formulas, games and network protocols

We describe a remarkable relation between the notion of valid formula of
predicate logic and the specification of network protocols. We give several
examples such as the acknowledgement of one packet or of a sequence of packets.
We show how to specify the composition of protocols.



On perfect, amicable, and sociable chains

Let $x = (x_0,...,x_{n-1})$ be an n-chain, i.e., an n-tuple of non-negative
integers $< n$. Consider the operator $s: x \mapsto x' = (x'_0,...,x'_{n-1})$,
where x'_j represents the number of $j$'s appearing among the components of x.
An n-chain x is said to be perfect if $s(x) = x$. For example, (2,1,2,0,0) is a
perfect 5-chain. Analogously to the theory of perfect, amicable, and sociable
numbers, one can define from the operator s the concepts of amicable pair and
sociable group of chains. In this paper we give an exhaustive list of all the
perfect, amicable, and sociable chains.



A Light-Based Device for Solving the Hamiltonian Path Problem

In this paper we suggest the use of light for performing useful computations.
Namely, we propose a special device which uses light rays for solving the
Hamiltonian path problem on a directed graph. The device has a graph-like
representation and the light is traversing it following the routes given by the
connections between nodes. In each node the rays are uniquely marked so that
they can be easily identified. At the destination node we will search only for
particular rays that have passed only once through each node. We show that the
proposed device can solve small and medium instances of the problem in
reasonable time.



Defensive forecasting for optimal prediction with expert advice

The method of defensive forecasting is applied to the problem of prediction
with expert advice for binary outcomes. It turns out that defensive forecasting
is not only competitive with the Aggregating Algorithm but also handles the
case of "second-guessing" experts, whose advice depends on the learner's
prediction; this paper assumes that the dependence on the learner's prediction
is continuous.



Solving the Hamiltonian path problem with a light-based computer

In this paper we propose a special computational device which uses light rays
for solving the Hamiltonian path problem on a directed graph. The device has a
graph-like representation and the light is traversing it by following the
routes given by the connections between nodes. In each node the rays are
uniquely marked so that they can be easily identified. At the destination node
we will search only for particular rays that have passed only once through each
node. We show that the proposed device can solve small and medium instances of
the problem in reasonable time.



A Data-Parallel Version of Aleph

This is to present work on modifying the Aleph ILP system so that it
evaluates the hypothesised clauses in parallel by distributing the data-set
among the nodes of a parallel or distributed machine. The paper briefly
discusses MPI, the interface used to access message- passing libraries for
parallel computers and clusters. It then proceeds to describe an extension of
YAP Prolog with an MPI interface and an implementation of data-parallel clause
evaluation for Aleph through this interface. The paper concludes by testing the
data-parallel Aleph on artificially constructed data-sets.



Resolution over Linear Equations and Multilinear Proofs

We develop and study the complexity of propositional proof systems of varying
strength extending resolution by allowing it to operate with disjunctions of
linear equations instead of clauses. We demonstrate polynomial-size refutations
for hard tautologies like the pigeonhole principle, Tseitin graph tautologies
and the clique-coloring tautologies in these proof systems. Using the
(monotone) interpolation by a communication game technique we establish an
exponential-size lower bound on refutations in a certain, considerably strong,
fragment of resolution over linear equations, as well as a general polynomial
upper bound on (non-monotone) interpolants in this fragment.
  We then apply these results to extend and improve previous results on
multilinear proofs (over fields of characteristic 0), as studied in
[RazTzameret06]. Specifically, we show the following:
  1. Proofs operating with depth-3 multilinear formulas polynomially simulate a
certain, considerably strong, fragment of resolution over linear equations.
  2. Proofs operating with depth-3 multilinear formulas admit polynomial-size
refutations of the pigeonhole principle and Tseitin graph tautologies. The
former improve over a previous result that established small multilinear proofs
only for the \emph{functional} pigeonhole principle. The latter are different
than previous proofs, and apply to multilinear proofs of Tseitin mod p graph
tautologies over any field of characteristic 0.
  We conclude by connecting resolution over linear equations with extensions of
the cutting planes proof system.



Construction of a 3-Dimensional MDS code

In this paper, we describe a procedure for constructing $q$--ary
$[N,3,N-2]$--MDS codes, of length $N\leq q+1$ (for $q$ odd) or $N\leq q+2$ (for
$q$ even), using a set of non--degenerate Hermitian forms in $PG(2,q^2)$.



Learning Phonotactics Using ILP

This paper describes experiments on learning Dutch phonotactic rules using
Inductive Logic Programming, a machine learning discipline based on inductive
logical operators. Two different ways of approaching the problem are
experimented with, and compared against each other as well as with related work
on the task. The results show a direct correspondence between the quality and
informedness of the background knowledge and the constructed theory,
demonstrating the ability of ILP to take good advantage of the prior domain
knowledge available. Further research is outlined.



Homogeneous temporal activity patterns in a large online communication space

The many-to-many social communication activity on the popular technology-news
website Slashdot has been studied. We have concentrated on the dynamics of
message production without considering semantic relations and have found
regular temporal patterns in the reaction time of the community to a news-post
as well as in single user behavior. The statistics of these activities follow
log-normal distributions. Daily and weekly oscillatory cycles, which cause
slight variations of this simple behavior, are identified. A superposition of
two log-normal distributions can account for these variations. The findings are
remarkable since the distribution of the number of comments per users, which is
also analyzed, indicates a great amount of heterogeneity in the community. The
reader may find surprising that only a few parameters allow a detailed
description, or even prediction, of social many-to-many information exchange in
this kind of popular public spaces.



Optimal Causal Inference: Estimating Stored Information and Approximating Causal Architecture

We introduce an approach to inferring the causal architecture of stochastic
dynamical systems that extends rate distortion theory to use causal
shielding---a natural principle of learning. We study two distinct cases of
causal inference: optimal causal filtering and optimal causal estimation.
  Filtering corresponds to the ideal case in which the probability distribution
of measurement sequences is known, giving a principled method to approximate a
system's causal structure at a desired level of representation. We show that,
in the limit in which a model complexity constraint is relaxed, filtering finds
the exact causal architecture of a stochastic dynamical system, known as the
causal-state partition. From this, one can estimate the amount of historical
information the process stores. More generally, causal filtering finds a graded
model-complexity hierarchy of approximations to the causal architecture. Abrupt
changes in the hierarchy, as a function of approximation, capture distinct
scales of structural organization.
  For nonideal cases with finite data, we show how the correct number of
underlying causal states can be found by optimal causal estimation. A
previously derived model complexity control term allows us to correct for the
effect of statistical fluctuations in probability estimates and thereby avoid
over-fitting.



Updating Probabilities with Data and Moments

We use the method of Maximum (relative) Entropy to process information in the
form of observed data and moment constraints. The generic "canonical" form of
the posterior distribution for the problem of simultaneous updating with data
and moments is obtained. We discuss the general problem of non-commuting
constraints, when they should be processed sequentially and when
simultaneously. As an illustration, the multinomial example of die tosses is
solved in detail for two superficially similar but actually very different
problems.



Designing a Collaborative Research Environment for Students and their Supervisors (CRESS)

In a previous paper the CSCR domain was defined. Here this is taken to the
next stage where the design of a particular Collaborative Research Environment
to support Students and Supervisors (CRESS) is considered. Following the CSCR
structure this paper deals with an analysis of 13 collaborative working
environments to determine a preliminary design for CRESS in order to discover
the most appropriate set of tools for its implementation.



Hybrid Branching-Time Logics

Hybrid branching-time logics are introduced as extensions of CTL-like logics
with state variables and the downarrow-binder. Following recent work in the
linear framework, only logics with a single variable are considered. The
expressive power and the complexity of satisfiability of the resulting logics
is investigated.
  As main result, the satisfiability problem for the hybrid versions of several
branching-time logics is proved to be 2EXPTIME-complete. These branching-time
logics range from strict fragments of CTL to extensions of CTL that can talk
about the past and express fairness-properties. The complexity gap relative to
CTL is explained by a corresponding succinctness result.
  To prove the upper bound, the automata-theoretic approach to branching-time
logics is extended to hybrid logics, showing that non-emptiness of alternating
one-pebble Buchi tree automata is 2EXPTIME-complete.



Design: One, but in different forms

This overview paper defends an augmented cognitively oriented generic-design
hypothesis: there are both significant similarities between the design
activities implemented in different situations and crucial differences between
these and other cognitive activities; yet, characteristics of a design
situation (related to the design process, the designers, and the artefact)
introduce specificities in the corresponding cognitive activities and
structures that are used, and in the resulting designs. We thus augment the
classical generic-design hypothesis with that of different forms of designing.
We review the data available in the cognitive design research literature and
propose a series of candidates underlying such forms of design, outlining a
number of directions requiring further elaboration.



Cryptanalysis of shifted conjugacy authentication protocol

In this paper we present the first practical attack on the shifted
conjugacy-based authentication protocol proposed by P. Dehornoy. We discuss the
weaknesses of that primitive and propose ways to improve the protocol.



Computational Simulation and 3D Virtual Reality Engineering Tools for Dynamical Modeling and Imaging of Composite Nanomaterials

An adventure at engineering design and modeling is possible with a Virtual
Reality Environment (VRE) that uses multiple computer-generated media to let a
user experience situations that are temporally and spatially prohibiting. In
this paper, an approach to developing some advanced architecture and modeling
tools is presented to allow multiple frameworks work together while being
shielded from the application program. This architecture is being developed in
a framework of workbench interactive tools for next generation
nanoparticle-reinforced damping/dynamic systems. Through the use of system, an
engineer/programmer can respectively concentrate on tailoring an engineering
design concept of novel system and the application software design while using
existing databases/software outputs.



Multiple-Description Coding by Dithered Delta-Sigma Quantization

We address the connection between the multiple-description (MD) problem and
Delta-Sigma quantization. The inherent redundancy due to oversampling in
Delta-Sigma quantization, and the simple linear-additive noise model resulting
from dithered lattice quantization, allow us to construct a symmetric and
time-invariant MD coding scheme. We show that the use of a noise shaping filter
makes it possible to trade off central distortion for side distortion.
Asymptotically as the dimension of the lattice vector quantizer and order of
the noise shaping filter approach infinity, the entropy rate of the dithered
Delta-Sigma quantization scheme approaches the symmetric two-channel MD
rate-distortion function for a memoryless Gaussian source and MSE fidelity
criterion, at any side-to-central distortion ratio and any resolution. In the
optimal scheme, the infinite-order noise shaping filter must be minimum phase
and have a piece-wise flat power spectrum with a single jump discontinuity. An
important advantage of the proposed design is that it is symmetric in rate and
distortion by construction, so the coding rates of the descriptions are
identical and there is therefore no need for source splitting.



A nearly tight memory-redundancy trade-off for one-pass compression

Let $s$ be a string of length $n$ over an alphabet of constant size $\sigma$
and let $c$ and $\epsilon$ be constants with (1 \geq c \geq 0) and (\epsilon >
0). Using (O (n)) time, (O (n^c)) bits of memory and one pass we can always
encode $s$ in (n H_k (s) + O (\sigma^k n^{1 - c + \epsilon})) bits for all
integers (k \geq 0) simultaneously. On the other hand, even with unlimited
time, using (O (n^c)) bits of memory and one pass we cannot always encode $s$
in (O (n H_k (s) + \sigma^k n^{1 - c - \epsilon})) bits for, e.g., (k = \lceil
(c + \epsilon / 2) \log_\sigma n \rceil).



On Edge-Disjoint Pairs Of Matchings

For a graph G, consider the pairs of edge-disjoint matchings whose union
consists of as many edges as possible. Let H be the largest matching among such
pairs. Let M be a maximum matching of G. We show that 5/4 is a tight upper
bound for |M|/|H|.



Lower Bounds for the Complexity of the Voronoi Diagram of Polygonal Curves under the Discrete Frechet Distance

We give lower bounds for the combinatorial complexity of the Voronoi diagram
of polygonal curves under the discrete Frechet distance. We show that the
Voronoi diagram of n curves in R^d with k vertices each, has complexity
Omega(n^{dk}) for dimension d=1,2 and Omega(n^{d(k-1)+2}) for d>2.



Exact Cover with light

We suggest a new optical solution for solving the YES/NO version of the Exact
Cover problem by using the massive parallelism of light. The idea is to build
an optical device which can generate all possible solutions of the problem and
then to pick the correct one. In our case the device has a graph-like
representation and the light is traversing it by following the routes given by
the connections between nodes. The nodes are connected by arcs in a special way
which lets us to generate all possible covers (exact or not) of the given set.
For selecting the correct solution we assign to each item, from the set to be
covered, a special integer number. These numbers will actually represent delays
induced to light when it passes through arcs. The solution is represented as a
subray arriving at a certain moment in the destination node. This will tell us
if an exact cover does exist or not.



Solving the subset-sum problem with a light-based device

We propose a special computational device which uses light rays for solving
the subset-sum problem. The device has a graph-like representation and the
light is traversing it by following the routes given by the connections between
nodes. The nodes are connected by arcs in a special way which lets us to
generate all possible subsets of the given set. To each arc we assign either a
number from the given set or a predefined constant. When the light is passing
through an arc it is delayed by the amount of time indicated by the number
placed in that arc. At the destination node we will check if there is a ray
whose total delay is equal to the target value of the subset sum problem (plus
some constants).



Who is the best connected EC researcher? Centrality analysis of the complex network of authors in evolutionary computation

Co-authorship graphs (that is, the graph of authors linked by co-authorship
of papers) are complex networks, which expresses the dynamics of a complex
system. Only recently its study has started to draw interest from the EC
community, the first paper dealing with it having been published two years ago.
In this paper we will study the co-authorship network of EC at a microscopic
level. Our objective is ascertaining which are the most relevant nodes (i.e.
authors) in it. For this purpose, we examine several metrics defined in the
complex-network literature, and analyze them both in isolation and combined
within a Pareto-dominance approach. The result of our analysis indicates that
there are some well-known researchers that appear systematically in top
rankings. This also provides some hints on the social behavior of our
community.



Nonantagonistic noisy duels of discrete type with an arbitrary number of actions

We study a nonzero-sum game of two players which is a generalization of the
antagonistic noisy duel of discrete type. The game is considered from the point
of view of various criterions of optimality. We prove existence of
epsilon-equilibrium situations and show that the epsilon-equilibrium strategies
that we have found are epsilon-maxmin. Conditions under which the equilibrium
plays are Pareto-optimal are given.
  Keywords: noisy duel, payoff function, strategy, equilibrium situation,
Pareto optimality, the value of a game.



Derivative of BICM Mutual Information

In this letter we determine the derivative of the mutual information
corresponding to bit-interleaved coded modulation systems. The derivative
follows as a linear combination of minimum-mean-squared error functions of
coded modulation sets. The result finds applications to the analysis of
communications systems in the wideband regime and to the design of power
allocation over parallel channels.



Repairing Inconsistent XML Write-Access Control Policies

XML access control policies involving updates may contain security flaws,
here called inconsistencies, in which a forbidden operation may be simulated by
performing a sequence of allowed operations. This paper investigates the
problem of deciding whether a policy is consistent, and if not, how its
inconsistencies can be repaired. We consider policies expressed in terms of
annotated DTDs defining which operations are allowed or denied for the XML
trees that are instances of the DTD. We show that consistency is decidable in
PTIME for such policies and that consistent partial policies can be extended to
unique "least-privilege" consistent total policies. We also consider repair
problems based on deleting privileges to restore consistency, show that finding
minimal repairs is NP-complete, and give heuristics for finding repairs.



Obstructions to Genericity in Study of Parametric Problems in Control Theory

We investigate systems of equations, involving parameters from the point of
view of both control theory and computer algebra. The equations might involve
linear operators such as partial (q-)differentiation, (q-)shift, (q-)difference
as well as more complicated ones, which act trivially on the parameters. Such a
system can be identified algebraically with a certain left module over a
non-commutative algebra, where the operators commute with the parameters. We
develop, implement and use in practice the algorithm for revealing all the
expressions in parameters, for which e.g. homological properties of a system
differ from the generic properties. We use Groebner bases and Groebner basics
in rings of solvable type as main tools. In particular, we demonstrate an
optimized algorithm for computing the left inverse of a matrix over a ring of
solvable type. We illustrate the article with interesting examples. In
particular, we provide a complete solution to the "two pendula, mounted on a
cart" problem from the classical book of Polderman and Willems, including the
case, where the friction at the joints is essential . To the best of our
knowledge, the latter example has not been solved before in a complete way.



Empirical entropy in context

We trace the history of empirical entropy, touching briefly on its relation
to Markov processes, normal numbers, Shannon entropy, the Chomsky hierarchy,
Kolmogorov complexity, Ziv-Lempel compression, de Bruijn sequences and
stochastic complexity.



Attribute Estimation and Testing Quasi-Symmetry

A Boolean function is symmetric if it is invariant under all permutations of
its arguments; it is quasi-symmetric if it is symmetric with respect to the
arguments on which it actually depends. We present a test that accepts every
quasi-symmetric function and, except with an error probability at most delta>0,
rejects every function that differs from every quasi-symmetric function on at
least a fraction epsilon>0 of the inputs. For a function of n arguments, the
test probes the function at O((n/epsilon)\log(n/delta)) inputs. Our
quasi-symmetry test acquires information concerning the arguments on which the
function actually depends. To do this, it employs a generalization of the
property testing paradigm that we call attribute estimation. Like property
testing, attribute estimation uses random sampling to obtain results that have
only "one-sided'' errors and that are close to accurate with high probability.



Provenance as Dependency Analysis

Provenance is information recording the source, derivation, or history of
some information. Provenance tracking has been studied in a variety of
settings; however, although many design points have been explored, the
mathematical or semantic foundations of data provenance have received
comparatively little attention. In this paper, we argue that dependency
analysis techniques familiar from program analysis and program slicing provide
a formal foundation for forms of provenance that are intended to show how (part
of) the output of a query depends on (parts of) its input. We introduce a
semantic characterization of such dependency provenance, show that this form of
provenance is not computable, and provide dynamic and static approximation
techniques.



Moderate Growth Time Series for Dynamic Combinatorics Modelisation

Here, we present a family of time series with a simple growth constraint.
This family can be the basis of a model to apply to emerging computation in
business and micro-economy where global functions can be expressed from local
rules. We explicit a double statistics on these series which allows to
establish a one-to-one correspondence between three other ballot-like
strunctures.



Collection analysis for Horn clause programs

We consider approximating data structures with collections of the items that
they contain. For examples, lists, binary trees, tuples, etc, can be
approximated by sets or multisets of the items within them. Such approximations
can be used to provide partial correctness properties of logic programs. For
example, one might wish to specify than whenever the atom $sort(t,s)$ is proved
then the two lists $t$ and $s$ contain the same multiset of items (that is, $s$
is a permutation of $t$). If sorting removes duplicates, then one would like to
infer that the sets of items underlying $t$ and $s$ are the same. Such results
could be useful to have if they can be determined statically and automatically.
We present a scheme by which such collection analysis can be structured and
automated. Central to this scheme is the use of linear logic as a omputational
logic underlying the logic of Horn clauses.



Focusing and Polarization in Intuitionistic Logic

A focused proof system provides a normal form to cut-free proofs that
structures the application of invertible and non-invertible inference rules.
The focused proof system of Andreoli for linear logic has been applied to both
the proof search and the proof normalization approaches to computation. Various
proof systems in literature exhibit characteristics of focusing to one degree
or another. We present a new, focused proof system for intuitionistic logic,
called LJF, and show how other proof systems can be mapped into the new system
by inserting logical connectives that prematurely stop focusing. We also use
LJF to design a focused proof system for classical logic. Our approach to the
design and analysis of these systems is based on the completeness of focusing
in linear logic and on the notion of polarity that appears in Girard's LC and
LU proof systems.



A Language for Generic Programming in the Large

Generic programming is an effective methodology for developing reusable
software libraries. Many programming languages provide generics and have
features for describing interfaces, but none completely support the idioms used
in generic programming. To address this need we developed the language G. The
central feature of G is the concept, a mechanism for organizing constraints on
generics that is inspired by the needs of modern C++ libraries. G provides
modular type checking and separate compilation (even of generics). These
characteristics support modular software development, especially the smooth
integration of independently developed components. In this article we present
the rationale for the design of G and demonstrate the expressiveness of G with
two case studies: porting the Standard Template Library and the Boost Graph
Library from C++ to G. The design of G shares much in common with the concept
extension proposed for the next C++ Standard (the authors participated in its
design) but there are important differences described in this article.



The study of a new gerrymandering methodology

This paper is to obtain a simple dividing-diagram of the congressional
districts, where the only limit is that each district should contain the same
population if possibly. In order to solve this problem, we introduce three
different standards of the "simple" shape. The first standard is that the final
shape of the congressional districts should be of a simplest figure and we
apply a modified "shortest split line algorithm" where the factor of the same
population is considered only. The second standard is that the gerrymandering
should ensure the integrity of the current administrative area as the
convenience for management. Thus we combine the factor of the administrative
area with the first standard, and generate an improved model resulting in the
new diagram in which the perimeters of the districts are along the boundaries
of some current counties. Moreover, the gerrymandering should consider the
geographic features.The third standard is introduced to describe this
situation. Finally, it can be proved that the difference between the supporting
ratio of a certain party in each district and the average supporting ratio of
that particular party in the whole state obeys the Chi-square distribution
approximately. Consequently, we can obtain an archetypal formula to check
whether the gerrymandering we propose is fair.



Capacity of the Degraded Half-Duplex Relay Channel

A discrete memoryless half-duplex relay channel is constructed from a
broadcast channel from the source to the relay and destination and a multiple
access channel from the source and relay to the destination. When the relay
listens, the channel operates in the broadcast mode. The channel switches to
the multiple access mode when the relay transmits. If the broadcast component
channel is physically degraded, the half-duplex relay channel will also be
referred to as physically degraded. The capacity of this degraded half-duplex
relay channel is examined. It is shown that the block Markov coding suggested
in the seminal paper by Cover and El Gamal can be modified to achieve capacity
for the degraded half-duplex relay channel. In the code construction, the
listen-transmit schedule of the relay is made to depend on the message to be
sent and hence the schedule carries information itself. If the schedule is
restricted to be deterministic, it is shown that the capacity can be achieved
by a simple management of information flows across the broadcast and multiple
access component channels.



Opportunism in Multiuser Relay Channels: Scheduling, Routing and Spectrum Reuse

In order to understand the key merits of multiuser diversity techniques in
relay-assisted cellular multihop networks, this paper analyzes the spectral
efficiency of opportunistic (i.e., channel-aware) scheduling algorithms over a
fading multiuser relay channel with $K$ users in the asymptotic regime of large
(but finite) number of users. Using tools from extreme-value theory, we
characterize the limiting distribution of spectral efficiency focusing on Type
I convergence and utilize it in investigating the large system behavior of the
multiuser relay channel as a function of the number of users and physical
channel signal-to-noise ratios (SNRs). Our analysis results in very accurate
formulas in the large (but finite) $K$ regime, provides insights on the
potential performance enhancements from multihop routing and spectrum reuse
policies in the presence of multiuser diversity gains from opportunistic
scheduling and helps to identify the regimes and conditions in which
relay-assisted multiuser communication provides a clear advantage over direct
multiuser communication.



Compositional Semantics Grounded in Commonsense Metaphysics

We argue for a compositional semantics grounded in a strongly typed ontology
that reflects our commonsense view of the world and the way we talk about it in
ordinary language. Assuming the existence of such a structure, we show that the
semantics of various natural language phenomena may become nearly trivial.



On Compact Routing for the Internet

While there exist compact routing schemes designed for grids, trees, and
Internet-like topologies that offer routing tables of sizes that scale
logarithmically with the network size, we demonstrate in this paper that in
view of recent results in compact routing research, such logarithmic scaling on
Internet-like topologies is fundamentally impossible in the presence of
topology dynamics or topology-independent (flat) addressing. We use analytic
arguments to show that the number of routing control messages per topology
change cannot scale better than linearly on Internet-like topologies. We also
employ simulations to confirm that logarithmic routing table size scaling gets
broken by topology-independent addressing, a cornerstone of popular
locator-identifier split proposals aiming at improving routing scaling in the
presence of network topology dynamics or host mobility. These pessimistic
findings lead us to the conclusion that a fundamental re-examination of
assumptions behind routing models and abstractions is needed in order to find a
routing architecture that would be able to scale ``indefinitely.''



Benefiting from Disorder: Source Coding for Unordered Data

The order of letters is not always relevant in a communication task. This
paper discusses the implications of order irrelevance on source coding,
presenting results in several major branches of source coding theory: lossless
coding, universal lossless coding, rate-distortion, high-rate quantization, and
universal lossy coding. The main conclusions demonstrate that there is a
significant rate savings when order is irrelevant. In particular, lossless
coding of n letters from a finite alphabet requires Theta(log n) bits and
universal lossless coding requires n + o(n) bits for many countable alphabet
sources. However, there are no universal schemes that can drive a strong
redundancy measure to zero. Results for lossy coding include distribution-free
expressions for the rate savings from order irrelevance in various high-rate
quantization schemes. Rate-distortion bounds are given, and it is shown that
the analogue of the Shannon lower bound is loose at all finite rates.



On Semimeasures Predicting Martin-Loef Random Sequences

Solomonoff's central result on induction is that the posterior of a universal
semimeasure M converges rapidly and with probability 1 to the true sequence
generating posterior mu, if the latter is computable. Hence, M is eligible as a
universal sequence predictor in case of unknown mu. Despite some nearby results
and proofs in the literature, the stronger result of convergence for all
(Martin-Loef) random sequences remained open. Such a convergence result would
be particularly interesting and natural, since randomness can be defined in
terms of M itself. We show that there are universal semimeasures M which do not
converge for all random sequences, i.e. we give a partial negative answer to
the open problem. We also provide a positive answer for some non-universal
semimeasures. We define the incomputable measure D as a mixture over all
computable measures and the enumerable semimeasure W as a mixture over all
enumerable nearly-measures. We show that W converges to D and D to mu on all
random sequences. The Hellinger distance measuring closeness of two
distributions plays a central role.



Unsatisfiable Linear k-CNFs Exist, for every k

We call a CNF formula linear if any two clauses have at most one variable in
common. Let Linear k-SAT be the problem of deciding whether a given linear
k-CNF formula is satisfiable. Here, a k-CNF formula is a CNF formula in which
every clause has size exactly k. It was known that for k >= 3, Linear k-SAT is
NP-complete if and only if an unsatisfiable linear k-CNF formula exists, and
that they do exist for k >= 4. We prove that unsatisfiable linear k-CNF
formulas exist for every k. Let f(k) be the minimum number of clauses in an
unsatisfiable linear k-CNF formula. We show that f(k) is Omega(k2^k) and
O(4^k*k^4), i.e., minimum size unsatisfiable linear k-CNF formulas are
significantly larger than minimum size unsatisfiable k-CNF formulas. Finally,
we prove that, surprisingly, linear k-CNF formulas do not allow for a larger
fraction of clauses to be satisfied than general k-CNF formulas.



Randomized algorithm for the k-server problem on decomposable spaces

We study the randomized k-server problem on metric spaces consisting of
widely separated subspaces. We give a method which extends existing algorithms
to larger spaces with the growth rate of the competitive quotients being at
most O(log k). This method yields o(k)-competitive algorithms solving the
randomized k-server problem, for some special underlying metric spaces, e.g.
HSTs of "small" height (but unbounded degree). HSTs are important tools for
probabilistic approximation of metric spaces.



Continuous and randomized defensive forecasting: unified view

Defensive forecasting is a method of transforming laws of probability (stated
in game-theoretic terms as strategies for Sceptic) into forecasting algorithms.
There are two known varieties of defensive forecasting: "continuous", in which
Sceptic's moves are assumed to depend on the forecasts in a (semi)continuous
manner and which produces deterministic forecasts, and "randomized", in which
the dependence of Sceptic's moves on the forecasts is arbitrary and
Forecaster's moves are allowed to be randomized. This note shows that the
randomized variety can be obtained from the continuous variety by smearing
Sceptic's moves to make them continuous.



On a constructive characterization of a class of trees related to pairs of disjoint matchings

For a graph consider the pairs of disjoint matchings which union contains as
many edges as possible, and define a parameter $\alpha$ which eqauls the
cardinality of the largest matching in those pairs. Also, define $\betta$ to be
the cardinality of a maximum matching of the graph.
  We give a constructive characterization of trees which satisfy the
$\alpha$=$\betta$ equality. The proof of our main theorem is based on a new
decomposition algorithm obtained for trees.



Key Agreement and Authentication Schemes Using Non-Commutative Semigroups

We give a new two-pass authentication scheme, whichis a generalisation of an
authentication scheme of Sibert-Dehornoy-Girault based on the Diffie-Hellman
conjugacy problem. Compared to the above scheme, for some parameters it is more
efficient with respect to multiplications. We sketch a proof that our
authentication scheme is secure. We give a new key agreement protocols.



On the AAGL Protocol

Recently the AAGL (Anshel-Anshel-Goldfeld-Lemieux) has been proposed which
can be used for RFID tags. We give algorithms for the problem (we call the
MSCSPv) on which the security of the AAGL protocol is based upon. Hence we give
various attacks for general parameters on the recent AAGL protocol proposed.
One of our attacks is a deterministic algorithm which has space complexity and
time complexity both atleast exponentialin the worst case. In a better case
using a probabilistic algorithm the time complexity canbe
O(|XSS(ui')^L5*(n^(1+e)) and the space complexity can be O(|XSS(ui')|^L6),
where the element ui' is part of a public key, n is the index of braid group,
XSS is a summit type set and e is a constant in a limit. The above shows the
AAGL protocol is potentially not significantly more secure as using key
agreement protocols based on the conjugacy problem such as the AAG
(Anshel-Anshel-Goldfeld) protocol because both protocols can be broken with
complexity which do not significantly differ. We think our attacks can be
improved.



User Participation in Social Media: Digg Study

The social news aggregator Digg allows users to submit and moderate stories
by voting on (digging) them. As is true of most social sites, user
participation on Digg is non-uniformly distributed, with few users contributing
a disproportionate fraction of content. We studied user participation on Digg,
to see whether it is motivated by competition, fueled by user ranking, or
social factors, such as community acceptance.
  For our study we collected activity data of the top users weekly over the
course of a year. We computed the number of stories users submitted, dugg or
commented on weekly. We report a spike in user activity in September 2006,
followed by a gradual decline, which seems unaffected by the elimination of
user ranking. The spike can be explained by a controversy that broke out at the
beginning of September 2006. We believe that the lasting acrimony that this
incident has created led to a decline of top user participation on Digg.



A structure from motion inequality

We state an elementary inequality for the structure from motion problem for m
cameras and n points. This structure from motion inequality relates space
dimension, camera parameter dimension, the number of cameras and number points
and global symmetry properties and provides a rigorous criterion for which
reconstruction is not possible with probability 1. Mathematically the
inequality is based on Frobenius theorem which is a geometric incarnation of
the fundamental theorem of linear algebra. The paper also provides a general
mathematical formalism for the structure from motion problem. It includes the
situation the points can move while the camera takes the pictures.



On Ullman's theorem in computer vision

Both in the plane and in space, we invert the nonlinear Ullman transformation
for 3 points and 3 orthographic cameras. While Ullman's theorem assures a
unique reconstruction modulo a reflection for 3 cameras and 4 points, we find a
locally unique reconstruction for 3 cameras and 3 points. Explicit
reconstruction formulas allow to decide whether picture data of three cameras
seeing three points can be realized as a point-camera configuration.



Space and camera path reconstruction for omni-directional vision

In this paper, we address the inverse problem of reconstructing a scene as
well as the camera motion from the image sequence taken by an omni-directional
camera. Our structure from motion results give sharp conditions under which the
reconstruction is unique. For example, if there are three points in general
position and three omni-directional cameras in general position, a unique
reconstruction is possible up to a similarity. We then look at the
reconstruction problem with m cameras and n points, where n and m can be large
and the over-determined system is solved by least square methods. The
reconstruction is robust and generalizes to the case of a dynamic environment
where landmarks can move during the movie capture. Possible applications of the
result are computer assisted scene reconstruction, 3D scanning, autonomous
robot navigation, medical tomography and city reconstructions.



On the subset sum problem over finite fields

The subset sum problem over finite fields is a well-known {\bf NP}-complete
problem. It arises naturally from decoding generalized Reed-Solomon codes. In
this paper, we study the number of solutions of the subset sum problem from a
mathematical point of view. In several interesting cases, we obtain explicit or
asymptotic formulas for the solution number. As a consequence, we obtain some
results on the decoding problem of Reed-Solomon codes.



Eigenvalue bounds on the pseudocodeword weight of expander codes

Four different ways of obtaining low-density parity-check codes from expander
graphs are considered. For each case, lower bounds on the minimum stopping set
size and the minimum pseudocodeword weight of expander (LDPC) codes are
derived. These bounds are compared with the known eigenvalue-based lower bounds
on the minimum distance of expander codes. Furthermore, Tanner's
parity-oriented eigenvalue lower bound on the minimum distance is generalized
to yield a new lower bound on the minimum pseudocodeword weight. These bounds
are useful in predicting the performance of LDPC codes under graph-based
iterative decoding and linear programming decoding.



Minimum Cost Homomorphisms to Reflexive Digraphs

For digraphs $G$ and $H$, a homomorphism of $G$ to $H$ is a mapping $f:\
V(G)\dom V(H)$ such that $uv\in A(G)$ implies $f(u)f(v)\in A(H)$. If moreover
each vertex $u \in V(G)$ is associated with costs $c_i(u), i \in V(H)$, then
the cost of a homomorphism $f$ is $\sum_{u\in V(G)}c_{f(u)}(u)$. For each fixed
digraph $H$, the {\em minimum cost homomorphism problem} for $H$, denoted
MinHOM($H$), is the following problem. Given an input digraph $G$, together
with costs $c_i(u)$, $u\in V(G)$, $i\in V(H)$, and an integer $k$, decide if
$G$ admits a homomorphism to $H$ of cost not exceeding $k$. We focus on the
minimum cost homomorphism problem for {\em reflexive} digraphs $H$ (every
vertex of $H$ has a loop). It is known that the problem MinHOM($H$) is
polynomial time solvable if the digraph $H$ has a {\em Min-Max ordering}, i.e.,
if its vertices can be linearly ordered by $<$ so that $i<j, s<r$ and $ir, js
\in A(H)$ imply that $is \in A(H)$ and $jr \in A(H)$. We give a forbidden
induced subgraph characterization of reflexive digraphs with a Min-Max
ordering; our characterization implies a polynomial time test for the existence
of a Min-Max ordering. Using this characterization, we show that for a
reflexive digraph $H$ which does not admit a Min-Max ordering, the minimum cost
homomorphism problem is NP-complete. Thus we obtain a full dichotomy
classification of the complexity of minimum cost homomorphism problems for
reflexive digraphs.



On the Complexity of the Minimum Cost Homomorphism Problem for Reflexive Multipartite Tournaments

For digraphs $D$ and $H$, a mapping $f: V(D)\dom V(H)$ is a homomorphism of
$D$ to $H$ if $uv\in A(D)$ implies $f(u)f(v)\in A(H).$ For a fixed digraph $H$,
the homomorphism problem is to decide whether an input digraph $D$ admits a
homomorphism to $H$ or not, and is denoted as HOMP($H$). Digraphs are allowed
to have loops, but not allowed to have parallel arcs.
  A natural optimization version of the homomorphism problem is defined as
follows. If each vertex $u \in V(D)$ is associated with costs $c_i(u), i \in
V(H)$, then the cost of the homomorphism $f$ is $\sum_{u\in V(D)}c_{f(u)}(u)$.
For each fixed digraph $H$, we have the {\em minimum cost homomorphism problem
for} $H$ and denote it as MinHOMP($H$). The problem is to decide, for an input
graph $D$ with costs $c_i(u),$ $u \in V(D), i\in V(H)$, whether there exists a
homomorphism of $D$ to $H$ and, if one exists, to find one of minimum cost.
  In a recent paper, we posed a problem of characterizing polynomial time
solvable and NP-hard cases of the minimum cost homomorphism problem for acyclic
multipartite tournaments with possible loops (w.p.l.). In this paper, we solve
the problem for reflexive multipartite tournaments and demonstrate a
considerate difficulty of the problem for the whole class of multipartite
tournaments w.p.l. using, as an example, acyclic 3-partite tournaments of order
4 w.p.l.\footnote{This paper was submitted to Discrete Mathematics on April 6,
2007}



Complexity of the Minimum Cost Homomorphism Problem for Semicomplete Digraphs with Possible Loops

For digraphs $D$ and $H$, a mapping $f: V(D)\dom V(H)$ is a homomorphism of
$D$ to $H$ if $uv\in A(D)$ implies $f(u)f(v)\in A(H).$ For a fixed digraph $H$,
the homomorphism problem is to decide whether an input digraph $D$ admits a
homomorphism to $H$ or not, and is denoted as HOM($H$).
  An optimization version of the homomorphism problem was motivated by a
real-world problem in defence logistics and was introduced in
\cite{gutinDAM154a}. If each vertex $u \in V(D)$ is associated with costs
$c_i(u), i \in V(H)$, then the cost of the homomorphism $f$ is $\sum_{u\in
V(D)}c_{f(u)}(u)$. For each fixed digraph $H$, we have the {\em minimum cost
homomorphism problem for} $H$ and denote it as MinHOM($H$). The problem is to
decide, for an input graph $D$ with costs $c_i(u),$ $u \in V(D), i\in V(H)$,
whether there exists a homomorphism of $D$ to $H$ and, if one exists, to find
one of minimum cost.
  Although a complete dichotomy classification of the complexity of MinHOM($H$)
for a digraph $H$ remains an unsolved problem, complete dichotomy
classifications for MinHOM($H$) were proved when $H$ is a semicomplete digraph
\cite{gutinDAM154b}, and a semicomplete multipartite digraph \cite{gutinDAM}.
In these studies, it is assumed that the digraph $H$ is loopless. In this
paper, we present a full dichotomy classification for semicomplete digraphs
with possible loops, which solves a problem in \cite{gutinRMS}.\footnote{This
paper was submitted to SIAM J. Discrete Math. on October 27, 2006}



Discrete Denoising with Shifts

We introduce S-DUDE, a new algorithm for denoising DMC-corrupted data. The
algorithm, which generalizes the recently introduced DUDE (Discrete Universal
DEnoiser) of Weissman et al., aims to compete with a genie that has access, in
addition to the noisy data, also to the underlying clean data, and can choose
to switch, up to $m$ times, between sliding window denoisers in a way that
minimizes the overall loss. When the underlying data form an individual
sequence, we show that the S-DUDE performs essentially as well as this genie,
provided that $m$ is sub-linear in the size of the data. When the clean data is
emitted by a piecewise stationary process, we show that the S-DUDE achieves the
optimum distribution-dependent performance, provided that the same
sub-linearity condition is imposed on the number of switches. To further
substantiate the universal optimality of the S-DUDE, we show that when the
number of switches is allowed to grow linearly with the size of the data,
\emph{any} (sequence of) scheme(s) fails to compete in the above senses. Using
dynamic programming, we derive an efficient implementation of the S-DUDE, which
has complexity (time and memory) growing only linearly with the data size and
the number of switches $m$. Preliminary experimental results are presented,
suggesting that S-DUDE has the capacity to significantly improve on the
performance attained by the original DUDE in applications where the nature of
the data abruptly changes in time (or space), as is often the case in practice.



